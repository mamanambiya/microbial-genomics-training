{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#microbial-genomics-metagenomics-training","title":"Microbial Genomics &amp; Metagenomics Training","text":"<p>Welcome to the comprehensive training course on Microbial Genomics &amp; Metagenomics for Clinical and Public Health Applications.</p> <ul> <li> <p> Course Overview</p> <p>Learn core principles and practical applications of microbial genomics and metagenomics in clinical and public health contexts.</p> <p> Learn more</p> </li> <li> <p> 10-Day Program</p> <p>Intensive hands-on training covering everything from command line basics to advanced outbreak investigation.</p> <p> View schedule</p> </li> <li> <p> Real-World Data</p> <p>Work with actual clinical datasets including M. tuberculosis and V. cholerae.</p> <p> Explore datasets</p> </li> <li> <p> Practical Skills</p> <p>Master bioinformatics tools, workflows, and reproducible analysis techniques used in clinical microbiology.</p> <p> See objectives</p> </li> </ul>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<p>This course provides hands-on experience with:</p>"},{"location":"#pathogen-analysis","title":"Pathogen Analysis","text":"<ul> <li>Analyze genomic diversity and evolutionary relationships</li> <li>Investigate antimicrobial resistance (AMR) profiles</li> <li>Study mobile genetic elements (MGE) in resistance spread</li> </ul>"},{"location":"#metagenomics","title":"Metagenomics","text":"<ul> <li>Explore microbial communities in clinical and environmental samples</li> <li>Apply advanced sequencing analysis techniques</li> <li>Interpret complex metagenomic datasets</li> </ul>"},{"location":"#reproducible-workflows","title":"Reproducible Workflows","text":"<ul> <li>Use version control with Git</li> <li>Work with containerized environments</li> <li>Implement Nextflow pipelines for scalable analysis</li> </ul>"},{"location":"#data-interpretation","title":"Data Interpretation","text":"<ul> <li>Generate publication-ready visualizations</li> <li>Conduct epidemiological investigations</li> <li>Present findings effectively</li> </ul>"},{"location":"#course-highlights","title":"Course Highlights","text":"<p>Interactive Learning</p> <ul> <li>Hands-on exercises with real datasets</li> <li>Group discussions and case studies  </li> <li>Individual project presentations</li> <li>Expert guest speakers</li> </ul> <p>Technical Requirements</p> <ul> <li>Laptop (Linux/macOS preferred, Git Bash for Windows)</li> <li>Basic Linux command-line knowledge</li> <li>HPC access (recommended)</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to begin your journey in microbial genomics? Start with our setup guide:</p> <p>Get Started </p>"},{"location":"#course-information","title":"Course Information","text":"<p>| Duration | 10 days (September 1-12, 2025) | | Format | Hands-on workshops with lectures | | Level | Intermediate | | Prerequisites | Basic bioinformatics knowledge |</p>"},{"location":"#support","title":"Support","text":"<p>Need help? Check our troubleshooting guide or reach out to the course instructors.</p> <ul> <li> <p> Repository</p> <p>Access all course materials, datasets, and workflows on GitHub.</p> <p> Visit repository</p> </li> <li> <p> Documentation</p> <p>Comprehensive guides, references, and additional resources.</p> <p> Browse docs</p> </li> </ul>"},{"location":"datasets/","title":"Datasets","text":""},{"location":"datasets/#training-datasets","title":"Training Datasets","text":""},{"location":"datasets/#overview","title":"Overview","text":"<p>This course uses carefully curated datasets representing real-world scenarios in microbial genomics. All data has been quality-controlled and prepared for educational use, focusing on Mycobacterium tuberculosis and Vibrio cholerae collections.</p>"},{"location":"datasets/#dataset-categories","title":"Dataset Categories","text":""},{"location":"datasets/#1-primary-training-datasets","title":"1. Primary Training Datasets","text":""},{"location":"datasets/#mycobacterium-tuberculosis-collection","title":"Mycobacterium tuberculosis Collection","text":"<ul> <li>Sample Size: 20 clinical isolates</li> <li>Geographic Origin: Global collection (South Africa, India, UK, Peru)</li> <li>Drug Resistance: Mixed MDR, XDR, and drug-susceptible strains</li> <li>Lineages: Representatives from Lineages 1-4</li> <li>Sequencing: Illumina paired-end (2\u00d7150bp, 80-120x coverage)</li> <li>Size: ~2.5 GB</li> <li>Use Cases: Drug resistance analysis, lineage typing, phylogenetic analysis</li> </ul>"},{"location":"datasets/#vibrio-cholerae-outbreak-investigation","title":"Vibrio cholerae Outbreak Investigation","text":"<ul> <li>Sample Size: 15 outbreak isolates + 3 environmental samples</li> <li>Source: Simulated cholera outbreak (based on real data)</li> <li>Timeframe: 6-month epidemic period</li> <li>Geographic: Coastal urban setting</li> <li>Sequencing: Illumina paired-end (2\u00d7150bp, 60-100x coverage)</li> <li>Size: ~1.8 GB</li> <li>Use Cases: Outbreak tracking, source attribution, transmission analysis</li> </ul>"},{"location":"datasets/#2-reference-materials","title":"2. Reference Materials","text":""},{"location":"datasets/#reference-genomes","title":"Reference Genomes","text":"<ul> <li>High-quality reference assemblies for M. tuberculosis and V. cholerae</li> <li>Annotation files (GFF, GenBank formats)</li> <li>Resistance gene databases</li> <li>Size: ~500 MB</li> </ul>"},{"location":"datasets/#validation-datasets","title":"Validation Datasets","text":"<ul> <li>Known outbreak collections with confirmed epidemiological links</li> <li>Quality control standards</li> <li>Benchmark datasets for method comparison</li> <li>Size: ~1.5 GB</li> </ul>"},{"location":"datasets/#dataset-access","title":"Dataset Access","text":""},{"location":"datasets/#file-organization","title":"File Organization","text":"<pre><code>datasets/\n\u251c\u2500\u2500 genomics/\n\u2502   \u251c\u2500\u2500 mtb/                    # M. tuberculosis isolates\n\u2502   \u2514\u2500\u2500 vibrio/                 # V. cholerae outbreak\n\u251c\u2500\u2500 references/\n\u2502   \u251c\u2500\u2500 genomes/                # Reference assemblies\n\u2502   \u251c\u2500\u2500 databases/              # Resistance/virulence databases\n\u2502   \u2514\u2500\u2500 annotations/            # Gene annotations\n\u2514\u2500\u2500 validation/\n    \u251c\u2500\u2500 benchmarks/             # Method comparison datasets\n    \u2514\u2500\u2500 qc_standards/           # Quality control references\n</code></pre>"},{"location":"datasets/#download-instructions","title":"Download Instructions","text":""},{"location":"datasets/#during-course","title":"During Course","text":"<p>Data is pre-loaded on course HPC systems: </p><pre><code># Access course data directory\ncd /data/course/datasets/\n\n# Copy to your workspace\ncp -r /data/course/datasets/ ~/workspace/\n</code></pre><p></p>"},{"location":"datasets/#post-course-access","title":"Post-Course Access","text":"<p>Datasets remain available through: </p><pre><code># Clone dataset repository\ngit clone https://github.com/CIDRI-Africa/microbial-genomics-datasets.git\n\n# Download specific collections\nwget https://datasets.microbial-genomics.org/mtb_collection.tar.gz\n</code></pre><p></p>"},{"location":"datasets/#data-formats","title":"Data Formats","text":""},{"location":"datasets/#raw-sequencing-data","title":"Raw Sequencing Data","text":"<ul> <li>Format: FASTQ (compressed with gzip)</li> <li>Quality: Phred+33 encoding</li> <li>Naming: <code>SampleID_R1.fastq.gz</code>, <code>SampleID_R2.fastq.gz</code></li> </ul>"},{"location":"datasets/#processed-data","title":"Processed Data","text":"<ul> <li>Assemblies: FASTA format</li> <li>Annotations: GFF3, GenBank</li> <li>Alignments: SAM/BAM format</li> <li>Variants: VCF format</li> </ul>"},{"location":"datasets/#metadata","title":"Metadata","text":"<ul> <li>Sample Information: CSV/TSV format</li> <li>Study Design: Detailed README files</li> <li>Quality Metrics: MultiQC reports included</li> </ul>"},{"location":"datasets/#metadata-schema","title":"Metadata Schema","text":""},{"location":"datasets/#genomic-samples","title":"Genomic Samples","text":"Field Description Example sample_id Unique identifier MTB_001 species Organism name Mycobacterium tuberculosis collection_date Sample date 2023-01-15 location Geographic origin Cape Town, South Africa resistance_profile Known resistance INH-R, RIF-R sequencing_platform Technology Illumina MiSeq coverage_depth Average coverage 85x"},{"location":"datasets/#quality-control","title":"Quality Control","text":""},{"location":"datasets/#pre-processing-standards","title":"Pre-processing Standards","text":"<ul> <li>Quality Score: Minimum Q30 for 80% of bases</li> <li>Contamination: &lt;2% non-target DNA</li> <li>Coverage: Minimum 30x for genomic samples</li> <li>Assembly Quality: N50 &gt;50kb, &lt;200 contigs</li> </ul>"},{"location":"datasets/#validation-procedures","title":"Validation Procedures","text":"<ul> <li>Species confirmation by 16S rRNA or genome similarity</li> <li>Contamination screening with multiple tools</li> <li>Assembly quality assessment with standard metrics</li> <li>Metadata validation and consistency checking</li> </ul>"},{"location":"datasets/#ethical-considerations","title":"Ethical Considerations","text":""},{"location":"datasets/#data-privacy","title":"Data Privacy","text":"<ul> <li>All clinical data de-identified according to HIPAA standards</li> <li>Geographic information limited to city/region level</li> <li>No patient identifiers or medical record linkage possible</li> </ul>"},{"location":"datasets/#usage-rights","title":"Usage Rights","text":"<ul> <li>Educational use permitted under Creative Commons License</li> <li>Commercial use requires separate permission</li> <li>Attribution required for publications using these datasets</li> <li>Redistribution allowed with proper citation</li> </ul>"},{"location":"datasets/#responsible-use","title":"Responsible Use","text":"<ul> <li>Data should not be used to identify individuals</li> <li>Results should not be used for clinical decision-making</li> <li>Sharing outside course requires instructor approval</li> </ul>"},{"location":"datasets/#dataset-specific-notes","title":"Dataset-Specific Notes","text":""},{"location":"datasets/#m-tuberculosis-collection","title":"M. tuberculosis Collection","text":"<ul> <li>Lineage assignments based on SNP typing</li> <li>Drug resistance confirmed by phenotypic testing</li> <li>Geographic sampling represents global diversity</li> <li>Suitable for phylogeographic analysis</li> </ul>"},{"location":"datasets/#v-cholerae-outbreak","title":"V. cholerae Outbreak","text":"<ul> <li>Temporal sampling allows transmission inference</li> <li>Environmental samples included for source attribution</li> <li>Metadata includes case demographics and exposure history</li> <li>Excellent dataset for outbreak investigation training</li> </ul>"},{"location":"datasets/#troubleshooting","title":"Troubleshooting","text":""},{"location":"datasets/#common-issues","title":"Common Issues","text":""},{"location":"datasets/#file-access-problems","title":"File Access Problems","text":"<pre><code># Check file permissions\nls -la datasets/\n# Fix permissions if needed\nchmod -R 755 datasets/\n</code></pre>"},{"location":"datasets/#corrupted-downloads","title":"Corrupted Downloads","text":"<pre><code># Verify file integrity\nmd5sum -c checksums.md5\n# Re-download corrupted files\nwget -c https://datasets.url/file.tar.gz\n</code></pre>"},{"location":"datasets/#storage-space-issues","title":"Storage Space Issues","text":"<pre><code># Check available space\ndf -h\n# Compress unused files\ngzip *.fastq\n# Remove temporary files\nrm -rf temp/\n</code></pre>"},{"location":"datasets/#citation-information","title":"Citation Information","text":"<p>When using these datasets in publications, please cite:</p> <p>CIDRI-Africa Microbial Genomics Training Consortium. (2024).  Comprehensive training datasets for microbial genomics and metagenomics education.  Microbial Genomics Education, Dataset Repository.</p> <p>Individual dataset citations available in respective README files.</p>"},{"location":"datasets/#support","title":"Support","text":"<p>For dataset-related questions: - Technical Issues: Submit issue on GitHub repository - Scientific Questions: Contact course instructors - Access Problems: Email dataset-admin@cidri-africa.org</p>"},{"location":"datasets/#updates-and-versioning","title":"Updates and Versioning","text":"<ul> <li>Current Version: v2.1 (September 2025)</li> <li>Update Frequency: Annually or as needed</li> <li>Change Log: Available in repository documentation</li> <li>Notification: Users notified of major updates via email</li> </ul> <p>Remember: These datasets represent real scientific data and should be treated with appropriate care and respect for the original sample donors and research contexts.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"troubleshooting/#setup-and-installation-issues","title":"Setup and Installation Issues","text":""},{"location":"troubleshooting/#git-configuration-problems","title":"Git Configuration Problems","text":"<p>Problem: Git not configured properly </p><pre><code># Check current configuration\ngit config --list\n\n# Error: Please tell me who you are\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre><p></p> <p>Problem: SSH key authentication fails </p><pre><code># Generate new SSH key\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Add to SSH agent\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\n# Test connection\nssh -T git@github.com\n</code></pre><p></p>"},{"location":"troubleshooting/#permission-denied-errors","title":"Permission Denied Errors","text":"<p>Problem: Cannot execute scripts or access files </p><pre><code># Fix script permissions\nchmod +x script.sh\n\n# Fix directory permissions\nchmod 755 directory/\nchmod -R 755 directory/  # Recursive\n\n# Fix SSH key permissions\nchmod 600 ~/.ssh/id_ed25519\nchmod 644 ~/.ssh/id_ed25519.pub\n</code></pre><p></p>"},{"location":"troubleshooting/#data-analysis-issues","title":"Data Analysis Issues","text":""},{"location":"troubleshooting/#fastqc-problems","title":"FastQC Problems","text":"<p>Problem: FastQC fails to run </p><pre><code># Check Java installation\njava -version\n\n# Install Java if missing (Ubuntu/Debian)\nsudo apt install default-jdk\n\n# Run FastQC with memory limit\nfastqc --memory 4096 *.fastq.gz\n</code></pre><p></p> <p>Problem: Out of memory errors </p><pre><code># Increase memory allocation\nfastqc --memory 8192 file.fastq.gz\n\n# Process files individually\nfor file in *.fastq.gz; do\n    fastqc --memory 4096 \"$file\"\ndone\n</code></pre><p></p>"},{"location":"troubleshooting/#assembly-issues","title":"Assembly Issues","text":"<p>Problem: SPAdes assembly fails </p><pre><code># Check available memory\nfree -h\n\n# Run with memory limit\nspades.py --memory 32 -1 R1.fastq.gz -2 R2.fastq.gz -o output/\n\n# Try different k-mer sizes\nspades.py -k 21,33,55 -1 R1.fastq.gz -2 R2.fastq.gz -o output/\n</code></pre><p></p> <p>Problem: Poor assembly quality (high fragmentation) </p><pre><code># Check input data quality first\nfastqc input_files.fastq.gz\n\n# Try more aggressive trimming\ntrimmomatic PE input_R1.fastq.gz input_R2.fastq.gz \\\n    output_R1.fastq.gz output_R1_unpaired.fastq.gz \\\n    output_R2.fastq.gz output_R2_unpaired.fastq.gz \\\n    LEADING:10 TRAILING:10 SLIDINGWINDOW:4:20 MINLEN:50\n\n# Use careful mode in SPAdes\nspades.py --careful -1 trimmed_R1.fastq.gz -2 trimmed_R2.fastq.gz -o careful_assembly/\n</code></pre><p></p>"},{"location":"troubleshooting/#tool-installation-and-dependencies","title":"Tool Installation and Dependencies","text":""},{"location":"troubleshooting/#condamamba-issues","title":"Conda/Mamba Issues","text":"<p>Problem: Environment creation fails </p><pre><code># Update conda\nconda update conda\n\n# Clear package cache\nconda clean --all\n\n# Create environment with specific Python version\nconda create -n genomics python=3.9\n\n# Use mamba for faster solving\nmamba create -n genomics python=3.9\n</code></pre><p></p> <p>Problem: Package conflicts </p><pre><code># Create minimal environment first\nconda create -n clean_env python=3.9\n\n# Activate and install packages one by one\nconda activate clean_env\nconda install -c bioconda fastqc\nconda install -c bioconda spades\n</code></pre><p></p>"},{"location":"troubleshooting/#dockersingularity-issues","title":"Docker/Singularity Issues","text":"<p>Problem: Permission denied with Docker </p><pre><code># Add user to docker group\nsudo usermod -aG docker $USER\n\n# Log out and back in, then test\ndocker run hello-world\n</code></pre><p></p> <p>Problem: Singularity image won't run </p><pre><code># Pull image explicitly\nsingularity pull docker://biocontainers/fastqc:v0.11.9_cv8\n\n# Run with specific bind paths\nsingularity exec -B /data:/data image.sif fastqc --version\n\n# Check image integrity\nsingularity verify image.sif\n</code></pre><p></p>"},{"location":"troubleshooting/#hpc-and-remote-access-issues","title":"HPC and Remote Access Issues","text":""},{"location":"troubleshooting/#ssh-connection-problems","title":"SSH Connection Problems","text":"<p>Problem: Connection timed out </p><pre><code># Test basic connectivity\nping hostname\n\n# Try different port\nssh -p 2222 username@hostname\n\n# Use verbose mode for debugging\nssh -v username@hostname\n</code></pre><p></p> <p>Problem: Key exchange failed </p><pre><code># Generate compatible key\nssh-keygen -t rsa -b 4096\n\n# Specify key explicitly\nssh -i ~/.ssh/specific_key username@hostname\n\n# Check SSH config\ncat ~/.ssh/config\n</code></pre><p></p>"},{"location":"troubleshooting/#slurm-job-issues","title":"SLURM Job Issues","text":"<p>Problem: Job stuck in queue </p><pre><code># Check queue status\nsqueue -u $USER\n\n# Check job details\nscontrol show job JOBID\n\n# Check partition availability\nsinfo\n</code></pre><p></p> <p>Problem: Job fails with memory errors </p><pre><code># Check job output\ncat slurm-JOBID.out\n\n# Increase memory request\n#SBATCH --mem=32G\n\n# Use multiple cores if available\n#SBATCH --cpus-per-task=8\n</code></pre><p></p>"},{"location":"troubleshooting/#data-processing-errors","title":"Data Processing Errors","text":""},{"location":"troubleshooting/#file-format-issues","title":"File Format Issues","text":"<p>Problem: Unexpected file format </p><pre><code># Check file type\nfile filename\nhead filename\n\n# Convert line endings if needed\ndos2unix filename\n\n# Check compression\ngunzip -t file.gz\n</code></pre><p></p> <p>Problem: Corrupt or truncated files </p><pre><code># Check file integrity\nmd5sum file.fastq.gz\n# Compare with provided checksum\n\n# Test gzip integrity\ngunzip -t file.fastq.gz\n\n# Repair if possible (may lose data)\ngzip -d file.fastq.gz\ngzip file.fastq\n</code></pre><p></p>"},{"location":"troubleshooting/#large-file-handling","title":"Large File Handling","text":"<p>Problem: Running out of disk space </p><pre><code># Check disk usage\ndf -h\ndu -sh directory/\n\n# Clean up temporary files\nrm -rf temp/\nrm *.tmp\n\n# Compress large files\ngzip *.fastq\ntar -czf archive.tar.gz directory/\n</code></pre><p></p> <p>Problem: Processing very large files </p><pre><code># Process in chunks\nsplit -l 4000000 large_file.fastq chunk_\n# Process each chunk separately\n\n# Use streaming where possible\nzcat file.fastq.gz | head -n 1000000 | tool\n\n# Use efficient tools\nseqtk sample file.fastq.gz 10000 &gt; sample.fastq\n</code></pre><p></p>"},{"location":"troubleshooting/#analysis-and-interpretation-issues","title":"Analysis and Interpretation Issues","text":""},{"location":"troubleshooting/#resistance-gene-detection","title":"Resistance Gene Detection","text":"<p>Problem: No resistance genes found (expected some) </p><pre><code># Check assembly quality\nquast.py assembly.fasta\n\n# Try multiple databases\nabricate --db resfinder assembly.fasta\nabricate --db card assembly.fasta\nabricate --db argannot assembly.fasta\n\n# Reduce stringency\nabricate --minid 80 --mincov 60 assembly.fasta\n</code></pre><p></p> <p>Problem: Too many false positives </p><pre><code># Increase stringency\nabricate --minid 95 --mincov 90 assembly.fasta\n\n# Verify hits manually\nblast -query resistance_gene.fasta -subject assembly.fasta\n\n# Check for truncated genes\nabricate --mincov 95 assembly.fasta\n</code></pre><p></p>"},{"location":"troubleshooting/#phylogenetic-analysis","title":"Phylogenetic Analysis","text":"<p>Problem: Tree looks wrong or unrealistic </p><pre><code># Check sequence alignment quality\naliview alignment.fasta\n\n# Remove problematic sequences\nseqtk subseq sequences.fasta good_ids.txt &gt; clean.fasta\n\n# Try different tree method\nFastTree -nt alignment.fasta &gt; tree.newick\niqtree -s alignment.fasta -m TEST\n</code></pre><p></p> <p>Problem: Low bootstrap support </p><pre><code># Increase bootstrap replicates\niqtree -s alignment.fasta -bb 1000\n\n# Check for recombination\ngubbins alignment.fasta\n\n# Use only core SNPs\nsnp-sites -c alignment.fasta &gt; core_snps.fasta\n</code></pre><p></p>"},{"location":"troubleshooting/#performance-and-resource-issues","title":"Performance and Resource Issues","text":""},{"location":"troubleshooting/#memory-management","title":"Memory Management","text":"<p>Problem: Out of memory errors </p><pre><code># Check memory usage\nfree -h\ntop\n\n# Limit memory usage\nulimit -v 8000000  # Limit to ~8GB\n\n# Use memory-efficient tools\nminimap2 instead of BWA-MEM for large references\n</code></pre><p></p> <p>Problem: Process running too slowly </p><pre><code># Use multiple cores\ntool -t 8 input output\n\n# Optimize I/O\n# Use local storage instead of network drives\ncp data /tmp/\ncd /tmp/\n# Run analysis\ncp results back/to/network/storage\n</code></pre><p></p>"},{"location":"troubleshooting/#storage-management","title":"Storage Management","text":"<p>Problem: Quota exceeded </p><pre><code># Find large files\nfind . -size +100M -ls\n\n# Clean up intermediate files\nrm *.sam  # Keep only BAM files\nrm temp_*\n\n# Compress old data\ntar -czf old_analysis.tar.gz old_directory/\nrm -rf old_directory/\n</code></pre><p></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#before-asking-for-help","title":"Before Asking for Help","text":"<ol> <li>Check error messages carefully - Often contain specific solutions</li> <li>Search documentation - Tool manuals usually have troubleshooting sections</li> <li>Try simple test cases - Use small datasets to isolate problems</li> <li>Check system resources - Memory, disk space, permissions</li> </ol>"},{"location":"troubleshooting/#how-to-ask-for-help","title":"How to Ask for Help","text":""},{"location":"troubleshooting/#include-essential-information","title":"Include Essential Information","text":"<ul> <li>Exact error message (copy-paste, don't retype)</li> <li>Command that failed (exact command with parameters)</li> <li>System information (OS, tool versions)</li> <li>Input file details (size, format, sample content)</li> </ul>"},{"location":"troubleshooting/#good-help-request-example","title":"Good Help Request Example","text":"<pre><code>Subject: SPAdes assembly fails with error code 1\n\nI'm running SPAdes on paired-end M. tuberculosis data:\nCommand: spades.py -1 sample_R1.fastq.gz -2 sample_R2.fastq.gz -o spades_out/\n\nError message:\n\"== Error ==  system call for: ['/usr/bin/python3', '/opt/spades/bin/spades_init.py'] finished abnormally, err code: 1\"\n\nSystem: Ubuntu 20.04, SPAdes v3.15.3\nInput files: 2x150bp Illumina, ~50x coverage, 2.3GB total\nAvailable memory: 32GB\nDisk space: 500GB free\n\nI've tried with --careful flag and different k-mer sizes but get the same error.\n</code></pre>"},{"location":"troubleshooting/#support-resources","title":"Support Resources","text":""},{"location":"troubleshooting/#course-support","title":"Course Support","text":"<ul> <li>Instructors: Available during course hours</li> <li>Slack Channel: <code>#troubleshooting</code></li> <li>Office Hours: Daily 17:00-18:00 (course week)</li> <li>Peer Support: Encouraged among participants</li> </ul>"},{"location":"troubleshooting/#online-resources","title":"Online Resources","text":"<ul> <li>Biostars: General bioinformatics Q&amp;A</li> <li>Stack Overflow: Programming and command line issues</li> <li>Tool Documentation: Always check official documentation</li> <li>Galaxy Training: Alternative tutorials and explanations</li> </ul>"},{"location":"troubleshooting/#emergency-contacts","title":"Emergency Contacts","text":"<ul> <li>Technical Issues: tech-support@course.org</li> <li>Data Access Problems: data-admin@course.org</li> <li>General Questions: instructors@course.org</li> </ul>"},{"location":"troubleshooting/#prevention-tips","title":"Prevention Tips","text":""},{"location":"troubleshooting/#best-practices","title":"Best Practices","text":"<ol> <li>Test with small datasets first</li> <li>Keep detailed logs of commands</li> <li>Use version control for scripts</li> <li>Regular backups of important results</li> <li>Document your workflow steps</li> </ol>"},{"location":"troubleshooting/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>Running analysis without checking input quality</li> <li>Using inappropriate parameters for your data type</li> <li>Ignoring error messages and logs</li> <li>Not checking intermediate results</li> <li>Working in directories with spaces in names</li> <li>Not backing up important data</li> </ul> <p>Remember: Most bioinformatics problems have been encountered before. Don't hesitate to search online and ask for help - the community is generally very supportive!</p>"},{"location":"course/objectives/","title":"Learning Objectives","text":""},{"location":"course/objectives/#learning-objectives","title":"Learning Objectives","text":"<p>After completing this workshop, participants will be able to:</p>"},{"location":"course/objectives/#core-competencies","title":"Core Competencies","text":""},{"location":"course/objectives/#1-pathogen-genomics-principles","title":"1. Pathogen Genomics Principles","text":"<ul> <li>Understand the principles of pathogen genomics and its role in infectious disease surveillance and control</li> <li>Explain how genomic data complements traditional epidemiological approaches</li> <li>Describe the impact of genomic surveillance on public health decision-making</li> </ul>"},{"location":"course/objectives/#2-data-generation-workflow-days-1-2","title":"2. Data Generation Workflow (Days 1-2)","text":"<ul> <li>Describe the complete workflow of genomic data generation, from sample collection to sequence analysis</li> <li>Evaluate different sequencing technologies and their applications (Day 1)</li> <li>Design appropriate sampling strategies for genomic surveillance</li> <li>Navigate high-performance computing systems (Day 2)</li> </ul>"},{"location":"course/objectives/#3-bioinformatics-analysis-days-2-3","title":"3. Bioinformatics Analysis (Days 2-3)","text":"<ul> <li>Apply basic bioinformatics tools to analyze pathogen genomic data</li> <li>Perform quality control, assembly, and annotation of microbial genomes (Days 2-3)</li> <li>Execute MLST typing and serotyping for strain characterization (Day 3)</li> <li>Implement reproducible analysis workflows using command-line tools</li> </ul>"},{"location":"course/objectives/#4-epidemiological-applications-day-4","title":"4. Epidemiological Applications (Day 4)","text":"<ul> <li>Interpret genomic data for epidemiological insights, including outbreak detection and tracking</li> <li>Construct and interpret phylogenetic trees for transmission analysis (Day 4)</li> <li>Perform pangenome analysis and comparative genomics (Day 4)</li> <li>Integrate genomic and epidemiological data for outbreak investigation</li> </ul>"},{"location":"course/objectives/#5-antimicrobial-resistance-day-3","title":"5. Antimicrobial Resistance (Day 3)","text":"<ul> <li>Understand the role of genomics in identifying antimicrobial resistance mechanisms</li> <li>Detect AMR genes and mutations in genomic data (Day 3)</li> <li>Analyze mobile genetic elements and resistance spread (Day 3)</li> <li>Predict resistance phenotypes from genomic data</li> </ul>"},{"location":"course/objectives/#6-metagenomics-days-5-6","title":"6. Metagenomics (Days 5-6)","text":"<ul> <li>Apply metagenomic approaches to study microbial communities (Day 5)</li> <li>Analyze microbiome composition and diversity metrics (Day 5)</li> <li>Identify pathogens in complex microbial communities (Day 6)</li> <li>Detect co-infections and community shifts (Day 6)</li> </ul>"},{"location":"course/objectives/#7-advanced-analysis-days-6-8","title":"7. Advanced Analysis (Days 6-8)","text":"<ul> <li>Perform variant calling and mutation analysis (Day 6)</li> <li>Establish genotype-phenotype correlations (Day 6)</li> <li>Develop reproducible Nextflow pipelines (Days 7-8)</li> <li>Optimize workflow performance and testing (Day 8)</li> </ul>"},{"location":"course/objectives/#8-data-management-and-reproducibility-days-7-8","title":"8. Data Management and Reproducibility (Days 7-8)","text":"<ul> <li>Implement version control for research projects</li> <li>Create reproducible analysis pipelines using workflow managers (Days 7-8)</li> <li>Apply best practices for data management and sharing</li> </ul>"},{"location":"course/objectives/#9-critical-analysis-and-communication-days-9-10","title":"9. Critical Analysis and Communication (Days 9-10)","text":"<ul> <li>Apply learned skills to real research data (Day 9)</li> <li>Troubleshoot analysis challenges independently (Day 9)</li> <li>Evaluate the quality and reliability of genomic analyses</li> <li>Communicate genomic findings to diverse audiences</li> <li>Present research findings effectively (Day 10)</li> </ul>"},{"location":"course/objectives/#practical-skills","title":"Practical Skills","text":"<p>Participants will gain hands-on experience with:</p> <ul> <li>Unix/Linux command-line interface</li> <li>Git version control</li> <li>Docker containerization</li> <li>Nextflow workflow management</li> <li>Popular bioinformatics tools and databases</li> <li>Data visualization and interpretation</li> <li>Scientific presentation and communication</li> </ul>"},{"location":"course/objectives/#assessment-criteria","title":"Assessment Criteria","text":"<p>Progress will be evaluated through:</p> <ul> <li>Completion of hands-on exercises</li> <li>Quality of individual analysis projects</li> <li>Participation in group discussions</li> <li>Final project presentation</li> <li>Understanding demonstrated in Q&amp;A sessions</li> </ul>"},{"location":"course/overview/","title":"Overview","text":""},{"location":"course/overview/#course-overview","title":"Course Overview","text":""},{"location":"course/overview/#about-the-course","title":"About the Course","text":"<p>This comprehensive 10-day training course introduces participants to the core principles and practical applications of microbial genomics and metagenomics in clinical and public health contexts.</p>"},{"location":"course/overview/#course-schedule-september-1-12-2025","title":"Course Schedule (September 1-12, 2025)","text":"<p>The training covers 10 days of intensive hands-on learning:</p>"},{"location":"course/overview/#week-1-foundations","title":"Week 1: Foundations","text":"<ul> <li>Day 1: Course welcome, genomic surveillance, sequencing technologies, PubMLST</li> <li>Day 2: HPC introduction, command line, quality control, species identification  </li> <li>Day 3: Genome assembly, annotation, MLST, serotyping, AMR detection</li> <li>Day 4: Comparative genomics, pangenomics, phylogenomics</li> <li>Day 5: Metagenomic profiling and microbiome analysis</li> </ul>"},{"location":"course/overview/#week-2-advanced-applications","title":"Week 2: Advanced Applications","text":"<ul> <li>Day 6: Co-infection detection, variant calling, genotype-phenotype correlation</li> <li>Day 7: Nextflow workflow development and nf-core</li> <li>Day 8: Advanced pipeline development and optimization</li> <li>Day 9: Bring-your-own-data analysis session</li> <li>Day 10: Final presentations and course wrap-up</li> </ul>"},{"location":"course/overview/#target-pathogens","title":"Target Pathogens","text":"<p>Using real-world datasets, participants will analyze:</p> <ul> <li>Mycobacterium tuberculosis - TB genomics and drug resistance analysis</li> <li>Vibrio cholerae - Outbreak investigation and epidemiological analysis</li> </ul>"},{"location":"course/overview/#key-focus-areas","title":"Key Focus Areas","text":""},{"location":"course/overview/#genomic-analysis","title":"Genomic Analysis","text":"<ul> <li>Analyze genomic diversity and evolutionary relationships</li> <li>Investigate antimicrobial resistance (AMR) profiles</li> <li>Study mobile genetic elements (MGE) in resistance spread</li> </ul>"},{"location":"course/overview/#metagenomics","title":"Metagenomics","text":"<ul> <li>Explore microbial communities in clinical and environmental samples</li> <li>Apply advanced sequencing analysis techniques</li> <li>Interpret complex metagenomic datasets</li> </ul>"},{"location":"course/overview/#reproducible-workflows","title":"Reproducible Workflows","text":"<ul> <li>Master version control with Git</li> <li>Work with containerized environments (Docker/Singularity)</li> <li>Implement Nextflow pipelines for scalable analysis</li> </ul>"},{"location":"course/overview/#course-format","title":"Course Format","text":"<ul> <li>Interactive workshops with hands-on exercises</li> <li>Real-world datasets from clinical and surveillance studies</li> <li>Expert instruction from experienced genomics researchers</li> <li>Group projects and individual presentations</li> <li>Bring-your-own-data analysis sessions</li> </ul>"},{"location":"course/overview/#expected-outcomes","title":"Expected Outcomes","text":"<p>By the end of this course, participants will be equipped with practical skills to conduct genomic surveillance, investigate outbreaks, and contribute to public health decision-making through genomic analysis.</p>"},{"location":"course/prerequisites/","title":"Prerequisites","text":""},{"location":"course/prerequisites/#prerequisites-and-requirements","title":"Prerequisites and Requirements","text":""},{"location":"course/prerequisites/#technical-prerequisites","title":"Technical Prerequisites","text":""},{"location":"course/prerequisites/#essential-requirements","title":"Essential Requirements","text":""},{"location":"course/prerequisites/#1-computing-equipment","title":"1. Computing Equipment","text":"<ul> <li>Laptop with at least 8GB RAM (16GB recommended)</li> <li>Operating System: Linux or macOS preferred</li> <li>Windows users must install Git Bash before the course</li> <li>Reliable internet connection for downloading datasets and software</li> </ul>"},{"location":"course/prerequisites/#2-basic-technical-knowledge","title":"2. Basic Technical Knowledge","text":"<ul> <li>Basic understanding of Linux command-line tools</li> <li>File navigation (cd, ls, pwd)</li> <li>File manipulation (cp, mv, rm, mkdir)</li> <li>Text viewing (cat, less, head, tail)</li> <li>Familiarity with text editors (nano, vim, or similar)</li> <li>Basic understanding of file systems and permissions</li> </ul>"},{"location":"course/prerequisites/#3-scientific-background","title":"3. Scientific Background","text":"<ul> <li>Undergraduate-level biology or microbiology</li> <li>Basic understanding of genetics and molecular biology</li> <li>Familiarity with concepts of:</li> <li>DNA sequencing</li> <li>Bacterial genetics</li> <li>Infectious diseases</li> <li>Public health surveillance</li> </ul>"},{"location":"course/prerequisites/#recommended-not-required","title":"Recommended (Not Required)","text":""},{"location":"course/prerequisites/#1-programming-experience","title":"1. Programming Experience","text":"<ul> <li>Basic scripting in Python, R, or shell scripting</li> <li>Experience with data analysis workflows</li> <li>Version control with Git (we'll cover this in Day 1)</li> </ul>"},{"location":"course/prerequisites/#2-bioinformatics-background","title":"2. Bioinformatics Background","text":"<ul> <li>Previous experience with sequence analysis tools</li> <li>Understanding of genomic file formats (FASTA, FASTQ, VCF)</li> <li>Familiarity with biological databases</li> </ul>"},{"location":"course/prerequisites/#3-high-performance-computing","title":"3. High-Performance Computing","text":"<ul> <li>Access to HPC resources (recommended but not essential)</li> <li>Experience with job schedulers (SLURM, PBS)</li> </ul>"},{"location":"course/prerequisites/#software-requirements","title":"Software Requirements","text":""},{"location":"course/prerequisites/#pre-course-installation","title":"Pre-course Installation","text":"<p>Please install the following software before the course begins:</p>"},{"location":"course/prerequisites/#1-git-all-operating-systems","title":"1. Git (All Operating Systems)","text":"<ul> <li>Linux: <code>sudo apt install git</code> or equivalent</li> <li>macOS: Install Xcode Command Line Tools or use Homebrew</li> <li>Windows: Download and install Git Bash from git-scm.com</li> </ul>"},{"location":"course/prerequisites/#2-text-editor","title":"2. Text Editor","text":"<p>Choose one of: - VS Code (recommended for beginners) - Sublime Text - Atom - Vim/Emacs (for advanced users)</p>"},{"location":"course/prerequisites/#3-ssh-client","title":"3. SSH Client","text":"<ul> <li>Linux/macOS: Built-in terminal</li> <li>Windows: Use Git Bash or Windows Subsystem for Linux (WSL)</li> </ul>"},{"location":"course/prerequisites/#course-provided-software","title":"Course-Provided Software","text":"<p>The following will be provided during the course:</p> <ul> <li>Docker/Singularity containers with pre-installed bioinformatics tools</li> <li>Nextflow for workflow management</li> <li>Access to high-performance computing resources</li> <li>Pre-configured analysis environments</li> </ul>"},{"location":"course/prerequisites/#data-requirements","title":"Data Requirements","text":""},{"location":"course/prerequisites/#practice-datasets","title":"Practice Datasets","text":"<p>We will provide: - Curated training datasets for all major pathogens - Quality-controlled sequence data in standard formats - Reference genomes and databases - Example analysis outputs for comparison</p>"},{"location":"course/prerequisites/#bring-your-own-data-optional","title":"Bring Your Own Data (Optional)","text":"<p>If you have your own datasets: - Raw sequencing data (FASTQ format) - Associated metadata (sample information, collection details) - Ethical approval for data sharing (if applicable) - Data should be &lt;100GB total for practical analysis</p>"},{"location":"course/prerequisites/#pre-course-preparation","title":"Pre-course Preparation","text":""},{"location":"course/prerequisites/#1-complete-the-setup-guide","title":"1. Complete the Setup Guide","text":"<p>Follow our detailed Setup Guide to prepare your computing environment.</p>"},{"location":"course/prerequisites/#2-review-basic-concepts","title":"2. Review Basic Concepts","text":"<p>Refresh your knowledge of: - Basic microbiology and infectious diseases - DNA sequencing principles - File formats in bioinformatics</p>"},{"location":"course/prerequisites/#3-practice-command-line","title":"3. Practice Command Line","text":"<p>If you're new to command line: - Complete an online tutorial (e.g., \"Command Line Crash Course\") - Practice basic file operations - Get comfortable with navigating directories</p>"},{"location":"course/prerequisites/#4-test-your-setup","title":"4. Test Your Setup","text":"<ul> <li>Verify Git installation: <code>git --version</code></li> <li>Test SSH connectivity (instructions will be provided)</li> <li>Ensure you can create and edit text files</li> </ul>"},{"location":"course/prerequisites/#accessibility-and-support","title":"Accessibility and Support","text":""},{"location":"course/prerequisites/#technical-support","title":"Technical Support","text":"<ul> <li>Pre-course: Contact instructors for setup assistance</li> <li>During course: Dedicated technical support available</li> <li>Post-course: Community forum for ongoing questions</li> </ul>"},{"location":"course/prerequisites/#accommodations","title":"Accommodations","text":"<p>Please inform instructors of any: - Accessibility requirements - Dietary restrictions (for course meals) - Special technical needs</p>"},{"location":"course/prerequisites/#language-support","title":"Language Support","text":"<ul> <li>Course materials are in English</li> <li>Instructors can provide support in multiple languages</li> <li>Translated resources available for key concepts</li> </ul>"},{"location":"course/prerequisites/#questions","title":"Questions?","text":"<p>If you have questions about prerequisites or setup:</p> <ol> <li>Check the Troubleshooting Guide</li> <li>Contact the course coordinators</li> <li>Join our pre-course Q&amp;A sessions (dates TBD)</li> </ol>"},{"location":"course/schedule/","title":"Schedule","text":""},{"location":"course/schedule/#course-schedule","title":"Course Schedule","text":""},{"location":"course/schedule/#overview","title":"Overview","text":"<p>The 10-day intensive training course covers fundamental to advanced topics in microbial genomics and metagenomics. Each day combines theoretical presentations with hands-on practical exercises.</p> <p>!!! info \"Course Information\"</p> <ul> <li>Duration: 10 days (September 1-12, 2025)</li> <li>Time: 09:00-13:00 CAT daily</li> <li>Venue: MAC room, level 2, Health Science UCT, Barnard Fuller Building</li> <li>Address: Anzio Rd, Observatory, Cape Town, 7935</li> <li>Format: Hands-on workshops with lectures</li> <li>Break: 11:00/11:30 AM coffee break</li> </ul> <p>!!! note \"Participant Requirements\"  - Participants are required to bring their own data to perform the analysis. If a participant does not have data, it will be made available to them.</p>"},{"location":"course/schedule/#day-1-welcome-to-the-course","title":"Day 1: Welcome to the Course","text":"<p>Date: September 1, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Introductions All 09:10 Overview of clinical pathogens and genomic surveillance Slides Ephifania Geza 09:40 Overview of sequencing technologies and data types Sindiswa Lukhele 10:00 Setting up and exploring PubMLST Sindiswa Lukhele 11:00 Break 11:30 Introduction to command line interface Practical Arash Iranzadeh <p>Key Learning Outcomes: Introduction to genomic surveillance, sequencing technologies, command line basics</p>"},{"location":"course/schedule/#day-2-introduction-to-commandline-and-guest-talk","title":"Day 2: Introduction to Commandline and Guest Talk","text":"<p>Date: September 2, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Introduction to command line interface Practical Arash Iranzadeh 11:30 Break 12:00 Guest talk: MtB and co-infection Bio. Presentation Bethlehem Adnew <p>Key Learning Outcomes: Command line proficiency, HPC fundamentals</p>"},{"location":"course/schedule/#day-3-accelerating-bioinformatics-hpc-qc-and-species-identification-essentials","title":"Day 3: Accelerating Bioinformatics: HPC, QC, and Species Identification Essentials","text":"<p>Date: September 3, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Introduction High Performance Computing (HPC) \u2013 Ilifu Notes \u2022 Practical 1 \u2022 Practical 2 Mamana Mbiyavanga 11:00 Break 12:00 Quality checking and control, as well as species identification Practical Arash Iranzadeh <p>Key Learning Outcomes: HPC fundamentals, Quality control fundamentals</p>"},{"location":"course/schedule/#day-4-genome-assembly-essentials-qc-identification-and-annotation","title":"Day 4: Genome Assembly Essentials: QC, Identification, and Annotation","text":"<p>Date: September 4, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Recap: Quality checking and control, and species identification Practical Arash Iranzadeh 10:30 Genome assembly, quality assessment Notes. Practical Ephifania Geza 11:00 Break 11:30 Genome assembly, quality assessment: Continuation Notes. Practical Ephifania Geza <p>Key Learning Outcomes: Quality control, Genome assembly, assessment</p>"},{"location":"course/schedule/#day-5-tracking-threats-genomic-detection-of-amr-virulence-and-plasmid-mobility","title":"Day 5: Tracking Threats: Genomic Detection of AMR, Virulence, and Plasmid Mobility","text":"<p>Date: September 5, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Recap: Genome assembly, quality assessment Notes. Practical 10:00 Genome annotation and Multi-Locus Sequence Typing Notes Arash Iranzadeh 11:30 Break 12:00 Antimicrobial Resistance gene detection and resistance prediction Ephifania Geza <p>Key Learning Outcomes: Genome quality and functional gene annotation fundamentals, AMR and virulence factors and plasmid detection</p>"},{"location":"course/schedule/#day-6-nextflow-pipeline-development","title":"Day 6: Nextflow Pipeline Development","text":"<p>Date: September 8, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Reproducible workflows with Nextflow and nf-core Mamana Mbiyavanga 10:30 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga 11:30 Break 12:00 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga <p>Key Learning Outcomes: Workflow reproducibility, Nextflow basics, pipeline development</p>"},{"location":"course/schedule/#day-7-nextflow-pipeline-development-version-control-with-github","title":"Day 7: Nextflow Pipeline Development &amp; Version Control with GitHub","text":"<p>Date: September 9, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Pipeline development continued and testing Mamana Mbiyavanga 11:30 Break 12:00 Introduction to Git and GitHub basics Mamana Mbiyavanga <p>Key Learning Outcomes: Pipeline completion, basic Git commands, GitHub repository management</p>"},{"location":"course/schedule/#day-8-metagenomic-profiling","title":"Day 8: Metagenomic profiling","text":"<p>Date: September 10, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Metagenomic sequencing Ephifania Geza 09:30 Quality control in metagenomic data Ephifania Geza 10:30 Microbiome profiling and diversity metrics (R or QIIME2) Ephifania Geza 11:30 Break 12:00 Microbiome profiling and diversity metrics (R or QIIME2) Ephifania Geza 13:00 Role of plasmids, integrons, and transposons in AMR spread Ephifania Geza <p>Key Learning Outcomes: Metagenomic sequencing principles, microbiome analysis, diversity metrics</p>"},{"location":"course/schedule/#day-9-comparative-genomics","title":"Day 9: Comparative Genomics","text":"<p>Date: September 11, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Pangenomics Arash Iranzadeh 10:30 Phylogenomics: Inferring evolutionary relationships from core SNPs Arash Iranzadeh 11:30 Break 12:00 Phylogenomics: Tree construction and visualisation Arash Iranzadeh <p>Key Learning Outcomes: Pan-genome analysis, phylogenetic inference, tree construction and visualization</p>"},{"location":"course/schedule/#day-10-wrap-up-session","title":"Day 10: Wrap-up session","text":"<p>Date: September 12, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Participant presentations 11:15 Short talks NGS-Academy/AfriGen-D/eLwazi ODSP 11:40 End of the course <p>Key Learning Outcomes: Applying learned skills to real data, Scientific presentation skills, course completion</p>"},{"location":"course/schedule/#trainers","title":"Trainers","text":""},{"location":"course/schedule/#core-training-team","title":"Core Training Team","text":"Trainer Role Expertise Ephifania Geza Lead Instructor Genomic surveillance, AMR analysis, metagenomics Arash Iranzadeh Technical Instructor Command line, QC, assembly, phylogenomics Sindiswa Lukhele Technical Instructor Sequencing technologies, PubMLST Mamana Mbiyavanga HPC/Workflow Specialist High-performance computing, Nextflow pipelines"},{"location":"course/schedule/#guest-speaker","title":"Guest Speaker","text":"Speaker Topic Date Bethlehem Adnew MtB and co-infection September 2, 2025 <p>This comprehensive training provides participants with both theoretical knowledge and practical skills needed for microbial genomics analysis in clinical and public health settings.</p>"},{"location":"course/setup/","title":"Setup","text":""},{"location":"course/setup/#setup-guide","title":"Setup Guide","text":""},{"location":"course/setup/#before-you-arrive","title":"Before You Arrive","text":"<p>Complete these setup steps at least one week before the course begins.</p>"},{"location":"course/setup/#1-system-requirements","title":"1. System Requirements","text":""},{"location":"course/setup/#minimum-specifications","title":"Minimum Specifications","text":"<ul> <li>RAM: 8GB (16GB recommended)</li> <li>Storage: 50GB free space</li> <li>OS: Linux, macOS, or Windows 10/11</li> <li>Internet: Stable broadband connection</li> </ul>"},{"location":"course/setup/#operating-system-setup","title":"Operating System Setup","text":"Linux (Ubuntu/Debian)macOSWindows <pre><code># Update package list\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install essential tools\nsudo apt install -y git curl wget build-essential\n</code></pre> <pre><code># Install Homebrew (if not already installed)\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install essential tools\nbrew install git curl wget\n</code></pre> <ol> <li>Install Git Bash: Download from git-scm.com</li> <li>Enable WSL2 (optional but recommended):    <pre><code>wsl --install\n</code></pre></li> <li>Install Windows Terminal from Microsoft Store</li> </ol>"},{"location":"course/setup/#2-git-configuration","title":"2. Git Configuration","text":"<p>Configure Git with your information:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\ngit config --global init.defaultBranch main\n</code></pre> <p>Verify configuration: </p><pre><code>git config --list\n</code></pre><p></p>"},{"location":"course/setup/#3-ssh-setup","title":"3. SSH Setup","text":"<p>Generate SSH key for secure connections:</p> <pre><code># Generate SSH key pair\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Start SSH agent\neval \"$(ssh-agent -s)\"\n\n# Add key to agent\nssh-add ~/.ssh/id_ed25519\n\n# Display public key (copy this for later use)\ncat ~/.ssh/id_ed25519.pub\n</code></pre>"},{"location":"course/setup/#4-text-editor","title":"4. Text Editor","text":"<p>Choose and install a text editor:</p> VS Code (Recommended)Sublime TextCommand Line Editors <ul> <li>Download from code.visualstudio.com</li> <li>Install useful extensions:</li> <li>Python</li> <li>Markdown All in One</li> <li>GitLens</li> </ul> <ul> <li>Download from sublimetext.com</li> <li>Consider Sublime Merge for Git integration</li> </ul> <pre><code># For vim users\nsudo apt install vim  # Linux\nbrew install vim      # macOS\n\n# For nano users (usually pre-installed)\nwhich nano\n</code></pre>"},{"location":"course/setup/#5-test-your-setup","title":"5. Test Your Setup","text":""},{"location":"course/setup/#basic-command-line-test","title":"Basic Command Line Test","text":"<pre><code># Check versions\ngit --version\ncurl --version\nwget --version\n\n# Test directory operations\nmkdir test_setup\ncd test_setup\necho \"Hello World\" &gt; test.txt\ncat test.txt\ncd ..\nrm -rf test_setup\n</code></pre>"},{"location":"course/setup/#git-test","title":"Git Test","text":"<pre><code># Clone a test repository\ngit clone https://github.com/CIDRI-Africa/microbial-genomics-training.git\ncd microbial-genomics-training\nls -la\ncd ..\nrm -rf microbial-genomics-training\n</code></pre>"},{"location":"course/setup/#6-course-specific-setup","title":"6. Course-Specific Setup","text":""},{"location":"course/setup/#hpc-access-if-provided","title":"HPC Access (If Provided)","text":"<p>You will receive: - SSH connection details - Username and login instructions - VPN setup (if required)</p>"},{"location":"course/setup/#dockersingularity-optional","title":"Docker/Singularity (Optional)","text":"<p>For local analysis (advanced users):</p> DockerSingularity <pre><code># Linux\nsudo apt install docker.io\nsudo usermod -aG docker $USER\n\n# macOS\n# Download Docker Desktop from docker.com\n\n# Test installation\ndocker --version\ndocker run hello-world\n</code></pre> <pre><code># Linux (Ubuntu/Debian)\nsudo apt install singularity-container\n\n# Test installation\nsingularity --version\n</code></pre>"},{"location":"course/setup/#7-download-course-materials","title":"7. Download Course Materials","text":"<p>One week before the course:</p> <pre><code># Create course directory\nmkdir -p ~/microbial-genomics-course\ncd ~/microbial-genomics-course\n\n# Clone course repository\ngit clone https://github.com/CIDRI-Africa/microbial-genomics-training.git\n\n# Check repository contents\ncd microbial-genomics-training\nls -la\n</code></pre>"},{"location":"course/setup/#8-pre-course-data-download","title":"8. Pre-course Data Download","text":"<p>Large datasets will be provided via: - Shared storage on HPC systems - Cloud storage links (Google Drive/Dropbox) - Direct download scripts (provided before course)</p>"},{"location":"course/setup/#9-connectivity-test","title":"9. Connectivity Test","text":""},{"location":"course/setup/#ssh-connection-test","title":"SSH Connection Test","text":"<pre><code># Test SSH connectivity (replace with provided details)\nssh -T username@hostname\n\n# If successful, you should see a welcome message\n</code></pre>"},{"location":"course/setup/#internet-speed-test","title":"Internet Speed Test","text":"<p>Ensure you have adequate bandwidth: - Minimum: 10 Mbps download - Recommended: 50+ Mbps download - Upload: 5+ Mbps for video calls</p>"},{"location":"course/setup/#10-backup-and-recovery","title":"10. Backup and Recovery","text":""},{"location":"course/setup/#create-backups","title":"Create Backups","text":"<pre><code># Backup SSH keys\ncp ~/.ssh/id_ed25519* ~/backup_location/\n\n# Backup Git configuration\ngit config --list &gt; ~/git_config_backup.txt\n</code></pre>"},{"location":"course/setup/#recovery-commands","title":"Recovery Commands","text":"<p>Keep these handy in case of issues: </p><pre><code># Reset Git configuration\ngit config --global --unset-all user.name\ngit config --global --unset-all user.email\n\n# Regenerate SSH keys\nrm ~/.ssh/id_ed25519*\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n</code></pre><p></p>"},{"location":"course/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"course/setup/#common-issues","title":"Common Issues","text":""},{"location":"course/setup/#git-issues","title":"Git Issues","text":"<pre><code># Fix permission issues (Linux/macOS)\nsudo chown -R $USER ~/.ssh/\n\n# Reset SSH agent\neval \"$(ssh-agent -k)\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n</code></pre>"},{"location":"course/setup/#network-issues","title":"Network Issues","text":"<pre><code># Test connectivity\nping google.com\ncurl -I https://github.com\n\n# Check proxy settings (if behind firewall)\necho $http_proxy\necho $https_proxy\n</code></pre>"},{"location":"course/setup/#permission-issues-linux","title":"Permission Issues (Linux)","text":"<pre><code># Fix common permission problems\nsudo chown -R $USER:$USER ~/\nchmod 755 ~/.ssh\nchmod 600 ~/.ssh/id_ed25519\nchmod 644 ~/.ssh/id_ed25519.pub\n</code></pre>"},{"location":"course/setup/#getting-help","title":"Getting Help","text":""},{"location":"course/setup/#before-the-course","title":"Before the Course","text":"<ul> <li>Email: instructors@microbial-genomics-training.org</li> <li>Slack: Join our pre-course channel</li> <li>Office Hours: Weekly setup sessions (schedule TBD)</li> </ul>"},{"location":"course/setup/#documentation","title":"Documentation","text":"<ul> <li>Command Line Tutorial</li> <li>Git Tutorial</li> <li>SSH Tutorial</li> </ul>"},{"location":"course/setup/#final-checklist","title":"Final Checklist","text":"<p>Before the course starts, verify:</p> <ul> <li> Git installed and configured</li> <li> SSH key generated and working</li> <li> Text editor installed and functional</li> <li> Course repository cloned</li> <li> HPC access tested (if provided)</li> <li> Internet connection stable</li> <li> Backup of important configurations created</li> </ul>"},{"location":"course/setup/#day-1-preview","title":"Day 1 Preview","text":"<p>On the first day (September 1, 2025), we'll: 1. Course introductions and welcome session 2. Overview of clinical pathogens and genomic surveillance 3. Sequencing technologies and data types overview 4. Hands-on PubMLST database exploration 5. Command line interface basics and R environment setup</p> <p>Come prepared with your laptop and the software installed above. Don't worry if you encounter setup problems \u2013 we'll troubleshoot together during the session!</p>"},{"location":"day2/hpc-ilifu-training/","title":"HPC and ILIFU Training Materials","text":""},{"location":"day2/hpc-ilifu-training/#hpc-and-ilifu-training-materials","title":"HPC and ILIFU Training Materials","text":""},{"location":"day2/hpc-ilifu-training/#getting-started","title":"Getting Started","text":"<p>This document provides an overview of HPC concepts and ILIFU infrastructure. </p> <p>For hands-on practice:</p> <ul> <li>SLURM Jobs: See High Performance Computing with SLURM: Practical Tutorial for step-by-step SLURM exercises</li> <li>Unix Commands: See Unix Commands for Pathogen Genomics - Practical Tutorial for genomics command-line basics</li> </ul> <p>Quick Setup (if needed for examples below): </p><pre><code>mkdir -p ~/hpc_practice &amp;&amp; cd ~/hpc_practice\ncp -r /cbio/training/courses/2025/micmet-genomics/sample-data/* .\n</code></pre><p></p>"},{"location":"day2/hpc-ilifu-training/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to High Performance Computing (HPC)</li> <li>ILIFU Infrastructure Overview</li> <li>Getting Started with ILIFU</li> <li>SLURM Job Scheduling</li> <li>Resource Allocation and Management</li> <li>Best Practices</li> <li>Practical Examples</li> <li>Troubleshooting</li> </ol>"},{"location":"day2/hpc-ilifu-training/#introduction-to-hpc","title":"Introduction to HPC","text":""},{"location":"day2/hpc-ilifu-training/#what-is-high-performance-computing","title":"What is High Performance Computing?","text":"<p>High Performance Computing (HPC) is the use of powerful computers with multiple processors working in parallel to solve complex computational problems that require significant processing power, memory, or time.</p> <p>Key Characteristics:</p> <ul> <li>Parallel processing: Multiple CPUs/cores work simultaneously on the same problem</li> <li>Cluster architecture: Hundreds or thousands of interconnected compute nodes</li> <li>High memory capacity: Large RAM for data-intensive computations</li> <li>Fast storage systems: High-speed file systems for handling large datasets</li> <li>Job scheduling: Queue management systems to optimize resource allocation</li> <li>Specialized hardware: GPUs, high-speed interconnects (InfiniBand), and custom processors</li> </ul>"},{"location":"day2/hpc-ilifu-training/#why-use-hpc","title":"Why Use HPC?","text":"<ul> <li>Speed: Complete computations faster than desktop computers</li> <li>Scale: Handle larger datasets and more complex problems</li> <li>Efficiency: Optimize resource utilization</li> <li>Cost-effective: Share expensive hardware among researchers</li> </ul>"},{"location":"day2/hpc-ilifu-training/#traditional-computing-vs-hpc","title":"Traditional Computing vs HPC","text":"<pre><code>Desktop Computer          HPC Cluster\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   1 CPU      \u2502   vs    \u2502Node1\u2502Node2\u2502Node3\u2502\n\u2502   8GB RAM    \u2502         \u2502 32  \u2502 64  \u2502128  \u2502\n\u2502   1TB Disk   \u2502         \u2502cores\u2502cores\u2502cores\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>HPC = Many computers working together</p> Traditional Computing HPC Cluster Single processor Hundreds of processors Limited memory (8-32GB) Massive shared memory (TB) Local storage (TB) Distributed storage (PB) Individual use Shared resources Desktop/Laptop Specialized data centers"},{"location":"day2/hpc-ilifu-training/#why-do-we-need-hpc","title":"Why Do We Need HPC?","text":""},{"location":"day2/hpc-ilifu-training/#real-world-problems-that-need-hpc","title":"Real-world Problems That Need HPC","text":""},{"location":"day2/hpc-ilifu-training/#astronomy-processing-telescope-data","title":"\ud83d\udd2c Astronomy: Processing telescope data","text":"<ul> <li>Data volume: 1-2 TB per night from modern telescopes</li> <li>Local machine: 3-4 weeks to process one night's data</li> <li>HPC cluster: 2-3 hours with parallel processing</li> <li>Example: MeerKAT telescope generates 2.5 TB/hour during observations</li> </ul>"},{"location":"day2/hpc-ilifu-training/#bioinformatics-genome-assembly","title":"\ud83e\uddec Bioinformatics: Genome assembly","text":"<ul> <li>Data size: 100-300 GB of raw sequencing reads</li> <li>Local machine (8GB RAM): Often fails due to memory limits</li> <li>Local machine (32GB RAM): 2-3 weeks for bacterial genome</li> <li>HPC cluster: 4-6 hours with 256GB RAM</li> <li>Example: Human genome assembly needs ~1TB RAM, impossible on most desktops</li> </ul>"},{"location":"day2/hpc-ilifu-training/#climate-modeling-weather-simulations","title":"\ud83c\udf21\ufe0f Climate Modeling: Weather simulations","text":"<ul> <li>Computation: Millions of grid points \u00d7 thousands of time steps</li> <li>Local machine: 6-8 months for regional model (if it runs at all)</li> <li>HPC cluster: 12-24 hours on 100+ cores</li> <li>Example: 10km resolution global model needs 10,000+ CPU hours</li> </ul>"},{"location":"day2/hpc-ilifu-training/#machine-learning-training-deep-neural-networks","title":"\ud83e\uddee Machine Learning: Training deep neural networks","text":"<ul> <li>Model size: GPT-3 has 175 billion parameters</li> <li>Local machine (single GPU): 355 years to train</li> <li>HPC cluster (1000 GPUs): 34 days</li> <li>Example: Training ResNet-50 on ImageNet: 2 weeks (laptop) \u2192 1 hour (8 GPUs)</li> </ul>"},{"location":"day2/hpc-ilifu-training/#pathogen-genomics-outbreak-analysis","title":"\ud83e\udda0 Pathogen Genomics: Outbreak analysis","text":"<ul> <li>Dataset: 1000 M. tuberculosis genomes for outbreak investigation</li> <li>Local machine tasks and times:</li> <li>Quality control: 50 hours (3 min/sample)</li> <li>Read alignment: 167 hours (10 min/sample)  </li> <li>Variant calling: 83 hours (5 min/sample)</li> <li>Phylogenetic tree: 48-72 hours</li> <li>Total: ~15 days of continuous processing</li> <li>HPC cluster: </li> <li>All samples in parallel: 4-6 hours total</li> <li>Tree construction on high-memory node: 2-3 hours</li> <li>Real example: COVID-19 surveillance processing 10,000 genomes weekly - impossible without HPC</li> </ul>"},{"location":"day2/hpc-ilifu-training/#additional-pathogen-genomics-use-cases","title":"Additional Pathogen Genomics Use Cases","text":""},{"location":"day2/hpc-ilifu-training/#bacterial-genome-assembly-illumina-nanopore","title":"\ud83e\uddec Bacterial Genome Assembly (Illumina + Nanopore)","text":"<ul> <li>Dataset: Hybrid assembly of 50 bacterial isolates</li> <li>Computational requirements:</li> <li>RAM: 16-32GB per genome</li> <li>CPU: 8-16 cores optimal per assembly</li> <li>Local machine (16GB RAM, 4 cores):</li> <li>One genome at a time only</li> <li>Per genome: 3-4 hours</li> <li>Total time: 150-200 hours (6-8 days)</li> <li>Risk of crashes with large genomes</li> <li>HPC cluster (256GB RAM, 32 cores/node):</li> <li>Process 8 genomes simultaneously per node</li> <li>Use 7 nodes for all 50 genomes</li> <li>Total time: 3-4 hours</li> <li>Speedup: 50x faster</li> </ul>"},{"location":"day2/hpc-ilifu-training/#amr-gene-detection-across-multiple-species","title":"\ud83d\udc8a AMR Gene Detection Across Multiple Species","text":"<ul> <li>Dataset: 5000 bacterial genomes from hospital surveillance</li> <li>Tools: AMRFinder, CARD-RGI, ResFinder</li> <li>Computational requirements:</li> <li>Database size: 2-5GB per tool</li> <li>RAM: 4-8GB per genome</li> <li>CPU time: 5-10 minutes per genome per tool</li> <li>Local machine (8 cores):</li> <li>Sequential processing: 5000 \u00d7 3 tools \u00d7 7.5 min = 1875 hours (78 days)</li> <li>Database loading overhead adds 20% more time</li> <li>HPC cluster (100 nodes, 32 cores each):</li> <li>Parallel processing across nodes</li> <li>Shared database in memory</li> <li>Total time: 6-8 hours</li> <li>Speedup: 230x faster</li> </ul>"},{"location":"day2/hpc-ilifu-training/#phylogeographic-analysis-of-cholera-outbreak","title":"\ud83c\udf0d Phylogeographic Analysis of Cholera Outbreak","text":"<ul> <li>Dataset: 2000 V. cholerae genomes from Haiti outbreak</li> <li>Computational requirements:</li> <li>Alignment: 100GB RAM for reference-based</li> <li>SNP calling: 4GB per genome</li> <li>Tree building (RAxML-NG): 64-128GB RAM</li> <li>BEAST analysis: 32GB RAM, 1000+ hours CPU time</li> <li>Local machine attempts:</li> <li>Alignment: Often fails (out of memory)</li> <li>If successful: 48 hours</li> <li>SNP calling: 133 hours (4 min/genome)</li> <li>RAxML tree: Fails on most laptops (needs &gt;64GB RAM)</li> <li>BEAST: 6-8 weeks for proper MCMC convergence</li> <li>HPC cluster:</li> <li>Alignment: 2 hours on high-memory node</li> <li>SNP calling: 2 hours (parallel)</li> <li>RAxML: 4-6 hours on 64 cores</li> <li>BEAST: 48 hours on 32 cores</li> <li>Total: 2-3 days vs 2-3 months</li> </ul>"},{"location":"day2/hpc-ilifu-training/#real-time-nanopore-sequencing-analysis","title":"\ud83d\udd2c Real-time Nanopore Sequencing Analysis","text":"<ul> <li>Scenario: Meningitis outbreak, need results in &lt;24 hours</li> <li>Data flow: 20 samples, 5GB data/sample, arriving over 12 hours</li> <li>Pipeline: Basecalling \u2192 QC \u2192 Assembly \u2192 Typing \u2192 AMR</li> <li>Local machine challenges:</li> <li>Can't keep up with data generation</li> <li>Basecalling alone: 2 hours/sample (40 hours total)</li> <li>Sequential processing: Miss the 24-hour deadline</li> <li>HPC solution:</li> <li>Real-time processing as data arrives</li> <li>GPU nodes for basecalling: 10 min/sample</li> <li>Parallel assembly and analysis</li> <li>Results available within 2-3 hours of sequencing</li> <li>Clinical impact: Treatment decisions in same day</li> </ul>"},{"location":"day2/hpc-ilifu-training/#computational-requirements-comparison-table","title":"Computational Requirements Comparison Table","text":"Task Local Machine HPC Cluster Speedup 100 TB genomes QC 8GB RAM, 5 hours 256GB RAM, 10 min 30x 1000 genome alignment 16GB RAM, 7 days 32GB/node \u00d7 50, 3 hours 56x Phylogenetic tree (5000 taxa) Often fails (&gt;64GB needed) 512GB RAM, 6 hours \u221e Pan-genome analysis (500 genomes) 32GB RAM, 2 weeks 256GB RAM, 8 hours 42x GWAS (10,000 samples) Impossible (&lt;1TB RAM) 1TB RAM node, 24 hours \u221e Metagenomic assembly 64GB RAM, 3 days 512GB RAM, 4 hours 18x"},{"location":"day2/hpc-ilifu-training/#why-these-tasks-fail-on-local-machines","title":"Why These Tasks Fail on Local Machines","text":"<ol> <li>Memory Walls:</li> <li>De novo assembly: Needs 100-1000x coverage data in RAM</li> <li>Tree building: O(n\u00b2) memory for distance matrices</li> <li> <p>Pan-genome: Stores all genomes simultaneously</p> </li> <li> <p>Time Constraints:</p> </li> <li>Outbreak response: Need results in hours, not weeks</li> <li>Grant deadlines: Can't wait months for analysis</li> <li> <p>Iterative analysis: Need to test multiple parameters</p> </li> <li> <p>Data Volume:</p> </li> <li>Modern sequencer: 100-500GB per run</li> <li>Surveillance programs: 100s of genomes weekly</li> <li>Can't even store data on laptop (typical: 256GB-1TB SSD)</li> </ol>"},{"location":"day2/hpc-ilifu-training/#ilifu-infrastructure","title":"ILIFU Infrastructure","text":""},{"location":"day2/hpc-ilifu-training/#what-is-ilifu","title":"What is ILIFU?","text":"<ul> <li>Inter-University Institute for Data Intensive Astronomy</li> <li>South African national research data facility</li> <li>Supports astronomy, bioinformatics, and other data-intensive sciences</li> <li>Located at University of Cape Town and University of the Western Cape</li> </ul>"},{"location":"day2/hpc-ilifu-training/#ilifu-services","title":"ILIFU Services","text":"<ol> <li>Compute Cluster: High-performance computing resources</li> <li>Storage: Large-scale data storage solutions</li> <li>Cloud Services: Virtualized computing environments</li> <li>Data Transfer: High-speed data movement capabilities</li> <li>Support: Technical assistance and training</li> </ol>"},{"location":"day2/hpc-ilifu-training/#ilifu-cluster-architecture","title":"ILIFU Cluster Architecture","text":"<p>ILIFU (Inter-University Institute for Data Intensive Astronomy) is a cloud computing infrastructure designed for data-intensive research in astronomy, bioinformatics, and other computational sciences. The facility operates on an OpenStack platform with containerized workloads using Singularity and job scheduling through SLURM.</p> <pre><code>graph TB\n    subgraph \"ILIFU Infrastructure\"\n        subgraph \"Cloud Platform\"\n            OS[OpenStack Cloud&lt;br/&gt;Infrastructure-as-a-Service]\n        end\n\n        subgraph \"Compute Resources\"\n            CN[Compute Nodes&lt;br/&gt;Max: 96 CPUs per job&lt;br/&gt;Max: 1500 GB RAM per job]\n        end\n\n        subgraph \"Container Platform\"\n            SP[Singularity Containers&lt;br/&gt;HPC-optimized&lt;br/&gt;Rootless execution]\n        end\n\n        subgraph \"Job Scheduler\"\n            SL[SLURM Workload Manager&lt;br/&gt;Max runtime: 336 hours]\n        end\n    end\n\n    subgraph \"Research Domains\"\n        AST[Astronomy&lt;br/&gt;MeerKAT, SKA]\n        BIO[Bioinformatics&lt;br/&gt;Genomics, Metagenomics]\n        DS[Data Science&lt;br/&gt;ML/AI Research]\n    end\n\n    OS --&gt; CN\n    CN --&gt; SP\n    SP --&gt; SL\n\n    AST --&gt; SL\n    BIO --&gt; SL\n    DS --&gt; SL\n\n    style OS fill:#e1f5fe\n    style CN fill:#e8f5e9\n    style SP fill:#fff3e0\n    style SL fill:#f3e5f5</code></pre> <p>Figure: ILIFU cloud infrastructure architecture supporting multiple research domains</p>"},{"location":"day2/hpc-ilifu-training/#resource-specifications","title":"Resource Specifications","text":"<p>Based on ILIFU's cloud infrastructure configuration:</p>"},{"location":"day2/hpc-ilifu-training/#maximum-job-resources","title":"Maximum Job Resources","text":"<ul> <li>CPUs: Up to 96 cores per job</li> <li>Memory: Up to 1500 GB (1.5 TB) RAM per job</li> <li>Runtime: Maximum 336 hours (14 days) per job</li> <li>Storage: Distributed file systems for large-scale data</li> </ul>"},{"location":"day2/hpc-ilifu-training/#key-features","title":"Key Features","text":"<ul> <li>OpenStack Platform: Provides flexible cloud computing resources</li> <li>Singularity Containers: Enables reproducible, portable workflows</li> <li>SLURM Scheduler: Manages resource allocation and job queuing</li> <li>Multi-domain Support: Serves astronomy, bioinformatics, and data science communities</li> </ul>"},{"location":"day2/hpc-ilifu-training/#access-methods","title":"Access Methods","text":"<ul> <li>SSH access to login nodes</li> <li>Jupyter notebooks for interactive computing</li> <li>Web-based interfaces for specific services</li> <li>API access for programmatic interaction</li> </ul>"},{"location":"day2/hpc-ilifu-training/#infrastructure-components","title":"Infrastructure Components","text":"Component Description Purpose OpenStack Cloud computing platform Infrastructure management and virtualization SLURM Workload manager Job scheduling and resource allocation Singularity Container platform Application deployment and portability CephFS Distributed storage High-performance shared file system Login Nodes Access points User entry and job submission Compute Nodes Processing units Actual computation execution <p>Note: ILIFU operates as a cloud infrastructure rather than a traditional fixed HPC cluster, allowing dynamic resource allocation based on user requirements. Specific hardware configurations may vary as resources are allocated on-demand through the OpenStack platform</p>"},{"location":"day2/hpc-ilifu-training/#getting-started-with-ilifu","title":"Getting Started with ILIFU","text":""},{"location":"day2/hpc-ilifu-training/#account-setup","title":"Account Setup","text":"<ol> <li>Request Access: Apply through your institution</li> <li>SSH Keys: Generate and register SSH key pairs</li> <li>VPN: Configure institutional VPN if required</li> <li>Initial Login: Connect to login nodes</li> </ol>"},{"location":"day2/hpc-ilifu-training/#basic-commands","title":"Basic Commands","text":"<pre><code># Login to ILIFU\nssh username@training.ilifu.ac.za\n\n# Check your home directory\nls -la ~\n\n# Check available modules\nmodule avail\n\n# Load a module\nmodule load python/3.12.3  # Or use system python3\n</code></pre> <p>\ud83d\udca1 Next Steps: After logging in, follow the hands-on exercises in High Performance Computing with SLURM: Practical Tutorial</p>"},{"location":"day2/hpc-ilifu-training/#file-system-layout","title":"File System Layout","text":"<pre><code>/home/username/          # Your home directory (limited space)\n/scratch/username/       # Temporary fast storage\n/data/project/          # Shared project data\n/software/              # Installed software\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#data-management","title":"Data Management","text":"<ul> <li>Home Directory: Small, backed up, permanent</li> <li>Scratch Space: Large, fast, temporary (auto-cleaned)</li> <li>Project Directories: Shared, persistent, for collaboration</li> </ul>"},{"location":"day2/hpc-ilifu-training/#slurm-basics","title":"SLURM Basics","text":"<p>\ud83d\udcda For detailed SLURM tutorials and exercises, see: High Performance Computing with SLURM: Practical Tutorial</p>"},{"location":"day2/hpc-ilifu-training/#what-is-slurm","title":"What is SLURM?","text":"<pre><code>graph TB\n    subgraph \"Users\"\n        U1[User 1]\n        U2[User 2]\n        U3[User 3]\n    end\n\n    subgraph \"Login Nodes\"\n        LN[Login Node&lt;br/&gt;- SSH Access&lt;br/&gt;- Job Submission&lt;br/&gt;- File Editing]\n    end\n\n    subgraph \"SLURM Controller\"\n        SC[SLURM Scheduler&lt;br/&gt;- Resource Allocation&lt;br/&gt;- Job Queuing&lt;br/&gt;- Priority Management]\n        DB[(Accounting&lt;br/&gt;Database)]\n    end\n\n    subgraph \"Compute Nodes\"\n        CN1[Compute Node 1&lt;br/&gt;CPUs: 32&lt;br/&gt;RAM: 128GB]\n        CN2[Compute Node 2&lt;br/&gt;CPUs: 32&lt;br/&gt;RAM: 128GB]\n        CN3[Compute Node 3&lt;br/&gt;CPUs: 32&lt;br/&gt;RAM: 128GB]\n        CNN[... More Nodes]\n    end\n\n    subgraph \"Storage\"\n        FS[Shared Filesystem&lt;br/&gt;/home&lt;br/&gt;/scratch&lt;br/&gt;/data]\n    end\n\n    U1 --&gt; LN\n    U2 --&gt; LN\n    U3 --&gt; LN\n\n    LN --&gt;|sbatch/srun| SC\n    SC --&gt; DB\n    SC --&gt;|Allocates| CN1\n    SC --&gt;|Allocates| CN2\n    SC --&gt;|Allocates| CN3\n    SC --&gt;|Allocates| CNN\n\n    CN1 --&gt; FS\n    CN2 --&gt; FS\n    CN3 --&gt; FS\n    CNN --&gt; FS\n    LN --&gt; FS\n\n    style U1 fill:#e1f5fe\n    style U2 fill:#e1f5fe\n    style U3 fill:#e1f5fe\n    style LN fill:#fff3e0\n    style SC fill:#f3e5f5\n    style DB fill:#f3e5f5\n    style CN1 fill:#e8f5e9\n    style CN2 fill:#e8f5e9\n    style CN3 fill:#e8f5e9\n    style CNN fill:#e8f5e9\n    style FS fill:#fce4ec</code></pre> <p>Figure: HPC cluster architecture showing the relationship between users, login nodes, SLURM scheduler, compute nodes, and shared storage</p>"},{"location":"day2/hpc-ilifu-training/#about-slurm","title":"About SLURM","text":"<p>Simple Linux Utility for Resource Management (SLURM) is a job scheduling and cluster management tool that:</p> <ul> <li>Job scheduler: Allocates compute resources efficiently among users</li> <li>Resource manager: Controls access to CPUs, memory, and other resources  </li> <li>Workload manager: Manages job queues and priorities based on fairness policies</li> <li>Framework components: Login nodes for access, compute nodes for execution, scheduler for coordination, and accounting database for tracking</li> </ul>"},{"location":"day2/hpc-ilifu-training/#key-slurm-concepts","title":"Key SLURM Concepts","text":"<ul> <li>Job: A computational task submitted to the cluster</li> <li>Partition: Group of nodes with similar characteristics</li> <li>Queue: Collection of jobs waiting for resources</li> <li>Node: Individual compute server</li> <li>Core/CPU: Processing unit within a node</li> </ul>"},{"location":"day2/hpc-ilifu-training/#basic-slurm-commands","title":"Basic SLURM Commands","text":"<pre><code># Submit a job\nsbatch job_script.sh\n\n# Check job status\nsqueue -u username\n\n# Cancel a job\nscancel job_id\n\n# Check node information\nsinfo\n\n# Check your job history\nsacct -u username\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-script-template","title":"Job Script Template","text":"<p>To create a job script, use the nano text editor:</p> <pre><code># Open nano editor to create your script\nnano my_job.sh\n</code></pre> <p>Then copy and paste the following template:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=my_job\n#SBATCH --partition=Main\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8GB\n#SBATCH --time=01:00:00\n#SBATCH --output=output_%j.log\n#SBATCH --error=error_%j.log\n\n# Load modules\nmodule load python/3.12.3  # Or use system python3\n\n# Run your program\npython my_script.py\n</code></pre> <p>To save and exit nano: - Press <code>Ctrl+X</code> to exit - Press <code>Y</code> to confirm save - Press <code>Enter</code> to accept the filename</p>"},{"location":"day2/hpc-ilifu-training/#slurm-directives-explained","title":"SLURM Directives Explained","text":"<ul> <li><code>--job-name</code>: Human-readable job name</li> <li><code>--partition</code>: Which partition to use</li> <li><code>--nodes</code>: Number of nodes required</li> <li><code>--ntasks-per-node</code>: Tasks per node</li> <li><code>--cpus-per-task</code>: CPUs per task</li> <li><code>--mem</code>: Memory requirement</li> <li><code>--time</code>: Maximum runtime</li> <li><code>--output/--error</code>: Log file locations</li> </ul>"},{"location":"day2/hpc-ilifu-training/#resource-management","title":"Resource Management","text":""},{"location":"day2/hpc-ilifu-training/#understanding-resources","title":"Understanding Resources","text":"<ul> <li>CPU Cores: Processing units</li> <li>Memory (RAM): Working memory</li> <li>GPU: Graphics processing units</li> <li>Storage: Disk space</li> <li>Network: Data transfer bandwidth</li> </ul>"},{"location":"day2/hpc-ilifu-training/#resource-allocation-strategies","title":"Resource Allocation Strategies","text":"<pre><code># CPU-intensive job\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=32GB\n\n# Memory-intensive job\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=64GB\n\n# GPU job\n#SBATCH --gres=gpu:1\n#SBATCH --partition=GPU\n\n# Parallel job\n#SBATCH --nodes=2\n#SBATCH --ntasks=32\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#monitoring-resource-usage","title":"Monitoring Resource Usage","text":"<pre><code># Check job efficiency\nseff job_id\n\n# Real-time job monitoring\nsstat job_id\n\n# Detailed job information\nscontrol show job job_id\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#best-practices","title":"Best Practices","text":""},{"location":"day2/hpc-ilifu-training/#job-submission","title":"Job Submission","text":"<ul> <li>Test small first: Start with short test runs</li> <li>Use checkpoints: Save progress regularly</li> <li>Estimate resources: Don't over-request</li> <li>Use appropriate partitions: Match job to partition</li> <li>Clean up: Remove temporary files</li> </ul>"},{"location":"day2/hpc-ilifu-training/#code-optimization","title":"Code Optimization","text":"<pre><code># Use parallel processing\n#SBATCH --cpus-per-task=8\n\n# In Python\nfrom multiprocessing import Pool\nwith Pool(8) as pool:\n    results = pool.map(my_function, data)\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#data-management_1","title":"Data Management","text":"<ul> <li>Use scratch space for temporary files</li> <li>Compress data when possible</li> <li>Clean up regularly</li> <li>Use appropriate file formats</li> </ul>"},{"location":"day2/hpc-ilifu-training/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":"<ul> <li>Requesting too many resources</li> <li>Running jobs on login nodes</li> <li>Not using version control</li> <li>Ignoring error messages</li> <li>Not testing scripts locally first</li> </ul>"},{"location":"day2/hpc-ilifu-training/#practical-examples","title":"Practical Examples","text":"<p>\ud83d\udcdd Complete Step-by-Step Tutorials: For detailed, hands-on SLURM exercises with explanations, see High Performance Computing with SLURM: Practical Tutorial</p>"},{"location":"day2/hpc-ilifu-training/#example-1-python-data-analysis","title":"Example 1: Python Data Analysis","text":"<p>To create this script:</p> <pre><code># Open nano editor\nnano data_analysis.sh\n\n# Copy and paste the script below, then:\n# Press Ctrl+X to exit\n# Press Y to save\n# Press Enter to confirm filename\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=data_analysis\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16GB\n#SBATCH --time=02:00:00\n#SBATCH --output=analysis_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n# Install with: pip install pandas numpy matplotlib\n\npython data_analysis.py input.csv\n</code></pre> <p>Submit with: <code>sbatch data_analysis.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#example-2-r-statistical-analysis","title":"Example 2: R Statistical Analysis","text":"<p>Create the script with nano:</p> <pre><code>nano r_stats.sh\n# Paste the script below, save with Ctrl+X, Y, Enter\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=r_stats\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8GB\n#SBATCH --time=01:30:00\n\nmodule load R/4.4.1  # Check available version\n\nRscript statistical_analysis.R\n</code></pre> <p>Submit with: <code>sbatch r_stats.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#example-3-gpu-machine-learning","title":"Example 3: GPU Machine Learning","text":"<p>Create the script:</p> <pre><code>nano ml_training.sh\n# Paste content, save with Ctrl+X, Y, Enter\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=ml_training\n#SBATCH --partition=GPU\n#SBATCH --gres=gpu:1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32GB\n#SBATCH --time=04:00:00\n\n# module load cuda  # Check if GPU/CUDA is available\nmodule load python/3.12.3  # Or use system python3\n\npython train_model.py\n</code></pre> <p>Submit with: <code>sbatch ml_training.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#example-4-array-jobs","title":"Example 4: Array Jobs","text":"<p>Create the array job script:</p> <pre><code>nano array_job.sh\n# Paste content, save with Ctrl+X, Y, Enter\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=array_job\n#SBATCH --partition=Main\n#SBATCH --array=1-100\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4GB\n#SBATCH --time=00:30:00\n\n# Process different files based on array index\ninput_file=\"data_${SLURM_ARRAY_TASK_ID}.txt\"\noutput_file=\"result_${SLURM_ARRAY_TASK_ID}.txt\"\n\npython process_data.py $input_file $output_file\n</code></pre> <p>Submit with: <code>sbatch array_job.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"day2/hpc-ilifu-training/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"day2/hpc-ilifu-training/#job-wont-start","title":"Job Won't Start","text":"<pre><code># Check partition limits\nscontrol show partition\n\n# Check job details\nscontrol show job job_id\n\n# Check node availability\nsinfo -N\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#out-of-memory-errors","title":"Out of Memory Errors","text":"<pre><code># Check memory usage\nsstat -j job_id --format=AveCPU,AvePages,AveRSS,AveVMSize\n\n# Increase memory request\n#SBATCH --mem=32GB\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-timeouts","title":"Job Timeouts","text":"<pre><code># Check time limits\nscontrol show partition\n\n# Increase time limit\n#SBATCH --time=04:00:00\n\n# Use checkpointing for long jobs\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#module-issues","title":"Module Issues","text":"<pre><code># List available modules\nmodule avail\n\n# Check module conflicts\nmodule list\n\n# Purge and reload\nmodule purge\nmodule load python/3.12.3  # Or use system python3\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Check ILIFU docs</li> <li>Help Desk: Submit support tickets</li> <li>Community: Ask on forums or Slack</li> <li>Training: Attend workshops</li> <li>Practical Tutorials: Work through High Performance Computing with SLURM: Practical Tutorial</li> </ul>"},{"location":"day2/hpc-ilifu-training/#quick-reference","title":"Quick Reference","text":""},{"location":"day2/hpc-ilifu-training/#essential-slurm-commands","title":"Essential SLURM Commands","text":"Command Purpose Example Output <code>sbatch script.sh</code> Submit job <code>Submitted batch job 10</code> <code>squeue -u $USER</code> Check your jobs Shows running/pending jobs <code>scancel job_id</code> Cancel job Terminates specified job <code>sinfo</code> Node information Shows partition and node status <code>sacct -j job_id</code> Job accounting Shows job completion details <code>seff job_id</code> Job efficiency Shows resource utilization"},{"location":"day2/hpc-ilifu-training/#example-command-outputs","title":"Example Command Outputs","text":""},{"location":"day2/hpc-ilifu-training/#checking-partition-information","title":"Checking Partition Information","text":"<pre><code>$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\ntraining*    up 14-00:00:0      7   idle compute-1-sep2025,compute-2-sep2025,compute-3-sep2025,compute-4-sep2025,compute-5-sep2025,compute-6-sep2025,compute-7-sep2025\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-submission-and-status","title":"Job Submission and Status","text":"<pre><code>$ sbatch hello.sh\nSubmitted batch job 10\n\n$ squeue -u mamana\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                10  training    hello   mamana  R       0:01      1 compute-1-sep2025\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-efficiency-report","title":"Job Efficiency Report","text":"<pre><code>$ seff 10\nJob ID: 10\nCluster: training\nUser/Group: mamana/training\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 1\nCPU Utilized: 00:00:00\nCPU Efficiency: 0.00% of 00:00:01 core-walltime\nJob Wall-clock time: 00:00:01\nMemory Utilized: 4.80 MB\nMemory Efficiency: 0.48% of 1.00 GB\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#common-sbatch-directives","title":"Common SBATCH Directives","text":"Directive Purpose Example <code>--job-name</code> Job name <code>my_analysis</code> <code>--partition</code> Partition <code>Main</code>, <code>GPU</code> <code>--cpus-per-task</code> CPU cores <code>4</code> <code>--mem</code> Memory <code>16GB</code> <code>--time</code> Runtime limit <code>02:00:00</code> <code>--gres</code> GPU resources <code>gpu:1</code>"},{"location":"day2/hpc-ilifu-training/#file-transfer","title":"File Transfer","text":"<pre><code># Upload data\nscp local_file.txt username@training.ilifu.ac.za:~/\n\n# Download results\nscp username@training.ilifu.ac.za:~/results.txt ./\n\n# Sync directories\nrsync -av local_dir/ username@training.ilifu.ac.za:~/remote_dir/\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#additional-resources","title":"Additional Resources","text":"<ul> <li>ILIFU Documentation: https://docs.ilifu.ac.za</li> <li>SLURM Documentation: https://slurm.schedmd.com/documentation.html</li> <li>HPC Best Practices: Various online resources</li> <li>Training Materials: Regular workshops and tutorials</li> </ul>"},{"location":"day2/slurm-practical-tutorial/","title":"High Performance Computing with SLURM: Practical Tutorial","text":""},{"location":"day2/slurm-practical-tutorial/#high-performance-computing-with-slurm-practical-tutorial","title":"High Performance Computing with SLURM: Practical Tutorial","text":""},{"location":"day2/slurm-practical-tutorial/#getting-started-setup-instructions","title":"Getting Started - Setup Instructions","text":"<p>Before starting the exercises, you need to set up your working environment and copy the sample data files to your home directory. Follow these steps:</p>"},{"location":"day2/slurm-practical-tutorial/#step-1-create-your-working-directory","title":"Step 1: Create Your Working Directory","text":"<pre><code># The mkdir command creates a new directory\n# The -p flag creates parent directories if they don't exist\nmkdir -p ~/hpc_practice\n\n# Change to your new working directory\n# The ~ symbol represents your home directory\ncd ~/hpc_practice\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-copy-sample-data-files","title":"Step 2: Copy Sample Data Files","text":"<pre><code># Copy all sample data from the shared course directory to your current directory\n# The -r flag means \"recursive\" - it copies directories and their contents\n# The * wildcard matches all files in the source directory\n# The . (dot) means \"current directory\" (where you are now)\ncp -r /cbio/training/courses/2025/micmet-genomics/sample-data/* .\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-verify-your-setup","title":"Step 3: Verify Your Setup","text":"<pre><code># List all files in your directory to confirm they copied correctly\n# The -l flag shows detailed information (permissions, size, date)\n# The -a flag shows all files including hidden ones (starting with .)\nls -la\n\n# You should see these files:\n# - sample.fastq.gz    : Compressed DNA sequencing data (gzipped FASTQ format)\n# - sample1.fastq      : Uncompressed sequencing reads for practice\n# - sample2.fastq      : Another set of sequencing reads\n# - reference.fasta    : Reference genome sequence for alignment exercises\n# - data.txt          : Tab-delimited data for text processing examples\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#what-these-files-contain","title":"What These Files Contain","text":"<ul> <li>FASTQ files: Contain DNA sequences and quality scores from sequencing machines</li> <li>FASTA files: Contain reference sequences without quality scores</li> <li>Text files: Contain structured data for analysis practice</li> </ul> <p>Now you're ready to start the exercises!</p>"},{"location":"day2/slurm-practical-tutorial/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites - Unix Commands</li> <li>Getting Started - Your First Job</li> <li>Basic Job Templates</li> <li>Python and Bash Examples</li> <li>Advanced Job Types</li> <li>Resource Optimization</li> <li>Troubleshooting Examples</li> </ol>"},{"location":"day2/slurm-practical-tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"day2/slurm-practical-tutorial/#essential-unix-commands-for-hpc","title":"Essential Unix Commands for HPC","text":"<p>Before submitting SLURM jobs, master these Unix commands for pathogen genomics:</p> <pre><code># Navigate and organize\nmkdir -p project/{data,results,scripts}\ncd project\npwd\n\n# Inspect FASTQ files\nzcat sample.fastq.gz | head -20\nzcat sample.fastq.gz | wc -l | awk '{print $1/4}'  # Count reads\n\n# Search and filter\ngrep \"^&gt;\" reference.fasta  # Find FASTA headers\ngrep -c \"PASS\" variants.vcf  # Count PASS variants\n\n# Process text\nawk '{print $1, $2}' data.txt\nsed 's/old/new/g' file.txt\n</code></pre> <p>\ud83d\udcda Full Unix guide: See Unix Commands for Pathogen Genomics - Practical Tutorial for comprehensive examples and exercises.</p>"},{"location":"day2/slurm-practical-tutorial/#tutorial-your-first-slurm-jobs-step-by-step","title":"Tutorial: Your First SLURM Jobs - Step by Step","text":""},{"location":"day2/slurm-practical-tutorial/#tutorial-overview","title":"Tutorial Overview","text":"<p>In this hands-on tutorial, you'll learn to:</p> <ol> <li>Write and submit your first SLURM job</li> <li>Monitor job status and view outputs</li> <li>Run Python scripts on HPC</li> <li>Process genomics data with SLURM</li> <li>Handle errors and optimize resources</li> </ol> <p>Time needed: 30-45 minutes Prerequisites: Basic Unix commands (covered above)</p>"},{"location":"day2/slurm-practical-tutorial/#tutorial-1-hello-world-on-hpc","title":"Tutorial 1: Hello World on HPC","text":""},{"location":"day2/slurm-practical-tutorial/#step-1-write-your-first-job-script","title":"Step 1: Write Your First Job Script","text":"<p>Create a simple SLURM job that prints a greeting:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=hello\n#SBATCH --time=00:05:00\n\necho \"Hello from HPC!\"\necho \"This job ran on node: $(hostname)\"\necho \"Current time: $(date)\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-save-the-script","title":"Step 2: Save the Script","text":"<pre><code># Use nano editor to create the file\nnano hello.sh\n\n# Paste the script above, then:\n# Press Ctrl+X to exit\n# Press Y to save\n# Press Enter to confirm filename\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-submit-your-job","title":"Step 3: Submit Your Job","text":"<pre><code># Submit the job to SLURM\nsbatch hello.sh\n</code></pre> <p>You'll see: <code>Submitted batch job 12345</code> (your job ID will differ)</p>"},{"location":"day2/slurm-practical-tutorial/#step-4-monitor-your-job","title":"Step 4: Monitor Your Job","text":"<pre><code># Check if your job is running\nsqueue -u $USER\n\n# You'll see something like:\n# JOBID PARTITION     NAME     USER ST       TIME  NODES\n# 12345      Main    hello  yourname  R       0:01      1\n# ST column: PD=Pending, R=Running, CG=Completing\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-5-view-the-output","title":"Step 5: View the Output","text":"<pre><code># Once job completes (status disappears from squeue)\n# View the output file (replace 12345 with your job ID)\ncat slurm-12345.out\n</code></pre> <p>Example run:</p> <pre><code>$ sbatch hello.sh\nSubmitted batch job 10\n\n$ squeue -u $USER\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                10  training    hello   mamana  R       0:01      1 compute-1-sep2025\n\n$ cat slurm-10.out\nHello from HPC!\nThis job ran on node: compute-1-sep2025\nCurrent time: Mon Sep 1 23:57:07 SAST 2025\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#tutorial-2-running-python-on-hpc","title":"Tutorial 2: Running Python on HPC","text":""},{"location":"day2/slurm-practical-tutorial/#step-1-create-a-python-job-script","title":"Step 1: Create a Python Job Script","text":"<p>Let's run Python code on the cluster:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=python_hello\n#SBATCH --time=00:10:00\n#SBATCH --mem=1GB\n\n# Load Python (or use system python3 if modules not available)\nmodule load python/3.12.3  # Or use system python3 || echo \"Using system Python\"\n\n# Run your Python script\npython3 &lt;&lt; 'EOF'\nprint(\"Hello from Python on HPC!\")\nimport os\nprint(f\"Running on: {os.uname().nodename}\")\n\n# Simple calculation\nresult = sum(range(1000))\nprint(f\"Sum of 0-999 = {result}\")\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-submit-and-monitor","title":"Step 2: Submit and Monitor","text":"<pre><code># Save the script\nnano python_job.sh\n# (paste script, save with Ctrl+X, Y, Enter)\n\n# Submit the job\nsbatch python_job.sh\n\n# Watch it run (updates every 2 seconds)\nwatch -n 2 squeue -u $USER\n# Press Ctrl+C to stop watching\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-check-the-output","title":"Step 3: Check the Output","text":"<pre><code># Find your output file\nls -lt slurm-*.out | head -5\n\n# View the results\ncat slurm-[JOBID].out\n</code></pre> <p>Expected output:</p> <pre><code>Hello from Python on HPC!\nRunning on: compute-1-sep2025\nSum of 0-999 = 499500\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"Problem Solution \"Module not found\" Use <code>python3</code> instead of loading module \"Python: command not found\" Check with <code>which python3</code> Job stays pending too long Check resources with <code>sinfo</code>"},{"location":"day2/slurm-practical-tutorial/#tutorial-3-real-genomics-analysis","title":"Tutorial 3: Real Genomics Analysis","text":""},{"location":"day2/slurm-practical-tutorial/#objective","title":"Objective","text":"<p>Process FASTQ files using SLURM, simulating a real bioinformatics pipeline.</p>"},{"location":"day2/slurm-practical-tutorial/#step-1-create-the-analysis-script","title":"Step 1: Create the Analysis Script","text":"<p>This script demonstrates a typical genomics workflow:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=fastq_analysis\n#SBATCH --time=00:05:00\n#SBATCH --mem=2GB\n#SBATCH --cpus-per-task=2\n\necho \"=== FASTQ Analysis Pipeline Starting ===\"\necho \"Job ID: $SLURM_JOB_ID\"\necho \"Node: $(hostname)\"\necho \"Start time: $(date)\"\n\n# Create sample FASTQ files for analysis\necho \"Creating sample FASTQ files...\"\ncat &gt; sample1.fastq &lt;&lt; 'EOF'\n@SEQ_1\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65\n@SEQ_2\nACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGT\n+\nBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\nEOF\n\ncat &gt; sample2.fastq &lt;&lt; 'EOF'\n@SEQ_3\nTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n+\nIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n@SEQ_4\nCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n+\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nEOF\n\necho \"Step 1: Initial file validation...\"\nsleep 45  # Simulate file checking and validation\n\necho \"Step 2: Sequence counting and basic stats...\"\nfor file in sample*.fastq; do\n    echo \"Processing $file...\"\n    sequences=$(wc -l &lt; \"$file\")\n    sequences=$((sequences / 4))\n    echo \"  Found $sequences sequences\"\n\n    # Simulate per-file analysis time\n    echo \"  Analyzing sequence lengths...\"\n    sleep 25  # Processing time per file\n\n    avg_length=60\n    echo \"  Average sequence length: ${avg_length}bp\"\ndone\n\necho \"Step 3: Quality score analysis...\"\necho \"Analyzing quality scores across all sequences...\"\nsleep 60  # Simulate quality analysis\n\necho \"Step 4: Generating contamination check...\"\necho \"Checking for adapter sequences and contaminants...\"\nsleep 45  # Simulate contamination screening\n\necho \"Step 5: Creating final summary report...\"\ntotal_sequences=0\nfor file in sample*.fastq; do\n    seqs=$(wc -l &lt; \"$file\")\n    seqs=$((seqs / 4))\n    total_sequences=$((total_sequences + seqs))\ndone\n\necho \"Step 6: Finalizing results and cleanup...\"\nsleep 20  # Final processing and cleanup\n\necho \"=== Analysis Complete ===\"\necho \"Total sequences analyzed: $total_sequences\"\necho \"Analysis completed at: $(date)\"\necho \"Total runtime: ~4 minutes\"\n\n# Create a summary file\ncat &gt; analysis_summary.txt &lt;&lt; EOF\nFASTQ Analysis Summary\n=====================\nTotal files processed: 2\nTotal sequences: $total_sequences\nAverage sequence length: 60bp\nQuality check: PASSED\nContamination check: CLEAN\nAnalysis date: $(date)\nEOF\n\necho \"Summary report saved to: analysis_summary.txt\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-submit-and-monitor-the-job","title":"Step 2: Submit and Monitor the Job","text":"<pre><code># Save the script\nnano fastq_analysis.sh\n\n# Submit the job\nsbatch fastq_analysis.sh\n# Note your job ID (e.g., \"Submitted batch job 12347\")\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-monitor-job-progress-in-real-time","title":"Step 3: Monitor Job Progress in Real-Time","text":"<p>Open multiple terminal windows to watch different aspects:</p> <p>Terminal 1: Submit and monitor queue</p> <pre><code># Submit the job\nsbatch fastq_analysis.sh\nSubmitted batch job 15\n\n# Watch it in the queue (run multiple times)\n# Watch the queue (repeat every 10 seconds)\nsqueue -u $USER\n# Status codes: PD=Pending, R=Running, CG=Completing\n</code></pre> <p>Terminal 2: Watch live output</p> <pre><code># Once job starts running (status = R), watch the output\ntail -f slurm-15.out\n# Press Ctrl+C to stop watching\n</code></pre> <p>Terminal 3: Check job details</p> <pre><code># Get detailed job information\nscontrol show job 15\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-4-understanding-job-states","title":"Step 4: Understanding Job States","text":"<p>During the 4-minute runtime, you'll observe these states:</p> Time Status What's Happening 0:00-0:05 PD (Pending) Job waiting for resources 0:05-4:00 R (Running) Job executing on compute node 4:00+ - (Completed) Job finished, no longer in queue <p>Timeline of analysis steps:</p> <ul> <li>0:00-0:45 - File validation</li> <li>0:45-1:35 - Sequence counting (sample1.fastq)</li> <li>1:35-2:25 - Sequence counting (sample2.fastq)  </li> <li>2:25-3:25 - Quality score analysis</li> <li>3:25-4:10 - Contamination screening</li> <li>4:10-4:30 - Final report generation</li> </ul> <p>Learning opportunity: This 4-minute window allows everyone to:</p> <ul> <li>Practice using <code>squeue</code> to monitor jobs multiple times</li> <li>See job state transitions and timing in real-time  </li> <li>Understand queue system behavior with sufficient time for discussion</li> <li>Watch live output with <code>tail -f</code> to see analysis progress</li> <li>Check intermediate results and final efficiency reports</li> </ul> <p>\ud83d\udca1 Training Tip: Have participants submit this job, then use the 4-minute window to demonstrate:</p> <ul> <li>Refreshing <code>squeue -u $USER</code> every 30 seconds to track progress</li> <li>Using <code>scontrol show job JOBID</code> for detailed job information</li> <li>Explaining what PENDING vs RUNNING states mean</li> <li>Demonstrating <code>tail -f slurm-JOBID.out</code> to watch live step-by-step output</li> <li>Discussing resource allocation while job runs</li> <li>Explaining the difference between walltime and CPU time</li> </ul> <p>Expected final output files:</p> <ul> <li><code>slurm-JOBID.out</code> - Complete log of all analysis steps</li> <li><code>analysis_summary.txt</code> - Final summary report</li> <li><code>sample1.fastq</code> &amp; <code>sample2.fastq</code> - Generated test data files</li> </ul> <p>Sample log output:</p> <pre><code>=== FASTQ Analysis Pipeline Starting ===\nJob ID: 15\nNode: compute-2-sep2025\nStart time: Mon Sep 2 10:15:23 SAST 2025\nCreating sample FASTQ files...\nStep 1: Initial file validation...\nStep 2: Sequence counting and basic stats...\nProcessing sample1.fastq...\n  Found 2 sequences\n  Analyzing sequence lengths...\n  Average sequence length: 60bp\nProcessing sample2.fastq...\n  Found 2 sequences\n  Analyzing sequence lengths...\n  Average sequence length: 60bp\nStep 3: Quality score analysis...\nAnalyzing quality scores across all sequences...\nStep 4: Generating contamination check...\nChecking for adapter sequences and contaminants...\nStep 5: Creating final summary report...\nStep 6: Finalizing results and cleanup...\n=== Analysis Complete ===\nTotal sequences analyzed: 4\nAnalysis completed at: Mon Sep 2 10:19:45 SAST 2025\nTotal runtime: ~4 minutes\nSummary report saved to: analysis_summary.txt\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#practice-exercises","title":"Practice Exercises","text":""},{"location":"day2/slurm-practical-tutorial/#exercise-1-modify-and-submit-a-job","title":"Exercise 1: Modify and Submit a Job","text":"<p>Task: Modify the hello.sh script to include your name and the current date.</p> <pre><code># Step 1: Edit the script\nnano hello.sh\n\n# Step 2: Add these lines:\necho \"Submitted by: [YOUR NAME]\"\necho \"Analysis date: $(date +%Y-%m-%d)\"\n\n# Step 3: Submit and check\nsbatch hello.sh\nsqueue -u $USER\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#exercise-2-resource-monitoring","title":"Exercise 2: Resource Monitoring","text":"<p>Task: Create a job that uses specific resources and monitor them.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=resource_test\n#SBATCH --time=00:02:00\n#SBATCH --mem=500MB\n#SBATCH --cpus-per-task=2\n\necho \"Allocated CPUs: $SLURM_CPUS_PER_TASK\"\necho \"Allocated Memory: $SLURM_MEM_PER_NODE MB\"\necho \"Running on node: $(hostname)\"\n\n# Use the allocated CPUs\nstress --cpu $SLURM_CPUS_PER_TASK --timeout 30s\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#exercise-3-array-jobs","title":"Exercise 3: Array Jobs","text":"<p>Task: Process multiple files in parallel using array jobs.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=array_demo\n#SBATCH --array=1-3\n#SBATCH --time=00:05:00\n\necho \"Processing file number: $SLURM_ARRAY_TASK_ID\"\n# Your processing command here\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#basic-templates","title":"Basic Templates","text":""},{"location":"day2/slurm-practical-tutorial/#1-standard-job-template","title":"1. Standard Job Template","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=my_job          # Give your job a name\n#SBATCH --time=01:00:00            # Max runtime (1 hour)\n#SBATCH --mem=4GB                  # Memory needed\n#SBATCH --output=output_%j.log     # Output file (%j = job ID)\n\n# Load software you need\nmodule load python/3.12.3  # Or use system python3\n\n# Run your command\necho \"Job started on $(hostname) at $(date)\"\npython my_script.py\necho \"Job completed at $(date)\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-multi-core-parallel-job","title":"2. Multi-core Parallel Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=parallel_job\n#SBATCH --partition=Main\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16GB\n#SBATCH --time=02:00:00\n#SBATCH --output=parallel_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n\n# Use all available cores\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\necho \"Using $SLURM_CPUS_PER_TASK CPU cores\"\npython parallel_script.py\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#python-and-bash-examples","title":"Python and Bash Examples","text":""},{"location":"day2/slurm-practical-tutorial/#python-jobs","title":"Python Jobs","text":""},{"location":"day2/slurm-practical-tutorial/#basic-python-analysis","title":"Basic Python Analysis","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=python_analysis\n#SBATCH --time=01:30:00\n#SBATCH --mem=8GB\n#SBATCH --cpus-per-task=4\n\n# Load Python module\nmodule load python/3.12.3  # Or use system python3\n\n# Run your analysis\npython genome_analysis.py sample_data.fasta\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#python-with-virtual-environment","title":"Python with Virtual Environment","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=python_venv\n#SBATCH --time=02:00:00\n#SBATCH --mem=16GB\n\nmodule load python/3.12.3  # Or use system python3\n\n# Create and activate virtual environment\npython -m venv pathogen_env\nsource pathogen_env/bin/activate\n\n# Install bioinformatics packages\npip install biopython pandas numpy matplotlib\n\n# Run pathogen analysis\npython pathogen_analysis.py\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#pathogen-genomics-snp-analysis","title":"Pathogen Genomics - SNP Analysis","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=snp_analysis\n#SBATCH --time=04:00:00\n#SBATCH --mem=32GB\n#SBATCH --cpus-per-task=8\n\nmodule load python/3.12.3  # Or use system python3\n\n# Python script for SNP analysis\npython &lt;&lt; 'EOF'\nimport pandas as pd\nfrom multiprocessing import Pool\nimport os\n\ndef analyze_sample(vcf_file):\n    \"\"\"Analyze SNPs in a VCF file\"\"\"\n    print(f\"Processing {vcf_file}\")\n\n    # Count SNPs (simplified example)\n    with open(vcf_file, 'r') as f:\n        snp_count = sum(1 for line in f if not line.startswith('#'))\n\n    return vcf_file, snp_count\n\n# Get all VCF files\nvcf_files = [f for f in os.listdir('.') if f.endswith('.vcf')]\n\n# Use all available CPU cores\nwith Pool(int(os.environ['SLURM_CPUS_PER_TASK'])) as pool:\n    results = pool.map(analyze_sample, vcf_files)\n\n# Save results\nresults_df = pd.DataFrame(results, columns=['Sample', 'SNP_Count'])\nresults_df.to_csv('snp_analysis_results.csv', index=False)\nprint(f\"Analyzed {len(vcf_files)} samples\")\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#bashshell-script-jobs","title":"Bash/Shell Script Jobs","text":""},{"location":"day2/slurm-practical-tutorial/#basic-fastq-processing","title":"Basic FASTQ Processing","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=fastq_processing\n#SBATCH --time=01:00:00\n#SBATCH --mem=4GB\n\n# Process multiple FASTQ files\nfor file in *.fastq; do\n    echo \"Processing $file...\"\n\n    # Count sequences (FASTQ has 4 lines per sequence)\n    sequences=$(wc -l &lt; \"$file\")\n    sequences=$((sequences / 4))\n\n    # Get basic stats\n    echo \"File: $file - Sequences: $sequences\"\n\n    # Count reads with quality scores above threshold\n    good_reads=$(awk 'NR%4==0 &amp;&amp; length($0)&gt;20' \"$file\" | wc -l)\n    echo \"High quality reads: $good_reads\"\ndone\n\necho \"Processing complete!\"\n</code></pre> <p>Expected output:</p> <pre><code>Processing sample1.fastq...\nFile: sample1.fastq - Sequences: 3\nHigh quality reads: 3\nProcessing sample2.fastq...\nFile: sample2.fastq - Sequences: 2\nHigh quality reads: 2\nProcessing complete!\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#pathogen-genomics-pipeline","title":"Pathogen Genomics Pipeline","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=pathogen_pipeline\n#SBATCH --time=06:00:00\n#SBATCH --mem=64GB\n#SBATCH --cpus-per-task=16\n\n# Load bioinformatics tools\nmodule load fastqc/0.12.1  # Check available version with 'module avail'\n# module load trimmomatic  # Install if needed\nmodule load bwa/github  # Check available version\nmodule load samtools/1.22.1\nmodule load bcftools/1.22\n\n# Sample information\nSAMPLE=\"pathogen_sample\"\nREFERENCE=\"reference_genome.fasta\"\n\necho \"=== Pathogen Genomics Pipeline Starting ===\"\necho \"Sample: $SAMPLE\"\necho \"Reference: $REFERENCE\"\necho \"CPUs: $SLURM_CPUS_PER_TASK\"\n\n# Step 1: Quality control\necho \"Step 1: Running FastQC...\"\nmkdir -p qc_reports\nfastqc \"${SAMPLE}_R1.fastq\" \"${SAMPLE}_R2.fastq\" -o qc_reports/\n\n# Step 2: Trim low-quality reads and adapters\necho \"Step 2: Trimming reads...\"\ntrimmomatic PE -threads $SLURM_CPUS_PER_TASK \\\n    \"${SAMPLE}_R1.fastq\" \"${SAMPLE}_R2.fastq\" \\\n    \"${SAMPLE}_R1_trimmed.fastq\" \"${SAMPLE}_R1_unpaired.fastq\" \\\n    \"${SAMPLE}_R2_trimmed.fastq\" \"${SAMPLE}_R2_unpaired.fastq\" \\\n    ILLUMINACLIP:adapters.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n\n# Step 3: Align reads to reference genome\necho \"Step 3: Aligning to reference genome...\"\nbwa mem -t $SLURM_CPUS_PER_TASK \"$REFERENCE\" \\\n    \"${SAMPLE}_R1_trimmed.fastq\" \"${SAMPLE}_R2_trimmed.fastq\" | \\\n    samtools sort -@ $SLURM_CPUS_PER_TASK -o \"${SAMPLE}_sorted.bam\"\n\n# Step 4: Index BAM file\necho \"Step 4: Indexing BAM file...\"\nsamtools index \"${SAMPLE}_sorted.bam\"\n\n# Step 5: Variant calling\necho \"Step 5: Calling variants...\"\nbcftools mpileup -f \"$REFERENCE\" \"${SAMPLE}_sorted.bam\" | \\\n    bcftools call -mv -Oz -o \"${SAMPLE}_variants.vcf.gz\"\n\n# Step 6: Index VCF and get basic stats\necho \"Step 6: Processing variants...\"\nbcftools index \"${SAMPLE}_variants.vcf.gz\"\nbcftools stats \"${SAMPLE}_variants.vcf.gz\" &gt; \"${SAMPLE}_variant_stats.txt\"\n\n# Summary\necho \"=== Pipeline Summary ===\"\necho \"Alignment stats:\"\nsamtools flagstat \"${SAMPLE}_sorted.bam\"\n\necho \"Variant counts:\"\nbcftools view -H \"${SAMPLE}_variants.vcf.gz\" | wc -l\n\necho \"=== Pathogen Genomics Pipeline Complete ===\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#multi-sample-outbreak-analysis","title":"Multi-Sample Outbreak Analysis","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=outbreak_analysis\n#SBATCH --time=08:00:00\n#SBATCH --mem=128GB\n#SBATCH --cpus-per-task=32\n\n# Load required modules\nmodule load python/3.12.3  # Or use system python3\nmodule load iqtree/2.2.0\nmodule load mafft/7.490\n\necho \"=== Multi-Sample Outbreak Analysis ===\"\n\n# Step 1: Concatenate all consensus sequences\necho \"Step 1: Preparing sequences for phylogenetic analysis...\"\ncat *.consensus.fasta &gt; all_samples.fasta\n\n# Step 2: Multiple sequence alignment\necho \"Step 2: Performing multiple sequence alignment...\"\nmafft --auto --thread $SLURM_CPUS_PER_TASK all_samples.fasta &gt; aligned_sequences.fasta\n\n# Step 3: Build phylogenetic tree\necho \"Step 3: Building phylogenetic tree...\"\niqtree2 -s aligned_sequences.fasta -nt $SLURM_CPUS_PER_TASK -bb 1000\n\n# Step 4: Calculate pairwise distances\necho \"Step 4: Calculating genetic distances...\"\npython &lt;&lt; 'EOF'\nfrom Bio import AlignIO\nfrom Bio.Phylo.TreeConstruction import DistanceCalculator\nimport pandas as pd\n\n# Read alignment\nalignment = AlignIO.read(\"aligned_sequences.fasta\", \"fasta\")\n\n# Calculate distances\ncalculator = DistanceCalculator('identity')\ndistance_matrix = calculator.get_distance(alignment)\n\n# Convert to DataFrame for easier handling\nsamples = [record.id for record in alignment]\ndist_df = pd.DataFrame(distance_matrix.matrix, \n                      index=samples, \n                      columns=samples)\n\n# Save distance matrix\ndist_df.to_csv('genetic_distances.csv')\n\n# Find closely related samples (distance &lt; 0.001)\nclose_pairs = []\nfor i, sample1 in enumerate(samples):\n    for j, sample2 in enumerate(samples[i+1:], i+1):\n        distance = distance_matrix.matrix[i][j]\n        if distance &lt; 0.001:  # Very similar sequences\n            close_pairs.append([sample1, sample2, distance])\n\nif close_pairs:\n    close_df = pd.DataFrame(close_pairs, \n                           columns=['Sample1', 'Sample2', 'Distance'])\n    close_df.to_csv('potential_transmission_links.csv', index=False)\n    print(f\"Found {len(close_pairs)} potential transmission links\")\nelse:\n    print(\"No closely related samples found\")\nEOF\n\necho \"=== Outbreak Analysis Complete ===\"\necho \"Results:\"\necho \"- Phylogenetic tree: aligned_sequences.fasta.treefile\"\necho \"- Genetic distances: genetic_distances.csv\"\necho \"- Potential links: potential_transmission_links.csv\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#advanced-job-types","title":"Advanced Job Types","text":""},{"location":"day2/slurm-practical-tutorial/#1-array-jobs","title":"1. Array Jobs","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=array_processing\n#SBATCH --partition=Main\n#SBATCH --array=1-100%10        # 100 jobs, max 10 concurrent\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4GB\n#SBATCH --time=00:30:00\n#SBATCH --output=array_%A_%a.log\n\nmodule load python/3.12.3  # Or use system python3\n\n# Use array task ID to process different files\nINPUT_FILE=\"input_${SLURM_ARRAY_TASK_ID}.txt\"\nOUTPUT_FILE=\"output_${SLURM_ARRAY_TASK_ID}.txt\"\n\necho \"Processing $INPUT_FILE on $(hostname)\"\npython process_file.py $INPUT_FILE $OUTPUT_FILE\n\necho \"Task $SLURM_ARRAY_TASK_ID completed\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-job-dependencies","title":"2. Job Dependencies","text":"<pre><code>#!/bin/bash\n# Submit first job\nJOB1=$(sbatch --parsable preprocess.sh)\n\n# Submit second job that depends on first\nJOB2=$(sbatch --parsable --dependency=afterok:$JOB1 analysis.sh)\n\n# Submit final job that depends on second\nsbatch --dependency=afterok:$JOB2 postprocess.sh\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#3-multi-node-mpi-job","title":"3. Multi-node MPI Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=mpi_job\n#SBATCH --partition=Main\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=16\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=2GB\n#SBATCH --time=04:00:00\n\n# module load openmpi  # Check if MPI is available\n\n# Total tasks = nodes * ntasks-per-node = 4 * 16 = 64\necho \"Running on $SLURM_NNODES nodes with $SLURM_NTASKS total tasks\"\n\nmpirun ./my_mpi_program input.dat\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#4-interactive-job","title":"4. Interactive Job","text":"<pre><code>graph TB\n    subgraph \"Interactive Jobs\"\n        I1[User logs in] --&gt; I2[Request interactive session&lt;br/&gt;sinteractive/srun --pty]\n        I2 --&gt; I3[Wait for resources]\n        I3 --&gt; I4[Get shell on compute node]\n        I4 --&gt; I5[Run commands interactively]\n        I5 --&gt; I6[See output in real-time]\n        I6 --&gt; I7[Exit when done]\n\n        style I4 fill:#e8f5e9\n        style I5 fill:#e8f5e9\n        style I6 fill:#e8f5e9\n    end\n\n    subgraph \"Batch Jobs\"\n        B1[User logs in] --&gt; B2[Write job script]\n        B2 --&gt; B3[Submit with sbatch]\n        B3 --&gt; B4[Job queued]\n        B4 --&gt; B5[Job runs automatically]\n        B5 --&gt; B6[Output to files]\n        B6 --&gt; B7[Check results later]\n\n        style B5 fill:#e1f5fe\n        style B6 fill:#e1f5fe\n    end\n\n    subgraph \"When to Use\"\n        UI[Interactive: Development,&lt;br/&gt;Testing, Debugging]\n        UB[Batch: Production runs,&lt;br/&gt;Long jobs, Multiple jobs]\n    end\n\n    I7 --&gt; UI\n    B7 --&gt; UB</code></pre> <p>Figure: Comparison between interactive and batch job workflows in SLURM</p> <pre><code># Request interactive session using sinteractive (ILIFU-specific)\nsinteractive -c 1 --time 03:00                    # 1 CPU for 3 hours (default)\nsinteractive -c 5 --time 5-00:00                 # 5 CPUs for 5 days (maximum)\n\n# Alternative: Use srun for interactive session\nsrun --partition=Main --cpus-per-task=4 --mem=8GB --time=02:00:00 --pty bash\n\n# Once in interactive session:\nmodule load python/3.12.3  # Or use system python3\npython  # Start interactive Python\n</code></pre> <p>Note: Resources on the Devel partition are shared (CPU and memory). For dedicated resources, use <code>srun</code> on the Main partition.</p>"},{"location":"day2/slurm-practical-tutorial/#5-jupyter-notebook-on-compute-node","title":"5. Jupyter Notebook on Compute Node","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=jupyter\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16GB\n#SBATCH --time=04:00:00\n#SBATCH --output=jupyter_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n\n# Install jupyter if needed\npip install --user jupyter\n\n# Get node info\nNODE=$(hostname -s)\nPORT=8888\n\necho \"Starting Jupyter notebook on node $NODE, port $PORT\"\necho \"SSH tunnel command:\"\necho \"ssh -N -L ${PORT}:${NODE}:${PORT} ${USER}@training.ilifu.ac.za\"\n\n# Start Jupyter\njupyter notebook --no-browser --port=$PORT --ip=0.0.0.0\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#resource-optimization","title":"Resource Optimization","text":""},{"location":"day2/slurm-practical-tutorial/#1-memory-optimization-examples","title":"1. Memory Optimization Examples","text":""},{"location":"day2/slurm-practical-tutorial/#low-memory-job","title":"Low Memory Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=low_mem\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=2GB           # Conservative memory request\n#SBATCH --time=01:00:00\n\nmodule load python/3.12.3  # Or use system python3\n\n# Process data in chunks to save memory\npython &lt;&lt; 'EOF'\nimport pandas as pd\n\n# Read in chunks instead of loading entire file\nchunk_size = 10000\nresults = []\n\nfor chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n    # Process chunk\n    processed = chunk.groupby('category').sum()\n    results.append(processed)\n\n# Combine results\nfinal_result = pd.concat(results)\nfinal_result.to_csv('output.csv')\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#memory-intensive-job","title":"Memory-intensive Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=high_mem\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=64GB          # High memory for large datasets\n#SBATCH --time=04:00:00\n\nmodule load python/3.12.3  # Or use system python3\n\n# Load large dataset into memory\npython &lt;&lt; 'EOF'\nimport pandas as pd\nimport numpy as np\n\n# Load entire large dataset\ndf = pd.read_csv('very_large_file.csv')\nprint(f\"Loaded dataset with shape: {df.shape}\")\n\n# Memory-intensive operations\ncorrelation_matrix = df.corr()\ncorrelation_matrix.to_csv('correlations.csv')\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-time-optimization","title":"2. Time Optimization","text":""},{"location":"day2/slurm-practical-tutorial/#checkpointing-example","title":"Checkpointing Example","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=checkpointed_job\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16GB\n#SBATCH --time=02:00:00\n#SBATCH --output=checkpoint_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n\npython &lt;&lt; 'EOF'\nimport pickle\nimport os\nimport time\n\ncheckpoint_file = 'checkpoint.pkl'\n\n# Try to load previous state\nif os.path.exists(checkpoint_file):\n    with open(checkpoint_file, 'rb') as f:\n        state = pickle.load(f)\n    start_iteration = state['iteration']\n    results = state['results']\n    print(f\"Resuming from iteration {start_iteration}\")\nelse:\n    start_iteration = 0\n    results = []\n    print(\"Starting from scratch\")\n\n# Main computation loop\nfor i in range(start_iteration, 1000):\n    # Simulate some work\n    time.sleep(1)\n    result = i ** 2\n    results.append(result)\n\n    # Save checkpoint every 100 iterations\n    if i % 100 == 0:\n        state = {'iteration': i + 1, 'results': results}\n        with open(checkpoint_file, 'wb') as f:\n            pickle.dump(state, f)\n        print(f\"Checkpoint saved at iteration {i}\")\n\nprint(\"Computation completed\")\n\n# Clean up checkpoint file\nif os.path.exists(checkpoint_file):\n    os.remove(checkpoint_file)\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#troubleshooting-examples","title":"Troubleshooting Examples","text":""},{"location":"day2/slurm-practical-tutorial/#1-debug-job-failures","title":"1. Debug Job Failures","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=debug_job\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4GB\n#SBATCH --time=00:15:00\n#SBATCH --output=debug_%j.log\n#SBATCH --error=debug_%j.err\n\n# Enable debugging\nset -e  # Exit on any error\nset -x  # Print commands as they execute\n\necho \"=== Environment Information ===\"\necho \"Node: $(hostname)\"\necho \"Date: $(date)\"\necho \"Working directory: $(pwd)\"\necho \"User: $(whoami)\"\necho \"SLURM Job ID: $SLURM_JOB_ID\"\necho \"SLURM CPUs: $SLURM_CPUS_PER_TASK\"\n\necho \"=== Module Information ===\"\nmodule list\n\necho \"=== Python Information ===\"\nmodule load python/3.12.3  # Or use system python3\nwhich python\npython --version\n\necho \"=== Running Script ===\"\npython my_script.py 2&gt;&amp;1 | tee python_output.log\n\necho \"=== Job Completed ===\"\necho \"Exit code: $?\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-memory-usage-monitoring","title":"2. Memory Usage Monitoring","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=memory_monitor\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8GB\n#SBATCH --time=01:00:00\n\n# Function to monitor memory usage\nmonitor_memory() {\n    while true; do\n        echo \"$(date): Memory usage: $(free -h | grep '^Mem' | awk '{print $3}')\"\n        sleep 30\n    done\n}\n\n# Start memory monitoring in background\nmonitor_memory &amp;\nMONITOR_PID=$!\n\n# Load modules and run main task\nmodule load python/3.12.3  # Or use system python3\npython memory_intensive_script.py\n\n# Stop monitoring\nkill $MONITOR_PID\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#3-file-permission-issues","title":"3. File Permission Issues","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=file_check\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=2GB\n#SBATCH --time=00:10:00\n\necho \"=== File System Checks ===\"\n\n# Check input files exist and are readable\nINPUT_FILES=(\"input1.txt\" \"input2.txt\" \"config.json\")\n\nfor file in \"${INPUT_FILES[@]}\"; do\n    if [[ -f \"$file\" ]]; then\n        if [[ -r \"$file\" ]]; then\n            echo \"\u2713 $file exists and is readable\"\n        else\n            echo \"\u2717 $file exists but is not readable\"\n            ls -l \"$file\"\n            exit 1\n        fi\n    else\n        echo \"\u2717 $file does not exist\"\n        exit 1\n    fi\ndone\n\n# Check output directory is writable\nOUTPUT_DIR=\"results\"\nif [[ ! -d \"$OUTPUT_DIR\" ]]; then\n    mkdir -p \"$OUTPUT_DIR\" || {\n        echo \"\u2717 Cannot create output directory $OUTPUT_DIR\"\n        exit 1\n    }\nfi\n\nif [[ -w \"$OUTPUT_DIR\" ]]; then\n    echo \"\u2713 Output directory $OUTPUT_DIR is writable\"\nelse\n    echo \"\u2717 Output directory $OUTPUT_DIR is not writable\"\n    ls -ld \"$OUTPUT_DIR\"\n    exit 1\nfi\n\necho \"All file checks passed!\"\n\n# Proceed with actual work\npython main_script.py\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#job-submission-scripts","title":"Job Submission Scripts","text":""},{"location":"day2/slurm-practical-tutorial/#batch-submit-multiple-jobs","title":"Batch Submit Multiple Jobs","text":"<pre><code>#!/bin/bash\n# submit_multiple.sh - Submit multiple related jobs\n\n# Array of input files\nINPUT_FILES=(data1.txt data2.txt data3.txt data4.txt)\n\n# Submit a job for each input file\nfor i in \"${!INPUT_FILES[@]}\"; do\n    input_file=\"${INPUT_FILES[$i]}\"\n    job_name=\"process_$(basename $input_file .txt)\"\n\n    echo \"Submitting job for $input_file\"\n\n    sbatch --job-name=\"$job_name\" \\\n           --output=\"${job_name}_%j.log\" \\\n           --export=INPUT_FILE=\"$input_file\" \\\n           process_template.sh\n\n    sleep 1  # Brief pause between submissions\ndone\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#template-with-environment-variables","title":"Template with Environment Variables","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=templated_job\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8GB\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.log  # %x = job name, %j = job id\n\n# Use environment variables passed from submission script\necho \"Processing file: $INPUT_FILE\"\necho \"Output directory: $OUTPUT_DIR\"\necho \"Parameters: $PARAMS\"\n\nmodule load python/3.12.3  # Or use system python3\n\n# Use the variables in your script\npython analysis.py \\\n    --input \"$INPUT_FILE\" \\\n    --output \"$OUTPUT_DIR\" \\\n    --params \"$PARAMS\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#performance-testing-template","title":"Performance Testing Template","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=performance_test\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16GB\n#SBATCH --time=01:00:00\n#SBATCH --output=perf_%j.log\n\necho \"=== Performance Test Started ===\"\necho \"Job ID: $SLURM_JOB_ID\"\necho \"Node: $(hostname)\"\necho \"CPUs allocated: $SLURM_CPUS_PER_TASK\"\necho \"Memory allocated: ${SLURM_MEM_PER_NODE}MB\"\necho \"Start time: $(date)\"\n\n# Record resource usage\necho \"=== Initial Resource Usage ===\"\nfree -h\ndf -h $HOME\ndf -h /scratch/$USER\n\nmodule load python/3.12.3  # Or use system python3\n\n# Time the main computation\necho \"=== Starting Main Computation ===\"\nstart_time=$(date +%s)\n\npython performance_test_script.py\n\nend_time=$(date +%s)\nruntime=$((end_time - start_time))\n\necho \"=== Performance Summary ===\"\necho \"Runtime: ${runtime} seconds\"\necho \"End time: $(date)\"\n\n# Check final resource usage\necho \"=== Final Resource Usage ===\"\nfree -h\n\necho \"=== Performance Test Completed ===\"\n</code></pre> <p>This comprehensive set of SLURM examples covers most common use cases and provides templates that can be adapted for specific needs. Each example includes comments explaining the key parameters and concepts.</p>"},{"location":"day2/unix-commands-pathogen-examples/","title":"Unix Commands for Pathogen Genomics - Practical Tutorial","text":""},{"location":"day2/unix-commands-pathogen-examples/#unix-commands-for-pathogen-genomics-practical-tutorial","title":"Unix Commands for Pathogen Genomics - Practical Tutorial","text":"<p>Adapted from Microbial-Genomics practice scripts</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-overview","title":"Tutorial Overview","text":""},{"location":"day2/unix-commands-pathogen-examples/#what-youll-learn","title":"What You'll Learn","text":"<p>This hands-on tutorial will teach you essential Unix commands for pathogen genomics analysis. By the end, you'll be able to: - Navigate and organize genomics project directories - Process FASTQ and FASTA files efficiently - Extract meaningful information from sequencing data - Build simple analysis pipelines - Prepare data for HPC analysis</p>"},{"location":"day2/unix-commands-pathogen-examples/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic terminal/command line access</li> <li>No prior Unix experience required</li> <li>Access to the training server</li> </ul>"},{"location":"day2/unix-commands-pathogen-examples/#time-required","title":"Time Required","text":"<ul> <li>Complete tutorial: 2-3 hours</li> <li>Quick essentials: 45 minutes</li> </ul>"},{"location":"day2/unix-commands-pathogen-examples/#learning-path","title":"Learning Path","text":"<ol> <li>Setup \u2192 2. Basic Navigation \u2192 3. File Operations \u2192 4. Data Processing \u2192 5. Pipeline Building</li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#setup-instructions","title":"Setup Instructions","text":"<p>Before starting the Unix command exercises, you need to prepare your workspace with sample genomics data. Here's how:</p>"},{"location":"day2/unix-commands-pathogen-examples/#step-1-create-your-practice-directory","title":"Step 1: Create Your Practice Directory","text":"<pre><code># Create a new directory for your practice exercises\n# mkdir = \"make directory\"\n# -p = create parent directories if needed (won't error if directory exists)\n# ~ = shortcut for your home directory (/home/username)\nmkdir -p ~/hpc_practice\n\n# Navigate into your new directory\n# cd = \"change directory\"\ncd ~/hpc_practice\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-copy-sample-genomics-data","title":"Step 2: Copy Sample Genomics Data","text":"<pre><code># Copy all sample files from the shared course folder to your current location\n# cp = \"copy\" command\n# -r = \"recursive\" - copy directories and all their contents\n# * = wildcard that matches all files\n# . = current directory (destination)\ncp -r /cbio/training/courses/2025/micmet-genomics/sample-data/* .\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-verify-your-files","title":"Step 3: Verify Your Files","text":"<pre><code># List all files with details to confirm the copy was successful\n# ls = \"list\" command\n# -l = long format (shows permissions, size, dates)\n# -a = show all files (including hidden files starting with .)\nls -la\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#understanding-your-sample-files","title":"Understanding Your Sample Files","text":"<p>The following files are now in your directory for practice:</p> File Description Used For <code>sample.fastq.gz</code> Compressed DNA sequencing reads Learning <code>zcat</code>, <code>gunzip</code>, file compression <code>sample1.fastq</code> Uncompressed sequencing data (3 reads) Practicing <code>grep</code>, <code>wc</code>, sequence counting <code>sample2.fastq</code> Another FASTQ file (2 reads) Array processing, file comparisons <code>reference.fasta</code> Reference genome sequences Learning <code>grep</code> with FASTA headers, sequence extraction <code>data.txt</code> Tab-delimited sample metadata Practicing <code>awk</code>, <code>sed</code>, <code>cut</code>, <code>sort</code> commands <p>File Formats Explained: - FASTQ: Contains sequences + quality scores (4 lines per read) - FASTA: Contains sequences only (2 lines per sequence: header + sequence) - GZ: Gzip compressed file (saves space, common in genomics)</p>"},{"location":"day2/unix-commands-pathogen-examples/#quick-command-reference-with-detailed-explanations","title":"Quick Command Reference with Detailed Explanations","text":""},{"location":"day2/unix-commands-pathogen-examples/#essential-commands-for-genomics-analysis","title":"Essential Commands for Genomics Analysis","text":"<p>Before diving into detailed modules, here's a quick reference of the most commonly used commands in pathogen genomics, with detailed explanations of what each component does:</p>"},{"location":"day2/unix-commands-pathogen-examples/#navigate-and-organize","title":"Navigate and Organize","text":"<pre><code># Create nested directories for a genomics project\nmkdir -p project/{data,results,scripts}\n# Explanation:\n# mkdir = make directory command\n# -p = create parent directories as needed (won't error if they exist)\n# project/ = main project folder\n# {data,results,scripts} = brace expansion creates 3 subdirectories at once\n#   - data/ for raw sequencing files\n#   - results/ for analysis outputs\n#   - scripts/ for your analysis code\n\n# Navigate to your project directory\ncd project\n# cd = change directory\n# project = destination directory (relative path from current location)\n\n# Show current working directory\npwd\n# pwd = print working directory\n# Returns absolute path like: /home/username/hpc_practice/project\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#inspect-fastq-files","title":"Inspect FASTQ Files","text":"<pre><code># View compressed FASTQ file content\nzcat sample.fastq.gz | head -20\n# Explanation:\n# zcat = view compressed file without extracting (z = gzip, cat = concatenate)\n# sample.fastq.gz = compressed FASTQ file (common in genomics to save space)\n# | = pipe operator, sends output to next command\n# head -20 = show first 20 lines (5 complete reads since FASTQ uses 4 lines/read)\n\n# Count number of reads in FASTQ file\nzcat sample.fastq.gz | wc -l | awk '{print $1/4}'\n# Explanation:\n# zcat sample.fastq.gz = decompress and output file content\n# wc -l = word count with -l flag counts lines\n# | = pipe the line count to awk\n# awk '{print $1/4}' = divide line count by 4 (FASTQ has 4 lines per read)\n#   - $1 = first field (the line count)\n#   - /4 = division to get read count\n# Example: 400 lines / 4 = 100 reads\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#search-and-filter","title":"Search and Filter","text":"<pre><code># Find all FASTA headers in reference genome\ngrep \"^&gt;\" reference.fasta\n# Explanation:\n# grep = global regular expression print (searches for patterns)\n# \"^&gt;\" = pattern to search for\n#   - ^ = start of line anchor (line must begin with &gt;)\n#   - &gt; = literal \"&gt;\" character (FASTA headers start with &gt;)\n# reference.fasta = file to search in\n# Output: Shows all sequence headers like \"&gt;chr1\", \"&gt;gene_ABC123\"\n\n# Count high-quality variants\ngrep -c \"PASS\" variants.vcf\n# Explanation:\n# grep = search command\n# -c = count matching lines instead of showing them\n# \"PASS\" = quality filter status in VCF files\n# variants.vcf = variant call format file\n# Returns: Number like \"1234\" (count of variants passing quality filters)\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#process-text-data","title":"Process Text Data","text":"<pre><code># Extract specific columns from data\nawk '{print $1, $2}' data.txt\n# Explanation:\n# awk = powerful text processing tool\n# '{print $1, $2}' = awk program\n#   - {} = action block\n#   - print = output command\n#   - $1 = first column/field\n#   - $2 = second column/field\n#   - , = adds space between fields in output\n# data.txt = input file\n# Example input:  \"Sample1 100 resistant\"\n# Example output: \"Sample1 100\"\n\n# Replace text in files\nsed 's/old/new/g' file.txt\n# Explanation:\n# sed = stream editor for text transformation\n# 's/old/new/g' = substitution command\n#   - s = substitute command\n#   - /old/ = pattern to find\n#   - /new/ = replacement text\n#   - g = global flag (replace all occurrences, not just first)\n# file.txt = input file\n# Example: Changes \"old_sample_name\" to \"new_sample_name\" throughout file\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#combined-pipeline-examples","title":"Combined Pipeline Examples","text":""},{"location":"day2/unix-commands-pathogen-examples/#example-1-quick-fastq-quality-check","title":"Example 1: Quick FASTQ Quality Check","text":"<pre><code># Count reads and check quality score distribution\nzcat sample.fastq.gz | \\\n  awk 'NR%4==0' | \\\n  cut -c1-10 | \\\n  sort | \\\n  uniq -c | \\\n  sort -rn\n\n# Line-by-line explanation:\n# zcat sample.fastq.gz = decompress FASTQ\n# awk 'NR%4==0' = get every 4th line (quality scores)\n#   - NR = line number\n#   - %4==0 = divisible by 4 (4th, 8th, 12th lines...)\n# cut -c1-10 = first 10 characters of quality string\n# sort = alphabetically sort quality patterns\n# uniq -c = count unique patterns\n# sort -rn = sort by count, highest first\n#   - -r = reverse order\n#   - -n = numerical sort\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#example-2-extract-high-quality-reads","title":"Example 2: Extract High-Quality Reads","text":"<pre><code># Get read IDs with average quality &gt; 30\nzcat sample.fastq.gz | \\\n  paste - - - - | \\\n  awk '{if(length($4) &gt; 0) print $1, length($4)}' | \\\n  grep \"^@\"\n\n# Explanation:\n# paste - - - - = combine every 4 lines into 1 tab-delimited line\n# awk = process the combined lines\n# $1 = read ID, $4 = quality string\n# grep \"^@\" = filter for valid read IDs\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#pro-tips-for-these-commands","title":"Pro Tips for These Commands","text":"<ol> <li> <p>Always preview before processing:    </p><pre><code>zcat file.gz | head -20  # Check format first\n</code></pre><p></p> </li> <li> <p>Count before and after filtering:    </p><pre><code># Before\ngrep -c \"^@\" input.fastq\n# After filtering\ngrep -c \"^@\" filtered.fastq\n</code></pre><p></p> </li> <li> <p>Use quotes for patterns with special characters:    </p><pre><code>grep \"^&gt;\" file.fasta     # Correct\ngrep ^&gt; file.fasta        # May fail - shell interprets &gt;\n</code></pre><p></p> </li> <li> <p>Combine commands efficiently:    </p><pre><code># Instead of creating intermediate files:\nzcat file.gz &gt; temp.txt\ngrep \"pattern\" temp.txt &gt; result.txt\n\n# Use pipes:\nzcat file.gz | grep \"pattern\" &gt; result.txt\n</code></pre><p></p> </li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#module-1-directory-organization-for-genomics-projects","title":"Module 1: Directory Organization for Genomics Projects","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives","title":"Learning Objectives","text":"<p>\u2713 Create organized project directories \u2713 Navigate between directories efficiently \u2713 Understand genomics project structure</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-11-creating-your-first-project-structure","title":"Tutorial 1.1: Creating Your First Project Structure","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-start-with-a-simple-structure","title":"Step 1: Start with a Simple Structure","text":"<pre><code># Create your main project directory\nmkdir my_first_project\n\n# Enter the directory\ncd my_first_project\n\n# Check where you are\npwd\n# Output: /home/username/hpc_practice/my_first_project\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-add-subdirectories","title":"Step 2: Add Subdirectories","text":"<pre><code># Create data directories\nmkdir data\nmkdir results\nmkdir scripts\n\n# List what you created\nls\n# Output: data  results  scripts\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-create-a-complex-structure","title":"Step 3: Create a Complex Structure","text":"<pre><code># Use -p to create nested directories\nmkdir -p data/{raw_reads,reference_genomes,metadata}\nmkdir -p results/{qc,alignment,variants,phylogeny}\nmkdir -p scripts logs tmp\n\n# View the structure\nls -la\n# The -la flags show: l=long format, a=all files\n</code></pre> <p>Try It Yourself: </p><pre><code># Exercise: Create this structure\n# project/\n#   \u251c\u2500\u2500 input/\n#   \u2502   \u251c\u2500\u2500 sequences/\n#   \u2502   \u2514\u2500\u2500 references/\n#   \u2514\u2500\u2500 output/\n#       \u251c\u2500\u2500 aligned/\n#       \u2514\u2500\u2500 reports/\n\n# Solution:\nmkdir -p project/{input/{sequences,references},output/{aligned,reports}}\n</code></pre><p></p> <p>Real-world application: </p><pre><code># Set up M. tuberculosis outbreak analysis\nmkdir -p mtb_outbreak_2025/{data,results,scripts,logs}\ncd mtb_outbreak_2025\nmkdir -p data/{fastq,references,clinical_metadata}\nmkdir -p results/{fastqc,trimming,bwa_alignment,vcf_files,phylogenetic_tree}\n</code></pre><p></p>"},{"location":"day2/unix-commands-pathogen-examples/#module-2-file-management-for-sequencing-data","title":"Module 2: File Management for Sequencing Data","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_1","title":"Learning Objectives","text":"<p>\u2713 Create and edit text files \u2713 Copy and rename sequencing files \u2713 Organize data systematically</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-21-working-with-sample-lists","title":"Tutorial 2.1: Working with Sample Lists","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-create-a-sample-list","title":"Step 1: Create a Sample List","text":"<pre><code># First, ensure you're in the right place\npwd\ncd ~/hpc_practice\n\n# Create an empty file\ntouch sample_list.txt\n\n# Check it was created\nls -la sample_list.txt\n# Output: -rw-r--r-- 1 user group 0 Sep 2 10:00 sample_list.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-add-content-to-the-file","title":"Step 2: Add Content to the File","text":"<pre><code># Method 1: Using echo (for single lines)\necho \"Sample_001\" &gt; sample_list.txt\necho \"Sample_002\" &gt;&gt; sample_list.txt  # &gt;&gt; appends, &gt; overwrites!\n\n# Method 2: Using nano editor (recommended for multiple lines)\nnano sample_list.txt\n# Type or paste the following content:\n# MTB_sample_001\n# MTB_sample_002\n# MTB_sample_003\n# MTB_sample_004\n# Then save with: Ctrl+X, Y, Enter\n\n# View what you created\ncat sample_list.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-count-and-verify","title":"Step 3: Count and Verify","text":"<pre><code># Count lines in file\nwc -l sample_list.txt\n# Output: 4 sample_list.txt\n\n# Count words\nwc -w sample_list.txt\n# Output: 4 sample_list.txt\n\n# Get full statistics\nwc sample_list.txt\n# Output: 4  4  58 sample_list.txt\n#        (lines, words, characters)\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-22-organizing-sequencing-files","title":"Tutorial 2.2: Organizing Sequencing Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-copy-files-safely","title":"Step 1: Copy Files Safely","text":"<pre><code># Copy sample files to practice with\ncp sample*.fastq .\n\n# List files before renaming\nls sample*.fastq\n# Output: sample1.fastq  sample2.fastq\n\n# Create a backup first (always!)\nmkdir backups\ncp sample*.fastq backups/\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-rename-files-systematically","title":"Step 2: Rename Files Systematically","text":"<pre><code># Rename a single file\nmv sample1.fastq patient001_reads.fastq\n\n# Batch rename using a loop\nfor file in sample*.fastq; do\n    # Extract the number from filename\n    num=$(echo $file | grep -o '[0-9]\\+')\n    # Create new name\n    newname=\"patient_${num}_sequences.fastq\"\n    echo \"Renaming $file to $newname\"\n    mv \"$file\" \"$newname\"\ndone\n\n# Verify the renaming\nls *.fastq\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#practice-exercise","title":"Practice Exercise:","text":"<pre><code># Exercise: Create copies with dates\n# Copy sample.fastq.gz to sample_20250902.fastq.gz\n\n# Solution:\ndate_stamp=$(date +%Y%m%d)\ncp sample.fastq.gz sample_${date_stamp}.fastq.gz\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#module-3-viewing-and-inspecting-genomics-files","title":"Module 3: Viewing and Inspecting Genomics Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_2","title":"Learning Objectives","text":"<p>\u2713 View compressed and uncompressed files \u2713 Count sequences in FASTQ/FASTA files \u2713 Extract specific parts of files</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-31-working-with-fastq-files","title":"Tutorial 3.1: Working with FASTQ Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-view-compressed-files","title":"Step 1: View Compressed Files","text":"<pre><code># View first 4 lines (1 complete read) of compressed file\nzcat sample.fastq.gz | head -4\n# Output:\n# @SEQ_001\n# ACGTACGTACGTACGTACGTACGTACGTACGTACGT\n# +\n# IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n\n# View first 2 reads (8 lines)\nzcat sample.fastq.gz | head -8\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-count-sequences","title":"Step 2: Count Sequences","text":"<pre><code># Count total lines\nzcat sample.fastq.gz | wc -l\n# Output: 12  (for 3 reads)\n\n# Count number of reads (FASTQ has 4 lines per read)\nzcat sample.fastq.gz | wc -l | awk '{print $1/4}'\n# Output: 3\n\n# Alternative: Count sequence headers\nzcat sample.fastq.gz | grep -c \"^@\"\n# Output: 3\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-view-with-line-numbers","title":"Step 3: View with Line Numbers","text":"<pre><code># Add line numbers to output\nzcat sample.fastq.gz | head -8 | cat -n\n# Output:\n#      1    @SEQ_001\n#      2    ACGTACGTACGTACGTACGTACGTACGTACGTACGT\n#      3    +\n#      4    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n#      5    @SEQ_002\n#      6    TGCATGCATGCATGCATGCATGCATGCATGCATGCA\n#      7    +\n#      8    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-32-working-with-fasta-files","title":"Tutorial 3.2: Working with FASTA Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-view-fasta-headers","title":"Step 1: View FASTA Headers","text":"<pre><code># View first line (header) of FASTA\nhead -1 reference.fasta\n# Output: &gt;Sequence_1 gene=ABC123\n\n# View all headers in file\ngrep \"^&gt;\" reference.fasta\n# Output:\n# &gt;Sequence_1 gene=ABC123\n# &gt;Sequence_2 gene=DEF456\n\n# Count sequences\ngrep -c \"^&gt;\" reference.fasta\n# Output: 2\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-extract-sequences","title":"Step 2: Extract Sequences","text":"<pre><code># View first 2 lines (header + sequence start)\nhead -2 reference.fasta\n\n# Skip header, view sequence only\ntail -n +2 reference.fasta | head -1\n# Output: ACGTACGTACGTACGTACGTACGTACGTACGTACGT\n\n# Get sequence length\ntail -n +2 reference.fasta | head -1 | wc -c\n# Output: 37 (includes newline)\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#practice-exercise_1","title":"Practice Exercise:","text":"<pre><code># Exercise: Count total bases in all sequences\n# Hint: Remove headers first\n\n# Solution:\ngrep -v \"^&gt;\" reference.fasta | tr -d '\\n' | wc -c\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#module-4-searching-and-filtering-genomics-data","title":"Module 4: Searching and Filtering Genomics Data","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_3","title":"Learning Objectives","text":"<p>\u2713 Search for patterns in files \u2713 Filter genomics data \u2713 Use regular expressions</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-41-basic-pattern-searching","title":"Tutorial 4.1: Basic Pattern Searching","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-simple-searches","title":"Step 1: Simple Searches","text":"<pre><code># Search for a word in a file\ngrep \"resistant\" data.txt\n# Output: Sample1 100 resistant\n#         Sample3 150 resistant\n\n# Case-insensitive search (-i flag)\ngrep -i \"SAMPLE\" data.txt\n# Finds: Sample1, Sample2, etc.\n\n# Count matches (-c flag)\ngrep -c \"resistant\" data.txt\n# Output: 2\n\n# Show line numbers (-n flag)\ngrep -n \"resistant\" data.txt\n# Output: 1:Sample1 100 resistant\n#         3:Sample3 150 resistant\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-search-in-genomics-files","title":"Step 2: Search in Genomics Files","text":"<pre><code># Find sequence headers in FASTA\ngrep \"^&gt;\" reference.fasta\n# ^ means \"start of line\"\n\n# Find adapter sequences in FASTQ\ngrep \"AGATCGGAAGAG\" sample1.fastq\n\n# Search compressed files\nzcat sample.fastq.gz | grep \"ACGT\"\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-42-advanced-pattern-matching","title":"Tutorial 4.2: Advanced Pattern Matching","text":""},{"location":"day2/unix-commands-pathogen-examples/#using-regular-expressions","title":"Using Regular Expressions","text":"<pre><code># Find lines with numbers\ngrep '[0-9]' data.txt\n# [0-9] matches any digit\n\n# Find lines ending with specific pattern\ngrep 'resistant$' data.txt\n# $ means \"end of line\"\n\n# Extract only the matching part (-o flag)\necho \"Sample123\" | grep -o '[0-9]\\+'\n# Output: 123\n# \\+ means \"one or more\"\n\n# Multiple patterns (OR)\ngrep -E 'resistant|sensitive' data.txt\n# -E enables extended regex\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#practice-exercise_2","title":"Practice Exercise:","text":"<pre><code># Exercise: Find all samples with values &gt; 150\n# Hint: Use awk instead of grep for numeric comparisons\n\n# Solution:\nawk '$2 &gt; 150' data.txt\n# Output: Sample2 200 sensitive\n#         Sample4 300 sensitive\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#module-5-text-processing-for-genomics","title":"Module 5: Text Processing for Genomics","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_4","title":"Learning Objectives","text":"<p>\u2713 Extract specific columns from files \u2713 Perform calculations on data \u2713 Transform text efficiently</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-51-using-awk-for-data-processing","title":"Tutorial 5.1: Using awk for Data Processing","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-extract-columns","title":"Step 1: Extract Columns","text":"<pre><code># Print specific columns (1st and 2nd)\nawk '{print $1, $2}' data.txt\n# Output: Sample1 100\n#         Sample2 200\n#         Sample3 150\n#         Sample4 300\n\n# Print with custom separator\nawk '{print $1 \",\" $2}' data.txt\n# Output: Sample1,100\n\n# Add text to output\nawk '{print \"Sample:\" $1 \" Value:\" $2}' data.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-perform-calculations","title":"Step 2: Perform Calculations","text":"<pre><code># Sum values in column 2\nawk '{sum+=$2} END {print \"Total:\", sum}' data.txt\n# Output: Total: 750\n\n# Calculate average\nawk '{sum+=$2; count++} END {print \"Average:\", sum/count}' data.txt\n# Output: Average: 187.5\n\n# Filter based on value\nawk '$2 &gt; 150 {print $0}' data.txt\n# Output: Sample2 200 sensitive\n#         Sample4 300 sensitive\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-52-using-sed-for-text-manipulation","title":"Tutorial 5.2: Using sed for Text Manipulation","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-basic-substitutions","title":"Step 1: Basic Substitutions","text":"<pre><code># Replace text (s = substitute)\necho \"Sample1\" | sed 's/1/A/'\n# Output: SampleA\n\n# Global replacement (g = global)\necho \"Sample111\" | sed 's/1/A/g'\n# Output: SampleAAA\n\n# Replace in file and save\nsed 's/Sample/Patient/g' data.txt &gt; patients.txt\n\n# Edit file in place (careful!)\nsed -i.bak 's/Sample/Patient/g' data.txt\n# Creates data.txt.bak as backup\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-advanced-manipulations","title":"Step 2: Advanced Manipulations","text":"<pre><code># Delete lines containing pattern\nsed '/sensitive/d' data.txt\n# Removes lines with \"sensitive\"\n\n# Add text to beginning of lines\nsed 's/^/PREFIX_/' data.txt\n# Adds PREFIX_ to each line start\n\n# Convert spaces to tabs\nsed 's/ /\\t/g' data.txt &gt; data.tsv\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#6-file-permissions-and-management","title":"6. File Permissions and Management","text":""},{"location":"day2/unix-commands-pathogen-examples/#managing-permissions","title":"Managing Permissions","text":"<pre><code># Make scripts executable\nchmod +x scripts/analysis_pipeline.sh\n\n# Protect raw data from accidental modification\nchmod 444 data/raw_reads/*.fastq.gz\n\n# Set directory permissions\nchmod 755 results/\n\n# Check permissions\nls -la data/raw_reads/\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#7-sorting-and-unique-operations","title":"7. Sorting and Unique Operations","text":""},{"location":"day2/unix-commands-pathogen-examples/#processing-sample-lists","title":"Processing Sample Lists","text":"<pre><code># Sort sample names\nsort data/sample_list.txt\n\n# Sort numerically by coverage\nsort -k2 -n coverage_stats.txt\n\n# Get unique mutations\ncut -f1,2 variants.txt | sort | uniq\n\n# Count occurrences of each mutation\ncut -f3 mutations.txt | sort | uniq -c | sort -rn\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#8-pipelines-and-redirection","title":"8. Pipelines and Redirection","text":""},{"location":"day2/unix-commands-pathogen-examples/#creating-simple-analysis-pipelines","title":"Creating Simple Analysis Pipelines","text":"<pre><code># Count reads per sample and save to file\nfor file in data/raw_reads/*.fastq.gz; do\n    sample=$(basename $file .fastq.gz)\n    count=$(zcat $file | wc -l | awk '{print $1/4}')\n    echo -e \"$sample\\t$count\"\ndone &gt; results/qc/read_counts.txt\n\n# Extract and count specific genes\ngrep \"^&gt;\" reference.fasta | cut -d' ' -f1 | sed 's/&gt;//' | sort | uniq -c &gt; gene_counts.txt\n\n# Process multiple VCF files\nfor vcf in results/variants/*.vcf; do\n    sample=$(basename $vcf .vcf)\n    pass_count=$(grep -c \"PASS\" $vcf)\n    total_count=$(grep -v \"^#\" $vcf | wc -l)\n    echo -e \"$sample\\t$total_count\\t$pass_count\"\ndone &gt; results/variant_summary.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#9-practical-slurm-integration","title":"9. Practical SLURM Integration","text":""},{"location":"day2/unix-commands-pathogen-examples/#preparing-files-for-hpc-analysis","title":"Preparing Files for HPC Analysis","text":"<p>To create a SLURM job script:</p> <pre><code># Open nano to create the script\nnano prep_pathogen_data.sh\n</code></pre> <p>Copy and paste the following content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=prep_pathogen_data\n#SBATCH --time=00:30:00\n#SBATCH --mem=4GB\n\n# Create directory structure\nmkdir -p ${SLURM_JOB_ID}_analysis/{data,results,logs}\n\n# Copy and organize files\ncp /shared/data/*.fastq.gz ${SLURM_JOB_ID}_analysis/data/\n\n# Generate file list for processing\nls ${SLURM_JOB_ID}_analysis/data/*.fastq.gz &gt; file_list.txt\n\n# Count and verify files\necho \"Total files to process: $(wc -l &lt; file_list.txt)\"\n\n# Create metadata file\nfor file in ${SLURM_JOB_ID}_analysis/data/*.fastq.gz; do\n    size=$(du -h $file | cut -f1)\n    reads=$(zcat $file | wc -l | awk '{print $1/4}')\n    echo \"$(basename $file)\\t$size\\t$reads\"\ndone &gt; ${SLURM_JOB_ID}_analysis/data/file_metadata.tsv\n\necho \"Preparation complete. Ready for analysis.\"\n</code></pre> <p>Save the file with: <code>Ctrl+X</code>, then <code>Y</code>, then <code>Enter</code></p> <p>Submit the job with: <code>sbatch prep_pathogen_data.sh</code></p>"},{"location":"day2/unix-commands-pathogen-examples/#hands-on-exercise-complete-pathogen-analysis-workflow","title":"Hands-On Exercise: Complete Pathogen Analysis Workflow","text":""},{"location":"day2/unix-commands-pathogen-examples/#exercise-overview","title":"Exercise Overview","text":"<p>Build a complete analysis pipeline step-by-step, applying all the skills you've learned.</p>"},{"location":"day2/unix-commands-pathogen-examples/#part-1-setup-and-data-preparation","title":"Part 1: Setup and Data Preparation","text":"<pre><code># Step 1: Create project structure\nmkdir -p pathogen_practice/{data,results,scripts}\ncd pathogen_practice\npwd  # Verify you're in the right place\n\n# Step 2: Create sample metadata\nnano data/samples.txt\n# Paste the following content:\n# Mtb_patient_001_resistant\n# Mtb_patient_002_susceptible  \n# Mtb_patient_003_resistant\n# Salmonella_outbreak_001\n# Salmonella_outbreak_002\n# Save with: Ctrl+X, Y, Enter\n\n# Verify the file was created\ncat data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#part-2-data-analysis-tasks","title":"Part 2: Data Analysis Tasks","text":""},{"location":"day2/unix-commands-pathogen-examples/#task-1-find-resistant-samples","title":"Task 1: Find Resistant Samples","text":"<pre><code># Use grep to find resistant samples\ngrep \"resistant\" data/samples.txt\n\n# Save results to file\ngrep \"resistant\" data/samples.txt &gt; results/resistant_samples.txt\n\n# Count how many\ngrep -c \"resistant\" data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#task-2-count-by-pathogen-type","title":"Task 2: Count by Pathogen Type","text":"<pre><code># Count MTB samples\ngrep -c \"Mtb\" data/samples.txt\n\n# Count Salmonella samples\ngrep -c \"Salmonella\" data/samples.txt\n\n# Save counts\necho \"MTB samples: $(grep -c 'Mtb' data/samples.txt)\" &gt; results/pathogen_counts.txt\necho \"Salmonella samples: $(grep -c 'Salmonella' data/samples.txt)\" &gt;&gt; results/pathogen_counts.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#task-3-generate-summary-report","title":"Task 3: Generate Summary Report","text":"<pre><code># Create a comprehensive summary using nano\nnano results/summary_report.txt\n\n# Type the following content (replace the values with actual counts):\n# === Pathogen Analysis Summary ===\n# Date: [current date]\n# Total samples: 5\n# Resistant samples: 2\n# Susceptible samples: 1\n# MTB samples: 3\n# Salmonella samples: 2\n# =================================\n# Save with: Ctrl+X, Y, Enter\n\n# Or use echo commands to generate it automatically:\necho \"=== Pathogen Analysis Summary ===\" &gt; results/summary_report.txt\necho \"Date: $(date)\" &gt;&gt; results/summary_report.txt\necho \"Total samples: $(wc -l &lt; data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"Resistant samples: $(grep -c \"resistant\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"Susceptible samples: $(grep -c \"susceptible\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"MTB samples: $(grep -c \"Mtb\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"Salmonella samples: $(grep -c \"Salmonella\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"=================================\" &gt;&gt; results/summary_report.txt\n\n# Display the report\ncat results/summary_report.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#part-3-challenge-exercises","title":"Part 3: Challenge Exercises","text":""},{"location":"day2/unix-commands-pathogen-examples/#challenge-1-extract-sample-ids","title":"Challenge 1: Extract Sample IDs","text":"<pre><code># Extract just the patient IDs (hint: use cut or awk)\n# Try it yourself first!\n\n# Solution:\ncut -d'_' -f2,3 data/samples.txt\n# Or using awk:\nawk -F'_' '{print $2\"_\"$3}' data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#challenge-2-sort-and-count-unique-pathogens","title":"Challenge 2: Sort and Count Unique Pathogens","text":"<pre><code># Extract pathogen names and count occurrences\n# Try it yourself first!\n\n# Solution:\ncut -d'_' -f1 data/samples.txt | sort | uniq -c\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#challenge-3-create-a-pipeline","title":"Challenge 3: Create a Pipeline","text":"<pre><code># Find all resistant MTB samples in one command\n# Try it yourself first!\n\n# Solution:\ngrep \"Mtb\" data/samples.txt | grep \"resistant\"\n# Or more elegantly:\ngrep \"Mtb.*resistant\" data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tips-for-pathogen-genomics-unix-usage","title":"Tips for Pathogen Genomics Unix Usage","text":"<ol> <li>Always work with copies of raw sequencing data</li> <li>Use meaningful file names with sample IDs and dates</li> <li>Document your commands in scripts for reproducibility</li> <li>Check file integrity after transfers (md5sum)</li> <li>Compress large files to save space (gzip/bgzip)</li> <li>Use screen or tmux for long-running processes</li> <li>Regular backups of analysis results</li> <li>Version control for scripts (git)</li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#common-file-formats-in-pathogen-genomics","title":"Common File Formats in Pathogen Genomics","text":"Extension Format View Command Description <code>.fastq.gz</code> Compressed FASTQ <code>zcat file.fastq.gz \\| head</code> Raw sequencing reads <code>.fasta</code> FASTA <code>cat file.fasta</code> Reference genomes <code>.sam/.bam</code> SAM/BAM <code>samtools view file.bam \\| head</code> Alignments <code>.vcf</code> VCF <code>cat file.vcf</code> Variant calls <code>.gff/.gtf</code> GFF/GTF <code>cat file.gff</code> Gene annotations <code>.newick/.tree</code> Newick <code>cat file.tree</code> Phylogenetic trees"},{"location":"day2/unix-commands-pathogen-examples/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"day2/unix-commands-pathogen-examples/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"day2/unix-commands-pathogen-examples/#issue-1-permission-denied","title":"Issue 1: \"Permission denied\"","text":"<pre><code># Problem: Can't access or modify a file\n# Solution: Check permissions\nls -la filename\n\n# Fix: Change permissions if you own the file\nchmod u+rw filename\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-2-no-such-file-or-directory","title":"Issue 2: \"No such file or directory\"","text":"<pre><code># Problem: File path is wrong\n# Solution: Check your current location\npwd\n\n# List files to verify\nls -la\n\n# Use absolute paths to be sure\n/full/path/to/file\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-3-command-not-found","title":"Issue 3: \"Command not found\"","text":"<pre><code># Problem: Tool not installed or not in PATH\n# Solution: Check if command exists\nwhich command_name\n\n# Load module if available\nmodule avail\nmodule load tool_name\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-4-file-is-empty-or-corrupted","title":"Issue 4: File is empty or corrupted","text":"<pre><code># Check file size\nls -lh filename\n\n# Check file type\nfile filename\n\n# For compressed files, test integrity\ngzip -t file.gz\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-5-out-of-disk-space","title":"Issue 5: Out of disk space","text":"<pre><code># Check available space\ndf -h\n\n# Find large files\ndu -sh * | sort -h\n\n# Clean up temporary files\nrm -rf tmp/*\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#best-practices-to-avoid-issues","title":"Best Practices to Avoid Issues","text":"<ol> <li> <p>Always backup before modifying </p><pre><code>cp important_file important_file.backup\n</code></pre><p></p> </li> <li> <p>Use tab completion to avoid typos    </p><pre><code>cat sam[TAB]  # Completes to sample.fastq\n</code></pre><p></p> </li> <li> <p>Preview commands with echo first </p><pre><code>echo mv *.fastq backup/  # See what would happen\nmv *.fastq backup/       # Then run for real\n</code></pre><p></p> </li> <li> <p>Check file contents before processing </p><pre><code>head -5 file.txt  # Preview first 5 lines\nwc -l file.txt    # Count total lines\n</code></pre><p></p> </li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#quick-reference-card","title":"Quick Reference Card","text":""},{"location":"day2/unix-commands-pathogen-examples/#essential-commands-summary","title":"Essential Commands Summary","text":"Task Command Example List files <code>ls -la</code> <code>ls -la *.fastq</code> Change directory <code>cd</code> <code>cd ~/hpc_practice</code> Create directory <code>mkdir -p</code> <code>mkdir -p data/reads</code> Copy files <code>cp -r</code> <code>cp sample.fastq backup/</code> Move/rename <code>mv</code> <code>mv old.txt new.txt</code> View compressed <code>zcat</code> <code>zcat file.gz \\| head</code> Count lines <code>wc -l</code> <code>wc -l sample.txt</code> Search text <code>grep</code> <code>grep \"pattern\" file</code> Extract columns <code>awk</code> <code>awk '{print $1}' file</code> Replace text <code>sed</code> <code>sed 's/old/new/g' file</code> Sort data <code>sort</code> <code>sort -n numbers.txt</code> Get unique <code>uniq</code> <code>sort file \\| uniq</code>"},{"location":"day2/unix-commands-pathogen-examples/#next-steps","title":"Next Steps","text":"<p>After mastering these Unix commands, you're ready to: 1. Submit SLURM jobs - See High Performance Computing with SLURM: Practical Tutorial 2. Learn HPC concepts - See HPC and ILIFU Training Materials 3. Build analysis pipelines with Nextflow 4. Perform quality control with FastQC 5. Align reads with BWA 6. Call variants with SAMtools/BCFtools</p> <p>Remember: Unix commands are the foundation of all bioinformatics pipelines!</p>"},{"location":"day3/genome_assembly_notes/","title":"Objectives","text":""},{"location":"day3/genome_assembly_notes/#objectives","title":"Objectives","text":"<ol> <li>To perform De novo genome assembly and reference-based genome assembly</li> <li>Assess assembly quality using quality metrics</li> </ol>"},{"location":"day3/genome_assembly_notes/#genome-assembly","title":"Genome assembly","text":"<p>Genome assembly involves reconstructing a genome from a set of fragmented DNA sequences (reads) obtained from sequencing technologies. </p> <p>It aims to piece together the reads to create a continuous sequence that represents the genome of an organism.</p>"},{"location":"day3/genome_assembly_notes/#key-concepts","title":"Key Concepts","text":"<ul> <li>Reads: Fragmented sequences of DNA obtained from sequencing technologies.</li> <li>Contigs: Continuous sequences formed by overlapping reads.</li> <li>Scaffolds: Ordered and oriented sets of contigs, sometimes with gaps, which represent larger regions of the genome.</li> <li>Coverage: The average number of times each base in the genome is sequenced, which affects the accuracy of the assembly.</li> <li>Assembly can be:<ul> <li>De novo assembly - the construction of the genome from scratch without a reference.</li> <li>Reference-guided assembly - uses a known reference genome to guide the assembly of the new genome.</li> </ul> </li> </ul>"},{"location":"day3/genome_assembly_notes/#steps-in-genome-assembly","title":"Steps in Genome Assembly","text":"<ol> <li>Preprocessing (quality control, adapter sequences and low-quality bases filtering with FASTQC, TRIMMOMATIC/FASTP/BBMap).</li> <li>Assembly</li> <li>Scaffolding (involves use of long-range information from paired-end or mate-pair reads to order and orient contigs into scaffolds (e.g., SSPACE).</li> <li>Gap Filling (use additional reads or assembly techniques to close gaps within scaffolds (e.g., GapCloser).</li> <li>Polishing (correcting sequencing errors and improving the accuracy of the assembly, e.g., Pilon).</li> <li>Evaluation (Assembly Metrics including N50 (length of the contig such that 50% of the total assembly length is in contigs of this length or longer); genome completeness; and accuracy, e.g., QUAST).</li> </ol>"},{"location":"day3/genome_assembly_notes/#genome-assembly-algorithms","title":"Genome Assembly Algorithms","text":""},{"location":"day3/genome_assembly_notes/#overlap-layout-consensus-olc","title":"Overlap Layout Consensus (OLC)","text":"<ul> <li>Suitable for long-read sequencing (e.g., PacBio, Oxford Nanopore)</li> <li>Determines overlap between reads</li> <li>Arranges reads based on ovelaps</li> <li>Resolves conflict to build the final sequences</li> <li>Handles long reads and complex regions well</li> <li>Computationally intensive</li> </ul>"},{"location":"day3/genome_assembly_notes/#de-bruijn-graph-dbg","title":"De Bruijn Graph (DBG)","text":"<ul> <li>Best for short-read sequencing (e.g., illumina).</li> <li>uses small overlapping sequences (k-mers) to build a graph, nodes: k-mers, and edges: overlaps.</li> <li>Fast and memory efficient for large datasets</li> <li>Struggles with repetitive regions</li> </ul>"},{"location":"day3/genome_assembly_notes/#hybrid-methods","title":"Hybrid Methods","text":"<ul> <li>Combine DBG and OLC strengths to improve assembly quality, especially for error-prone long reads.\u200b</li> </ul>"},{"location":"day3/genome_assembly_notes/#assembler-classification-by-read-type","title":"Assembler Classification by Read Type","text":""},{"location":"day3/genome_assembly_notes/#short-read-assemblers","title":"Short-Read Assemblers\u200b","text":"<ul> <li>Short-read assemblers process high volumes of short sequences using DBG algorithms</li> <li>Ideal for technologies like Illumina.\u200b</li> <li>SPAdes (widely used for small genomes)</li> <li>Velvet (older though useful)</li> <li>SOAPdenovo (short-read assembler for larger genomes),</li> <li>ABySS (large genomes).</li> </ul>"},{"location":"day3/genome_assembly_notes/#long-read-assemblers","title":"Long-Read Assemblers\u200b","text":"<ul> <li>Long-read assemblers handle fewer, longer sequences using OLC algorithms</li> <li>Can span entire genes and repetitive regions</li> <li>Optimized for platforms like PacBio and ONT.\u200b</li> <li>Higher error rates (can be corrected by short reads or polishing)</li> <li>Canu, Celera, Flye</li> </ul>"},{"location":"day3/genome_assembly_notes/#hybrid-assemblers","title":"Hybrid Assemblers\u200b","text":"<ul> <li>Combine short and long reads, integrating DBG and OLC methods for improved accuracy and contiguity.\u200b</li> <li>Balances accuracy (short reads) and completeness (long reads, resolving repeats and structural variants)</li> <li>Requires careful data integration</li> <li>Unicycler, MaSuRCA</li> </ul>"},{"location":"day3/genome_assembly_notes/#reference-guided-assembly","title":"Reference-guided Assembly","text":"<ul> <li>Works well given a closely related reference genome</li> <li>Aligns reads to a reference genome and fills gaps</li> <li>Faster and less computationally demanding</li> <li>May miss novel sequences or structural variations</li> </ul>"},{"location":"day3/genome_assembly_notes/#note","title":"NOTE","text":"Assembly Strategy Subtypes Read Type Compatibility Examples Notes De novo assembly DBG, OLC, Hybrid Short, long, hybrid Velvet, SPAdes, Canu, Flye, MaSuRCA No reference genome used Reference-guided assembly Mapping-based Short, long BWA, Bowtie2, Novoalign, Minimap2 Aligns reads to a known reference genome"},{"location":"day3/genome_assembly_notes/#assembler-selection-factors","title":"Assembler Selection Factors\u200b","text":"<p>Choosing an assembler depends on\" - Sequencing technology, - Project goals, and - Available computational resources.\u200b</p>"},{"location":"day3/genome_assembly_notes/#best-practices-for-genome-assembly","title":"Best Practices for Genome Assembly","text":"<ul> <li>Start with high-quality, high-coverage sequencing data to improve the accuracy of the assembly.</li> <li>Try multiple assemblers and compare results, as different tools may perform better for different data types.</li> <li>Use iterative rounds of assembly, scaffolding, and polishing to gradually improve the assembly.</li> <li>Validate the final assembly using independent data, such as long-read sequencing or optical mapping.</li> <li>Keep detailed records of all parameters and steps used in the assembly process for reproducibility.</li> </ul>"},{"location":"day3/practical_genome_assmbly/","title":"De novo Genome Assembly Practical","text":""},{"location":"day3/practical_genome_assmbly/#de-novo-genome-assembly-practical","title":"De novo Genome Assembly Practical","text":"<p>Before setting up you need to know the current workig directory  </p><pre><code># Check current working directory\npwd\n\n# List the contents of the working diresctory\nls\n\n# Create relevant output directories. -p so that it creates parent dir if it doesn't exist\nmkdir -p /users/${USER}/results/02_assembly/tb\nmkdir -p /users/${USER}/results/02_assembly/vc\nmkdir -p /users/${USER}/results/03_quality/tb\nmkdir -p /users/${USER}/results/03_quality/vc\n</code></pre><p></p> <p>Delete /users/${USER}/results/02_assembly and /users/user24/results/03_quality and create thenm using one line code and not four as above</p> <pre><code>mkdir /users/${USER}/results/02_assembly/tb /users/user24/results/02_assembly/vc \\\n  /users/${USER}/results/03_quality/tb /users/user24/results/03_quality/vc\n\n# Request for an interactive node\nsrun --cpus-per-tasks=32 --mem=128g --time 5:00:00 --job-name \"ephie\" --pty /bin/bash\n\n# # Clear all modules\nmodule purge\n\n# Load the required modules\nmodule load spades/4.2.0\n\n## How can I get the help documentation?\nspades.py --help\n\n# Lets run spades with a test dataset\n/software/bio/spades/4.2.0/bin/spades.py  --test  --careful\n\n# Run spades with test data but with more details on the reads\nspades.py  -1 /software/bio/spades/4.2.0/share/spades/test_dataset/ecoli_1K_1.fq.gz \\\n    -2 /software/bio/spades/4.2.0/share/spades/test_dataset/ecoli_1K_2.fq.gz \\\n  --careful -o results/02_assembly-test\n\n## Subset the data\nhead -5 tb_IDs &gt; 02_tb_IDs\nhead -5 vc_IDs &gt; 02_vc_IDs\n\n# Perform denovo assembly for all TB samples\nmkdir -p /users/${USER}/scripts\nnano /users/${USER}/scripts/02_assembly.sh\n\n#!/bin/bash\n#SBATCH --job-name='02_assembly'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=120GB\n#SBATCH --output=/users/${USER}/logs/02_assembly-stdout.txt\n#SBATCH --error=/users/${USER}/logs/02_assembly-stdout.txt\n#SBATCH --time=12:00:00\n\nfor SAMPLE in $(cat tb_IDs); do\n  echo \"[TB|SPADES] $SAMPLE\"\n  spades.py -1 /users/${USER}/results/trimmed_trimmomatic/tb/${SAMPLE}_1.fastq.gz \\\n    -2 /users/${USER}/results/trimmed_trimmomatic/tb/${SAMPLE}_2.fastq.gz \\\n    --careful \\\n    -o /users/${USER}/results/02_assembly/tb/${SAMPLE}\ndone\n\n## Save\n# Ctrl + O\n\n# CLOSE\n# ctrl + x\n\nsbatch /users/${USER}/scripts/02_assembly.sh\n</code></pre>"},{"location":"day3/practical_genome_assmbly/#notes","title":"Notes","text":"<ul> <li>-1                file with forward paired-end reads</li> <li>-2                file with reverse paired-end reads</li> <li>--careful                   will reduce number of mismatches and short indels</li> <li>-o                          path to output file (directory shouldn't be existing)</li> </ul>"},{"location":"day3/practical_genome_assmbly/#quality-assessment","title":"Quality Assessment","text":"<p>We need assess the quality of the contigs so that we are confident that they are of good quality.</p>"},{"location":"day4/genome_assembly/","title":"Objectives","text":""},{"location":"day4/genome_assembly/#objectives","title":"Objectives","text":"<ol> <li>To perform De novo genome assembly and reference-based genome assembly</li> <li>Assess assembly quality using quality metrics</li> </ol>"},{"location":"day4/genome_assembly/#target-organisms","title":"Target Organisms","text":""},{"location":"day4/genome_assembly/#mycobacterium-tuberculosis","title":"Mycobacterium tuberculosis","text":"<ul> <li>Genome size: ~4.4 Mb</li> <li>Characteristics: High GC content ~65.6%, complex secondary structures</li> <li>Genes ~4,000 protein-coding genes</li> <li>Clinical relevance: Major global pathogen, drug resistance concerns</li> <li>Assembly challenges: Repetitive sequences, PE/PPE gene families</li> </ul>"},{"location":"day4/genome_assembly/#vibrio-cholerae","title":"Vibrio cholerae","text":"<ul> <li>Characteristics: Dual chromosome structure</li> <li>Genome size: ~4.0 Mb (two chromosomes: ~3.0 Mb + ~1.1 Mb)</li> <li>GC content: ~47.7%</li> <li>Genes: ~3,800 protein-coding genes</li> <li>Clinical relevance: Cholera pandemics, epidemic strain tracking</li> <li>Assembly challenges: Chromosome separation, mobile genetic elements</li> </ul>"},{"location":"day4/genome_assembly/#genome-assembly","title":"Genome assembly","text":"<p>Genome assembly involves reconstructing a genome from a set of fragmented DNA sequences (reads) obtained from sequencing technologies. </p> <p>It aims to piece together the reads to create a continuous sequence that represents the genome of an organism.</p>"},{"location":"day4/genome_assembly/#key-concepts","title":"Key Concepts","text":"<ul> <li>Reads: Fragmented sequences of DNA obtained from sequencing technologies.</li> <li>Contigs: Continuous sequences formed by overlapping reads.</li> <li>Scaffolds: Ordered and oriented sets of contigs, sometimes with gaps, which represent larger regions of the genome.</li> <li>Coverage: The average number of times each base in the genome is sequenced, which affects the accuracy of the assembly.</li> <li>Assembly can be:<ul> <li>De novo assembly - the construction of the genome from scratch without a reference.</li> <li>Reference-guided assembly - uses a known reference genome to guide the assembly of the new genome.</li> </ul> </li> </ul>"},{"location":"day4/genome_assembly/#steps-in-genome-assembly","title":"Steps in Genome Assembly","text":"<ol> <li>Preprocessing (quality control, adapter sequences and low-quality bases filtering with FASTQC, TRIMMOMATIC/FASTP/BBMap).</li> <li>Assembly</li> <li>Scaffolding (involves use of long-range information from paired-end or mate-pair reads to order and orient contigs into scaffolds (e.g., SSPACE).</li> <li>Gap Filling (use additional reads or assembly techniques to close gaps within scaffolds (e.g., GapCloser).</li> <li>Polishing (correcting sequencing errors and improving the accuracy of the assembly, e.g., Pilon).</li> <li>Evaluation (Assembly Metrics including N50 (length of the contig such that 50% of the total assembly length is in contigs of this length or longer); genome completeness; and accuracy, e.g., QUAST).</li> </ol>"},{"location":"day4/genome_assembly/#genome-assembly-algorithms","title":"Genome Assembly Algorithms","text":""},{"location":"day4/genome_assembly/#overlap-layout-consensus-olc","title":"Overlap Layout Consensus (OLC)","text":"<ul> <li>Suitable for long-read sequencing (e.g., PacBio, Oxford Nanopore)</li> <li>Determines overlap between reads</li> <li>Arranges reads based on ovelaps</li> <li>Resolves conflict to build the final sequences</li> <li>Handles long reads and complex regions well</li> <li>Computationally intensive</li> </ul>"},{"location":"day4/genome_assembly/#de-bruijn-graph-dbg","title":"De Bruijn Graph (DBG)","text":"<ul> <li>Best for short-read sequencing (e.g., illumina).</li> <li>Uses small overlapping sequences (k-mers) to build a graph, nodes: k-mers, and edges: overlaps.</li> <li>Fast and memory efficient for large datasets</li> <li>Struggles with repetitive regions</li> </ul>"},{"location":"day4/genome_assembly/#hybrid-methods","title":"Hybrid Methods","text":"<ul> <li>Combine DBG and OLC strengths to improve assembly quality, especially for error-prone long reads.\u200b</li> </ul>"},{"location":"day4/genome_assembly/#assembler-classification-by-read-type","title":"Assembler Classification by Read Type","text":""},{"location":"day4/genome_assembly/#short-read-assemblers","title":"Short-Read Assemblers\u200b","text":"<ul> <li>Short-read assemblers process high volumes of short sequences using DBG algorithms</li> <li>Ideal for technologies like Illumina.\u200b</li> <li>SPAdes (widely used for small genomes)</li> <li>Velvet (older though useful)</li> <li>SOAPdenovo (short-read assembler for larger genomes),</li> <li>ABySS (large genomes).</li> </ul>"},{"location":"day4/genome_assembly/#long-read-assemblers","title":"Long-Read Assemblers\u200b","text":"<ul> <li>Long-read assemblers handle fewer, longer sequences using OLC algorithms</li> <li>Can span entire genes and repetitive regions</li> <li>Optimized for platforms like PacBio and ONT.\u200b</li> <li>Higher error rates (can be corrected by short reads or polishing)</li> </ul>"},{"location":"day4/genome_assembly/#examples-of-long-read-assemblers-for-bacterial-genomes","title":"Examples of Long-Read Assemblers for Bacterial Genomes","text":"<ol> <li> <p>FLYE (Recommended for most bacterial genomes)</p> </li> <li> <p>Excellent for PacBio and Oxford Nanopore</p> </li> <li>Good repeat resolution</li> <li> <p>Usage: flye --nano-raw reads.fastq --out-dir output --genome-size 4.5m</p> </li> <li> <p>Canu (High accuracy, slower)</p> </li> <li> <p>Gold standard for accuracy</p> </li> <li>Requires significant computational resources</li> <li> <p>Usage: canu -p prefix -d output genomeSize=4.5m -nanopore reads.fastq</p> </li> <li> <p>Unicycler (Hybrid approach)</p> </li> <li> <p>Combines short and long reads</p> </li> <li>Excellent for complete genomes</li> <li> <p>Usage: unicycler -1 short_R1.fq -2 short_R2.fq -l long_reads.fq -o output</p> </li> <li> <p>Raven (Fast, lightweight)</p> </li> <li> <p>Quick assemblies for preliminary analysis</p> </li> <li>Good for large datasets</li> <li> <p>Usage: raven reads.fastq &gt; assembly.fasta</p> </li> <li> <p>NextDenovo (High accuracy for Nanopore)</p> </li> <li> <p>Specialized for Oxford Nanopore data</p> </li> <li>Good error correction</li> <li>Usage: nextDenovo config.txt</li> </ol> <p>Polishing Tools:</p> <ul> <li>Medaka (Nanopore): medaka_consensus -i reads.fastq -d assembly.fasta -o polished</li> <li>Pilon (with short reads): pilon --genome assembly.fasta --frags mapped_reads.bam</li> <li>Racon: racon reads.fastq mappings.paf assembly.fasta</li> </ul>"},{"location":"day4/genome_assembly/#hybrid-assemblers","title":"Hybrid Assemblers\u200b","text":"<ul> <li>Combine short and long reads, integrating DBG and OLC methods for improved accuracy and contiguity.\u200b</li> <li>Balances accuracy (short reads) and completeness (long reads, resolving repeats and structural variants)</li> <li>Requires careful data integration</li> <li>Unicycler, MaSuRCA</li> </ul>"},{"location":"day4/genome_assembly/#reference-guided-assembly","title":"Reference-guided Assembly","text":"<ul> <li>Works well given a closely related reference genome</li> <li>Aligns reads to a reference genome and fills gaps</li> <li>Faster and less computationally demanding</li> <li>May miss novel sequences or structural variations</li> </ul>"},{"location":"day4/genome_assembly/#note","title":"NOTE","text":"Assembly Strategy Subtypes Read Type Compatibility Examples Notes De novo assembly DBG, OLC, Hybrid Short, long, hybrid Velvet, SPAdes, Canu, Flye, MaSuRCA, UniCycler No reference genome used Reference-guided assembly Mapping-based Short, long BWA, Bowtie2, Novoalign, Minimap2 Aligns reads to a known reference genome"},{"location":"day4/genome_assembly/#assembler-selection-factors","title":"Assembler Selection Factors\u200b","text":"<p>Choosing an assembler depends on\" - Sequencing technology, - Project goals, and - Available computational resources.\u200b</p>"},{"location":"day4/genome_assembly/#best-practices-for-genome-assembly","title":"Best Practices for Genome Assembly","text":"<ul> <li>Start with high-quality, high-coverage sequencing data to improve the accuracy of the assembly.</li> <li>Try multiple assemblers and compare results, as different tools may perform better for different data types.</li> <li>Use iterative rounds of assembly, scaffolding, and polishing to gradually improve the assembly.</li> <li>Validate the final assembly using independent data, such as long-read sequencing or optical mapping.</li> <li>Keep detailed records of all parameters and steps used in the assembly process for reproducibility.</li> </ul>"},{"location":"day4/practical_assembly/","title":"De novo Genome Assembly Practical","text":""},{"location":"day4/practical_assembly/#de-novo-genome-assembly-practical","title":"De novo Genome Assembly Practical","text":"<p>Before setting up you need to know the current workig directory  </p><pre><code># Check current working directory\npwd\n# List the contents of the working diresctory\nls\n# Define output dir\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n# Create relevant output directories. -p so that it creates parent dir if it doesn't exist\nmkdir -p ${outspades}\"tb\"\nmkdir -p ${outspades}\"vc\"\nmkdir -p ${outquast}\"tb\"\nmkdir -p ${outquast}\"vc\"\n</code></pre><p></p> <p>Delete ${outspades} and ${outquast} and create them using one line of code and not four as above</p> <p></p><pre><code># Request for an interactive node (Resources)\n## Use this command to retrieve the previously used \"srun\" command\nhistory | grep \"srun\"\nsrun --cpus-per-tasks=16 --mem=128g --time 3:00:00 --job-name \"${USER}-assembly\" --pty /bin/bash\n\n# # Clear all modules\nmodule purge\n# Load the required modules\nmodule load spades/4.2.0\n## How can I get the help documentation?\nspades.py --help | less\n# Lets run spades with a test dataset\n/software/bio/spades/4.2.0/bin/spades.py  --test  --careful\n</code></pre> We  will use trimmed reads and file with IDs that we created during the initial data cleaning process. Check where the ID files are  and CHANGE DIR to where these are. Use \"cd /path/to/tb_IDs\"<p></p> <pre><code>## Subset the data\nhead -2 /data/users/${USER}/data_analysis/tb_IDs &gt; /data/users/${USER}/02_tb_IDs\nhead -2 /data/users/${USER}/data_analysis/vc_IDs &gt; /data/users/${USER}/02_vc_IDs\n# Define dirs\nindir=\"/data/users/${USER}/data_analysis/trimmed_trimmomatic/\"\noutdir=\"/data/users/${USER}/data_analysis/assembly-test/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outspades}tb ${outspades}vc ${outquast}tb ${outquast}vc\n\ncd /data/users/${USER}/\n# Run SPAdes for M. tuberculosis\necho \"Starting M. tuberculosis assembly at $(date)\"\n\nfor SAMPLE in $(cat 02_tb_IDs)\ndo\n  echo \"[TB|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}tb/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}tb/${SAMPLE}_2.fastq.gz \\\n    --careful \\\n    --cov-cutoff auto \\\n    -t 16 \\\n    -o ${outspades}tb/${SAMPLE}\ndone\n</code></pre>"},{"location":"day4/practical_assembly/#notes","title":"Notes","text":"<ul> <li>-1                file with forward paired-end reads</li> <li>-2                file with reverse paired-end reads</li> <li>--careful                   error and mismatch correction</li> <li>-o                          path to output file (directory shouldn't be existing)</li> <li>--cov-cutoff auto           Automatic coverage cutoff (usiful mostly for high GC genomes)</li> </ul> <pre><code># Perform denovo assembly for all TB ands VC samples\nmkdir -p /users/${USER}/scripts /users/${USER}/logs\n\n## Now let's write our submission script\nnano /users/${USER}/scripts/assembly_01.sh\n\n#!/bin/bash\n#SBATCH --job-name='assembly-${USER}'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=128GB\n#SBATCH --output=/users/${USER}/logs/assembly_01-stdout.txt\n#SBATCH --error=/users/${USER}/logs/assembly_01-stdout.txt\n#SBATCH --time=12:00:00\n\n# Define dir\nindir=\"/data/users/${USER}/data_analysis/trimmed_trimmomatic/\"\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\n# Change into the DIR with \"tb_IDs\"\ncd /data/users/${USER}/data_analysis/\n# Run SPAdes for M. tuberculosis\necho \"Starting M. tuberculosis assembly at $(date)\"\nfor SAMPLE in $(cat tb_IDs); do\n  echo \"[TB|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}tb/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}tb/${SAMPLE}_2.fastq.gz \\\n    --careful --cov-cutoff auto -t 16 \\\n    -o ${outspades}tb/${SAMPLE}\ndone\necho \"M. tuberculosis assembly completed at $(date)\"\n\nfor SAMPLE in $(cat vc_IDs); do\n  echo \"[VC|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}vc/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}vc/${SAMPLE}_2.fastq.gz \\\n    --careful -t 16 \\\n    -o ${outspades}vc/${SAMPLE}\ndone\n## SAVE THIS IN NANO BY\n# Ctrl + O\n\n# CLOSE THE FILE\n# ctrl + x\n</code></pre> Before submission of your script, verify if all the input directory and files exist. <pre><code>## SUBMIT THE ASSEMBLY SCRIPT\nsbatch /users/${USER}/scripts/assembly_01.sh\n</code></pre>"},{"location":"day4/practical_assembly/#genome-assembly-quality","title":"Genome Assembly Quality","text":"<p>Assess the quality of your assemblies to be confident of your downstream analysis. </p>"},{"location":"day4/practical_assembly/#common-assembly-quality-metrics","title":"Common Assembly Quality Metrics","text":"<ul> <li>Expected genome size</li> <li>Expected GC content %</li> <li>Number of contigs</li> <li>N50 and L50</li> </ul>"},{"location":"day4/practical_assembly/#common-assembly-issues-and-solutions","title":"Common Assembly Issues and Solutions","text":"<p>===================================</p> \ud83d\udea8 Issue \ud83d\udd0d Possible Cause \ud83d\udee0\ufe0f Solution High number of contigs (&gt;100) \u2022 Low coverage (&lt;30x)  \u2022 Poor quality reads  \u2022 Highly repetitive genome  \u2022 Contamination \u2022 Increase sequencing depth  \u2022 Better read trimming/filtering  \u2022 Use hybrid assembly with long reads  \u2022 Check contamination with Kraken2 Low N50 (&lt;50kb for bacteria) \u2022 Repetitive sequences  \u2022 Low coverage  \u2022 Assembly parameter issues \u2022 Adjust k-mer sizes in SPAdes  \u2022 Use <code>--careful</code> flag  \u2022 Try different assemblers (Unicycler, SKESA) Assembly size much larger than expected \u2022 Contamination  \u2022 Duplication in assembly  \u2022 Presence of plasmids \u2022 Perform contamination screening  \u2022 Check duplication ratio in QUAST  \u2022 Separate plasmid sequences High number of hypothetical proteins (&gt;50%) \u2022 Novel organism/strain  \u2022 Poor annotation database match  \u2022 Assembly quality issues \u2022 Use specialized databases (RefSeq, UniProt)  \u2022 Manual curation of key genes  \u2022 Functional annotation with KEGG/COG"},{"location":"day4/practical_assembly/#running-quast-tool","title":"Running QUAST Tool","text":"<pre><code># Run QUAST for a subset of M. tuberculosis dataset\necho \"Running QUAST analysis for M. tuberculosis...\"\n\nindir=\"/data/users/${USER}/data_analysis/trimmed_trimmomatic/\"\noutdir=\"/data/users/${USER}/data_analysis/assembly-test/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outdir}\"02_quast/\"\n# Remove modules in your env\nmodules purge\n# Load module\nmodule load quast\n# Use the subset of our data\ncd /data/users/${USER}/\nfor SAMPLE in $(cat 02_tb_IDs)\ndo\n  echo \"[TB|SPADES] $SAMPLE\"\n  quast.py ${outspades}tb/${SAMPLE}/contigs.fasta \\\n    #-r /data/TB_H37Rv.fasta -g /data/TB_H37rv.gff \\\n    -o ${outquast}tb/${SAMPLE} \\\n    --threads 4 --min-contig 200 \\\n    --labels \"MTB_Assembly\"\ndone\n</code></pre>"},{"location":"day4/practical_assembly/#quast-submission-script-for-all-samples","title":"Quast Submission script for all samples","text":"<pre><code># Assess all assembly results for all TB ands VC samples\n# In case you didn't create these above\nmkdir -p /users/${USER}/scripts /users/${USER}/logs\n\n## Now let's write our submission script\nnano /users/${USER}/scripts/assembly_02.sh\n\n#!/bin/bash\n#SBATCH --job-name='quast-${USER}'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=128GB\n#SBATCH --output=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --error=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --time=12:00:00\n\n# Define dir\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outdir}\"01_spades\" ${outdir}\"02_quast\"\n# Run QUAST for a subset of M. tuberculosis dataset\necho \"Running QUAST analysis for M. tuberculosis...\"\n# Remove modules in your env\nmodules purge\n# Load module\nmodule load quast\n# Use the sample IDs for our data\ncd /data/users/${USER}/data_analysis\nfor SAMPLE in $(cat tb_IDs)\ndo\n  echo \"[TB|QUAST] $SAMPLE\"\n  quast.py ${outspades}tb/${SAMPLE}/contigs.fasta \\\n    #-r /data/TB_H37Rv.fasta -g /data/TB_H37rv.gff \\\n    -o ${outquast}tb/${SAMPLE} \\\n    --threads 4 --min-contig 200 \\\n    --labels \"MTB_Assembly\"\ndone\n\nfor SAMPLE in $(cat vc_IDs); do\n  echo \"[VC|QUAST] $SAMPLE\"\n  quast.py ${outspades}vc/${SAMPLE}/contigs.fasta \\\n    #-r /data/TB_H37Rv.fasta -g /data/TB_H37rv.gff \\\n    -o ${outquast}tb/${SAMPLE} \\\n    --threads 4 --min-contig 200 \\\n    --labels \"MTB_Assembly\"\ndone\n## SAVE THIS IN NANO BY\n# Ctrl + O\n\n# CLOSE THE FILE\n# ctrl + x\n</code></pre> <pre><code># Assess all assembly results for all TB ands VC samples\n# In case you didn't create these above\nmkdir -p /users/${USER}/scripts /users/${USER}/logs\n\n## Now let's write our submission script\nnano /users/${USER}/scripts/assembly_02.sh\n\n#!/bin/bash\n#SBATCH --job-name='quast-${USER}'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=128GB\n#SBATCH --output=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --error=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --time=12:00:00\n\n# Define dir\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outdir}\"02_quast/tb\" ${outdir}\"02_quast/vc\"\n# Run Prokka for M. tuberculosis\necho \"Starting M. tuberculosis annotation at $(date)\"\nfor SAMPLE in $(cat tb_IDs); do\n  echo \"[TB|SPADES] $SAMPLE\"\n  prokka ${outspades}tb/${SAMPLE}/contigs.fasta \\\n       --outdir ${outquast}/${SAMPLE} \\\n       --cpus 8 --genus Mycobacterium\ndone\necho \"M. tuberculosis annotation completed at $(date)\"\n\nfor SAMPLE in $(cat vc_IDs); do\n  echo \"[VC|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}vc/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}vc/${SAMPLE}_2.fastq.gz \\\n    --careful -t 8 \\\n    -o ${outspades}vc/${SAMPLE}\ndone\n## SAVE THIS IN NANO BY\n# Ctrl + O\n\n# CLOSE THE FILE\n# ctrl + x\n</code></pre> <pre><code>## Clean-up\nrm -rf /data/users/${USER}/data_analysis/assembly-test/\n</code></pre>"},{"location":"day5/amr_prediction/","title":"Amr prediction","text":""},{"location":"day5/amr_prediction/#antimicrobial-resistance","title":"Antimicrobial Resistance","text":""},{"location":"modules/day1/","title":"Day 1 - Welcome to the Course!","text":""},{"location":"modules/day1/#day-1-welcome-to-the-course","title":"Day 1: Welcome to the Course!","text":"<p>Date: September 1, 2025 Duration: 09:00-13:00 CAT Focus: Course introduction, genomic surveillance overview, sequencing technologies</p>"},{"location":"modules/day1/#overview","title":"Overview","text":"<p>Welcome to the Microbial Genomics &amp; Metagenomics Training Course! Day 1 introduces the course, provides essential background on genomic surveillance, covers sequencing technologies, and introduces key databases and tools used throughout the training.</p>"},{"location":"modules/day1/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 1, you will be able to:</p> <ul> <li>Understand the role of genomic surveillance in public health</li> <li>Recognize different sequencing technologies and their applications</li> <li>Navigate and use PubMLST database resources</li> <li>Perform basic command line operations</li> <li>Set up and configure analysis environments</li> </ul>"},{"location":"modules/day1/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Introductions All 09:10 Overview of clinical pathogens and genomic surveillance Slides Ephifania Geza 09:40 Overview of sequencing technologies and data types Sindiswa Lukhele 10:00 Setting up and exploring PubMLST Sindiswa Lukhele 11:00 Break 11:30 Introduction to command line interface Practical Arash Iranzadeh"},{"location":"modules/day1/#key-topics","title":"Key Topics","text":""},{"location":"modules/day1/#1-course-introduction-and-participant-introductions","title":"1. Course Introduction and Participant Introductions","text":"<ul> <li>Welcome and course overview</li> <li>Trainer and participant introductions</li> <li>Course objectives and structure</li> <li>Training schedule and logistics</li> </ul>"},{"location":"modules/day1/#2-clinical-pathogens-and-genomic-surveillance","title":"2. Clinical Pathogens and Genomic Surveillance","text":"<ul> <li>Role of genomics in infectious disease surveillance</li> <li>Applications in outbreak investigation</li> <li>Antimicrobial resistance monitoring</li> <li>Integration with epidemiological data</li> </ul>"},{"location":"modules/day1/#3-sequencing-technologies-and-data-types","title":"3. Sequencing Technologies and Data Types","text":"<ul> <li>Next-generation sequencing platforms</li> <li>Illumina, Oxford Nanopore, PacBio comparison</li> <li>Short-read vs long-read technologies</li> <li>Data quality considerations and file formats</li> </ul>"},{"location":"modules/day1/#4-pubmlst-database-system","title":"4. PubMLST Database System","text":"<ul> <li>Multi-locus sequence typing (MLST) concepts</li> <li>Database navigation and search functions</li> <li>Species-specific typing schemes</li> <li>Data submission and retrieval</li> </ul>"},{"location":"modules/day1/#5-command-line-interface-basics","title":"5. Command Line Interface Basics","text":"<ul> <li>Introduction to Unix/Linux command line</li> <li>Git Bash setup for Windows users</li> <li>Basic file operations and navigation</li> <li>Introduction to R statistical environment</li> </ul>"},{"location":"modules/day1/#tools-and-resources","title":"Tools and Resources","text":""},{"location":"modules/day1/#databases-explored","title":"Databases Explored","text":"<ul> <li>PubMLST - Public databases for molecular typing</li> <li>Pathogen databases - Species-specific resources</li> <li>MLST schemes - Standardized typing protocols</li> </ul>"},{"location":"modules/day1/#software-introduced","title":"Software Introduced","text":"<ul> <li>Git Bash - Command line interface for Windows</li> <li>R/RStudio - Statistical computing environment</li> <li>Web browsers - For database navigation</li> <li>Terminal applications - Command line access</li> </ul>"},{"location":"modules/day1/#hands-on-activities","title":"Hands-on Activities","text":""},{"location":"modules/day1/#exercise-1-pubmlst-exploration-30-minutes","title":"Exercise 1: PubMLST Exploration (30 minutes)","text":"<p>Navigate the PubMLST website and explore available databases for different pathogens.</p>"},{"location":"modules/day1/#exercise-2-basic-command-line-operations-45-minutes","title":"Exercise 2: Basic Command Line Operations (45 minutes)","text":"<p>Practice essential Unix commands and file system navigation.</p>"},{"location":"modules/day1/#exercise-3-r-environment-setup-30-minutes","title":"Exercise 3: R Environment Setup (30 minutes)","text":"<p>Install and configure R/RStudio for data analysis.</p>"},{"location":"modules/day1/#exercise-4-database-search-practice-15-minutes","title":"Exercise 4: Database Search Practice (15 minutes)","text":"<p>Search for MLST data for specific bacterial isolates.</p>"},{"location":"modules/day1/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day1/#genomic-surveillance-applications","title":"Genomic Surveillance Applications","text":"<ul> <li>Outbreak investigation: Tracking transmission patterns</li> <li>Antimicrobial resistance: Monitoring resistance emergence</li> <li>Epidemiological studies: Population structure analysis</li> <li>Public health response: Informing control measures</li> </ul>"},{"location":"modules/day1/#sequencing-technology-comparison","title":"Sequencing Technology Comparison","text":"Platform Read Length Accuracy Throughput Cost Best For Illumina 150-300 bp &gt;99% High Low Routine surveillance Oxford Nanopore 1-100 kb ~95% Medium Medium Structural variants PacBio 10-25 kb &gt;99% Medium High Complete genomes"},{"location":"modules/day1/#mlst-fundamentals","title":"MLST Fundamentals","text":"<ul> <li>Housekeeping genes: Conserved sequences for typing</li> <li>Allelic profiles: Unique combinations define sequence types</li> <li>Population structure: Understanding strain relationships</li> <li>Standardization: Reproducible typing across laboratories</li> </ul>"},{"location":"modules/day1/#resources","title":"Resources","text":""},{"location":"modules/day1/#essential-websites","title":"Essential Websites","text":"<ul> <li>PubMLST - Public databases for molecular typing</li> <li>Pathogen Watch - Genomic surveillance platform</li> <li>NCBI SRA - Sequence Read Archive</li> </ul>"},{"location":"modules/day1/#documentation","title":"Documentation","text":"<ul> <li>Git Bash User Guide</li> <li>R Project Documentation</li> <li>PubMLST User Guide</li> </ul>"},{"location":"modules/day1/#training-materials","title":"Training Materials","text":"<ul> <li>Command line cheat sheets</li> <li>MLST database tutorials</li> <li>Sequencing technology overviews</li> </ul>"},{"location":"modules/day1/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day1/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Navigate PubMLST interface successfully</li> <li>Execute basic command line operations</li> <li>Identify appropriate sequencing platforms for different applications</li> <li>Understand MLST typing principles</li> </ul>"},{"location":"modules/day1/#group-discussion","title":"Group Discussion","text":"<ul> <li>Share experiences with different pathogens</li> <li>Discuss genomic surveillance challenges in different settings</li> <li>Compare sequencing technology applications</li> <li>Explore database search strategies</li> </ul>"},{"location":"modules/day1/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day1/#command-line-anxiety","title":"Command Line Anxiety","text":"<p>Many participants are new to command line interfaces. We provide: - Patient, step-by-step instruction - Plenty of practice time - Peer support and collaboration - Reference materials for later use</p>"},{"location":"modules/day1/#technical-setup-issues","title":"Technical Setup Issues","text":"<pre><code># Common Git Bash issues on Windows\n# Check if Git Bash is properly installed\ngit --version\n\n# Verify R installation\nR --version\n</code></pre>"},{"location":"modules/day1/#database-navigation","title":"Database Navigation","text":"<ul> <li>Start with simple searches</li> <li>Use guided examples</li> <li>Practice with known organisms</li> <li>Build confidence gradually</li> </ul>"},{"location":"modules/day1/#looking-ahead","title":"Looking Ahead","text":"<p>Day 2 Preview: Introduction to Command Line, HPC, &amp; Quality Control including: - High Performance Computing (HPC) introduction - Advanced command line operations - Quality checking and control methods - Species identification techniques - Guest talk on M. tuberculosis and co-infection</p> <p>Key Learning Outcome: Day 1 establishes the foundational knowledge of genomic surveillance principles, sequencing technologies, and essential database resources that underpin all subsequent training activities.</p>"},{"location":"modules/day10/","title":"Day 10 - Wrap-up session","text":""},{"location":"modules/day10/#day-10-wrap-up-session","title":"Day 10: Wrap-up session","text":"<p>Date: September 12, 2025 Duration: 09:00-11:40 CAT Focus: Course conclusion, presentations, and future directions</p>"},{"location":"modules/day10/#overview","title":"Overview","text":"<p>The final day of the microbial genomics training course brings together all learning experiences through participant presentations, showcases ongoing research initiatives, and provides guidance for continued professional development in computational biology.</p>"},{"location":"modules/day10/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 10, you will be able to:</p> <ul> <li>Present bioinformatics analysis results effectively to scientific audiences</li> <li>Demonstrate mastery of key concepts covered throughout the course</li> <li>Identify resources for continued learning and professional development</li> <li>Connect with ongoing research initiatives and training opportunities</li> <li>Plan next steps for implementing learned skills in your research</li> <li>Build professional networks within the computational biology community</li> </ul>"},{"location":"modules/day10/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Participant presentations 11:15 Short talks NGS-Academy/AfriGen-D/eLwazi ODSP 11:40 End of the course [Resources]"},{"location":"modules/day10/#presentation-session-0900-1115","title":"Presentation Session (09:00-11:15)","text":""},{"location":"modules/day10/#presentation-guidelines","title":"Presentation Guidelines","text":"<p>Each participant will deliver a 5-minute presentation covering their Day 9 analysis work:</p>"},{"location":"modules/day10/#presentation-structure","title":"Presentation Structure","text":"<ol> <li>Introduction (1 minute)</li> <li>Research question or objective</li> <li>Brief background context</li> <li> <p>Dataset description</p> </li> <li> <p>Methods (1.5 minutes)</p> </li> <li>Analysis workflow overview</li> <li>Key tools and techniques used</li> <li> <p>Parameter choices and rationale</p> </li> <li> <p>Results (2 minutes)</p> </li> <li>Major findings from analysis</li> <li>Key figures or summary statistics</li> <li> <p>Interpretation of results</p> </li> <li> <p>Challenges &amp; Solutions (30 seconds)</p> </li> <li>Main obstacles encountered</li> <li>How they were addressed</li> <li>Lessons learned</li> </ol>"},{"location":"modules/day10/#technical-requirements","title":"Technical Requirements","text":"<ul> <li>Format: PDF slides or live demonstration</li> <li>Time limit: Strictly enforced 5 minutes</li> <li>Q&amp;A: 2-3 minutes for questions after each presentation</li> <li>Backup: Have presentation files ready on USB drive</li> </ul>"},{"location":"modules/day10/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>Presentations will be assessed on:</p> <ul> <li>Scientific rigor: Appropriate methods and interpretation</li> <li>Technical competence: Correct use of bioinformatics tools</li> <li>Communication clarity: Clear explanation of complex concepts</li> <li>Problem-solving: Evidence of troubleshooting and adaptation</li> <li>Course integration: Application of multiple course concepts</li> </ul>"},{"location":"modules/day10/#sample-presentation-topics","title":"Sample Presentation Topics","text":"<p>Based on participant data types, presentations may cover:</p>"},{"location":"modules/day10/#genomic-analysis-examples","title":"Genomic Analysis Examples","text":"<ul> <li>\"Antimicrobial resistance profiling in Mycobacterium tuberculosis isolates\"</li> <li>\"Phylogenetic analysis of Mycobacterium tuberculosis outbreak strains\"</li> <li>\"Comparative genomics of Vibrio cholerae from environmental samples\"</li> <li>\"Transmission analysis of Vibrio cholerae epidemic strains\"</li> </ul>"},{"location":"modules/day10/#methodological-examples","title":"Methodological Examples","text":"<ul> <li>\"Optimizing assembly parameters for low-coverage genomes\"</li> <li>\"Developing Nextflow pipeline for routine surveillance\"</li> <li>\"Quality control strategies for degraded DNA samples\"</li> </ul>"},{"location":"modules/day10/#presentation-order","title":"Presentation Order","text":"<p>Presentations will be grouped by analysis type to facilitate discussion:</p> <ol> <li>Genomic Surveillance (9:00-9:45)</li> <li>Methodological &amp; Pipeline Development (9:45-11:15)</li> </ol>"},{"location":"modules/day10/#special-presentations-1115-1140","title":"Special Presentations (11:15-11:40)","text":""},{"location":"modules/day10/#ngs-academy-initiative","title":"NGS-Academy Initiative","text":"<p>Overview of next-generation sequencing training programs - Advanced training opportunities - Certification pathways - Research collaboration opportunities - International exchange programs</p>"},{"location":"modules/day10/#afrigen-d-project","title":"AfriGen-D Project","text":"<p>African Genome Diversity Project updates - Current research initiatives - Collaboration opportunities for participants - Data sharing and analysis platforms - Future funding opportunities</p>"},{"location":"modules/day10/#elwazi-odsp-open-data-science-platform","title":"eLwazi ODSP (Open Data Science Platform)","text":"<p>Data science infrastructure and resources - Platform capabilities and access - Training modules and resources - Research project support - Community building initiatives</p>"},{"location":"modules/day10/#course-completion","title":"Course Completion","text":""},{"location":"modules/day10/#certificate-requirements","title":"Certificate Requirements","text":"<p>To receive course completion certificate, participants must:</p> <ul> <li> Attend at least 8 out of 10 training days</li> <li> Complete all hands-on exercises</li> <li> Submit Day 9 analysis documentation</li> <li> Deliver Day 10 presentation</li> <li> Participate in course evaluation</li> </ul>"},{"location":"modules/day10/#skills-assessment-summary","title":"Skills Assessment Summary","text":"<p>By course completion, participants will have demonstrated:</p>"},{"location":"modules/day10/#technical-skills","title":"Technical Skills","text":"<ul> <li>Command line proficiency: Navigation, file management, and tool execution</li> <li>Quality control: Assessment and improvement of sequencing data</li> <li>Genome assembly: De novo assembly and quality assessment</li> <li>Annotation: Functional and structural genome annotation</li> <li>Phylogenetics: Tree construction and interpretation</li> <li>Workflow development: Nextflow pipeline creation and optimization</li> </ul>"},{"location":"modules/day10/#analytical-skills","title":"Analytical Skills","text":"<ul> <li>Data interpretation: Drawing biological conclusions from computational results</li> <li>Method selection: Choosing appropriate tools for specific analyses</li> <li>Parameter optimization: Adjusting analysis parameters for data characteristics</li> <li>Quality assessment: Evaluating reliability of computational results</li> <li>Troubleshooting: Diagnosing and solving technical problems</li> </ul>"},{"location":"modules/day10/#professional-skills","title":"Professional Skills","text":"<ul> <li>Documentation: Maintaining analysis logs and reproducible workflows</li> <li>Presentation: Communicating results to scientific audiences</li> <li>Collaboration: Working effectively in computational research teams</li> <li>Continuous learning: Accessing resources for ongoing skill development</li> </ul>"},{"location":"modules/day10/#post-course-resources","title":"Post-Course Resources","text":""},{"location":"modules/day10/#immediate-support-next-3-months","title":"Immediate Support (Next 3 months)","text":"<ul> <li>Email support: Continued access to trainer expertise</li> <li>Online forum: Participant discussion platform</li> <li>Monthly virtual meetups: Progress sharing and troubleshooting</li> <li>Resource sharing: Access to course materials and datasets</li> </ul>"},{"location":"modules/day10/#long-term-development","title":"Long-term Development","text":"<ul> <li>Advanced training: Information about specialized workshops</li> <li>Research collaboration: Connections to ongoing projects</li> <li>Professional networks: Links to regional and international communities</li> <li>Career opportunities: Job postings and fellowship announcements</li> </ul>"},{"location":"modules/day10/#recommended-next-steps","title":"Recommended Next Steps","text":""},{"location":"modules/day10/#for-beginners","title":"For Beginners","text":"<ol> <li>Practice with additional datasets</li> <li>Complete online tutorials for specific tools</li> <li>Join local bioinformatics user groups</li> <li>Consider formal coursework in computational biology</li> </ol>"},{"location":"modules/day10/#for-intermediate-users","title":"For Intermediate Users","text":"<ol> <li>Develop specialized analysis pipelines</li> <li>Contribute to open-source bioinformatics projects</li> <li>Attend specialized conferences and workshops</li> <li>Mentor others in computational skills</li> </ol>"},{"location":"modules/day10/#for-advanced-users","title":"for Advanced Users","text":"<ol> <li>Lead research projects using learned techniques</li> <li>Develop novel analytical methods</li> <li>Teach and train others in the community</li> <li>Collaborate on large-scale genomics initiatives</li> </ol>"},{"location":"modules/day10/#course-evaluation","title":"Course Evaluation","text":""},{"location":"modules/day10/#feedback-categories","title":"Feedback Categories","text":""},{"location":"modules/day10/#content-assessment","title":"Content Assessment","text":"<ul> <li>Relevance to research needs</li> <li>Appropriate level of technical detail</li> <li>Balance of theory and practical application</li> <li>Currency of methods and tools</li> </ul>"},{"location":"modules/day10/#delivery-evaluation","title":"Delivery Evaluation","text":"<ul> <li>Trainer expertise and communication</li> <li>Hands-on exercise quality</li> <li>Technical support adequacy</li> <li>Course pacing and organization</li> </ul>"},{"location":"modules/day10/#impact-measurement","title":"Impact Measurement","text":"<ul> <li>Confidence in using bioinformatics tools</li> <li>Likelihood of applying learned skills</li> <li>Interest in advanced training</li> <li>Recommendations to colleagues</li> </ul>"},{"location":"modules/day10/#improvement-suggestions","title":"Improvement Suggestions","text":"<p>Participants are encouraged to provide specific suggestions for: - Additional topics to cover - Alternative teaching methods - Better integration of concepts - Enhanced practical exercises - Improved course materials</p>"},{"location":"modules/day10/#networking-and-community-building","title":"Networking and Community Building","text":""},{"location":"modules/day10/#contact-information-exchange","title":"Contact Information Exchange","text":"<ul> <li>WhatsApp group for ongoing communication</li> <li>LinkedIn professional network connections</li> <li>GitHub collaboration on analysis projects</li> <li>Research ResearchGate connections</li> </ul>"},{"location":"modules/day10/#regional-initiatives","title":"Regional Initiatives","text":"<ul> <li>South African Bioinformatics Society: Local meetings and conferences</li> <li>H3ABioNet: Pan-African bioinformatics network</li> <li>ISCB Regional Student Groups: International student connections</li> <li>Local university partnerships: Ongoing collaboration opportunities</li> </ul>"},{"location":"modules/day10/#final-remarks","title":"Final Remarks","text":""},{"location":"modules/day10/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Bioinformatics is a journey: Continuous learning and adaptation required</li> <li>Community matters: Collaboration accelerates learning and research</li> <li>Documentation is crucial: Reproducible research starts with good record-keeping</li> <li>Practice makes perfect: Regular use of skills prevents deterioration</li> <li>Stay current: Field evolves rapidly, ongoing education essential</li> </ol>"},{"location":"modules/day10/#words-of-encouragement","title":"Words of Encouragement","text":"<p>The computational biology field welcomes contributors from diverse backgrounds. Your unique perspective, combined with the skills learned in this course, positions you to make meaningful contributions to understanding microbial genomics and its applications to human health.</p>"},{"location":"modules/day10/#course-legacy","title":"Course Legacy","text":"<p>This training represents an investment in the future of computational biology in Africa. As you apply these skills in your research and career, you become part of a growing network of scientists advancing genomic medicine and public health through computational approaches.</p>"},{"location":"modules/day10/#acknowledgments","title":"Acknowledgments","text":""},{"location":"modules/day10/#training-team-recognition","title":"Training Team Recognition","text":"<p>Special thanks to the course instructors: - Ephifania Geza: Lead instructor and course coordinator - Arash Iranzadeh: Technical instruction and phylogenomics expertise - Sindiswa Lukhele: Sequencing technologies and quality control - Mamana Mbiyavanga: HPC systems and workflow development - Bethlehem Adnew: Guest expertise on tuberculosis genomics</p>"},{"location":"modules/day10/#institutional-support","title":"Institutional Support","text":"<ul> <li>University of Cape Town Computational Biology Division</li> <li>CIDRI-Africa research infrastructure</li> <li>High-performance computing facility access</li> <li>Guest lecture coordination and logistics</li> </ul>"},{"location":"modules/day10/#community-contributions","title":"Community Contributions","text":"<ul> <li>Dataset providers and research collaborators</li> <li>Open-source software developers</li> <li>International training program partnerships</li> <li>Participant engagement and peer learning</li> </ul>"},{"location":"modules/day10/#contact-information","title":"Contact Information","text":""},{"location":"modules/day10/#immediate-questions","title":"Immediate Questions","text":"<p>Course Coordinator: Ephifania Geza Email: ephifania.geza@uct.ac.za</p>"},{"location":"modules/day10/#technical-support","title":"Technical Support","text":"<p>HPC and Workflows: Mamana Mbiyavanga Email: mamana.mbiyavanga@uct.ac.za</p>"},{"location":"modules/day10/#general-inquiries","title":"General Inquiries","text":"<p>Training Program: microbial-genomics-training@uct.ac.za</p>"},{"location":"modules/day10/#follow-up-resources","title":"Follow-up Resources","text":"<ul> <li>Course Materials: GitHub repository access maintained</li> <li>Discussion Forum: Access links provided via email</li> <li>Newsletter: Quarterly updates on opportunities and resources</li> </ul> <p>Final Learning Outcome: Completion of this intensive training program provides participants with both the technical skills and professional network needed to pursue independent research in microbial genomics, contributing to advances in infectious disease understanding, antimicrobial resistance surveillance, and public health genomics.</p>"},{"location":"modules/day2/","title":"Day 2 - Introduction to Commandline, High Performance Computing, & Quality Control","text":""},{"location":"modules/day2/#day-2-introduction-to-commandline","title":"Day 2: Introduction to Commandline","text":"<p>Date: September 2, 2025 Duration: 09:00-13:00 CAT Focus: Command line proficiency, M. tuberculosis genomics</p>"},{"location":"modules/day2/#overview","title":"Overview","text":"<p>Day 2 focuses on building strong command line skills essential for bioinformatics work. This day provides the computational foundation needed for all subsequent genomic analyses in the course.</p>"},{"location":"modules/day2/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 2, you will be able to:</p> <ul> <li>Master essential Unix/Linux command line operations for bioinformatics workflows</li> <li>Understand M. tuberculosis genomics and co-infection patterns</li> </ul>"},{"location":"modules/day2/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Introduction to command line interface Practical Arash Iranzadeh 11:30 Break 12:00 Guest talk: MtB and co-infection Speaker Bio Bethlehem Adnew"},{"location":"modules/day2/#key-topics","title":"Key Topics","text":""},{"location":"modules/day2/#1-command-line-interface-fundamentals","title":"1. Command Line Interface Fundamentals","text":"<ul> <li>Unix/Linux file system navigation</li> <li>Essential commands for bioinformatics (grep, awk, sed)</li> <li>File manipulation and text processing</li> <li>Pipes and command chaining</li> <li>Working with compressed files (gzip, tar)</li> <li>Shell scripting basics for automation</li> </ul>"},{"location":"modules/day2/#2-m-tuberculosis-and-co-infection","title":"2. M. tuberculosis and Co-infection","text":"<ul> <li>TB genomics and strain typing</li> <li>Co-infection patterns and detection</li> <li>Clinical implications</li> <li>Molecular epidemiology approaches</li> <li>Drug resistance mechanisms</li> <li>Public health applications</li> </ul>"},{"location":"modules/day2/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day2/#command-line-tools","title":"Command Line Tools","text":"<ul> <li>Bash shell - Command line interface and scripting</li> <li>GNU coreutils - Essential Unix utilities (ls, cd, grep, etc.)</li> <li>Text processing - awk, sed, cut, sort, uniq</li> <li>File compression - gzip, tar, zip</li> <li>tmux/screen - Terminal session management</li> <li>rsync/scp - File transfer and synchronization</li> </ul>"},{"location":"modules/day2/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day2/#exercise-1-command-line-fundamentals-90-minutes","title":"Exercise 1: Command Line Fundamentals (90 minutes)","text":"<p>Master essential Unix commands for bioinformatics through practical exercises.</p> <pre><code># Navigate file systems and manipulate files\ncd ~/data\nls -la\nmkdir analysis_output\n\n# Process text files with Unix tools\ngrep \"^&gt;\" sequences.fasta | wc -l  # Count sequences\ncat sample.fastq | head -20         # View file contents\n\n# Work with compressed files\ngzip large_file.txt\ngunzip -c compressed.gz | head\n\n# Use pipes and redirection\ncat data.txt | sort | uniq &gt; unique_values.txt\n</code></pre>"},{"location":"modules/day2/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day2/#command-line-essentials","title":"Command Line Essentials","text":"<ul> <li>File system navigation: Understanding directory structure and paths</li> <li>Text processing: Using grep, sed, awk for data manipulation</li> <li>Pipes and redirection: Chaining commands for complex operations</li> <li>Shell scripting: Automating repetitive tasks</li> <li>Regular expressions: Pattern matching in bioinformatics data</li> </ul>"},{"location":"modules/day2/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day2/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Successfully connect to Ilifu HPC system</li> <li>Navigate Unix file system and manipulate files</li> <li>Complete command line exercises for pathogen genomics data</li> </ul>"},{"location":"modules/day2/#group-discussion","title":"Group Discussion","text":"<ul> <li>Share command line tips and tricks</li> <li>Discuss HPC resource management strategies</li> <li>Troubleshoot connection and job submission issues</li> <li>Compare different approaches to batch processing</li> </ul>"},{"location":"modules/day2/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day2/#command-line-challenges","title":"Command Line Challenges","text":"<pre><code># Permission denied errors\nchmod +x script.sh    # Make script executable\nls -la                # Check file permissions\n\n# Path issues\necho $PATH            # Check current PATH\nexport PATH=$PATH:/new/path  # Add to PATH\n</code></pre>"},{"location":"modules/day2/#command-line-resources","title":"Command Line Resources","text":"<ul> <li>Unix for Bioinformatics</li> <li>Bash Scripting Guide</li> <li>Command Line for Genomics</li> </ul>"},{"location":"modules/day2/#m-tuberculosis-resources","title":"M. tuberculosis Resources","text":"<ul> <li>TB-Profiler</li> <li>ReSeqTB</li> <li>TBDB</li> </ul>"},{"location":"modules/day2/#guest-lecture-mtb-and-co-infection","title":"Guest Lecture: MtB and Co-infection","text":""},{"location":"modules/day2/#speaker-bethlehem-adnew","title":"Speaker: Bethlehem Adnew","text":""},{"location":"modules/day2/#key-topics-covered","title":"Key Topics Covered","text":"<ul> <li>M. tuberculosis genomics: Strain diversity and typing methods</li> <li>Co-infection dynamics: TB-HIV and other respiratory pathogens</li> <li>Diagnostic challenges: Molecular detection in complex samples</li> <li>Treatment implications: Drug resistance in co-infected patients</li> <li>Epidemiological insights: Transmission patterns and control strategies</li> </ul>"},{"location":"modules/day2/#interactive-discussion-points","title":"Interactive Discussion Points","text":"<ul> <li>Current challenges in TB diagnosis</li> <li>Role of genomics in outbreak investigation</li> <li>Future directions in TB research</li> <li>Integration of genomic and clinical data</li> </ul>"},{"location":"modules/day2/#looking-ahead","title":"Looking Ahead","text":"<p>Day 3 Preview:  - Command line proficiency, - HPC fundamentals - Quality checking and control with FastQC - Species identification using Kraken2</p> <p>Key Learning Outcome: Mastery of command line operations and HPC infrastructure usage provides the essential computational foundation for all subsequent genomic analyses in the course. </p>"},{"location":"modules/day3/","title":"Day 3 - Genomic Characterization","text":""},{"location":"modules/day3/#day-3-accelerating-bioinformatics-hpc-qc-and-species-identification-essentials","title":"Day 3: Accelerating Bioinformatics: HPC, QC, and Species Identification Essentials","text":"<p>Date: September 3, 2025 Duration: 09:00-13:00 CAT Focus: HPC infrastructure, quality control, species identification</p>"},{"location":"modules/day3/#overview","title":"Overview","text":"<p>Day 3 continues building computational skills with an introduction to the Ilifu HPC infrastructure, then introduces essential genomic characterization techniques including quality control, species identification. These foundational skills are critical for all downstream genomic analyses.</p>"},{"location":"modules/day3/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 3, you will be able to:</p> <ul> <li>Connect to and navigate the Ilifu high-performance computing cluster</li> <li>Submit and manage jobs using the SLURM scheduler</li> <li>Understand resource allocation and job queue management on HPC systems</li> <li>Perform quality checking and control on sequencing data using FastQC</li> <li>Identify species from genomic data using Kraken2 and other tools</li> </ul>"},{"location":"modules/day3/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Introduction High Performance Computing (HPC) \u2013 Ilifu Notes \u2022 Practical 1 \u2022 Practical 2 Mamana Mbiyavanga 11:00 Break 11:30 Quality checking and control, as well as species identification Practical Arash Iranzadeh"},{"location":"modules/day3/#key-topics","title":"Key Topics","text":""},{"location":"modules/day3/#1-qc-and-species-identification-essentials","title":"1. QC and Species Identification Essentials","text":"<ul> <li>Quality checking, and adapter and low read quality filtering</li> <li>Contamination detection and removal</li> <li>Species identification or confirmation</li> </ul>"},{"location":"modules/day3/#2-high-performance-computing-hpc-ilifu-infrastructure","title":"2. High Performance Computing (HPC) - Ilifu Infrastructure","text":"<ul> <li>Introduction to cluster computing concepts</li> <li>Ilifu cluster architecture and capabilities</li> <li>SSH connections and authentication</li> <li>SLURM job scheduling system</li> <li>Resource allocation (CPU, memory, time)</li> <li>Module system for software management</li> </ul>"},{"location":"modules/day3/#3-slurm-job-management","title":"3. SLURM Job Management","text":"<ul> <li>Writing and submitting batch scripts</li> <li>Interactive vs batch jobs</li> <li>Job monitoring and queue management</li> <li>Resource specification and optimization</li> <li>Output and error file handling</li> <li>Best practices for efficient HPC usage</li> </ul>"},{"location":"modules/day3/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day3/#hpc-environment","title":"HPC Environment","text":"<ul> <li>Ilifu cluster - High-performance computing infrastructure</li> <li>SLURM - Job scheduling system</li> <li>Module system - Software environment management</li> <li>SSH clients - Remote connection tools</li> </ul>"},{"location":"modules/day3/#quality-control-tools","title":"Quality Control Tools","text":"<ul> <li>FASTQC - Quality checking</li> <li>MULTIQC - Quality checking and amalgation of reports</li> <li>Trimmomatic - Filter adapters and low quality reads</li> <li>Fastp -  Filter adapters and low quality reads</li> <li>KRAKEN2 - Species Identification</li> </ul>"},{"location":"modules/day3/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day3/#exercise-2-ilifu-hpc-connection-and-setup-30-minutes","title":"Exercise 2: Ilifu HPC Connection and Setup (30 minutes)","text":"<p>Connect to the Ilifu cluster and set up your working environment.</p> <pre><code># Connect to Ilifu via SSH\nssh username@slurm.ilifu.ac.za\n\n# Explore the HPC environment\npwd                    # Check current directory\nmodule avail          # List available software\nmodule load python    # Load software module\n\n# Check cluster resources\nsinfo                 # View cluster partitions\nsqueue               # Check job queue\n</code></pre>"},{"location":"modules/day3/#exercise-3-slurm-job-submission-60-minutes","title":"Exercise 3: SLURM Job Submission (60 minutes)","text":"<p>Learn to submit and manage jobs on the HPC cluster.</p> <pre><code># Create a simple batch script\ncat &gt; my_first_job.sh &lt;&lt; 'EOF'\n#!/bin/bash\n#SBATCH --job-name=test_job\n#SBATCH --time=00:10:00\n#SBATCH --mem=1G\n#SBATCH --cpus-per-task=1\n\necho \"Hello from HPC!\"\necho \"Running on node: $HOSTNAME\"\ndate\nEOF\n\n# Submit the job\nsbatch my_first_job.sh\n\n# Monitor job progress\nsqueue -u $USER\n</code></pre>"},{"location":"modules/day3/#exercise-1-species-identification-60-minutes","title":"Exercise 1: Species Identification (60 minutes)","text":"<p>Species identification and contamination screening</p>"},{"location":"modules/day3/#contamination-screening","title":"Contamination screening","text":"<p>kraken2 --db minikraken2_v2 assembly_output/scaffolds.fasta --report contamination_check.txt </p><pre><code>## Key Concepts\n\n### HPC Computing Principles\n- **Cluster architecture**: Login nodes vs compute nodes\n- **Job scheduling**: SLURM queue management and priority\n- **Resource allocation**: CPU, memory, and time specifications\n- **Module system**: Managing software environments\n- **Parallel processing**: Utilizing multiple cores efficiently\n\n### SLURM Job Management\n| Component | Description | Example |\n|-----------|-------------|---------|\n| Partition | Compute resource group | `main`, `gpu`, `bigmem` |\n| Job State | Current job status | `PD` (pending), `R` (running) |\n| Resources | CPU/Memory/Time | `--cpus-per-task=4 --mem=8G` |\n| Output | Job results and logs | `slurm-jobid.out` |\n\n## Assessment Activities\n\n### Individual Analysis\n- Write and submit a SLURM batch script\n- Monitor job status and retrieve results\n- Complete genome assembly workflow\n- Perform quality assessment and interpretation\n\n### HPC Connection Issues\n```bash\n# SSH key problems\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nssh-copy-id username@slurm.ilifu.ac.za\n\n# Module loading issues\nmodule avail    # List available modules\nmodule list     # Show loaded modules\nmodule purge    # Clear all modules\n</code></pre><p></p>"},{"location":"modules/day3/#slurm-job-troubleshooting","title":"SLURM Job Troubleshooting","text":"<pre><code># Job stuck in pending\nscontrol show job &lt;jobid&gt;  # Check job details\nsqueue -j &lt;jobid&gt;          # Check specific job\n\n# Resource issues\nsacct -j &lt;jobid&gt; --format=JobID,State,ExitCode,MaxRSS,Elapsed\n</code></pre>"},{"location":"modules/day3/#resources","title":"Resources","text":""},{"location":"modules/day3/#hpc-documentation","title":"HPC Documentation","text":"<ul> <li>Ilifu User Guide</li> <li>SLURM Quick Start</li> <li>SSH Key Management</li> </ul>"},{"location":"modules/day3/#assembly-issues","title":"Assembly Issues","text":"<pre><code># Low coverage assemblies\nspades.py --careful --cov-cutoff 5 -1 R1.fastq -2 R2.fastq -o low_cov_assembly/\n\n# Contamination removal\n# Remove contaminant contigs based on taxonomy\nseqtk subseq scaffolds.fasta clean_contigs.txt &gt; clean_assembly.fasta\n</code></pre>"},{"location":"modules/day3/#clinical-applications","title":"Clinical Applications","text":""},{"location":"modules/day3/#routine-surveillance","title":"Routine Surveillance","text":"<ul> <li>Mantaining data quality control standards</li> <li>Rapid species identification and typing</li> </ul>"},{"location":"modules/day3/#looking-ahead","title":"Looking Ahead","text":"<p>Day 4 Preview:  - Genome assembly and - Genome quality assessment - Genome annotation with Prokka</p> <p>Key Learning Outcome: Quality genomes to increase our confidence in our characterization capabilities essential for clinical genomics and public health surveillance.</p>"},{"location":"modules/day4/","title":"Day 4 - Genomic Characterization","text":""},{"location":"modules/day4/#day-4-genome-assembly-essentials-qc-identification-and-assembly","title":"Day 4: Genome Assembly Essentials: QC, Identification, and assembly","text":"<p>Date: September 4, 2025 Duration: 09:00-13:00 CAT Focus: Genome assembly and assessment</p>"},{"location":"modules/day4/#overview","title":"Overview","text":"<p>Day 4 builds on Day 3 by starting with a recap on quality control, followed by genome assembly and assessment of assemblies to ensure.</p>"},{"location":"modules/day4/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 4, you will be able to:</p> <ul> <li>Execute de novo genome assembly using SPAdes or other assemblers</li> <li>Assess assembly quality using QUAST and other metrics</li> </ul>"},{"location":"modules/day4/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Recap: Quality checking and control, and species identification Practical Arash Iranzadeh 10:30 Genome assembly, quality assessment Notes. Ephifania Geza 11:00 Break 11:30 Genome assembly, quality assessment: Continuation Practical Ephifania Geza"},{"location":"modules/day4/#key-topics","title":"Key Topics","text":""},{"location":"modules/day4/#1-genome-assembly-and-assessing-contigs","title":"1. Genome assembly and assessing contigs","text":"<ul> <li>De novo assembly algorithms and approaches</li> <li>Short-read vs long-read assembly strategies</li> <li>Assembly quality metrics and interpretation</li> <li>Assembly polishing and gap filling</li> </ul>"},{"location":"modules/day4/#datasets-used","title":"Datasets Used","text":""},{"location":"modules/day4/#v-cholerae-outbreak-collection","title":"V. cholerae Outbreak Collection","text":"<ul> <li>Source: Multi-country cholera outbreak (2019)</li> <li>Samples: 25 clinical isolates + environmental samples</li> <li>Timespan: 8-month outbreak period</li> <li>Geographic: Three countries, coastal regions</li> <li>Epidemiological data: Case demographics, travel history</li> </ul>"},{"location":"modules/day4/#tools-introduced","title":"Tools Introduced","text":""},{"location":"modules/day4/#pangenome-analysis","title":"Pangenome Analysis","text":"<ul> <li>SPAdes - De novo genome assembler</li> <li>Unicycler - Hybrid assembly pipeline</li> <li>Flye - Long-read assembly</li> <li>QUAST - Assembly quality assessment</li> </ul>"},{"location":"modules/day4/#annotation-tools","title":"Annotation Tools","text":"<ul> <li>Prokka - Automated prokaryotic annotation</li> <li>RAST - Rapid Annotation using Subsystem Technology</li> <li>NCBI PGAP - Prokaryotic Genome Annotation Pipeline</li> <li> <p>Bakta - Rapid bacterial genome annotation</p> </li> <li> <p>Panaroo - Pangenome pipeline</p> </li> <li>Roary - Rapid large-scale prokaryote pangenome analysis</li> <li>PPanGGOLiN - Depicting microbial diversity via pangenomes</li> </ul>"},{"location":"modules/day4/#phylogenetic-analysis","title":"Phylogenetic Analysis","text":"<ul> <li>IQ-TREE - Maximum likelihood phylogenetic inference</li> <li>RAxML - Randomized Axelerated Maximum Likelihood</li> <li>FastTree - Approximately maximum-likelihood trees</li> </ul>"},{"location":"modules/day4/#snp-analysis","title":"SNP Analysis","text":"<ul> <li>Snippy - Rapid haploid variant calling</li> <li>ParSNP - Rapid core genome SNP typing</li> <li>Gubbins - Recombination detection in bacterial genomes</li> </ul>"},{"location":"modules/day4/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day4/#exercise-1-genome-assembly-and-quality-assessment-60-minutes","title":"Exercise 1: Genome Assembly and Quality Assessment (60 minutes)","text":"<p>Assemble bacterial genomes and evaluate assembly quality.</p> <pre><code># De novo assembly with SPAdes\nspades.py --careful -1 sample_R1.fastq.gz -2 sample_R2.fastq.gz -o assembly_output/\n\n# Assembly quality assessment\nquast.py assembly_output/scaffolds.fasta -o quast_results/\n\n# Check assembly statistics\nassembly-stats assembly_output/scaffolds.fasta\n\n\n### Exercise 1: Pangenome Analysis (60 minutes)\nAnalyze the core and accessory genome of *V. cholerae* outbreak strains.\n\n```bash\n# Run Panaroo pangenome analysis\npanaroo -i *.gff -o panaroo_output --clean-mode strict\n\n# Visualize results\npython3 scripts/visualize_pangenome.py panaroo_output/\n</code></pre>"},{"location":"modules/day4/#exercise-2-phylogenetic-tree-construction-60-minutes","title":"Exercise 2: Phylogenetic Tree Construction (60 minutes)","text":"<p>Build maximum likelihood trees from core genome alignments.</p> <pre><code># Generate core genome alignment\nsnippy-core --ref reference.gbk snippy_output/*\n\n# Build phylogenetic tree\niqtree -s core_alignment.aln -m GTR+G -bb 1000 -nt 4\n\n# Visualize tree\nfigtree core_alignment.aln.treefile\n</code></pre>"},{"location":"modules/day4/#exercise-3-outbreak-investigation-45-minutes","title":"Exercise 3: Outbreak Investigation (45 minutes)","text":"<p>Integrate genomic and epidemiological data to investigate transmission.</p> <pre><code># Calculate pairwise SNP distances\nsnp-dists core_alignment.aln &gt; snp_distances.tsv\n\n# Identify transmission clusters\ncluster_analysis.py --snp-threshold 10 --epi-data metadata.csv\n</code></pre>"},{"location":"modules/day4/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day4/#assembly-quality-metrics","title":"Assembly Quality Metrics","text":"Metric Good Assembly Poor Assembly Action N50 &gt;50 kb &lt;10 kb Optimize parameters Contigs &lt;100 &gt;500 Check contamination Genome size Expected \u00b110% &gt;20% difference Review input data Coverage &gt;50x &lt;20x Sequence more"},{"location":"modules/day4/#pangenome-structure","title":"Pangenome Structure","text":"<ul> <li>Core genome: Genes present in all strains (housekeeping functions)</li> <li>Accessory genome: Variable genes (adaptation, virulence, resistance)</li> <li>Singleton genes: Present in single strain only</li> <li>Shell genes: Present in several but not all strains</li> </ul>"},{"location":"modules/day4/#phylogenetic-inference","title":"Phylogenetic Inference","text":"<ul> <li>Substitution models: GTR, HKY, JC69 for nucleotide evolution</li> <li>Rate heterogeneity: Gamma distribution for variable sites</li> <li>Bootstrap support: Statistical confidence in tree topology</li> <li>Branch lengths: Evolutionary distance (substitutions per site)</li> </ul>"},{"location":"modules/day4/#snp-thresholds-for-outbreak-investigation","title":"SNP Thresholds for Outbreak Investigation","text":"Pathogen SNP Threshold Timeframe Context M. tuberculosis 0-5 SNPs Recent transmission Same household S. Typhi 0-20 SNPs Outbreak cluster Weeks to months V. cholerae 0-10 SNPs Epidemic spread Days to weeks E. coli O157 0-15 SNPs Foodborne outbreak Days"},{"location":"modules/day4/#advanced-topics","title":"Advanced Topics","text":""},{"location":"modules/day4/#molecular-dating","title":"Molecular Dating","text":"<ul> <li>Tip dating: Using collection dates for molecular clock</li> <li>Bayesian methods: BEAST, MrBayes for time-resolved phylogenies</li> <li>Substitution rates: Pathogen-specific evolutionary rates</li> </ul>"},{"location":"modules/day4/#network-analysis","title":"Network Analysis","text":"<ul> <li>Minimum spanning trees: Alternative to bifurcating phylogenies</li> <li>Median-joining networks: Visualization of reticulate evolution</li> <li>Transmission networks: Direct transmission inference</li> </ul>"},{"location":"modules/day4/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day4/#individual-analysis","title":"Individual Analysis","text":"<ul> <li>Generate pangenome summary statistics</li> <li>Construct phylogenetic tree with bootstrap support</li> <li>Calculate SNP distances between outbreak isolates</li> <li>Identify transmission clusters based on genomic data</li> </ul>"},{"location":"modules/day4/#group-discussion","title":"Group Discussion","text":"<ul> <li>Compare assembly strategies and results</li> </ul>"},{"location":"modules/day4/#common-challenges","title":"Common Challenges","text":"<ul> <li>Interpret pangenome diversity in outbreak context</li> <li>Evaluate phylogenetic support for transmission hypotheses</li> <li>Discuss integration of genomic and epidemiological evidence</li> <li>Consider limitations of genomic outbreak investigation</li> </ul>"},{"location":"modules/day4/#common-challenges_1","title":"Common Challenges","text":""},{"location":"modules/day4/#low-phylogenetic-resolution","title":"Low Phylogenetic Resolution","text":"<pre><code># Try different substitution models\niqtree -s alignment.aln -m MFP -bb 1000  # Model selection\n\n# Use more informative sites only\niqtree -s alignment.aln -m GTR+G -bb 1000 --rate\n</code></pre>"},{"location":"modules/day4/#recombination-issues","title":"Recombination Issues","text":"<pre><code># Detect and remove recombinant regions\ngubbins alignment.aln\n\n# Use recombination-free alignment\niqtree -s alignment.filtered_polymorphic_sites.fasta\n</code></pre>"},{"location":"modules/day4/#missing-epidemiological-links","title":"Missing Epidemiological Links","text":"<pre><code># Lower SNP threshold for exploration\ncluster_analysis.py --snp-threshold 20 --epi-data metadata.csv\n\n# Consider longer transmission chains\ntransmission_chains.py --max-generations 3\n</code></pre>"},{"location":"modules/day4/#resources","title":"Resources","text":""},{"location":"modules/day4/#resources_1","title":"Resources","text":""},{"location":"modules/day4/#assembly-resources","title":"Assembly Resources","text":"<ul> <li>SPAdes Manual</li> <li>QUAST Documentation</li> <li>Assembly Best Practices</li> </ul>"},{"location":"modules/day4/#key-publications","title":"Key Publications","text":"<ul> <li>Page et al. (2015). Roary: rapid large-scale prokaryote pangenome analysis</li> <li>Tonkin-Hill et al. (2020). Producing polished prokaryotic pangenomes</li> <li>Croucher et al. (2015). Rapid phylogenetic analysis of bacterial genomes</li> </ul>"},{"location":"modules/day4/#software-documentation","title":"Software Documentation","text":"<ul> <li>IQ-TREE Manual</li> <li>Panaroo Documentation</li> <li>Snippy Manual</li> </ul>"},{"location":"modules/day4/#online-resources","title":"Online Resources","text":"<ul> <li>Microreact - Visualization platform</li> <li>iTOL - Interactive tree visualization</li> <li>FigTree - Tree viewing software</li> </ul>"},{"location":"modules/day4/#looking-ahead","title":"Looking Ahead","text":"<p>Day 5 Preview: Metagenomics analysis including: - Microbiome profiling from clinical samples - Pathogen detection in complex communities - Functional analysis of metagenomes - Association with host health outcomes</p>"},{"location":"modules/day4/#homework-optional","title":"Homework (Optional)","text":"<ol> <li>Analyze pangenome diversity in additional pathogen datasets</li> <li>Experiment with different phylogenetic methods and compare results</li> <li>Read case studies of genomic outbreak investigations</li> <li>Practice tree interpretation with published examples</li> </ol> <p>Key Takeaway: Genomic analysis provides unprecedented resolution for understanding pathogen evolution and transmission, but interpretation requires careful integration with epidemiological context and understanding of method limitations.</p>"},{"location":"modules/day5/","title":"Day 5 - Nextflow Pipeline Development","text":""},{"location":"modules/day5/#day-5-nextflow-pipeline-development","title":"Day 5: Nextflow Pipeline Development","text":"<p>Date: September 5, 2025 Duration: 09:00-13:00 CAT Focus: Workflow reproducibility, Nextflow basics, pipeline development</p>"},{"location":"modules/day5/#overview","title":"Overview","text":"<p>Day 5 introduces Nextflow, a powerful workflow management system for creating reproducible and scalable bioinformatics pipelines. We'll explore the fundamentals of Nextflow, the nf-core community standards, and begin developing a pipeline for genomic analysis including QC, assembly, quality assessment, and annotation.</p>"},{"location":"modules/day5/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 5, you will be able to:</p> <ul> <li>Understand the principles of reproducible computational workflows</li> <li>Write basic Nextflow scripts with processes and channels</li> <li>Utilize nf-core tools and community pipelines</li> <li>Design workflow architecture for genomic analysis</li> <li>Implement data flow using Nextflow channels</li> <li>Begin developing a pipeline for QC, assembly, and annotation</li> </ul>"},{"location":"modules/day5/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Reproducible workflows with Nextflow and nf-core Mamana Mbiyavanga 10:30 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga 11:30 Break 12:00 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga"},{"location":"modules/day5/#key-topics","title":"Key Topics","text":""},{"location":"modules/day5/#1-introduction-to-workflow-management","title":"1. Introduction to Workflow Management","text":"<ul> <li>Challenges in bioinformatics reproducibility</li> <li>Benefits of workflow management systems</li> <li>Nextflow vs other workflow systems (Snakemake, CWL, WDL)</li> <li>Container technologies (Docker, Singularity)</li> </ul>"},{"location":"modules/day5/#2-nextflow-fundamentals","title":"2. Nextflow Fundamentals","text":"<ul> <li>Nextflow architecture and concepts</li> <li>Processes, channels, and operators</li> <li>Configuration files and profiles</li> <li>Resource management and executors</li> <li>Error handling and resume capabilities</li> </ul>"},{"location":"modules/day5/#3-nf-core-community-and-standards","title":"3. nf-core Community and Standards","text":"<ul> <li>nf-core pipeline structure</li> <li>Community guidelines and best practices</li> <li>Using nf-core tools</li> <li>Available nf-core pipelines for genomics</li> <li>Contributing to nf-core</li> </ul>"},{"location":"modules/day5/#4-building-a-genomic-analysis-pipeline","title":"4. Building a Genomic Analysis Pipeline","text":"<ul> <li>Pipeline design and planning</li> <li>Implementing QC processes (FastQC, MultiQC)</li> <li>Assembly process integration (SPAdes)</li> <li>Quality assessment steps (QUAST)</li> <li>Annotation process (Prokka)</li> </ul>"},{"location":"modules/day5/#5-nextflow-scripting","title":"5. Nextflow Scripting","text":"<ul> <li>Writing process definitions</li> <li>Channel operations and data flow</li> <li>Parameter handling</li> <li>Conditional execution</li> <li>Module organization</li> </ul>"},{"location":"modules/day5/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day5/#workflow-management","title":"Workflow Management","text":"<ul> <li>Nextflow - Workflow orchestration system</li> <li>nf-core tools - Pipeline development framework</li> <li>Tower - Workflow monitoring platform</li> </ul>"},{"location":"modules/day5/#containerization","title":"Containerization","text":"<ul> <li>Docker - Container platform</li> <li>Singularity - HPC-friendly containers</li> <li>Conda - Package management</li> </ul>"},{"location":"modules/day5/#pipeline-components","title":"Pipeline Components","text":"<ul> <li>FastQC - Read quality control</li> <li>MultiQC - Aggregate reporting</li> <li>SPAdes - Genome assembly</li> <li>QUAST - Assembly assessment</li> <li>Prokka - Genome annotation</li> </ul>"},{"location":"modules/day5/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day5/#exercise-1-first-nextflow-script-30-minutes","title":"Exercise 1: First Nextflow Script (30 minutes)","text":"<p>Create and run a simple Nextflow pipeline.</p> <pre><code>#!/usr/bin/env nextflow\n\n// Define parameters\nparams.input = \"data/*.fastq\"\nparams.outdir = \"results\"\n\n// Create a channel from input files\nChannel\n    .fromPath(params.input)\n    .set { fastq_ch }\n\n// Define a process\nprocess countReads {\n    input:\n    path fastq from fastq_ch\n\n    output:\n    path \"*.count\" into counts_ch\n\n    script:\n    \"\"\"\n    echo \"Processing ${fastq}\"\n    wc -l ${fastq} &gt; ${fastq.baseName}.count\n    \"\"\"\n}\n\n// View the results\ncounts_ch.view()\n</code></pre>"},{"location":"modules/day5/#exercise-2-building-a-qc-pipeline-60-minutes","title":"Exercise 2: Building a QC Pipeline (60 minutes)","text":"<p>Implement quality control with FastQC and MultiQC.</p> <pre><code>process fastqc {\n    container 'biocontainers/fastqc:v0.11.9'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -t ${task.cpus} ${reads}\n    \"\"\"\n}\n\nprocess multiqc {\n    publishDir params.outdir, mode: 'copy'\n    container 'ewels/multiqc:latest'\n\n    input:\n    path '*' from fastqc_results.collect()\n\n    output:\n    path 'multiqc_report.html'\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day5/#exercise-3-integrating-assembly-90-minutes","title":"Exercise 3: Integrating Assembly (90 minutes)","text":"<p>Add genome assembly to the pipeline.</p> <pre><code>process spades_assembly {\n    container 'staphb/spades:latest'\n    cpus 4\n    memory '8 GB'\n\n    input:\n    tuple val(sample_id), path(reads1), path(reads2)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_contigs.fasta\")\n\n    script:\n    \"\"\"\n    spades.py \\\n        -1 ${reads1} \\\n        -2 ${reads2} \\\n        -o spades_output \\\n        -t ${task.cpus} \\\n        --careful\n\n    cp spades_output/contigs.fasta ${sample_id}_contigs.fasta\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day5/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day5/#workflow-principles","title":"Workflow Principles","text":"<ul> <li>Reproducibility: Same input \u2192 same output</li> <li>Portability: Run anywhere (laptop, HPC, cloud)</li> <li>Scalability: Handle any data volume</li> <li>Resumability: Restart from failure points</li> </ul>"},{"location":"modules/day5/#nextflow-components","title":"Nextflow Components","text":"Component Description Example Process Computational step <code>process fastqc { ... }</code> Channel Data flow connection <code>Channel.fromPath()</code> Operator Channel transformation <code>.map()</code>, <code>.filter()</code> Directive Process configuration <code>cpus 4</code>"},{"location":"modules/day5/#best-practices","title":"Best Practices","text":"<ol> <li>Use containers: Ensure environment reproducibility</li> <li>Parameterize everything: Make pipelines flexible</li> <li>Version control: Track pipeline changes</li> <li>Document thoroughly: Help users and future self</li> <li>Test incrementally: Build and test step by step</li> </ol>"},{"location":"modules/day5/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day5/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Create a basic Nextflow script with at least 2 processes</li> <li>Successfully run a pipeline with test data</li> <li>Modify pipeline parameters and observe changes</li> <li>Debug a pipeline with intentional errors</li> <li>Document pipeline usage</li> </ul>"},{"location":"modules/day5/#group-discussion","title":"Group Discussion","text":"<ul> <li>Compare Nextflow with traditional shell scripting</li> <li>Discuss reproducibility challenges and solutions</li> <li>Share pipeline design strategies</li> <li>Explore nf-core pipeline catalog</li> </ul>"},{"location":"modules/day5/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day5/#installation-issues","title":"Installation Issues","text":"<pre><code># Install Nextflow\ncurl -s https://get.nextflow.io | bash\n./nextflow run hello\n\n# Set up environment\nexport PATH=$PATH:$PWD\nexport NXF_VER=23.10.0\n</code></pre>"},{"location":"modules/day5/#channel-operations","title":"Channel Operations","text":"<pre><code>// Common channel patterns\nChannel\n    .fromFilePairs(params.reads)\n    .ifEmpty { error \"No read files found!\" }\n    .set { read_pairs_ch }\n\n// Combining channels\nfastqc_ch\n    .join(assembly_ch)\n    .map { sample, qc, assembly -&gt; \n        [sample, qc, assembly]\n    }\n</code></pre>"},{"location":"modules/day5/#resource-management","title":"Resource Management","text":"<pre><code>process memory_intensive {\n    memory { 2.GB * task.attempt }\n    maxRetries 3\n    errorStrategy 'retry'\n\n    script:\n    \"\"\"\n    # Your command here\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day5/#resources","title":"Resources","text":""},{"location":"modules/day5/#documentation","title":"Documentation","text":"<ul> <li>Nextflow Documentation</li> <li>nf-core Website</li> <li>Nextflow Training</li> </ul>"},{"location":"modules/day5/#tutorials","title":"Tutorials","text":"<ul> <li>Nextflow Tutorial</li> <li>nf-core Tutorials</li> <li>Seqera Labs Training</li> </ul>"},{"location":"modules/day5/#community","title":"Community","text":"<ul> <li>Nextflow Slack</li> <li>nf-core Slack</li> <li>GitHub Discussions</li> </ul>"},{"location":"modules/day5/#looking-ahead","title":"Looking Ahead","text":"<p>Day 6 Preview: Nextflow Pipeline Development - Continue building the genomic analysis pipeline - Advanced Nextflow features and optimization - Pipeline testing and validation - Deployment strategies</p> <p>Key Learning Outcome: Understanding workflow management principles and gaining hands-on experience with Nextflow enables creation of reproducible, scalable bioinformatics pipelines essential for modern genomic analysis.</p>"},{"location":"modules/day6/","title":"Day 6 - Nextflow Pipeline Development & Version Control with GitHub","text":""},{"location":"modules/day6/#day-6-nextflow-foundations-core-concepts","title":"Day 6: Nextflow Foundations &amp; Core Concepts","text":"<p>Date: September 8, 2025 Duration: 09:00-13:00 CAT Focus: Introduction to workflow management, Nextflow fundamentals, and first pipelines</p>"},{"location":"modules/day6/#overview","title":"Overview","text":"<p>Day 6 introduces participants to workflow management systems and Nextflow fundamentals. This comprehensive session covers the theoretical foundations of reproducible workflows, core Nextflow concepts, and hands-on development of basic pipelines. Participants will understand why workflow management is crucial for bioinformatics and gain practical experience with Nextflow's core components.</p>"},{"location":"modules/day6/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 6, you will be able to:</p> <ul> <li>Understand the challenges in bioinformatics reproducibility and benefits of workflow management systems</li> <li>Explain Nextflow's core features and architecture</li> <li>Identify the main components of a Nextflow script (processes, channels, workflows)</li> <li>Write and execute basic Nextflow processes and workflows</li> <li>Use channels to manage data flow between processes</li> <li>Configure Nextflow for different execution environments</li> <li>Debug common Nextflow issues and understand error messages</li> <li>Apply best practices for pipeline development</li> </ul>"},{"location":"modules/day6/#schedule","title":"Schedule","text":"Time (CAT) Topic Duration Trainer 09:00 Foundation: Command line, Git, Containers review 30 min Mamana Mbiyavanga 09:30 Introduction to Workflow Management Systems 45 min Mamana Mbiyavanga 10:15 Nextflow Basics: Core concepts and first pipelines 75 min Mamana Mbiyavanga 11:30 Break 15 min 11:45 Hands-on: Building your first Nextflow pipeline 75 min Mamana Mbiyavanga"},{"location":"modules/day6/#key-topics","title":"Key Topics","text":""},{"location":"modules/day6/#1-foundation-review-30-minutes","title":"1. Foundation Review (30 minutes)","text":"<ul> <li>Command line proficiency check</li> <li>Git basics and version control concepts</li> <li>Container technologies (Docker/Singularity) overview</li> <li>Setting up the development environment</li> </ul>"},{"location":"modules/day6/#2-introduction-to-workflow-management-45-minutes","title":"2. Introduction to Workflow Management (45 minutes)","text":"<ul> <li>The challenge of complex genomics analyses</li> <li>Problems with traditional scripting approaches</li> <li>Benefits of workflow management systems</li> <li>Nextflow vs other systems (Snakemake, CWL, WDL)</li> <li>Reproducibility, portability, and scalability</li> </ul>"},{"location":"modules/day6/#3-nextflow-core-concepts-75-minutes","title":"3. Nextflow Core Concepts (75 minutes)","text":"<ul> <li>Nextflow architecture and execution model</li> <li>Processes: encapsulated tasks with inputs, outputs, and scripts</li> <li>Channels: asynchronous data streams connecting processes</li> <li>Workflows: orchestrating process execution and data flow</li> <li>The work directory structure and caching mechanism</li> <li>Executors and execution platforms</li> </ul>"},{"location":"modules/day6/#4-hands-on-pipeline-development-75-minutes","title":"4. Hands-on Pipeline Development (75 minutes)","text":"<ul> <li>Writing your first Nextflow process</li> <li>Creating channels and managing data flow</li> <li>Building a simple QC workflow</li> <li>Testing and debugging pipelines</li> <li>Understanding the work directory</li> </ul>"},{"location":"modules/day6/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day6/#core-requirements","title":"Core Requirements","text":"<ul> <li>Nextflow (version 20.10.0 or later) - Workflow orchestration system</li> <li>Java (version 11 or later) - Required for Nextflow execution</li> <li>Docker or Singularity - Container platforms for reproducibility</li> <li>Text editor - VS Code with Nextflow extension recommended</li> </ul>"},{"location":"modules/day6/#bioinformatics-tools","title":"Bioinformatics Tools","text":"<ul> <li>FastQC - Read quality control assessment</li> <li>MultiQC - Aggregate quality control reports</li> <li>Trimmomatic - Read trimming and filtering</li> <li>SPAdes - Genome assembly (for later exercises)</li> </ul>"},{"location":"modules/day6/#development-environment","title":"Development Environment","text":"<ul> <li>Git - Version control system</li> <li>GitHub account - For sharing and collaboration</li> <li>Terminal/Command line - For running Nextflow commands</li> </ul>"},{"location":"modules/day6/#part-1-the-challenge-of-complex-genomics-analyses","title":"Part 1: The Challenge of Complex Genomics Analyses","text":""},{"location":"modules/day6/#why-workflow-management-matters","title":"Why Workflow Management Matters","text":"<p>Consider analyzing 100 bacterial genomes without workflow management:</p> <pre><code># Manual approach - tedious and error-prone\nfor sample in sample1 sample2 sample3 ... sample100; do\n    fastqc ${sample}_R1.fastq ${sample}_R2.fastq\n    if [ $? -ne 0 ]; then echo \"FastQC failed\"; exit 1; fi\n\n    trimmomatic PE ${sample}_R1.fastq ${sample}_R2.fastq \\\n        ${sample}_R1_trimmed.fastq ${sample}_R1_unpaired.fastq \\\n        ${sample}_R2_trimmed.fastq ${sample}_R2_unpaired.fastq \\\n        SLIDINGWINDOW:4:20\n    if [ $? -ne 0 ]; then echo \"Trimming failed\"; exit 1; fi\n\n    spades.py -1 ${sample}_R1_trimmed.fastq -2 ${sample}_R2_trimmed.fastq \\\n        -o ${sample}_assembly\n    if [ $? -ne 0 ]; then echo \"Assembly failed\"; exit 1; fi\n\n    # What if step 3 fails for sample 67?\n    # How do you restart from where it failed?\n    # How do you run samples in parallel efficiently?\n    # How do you ensure reproducibility across different systems?\ndone\n</code></pre>"},{"location":"modules/day6/#problems-with-traditional-approaches","title":"Problems with Traditional Approaches","text":"<ol> <li>Error Handling: Manual error checking is verbose and error-prone</li> <li>Parallelization: Difficult to efficiently use multiple cores/nodes</li> <li>Resumability: No easy way to restart from failed steps</li> <li>Reproducibility: Hard to ensure same results across different systems</li> <li>Scalability: Doesn't scale well from laptop to HPC to cloud</li> <li>Dependency Management: Software installation and version conflicts</li> <li>Resource Management: No automatic optimization of CPU/memory usage</li> </ol>"},{"location":"modules/day6/#the-workflow-management-solution","title":"The Workflow Management Solution","text":"<p>With Nextflow, you define the workflow once and it handles:</p> <ul> <li>Automatic parallelization of all 100 samples</li> <li>Intelligent resource management (memory, CPUs)</li> <li>Automatic retry of failed tasks with different resources</li> <li>Resume capability from the last successful step</li> <li>Container integration for reproducibility</li> <li>Detailed execution reports and monitoring</li> <li>Platform portability (laptop \u2192 HPC \u2192 cloud)</li> </ul>"},{"location":"modules/day6/#part-2-nextflow-architecture-and-core-concepts","title":"Part 2: Nextflow Architecture and Core Concepts","text":""},{"location":"modules/day6/#nextflows-key-components","title":"Nextflow's Key Components","text":""},{"location":"modules/day6/#1-nextflow-engine","title":"1. Nextflow Engine","text":"<p>The core runtime that interprets and executes your pipeline:</p> <ul> <li>Parses the workflow script</li> <li>Manages task scheduling and execution</li> <li>Handles data flow between processes</li> <li>Provides caching and resume capabilities</li> </ul>"},{"location":"modules/day6/#2-work-directory","title":"2. Work Directory","text":"<p>Where Nextflow stores intermediate files and task execution:</p> <pre><code>work/\n\u251c\u2500\u2500 12/\n\u2502   \u2514\u2500\u2500 3456789abcdef.../\n\u2502       \u251c\u2500\u2500 .command.sh      # The actual script executed\n\u2502       \u251c\u2500\u2500 .command.run     # Wrapper script\n\u2502       \u251c\u2500\u2500 .command.out     # Standard output\n\u2502       \u251c\u2500\u2500 .command.err     # Standard error\n\u2502       \u251c\u2500\u2500 .command.log     # Execution log\n\u2502       \u251c\u2500\u2500 .exitcode       # Exit status\n\u2502       \u2514\u2500\u2500 input_file.fastq # Staged input files\n\u2514\u2500\u2500 ab/\n    \u2514\u2500\u2500 cdef123456789.../\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"modules/day6/#3-executors","title":"3. Executors","text":"<p>Interface with different computing platforms:</p> <ul> <li>Local: Run on your laptop/desktop</li> <li>SLURM: Submit jobs to HPC clusters</li> <li>AWS Batch: Execute on Amazon cloud</li> <li>Kubernetes: Run on container orchestration platforms</li> </ul>"},{"location":"modules/day6/#core-nextflow-components","title":"Core Nextflow Components","text":""},{"location":"modules/day6/#process","title":"Process","text":"<p>A process defines a task to be executed. It's the basic building block of a Nextflow pipeline:</p> <pre><code>process FASTQC {\n    // Process directives\n    tag \"$sample_id\"\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*_fastqc.{html,zip}\"), emit: reports\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n</code></pre> <p>Key Elements:</p> <ul> <li>Directives: Configure how the process runs (container, resources, etc.)</li> <li>Input: Define what data the process expects</li> <li>Output: Define what data the process produces</li> <li>Script: The actual command(s) to execute</li> </ul>"},{"location":"modules/day6/#channel","title":"Channel","text":"<p>Channels are asynchronous data streams that connect processes:</p> <pre><code>// Create channel from file pairs\nreads_ch = Channel.fromFilePairs(\"data/*_R{1,2}.fastq.gz\")\n\n// Create channel from a list\nsamples_ch = Channel.from(['sample1', 'sample2', 'sample3'])\n\n// Create channel from a file\nreference_ch = Channel.fromPath(\"reference.fasta\")\n</code></pre> <p>Channel Types:</p> <ul> <li>Queue channels: Can be consumed only once</li> <li>Value channels: Can be consumed multiple times</li> <li>File channels: Handle file paths and staging</li> </ul>"},{"location":"modules/day6/#workflow","title":"Workflow","text":"<p>The workflow block orchestrates process execution:</p> <pre><code>workflow {\n    // Define input channels\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    // Execute processes\n    FASTQC(reads_ch)\n\n    // Chain processes together\n    TRIMMOMATIC(reads_ch)\n    SPADES(TRIMMOMATIC.out.trimmed)\n\n    // Access outputs\n    FASTQC.out.reports.view()\n}\n</code></pre>"},{"location":"modules/day6/#part-3-hands-on-exercises","title":"Part 3: Hands-on Exercises","text":""},{"location":"modules/day6/#exercise-1-installation-and-setup-15-minutes","title":"Exercise 1: Installation and Setup (15 minutes)","text":"<p>Objective: Install Nextflow and verify the environment</p> <pre><code># Check Java version (must be 11 or later)\njava -version\n\n# Install Nextflow\ncurl -s https://get.nextflow.io | bash\n\n# Make executable and add to PATH\nchmod +x nextflow\nsudo mv nextflow /usr/local/bin/\n\n# Verify installation\nnextflow info\n\n# Test with hello world\nnextflow run hello\n</code></pre>"},{"location":"modules/day6/#exercise-2-your-first-nextflow-script-30-minutes","title":"Exercise 2: Your First Nextflow Script (30 minutes)","text":"<p>Objective: Create and run a simple Nextflow pipeline</p> <p>Create a file called <code>word_count.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Pipeline parameters\nparams.input = \"data/yeast/reads/ref1_1.fq.gz\"\n\n// Input channel\ninput_ch = Channel.fromPath(params.input)\n\n// Main workflow\nworkflow {\n    NUM_LINES(input_ch)\n    NUM_LINES.out.view()\n}\n\n// Process definition\nprocess NUM_LINES {\n    input:\n    path read\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    printf '${read}\\\\t'\n    gunzip -c ${read} | wc -l\n    \"\"\"\n}\n</code></pre> <p>Run the pipeline:</p> <pre><code># Create test data directory\nmkdir -p data/yeast/reads\n\n# Download test file (or create a dummy file)\necho -e \"@read1\\nACGT\\n+\\nIIII\" | gzip &gt; data/yeast/reads/ref1_1.fq.gz\n\n# Run the pipeline\nnextflow run word_count.nf\n\n# Examine the work directory\nls -la work/\n</code></pre>"},{"location":"modules/day6/#exercise-3-understanding-channels-20-minutes","title":"Exercise 3: Understanding Channels (20 minutes)","text":"<p>Objective: Learn different ways to create and manipulate channels</p> <p>Create <code>channel_examples.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\nworkflow {\n    // Channel from file pairs\n    reads_ch = Channel.fromFilePairs(\"data/*_R{1,2}.fastq.gz\")\n    reads_ch.view { sample, files -&gt; \"Sample: $sample, Files: $files\" }\n\n    // Channel from list\n    samples_ch = Channel.from(['sample1', 'sample2', 'sample3'])\n    samples_ch.view { \"Processing: $it\" }\n\n    // Channel from path pattern\n    ref_ch = Channel.fromPath(\"*.fasta\")\n    ref_ch.view { \"Reference: $it\" }\n}\n</code></pre> <p>Save and push to GitHub: </p><pre><code>git add README.md\ngit commit -m \"Add README documentation\"\ngit push\n</code></pre><p></p>"},{"location":"modules/day6/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day6/#version-control-benefits","title":"Version Control Benefits","text":"<ul> <li>Track Changes: See what changed, when, and why</li> <li>Backup: Your code is safe on GitHub</li> <li>Collaboration: Work with others easily</li> <li>Reproducibility: Others can use your exact pipeline version</li> </ul>"},{"location":"modules/day6/#git-workflow","title":"Git Workflow","text":"<pre><code>Working Directory \u2192 Staging Area \u2192 Local Repository \u2192 Remote Repository\n     (edit)           (git add)      (git commit)        (git push)\n</code></pre>"},{"location":"modules/day6/#commit-best-practices","title":"Commit Best Practices","text":"<ul> <li>Write clear, descriptive messages</li> <li>Commit logical units of change</li> <li>Commit frequently</li> <li>Don't commit generated files (use .gitignore)</li> </ul>"},{"location":"modules/day6/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"modules/day6/#git-configuration","title":"Git Configuration","text":"<pre><code># Set up your identity (first time only)\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre>"},{"location":"modules/day6/#authentication-issues","title":"Authentication Issues","text":"<pre><code># If password authentication fails, use personal access token:\n# GitHub \u2192 Settings \u2192 Developer settings \u2192 Personal access tokens\n# Generate new token with 'repo' permissions\n# Use token instead of password when pushing\n</code></pre>"},{"location":"modules/day6/#common-git-commands","title":"Common Git Commands","text":"Command Purpose Example <code>git status</code> Check repository state <code>git status</code> <code>git diff</code> See changes <code>git diff main.nf</code> <code>git add .</code> Stage all changes <code>git add .</code> <code>git commit -m</code> Save changes <code>git commit -m \"Fix bug\"</code> <code>git push</code> Upload to GitHub <code>git push origin main</code> <code>git pull</code> Download from GitHub <code>git pull origin main</code>"},{"location":"modules/day6/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day6/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Successfully complete and run the Nextflow pipeline</li> <li>Initialize a Git repository for your pipeline</li> <li>Create a GitHub account and repository</li> <li>Push your pipeline to GitHub</li> <li>Create a comprehensive README file</li> </ul>"},{"location":"modules/day6/#group-discussion","title":"Group Discussion","text":"<ul> <li>Share GitHub repository URLs with the class</li> <li>Discuss pipeline design choices</li> <li>Review each other's README files</li> <li>Troubleshoot Git/GitHub issues together</li> </ul>"},{"location":"modules/day6/#resources","title":"Resources","text":""},{"location":"modules/day6/#nextflow-resources","title":"Nextflow Resources","text":"<ul> <li>Nextflow Documentation</li> <li>nf-core pipelines</li> </ul>"},{"location":"modules/day6/#gitgithub-resources","title":"Git/GitHub Resources","text":"<ul> <li>GitHub Guides</li> <li>Git Handbook</li> <li>GitHub Learning Lab</li> <li>Pro Git Book (free)</li> </ul>"},{"location":"modules/day6/#looking-ahead","title":"Looking Ahead","text":"<p>Day 7 Preview: Applied Genomics &amp; Advanced Topics</p> <ul> <li>MTB analysis pipeline development</li> <li>Genome assembly workflows</li> <li>Advanced Nextflow features and optimization</li> <li>Pipeline deployment and best practices</li> </ul>"},{"location":"modules/day6/#exercise-4-building-a-qc-process-30-minutes","title":"Exercise 4: Building a QC Process (30 minutes)","text":"<p>Objective: Create a real bioinformatics process</p> <p>Create <code>fastqc_pipeline.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Parameters\nparams.reads = \"data/*_R{1,2}.fastq.gz\"\nparams.outdir = \"results\"\n\n// Main workflow\nworkflow {\n    // Create channel from paired reads\n    reads_ch = Channel.fromFilePairs(params.reads, checkIfExists: true)\n\n    // Run FastQC\n    FASTQC(reads_ch)\n\n    // View results\n    FASTQC.out.view { sample, reports -&gt;\n        \"FastQC completed for $sample: $reports\"\n    }\n}\n\n// FastQC process\nprocess FASTQC {\n    tag \"$sample_id\"\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*_fastqc.{html,zip}\")\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n</code></pre> <p>Test the pipeline:</p> <pre><code># Create test data\nmkdir -p data\necho -e \"@read1\\nACGTACGT\\n+\\nIIIIIIII\" &gt; data/sample1_R1.fastq\necho -e \"@read1\\nTGCATGCA\\n+\\nIIIIIIII\" &gt; data/sample1_R2.fastq\n\n# Run pipeline\nnextflow run fastqc_pipeline.nf\n\n# Check results\nls -la results/fastqc/\n</code></pre>"},{"location":"modules/day6/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"modules/day6/#installation-issues","title":"Installation Issues","text":"<pre><code># Java version problems\njava -version  # Must be 11 or later\n\n# Nextflow not found\necho $PATH\nwhich nextflow\n\n# Permission issues\nchmod +x nextflow\n</code></pre>"},{"location":"modules/day6/#pipeline-debugging","title":"Pipeline Debugging","text":"<pre><code># Verbose output\nnextflow run pipeline.nf -with-trace -with-report -with-timeline\n\n# Check work directory\nls -la work/\n\n# Resume from failure\nnextflow run pipeline.nf -resume\n</code></pre> <p>Key Learning Outcome: Understanding workflow management fundamentals and Nextflow core concepts provides the foundation for building reproducible, scalable bioinformatics pipelines.</p>"},{"location":"modules/day7/","title":"Day 7 - Metagenomic profiling","text":""},{"location":"modules/day7/#day-7-applied-genomics-advanced-nextflow-topics","title":"Day 7: Applied Genomics &amp; Advanced Nextflow Topics","text":"<p>Date: September 9, 2025 Duration: 09:00-13:00 CAT Focus: MTB analysis pipelines, genome assembly workflows, advanced features, and deployment</p>"},{"location":"modules/day7/#overview","title":"Overview","text":"<p>Day 7 builds upon the Nextflow foundations from Day 6, focusing on real-world genomic applications and advanced pipeline development. Participants will develop comprehensive MTB (Mycobacterium tuberculosis) analysis pipelines, learn genome assembly workflows, explore advanced Nextflow features, and understand deployment strategies for production environments.</p>"},{"location":"modules/day7/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 7, you will be able to:</p> <ul> <li>Develop complete MTB genomic analysis pipelines for clinical applications</li> <li>Build genome assembly and annotation workflows</li> <li>Implement advanced Nextflow features (modules, subworkflows, operators)</li> <li>Optimize pipeline performance and resource management</li> <li>Test and validate pipeline components with real data</li> <li>Deploy pipelines across different execution platforms (local, HPC, cloud)</li> <li>Apply production-ready best practices and error handling</li> <li>Use nf-core tools and community resources</li> </ul>"},{"location":"modules/day7/#schedule","title":"Schedule","text":"Time (CAT) Topic Duration Trainer 09:00 Applied Genomics: MTB Analysis Pipeline 75 min Mamana Mbiyavanga 10:15 Genome Assembly Workflows 75 min Mamana Mbiyavanga 11:30 Break 15 min 11:45 Advanced Topics: Optimization &amp; Deployment 75 min Mamana Mbiyavanga"},{"location":"modules/day7/#key-topics","title":"Key Topics","text":""},{"location":"modules/day7/#1-applied-genomics-mtb-analysis-pipeline-75-minutes","title":"1. Applied Genomics: MTB Analysis Pipeline (75 minutes)","text":"<ul> <li>Clinical TB genomics workflow requirements</li> <li>Quality control and read preprocessing</li> <li>Reference-based variant calling pipeline</li> <li>Drug resistance prediction and lineage typing</li> <li>Phylogenetic analysis and transmission clustering</li> <li>Clinical reporting and interpretation</li> </ul>"},{"location":"modules/day7/#2-genome-assembly-workflows-75-minutes","title":"2. Genome Assembly Workflows (75 minutes)","text":"<ul> <li>De novo assembly pipeline development</li> <li>Assembly quality assessment and validation</li> <li>Genome annotation and functional analysis</li> <li>Comparative genomics approaches</li> <li>Multi-sample assembly processing</li> <li>Integration with downstream analyses</li> </ul>"},{"location":"modules/day7/#3-advanced-topics-optimization-deployment-75-minutes","title":"3. Advanced Topics: Optimization &amp; Deployment (75 minutes)","text":"<ul> <li>Advanced Nextflow features and modules</li> <li>Performance optimization and resource management</li> <li>Testing frameworks and continuous integration</li> <li>Deployment strategies (local, HPC, cloud)</li> <li>Production deployment and monitoring</li> <li>Best practices for collaborative development</li> </ul>"},{"location":"modules/day7/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day7/#bioinformatics-tools-for-mtb-analysis","title":"Bioinformatics Tools for MTB Analysis","text":"<ul> <li>BWA - Read alignment to reference genome</li> <li>SAMtools/BCFtools - SAM/BAM manipulation and variant calling</li> <li>GATK - Genome Analysis Toolkit for variant calling</li> <li>FastQC/MultiQC - Quality control and reporting</li> <li>Trimmomatic - Read trimming and filtering</li> <li>TB-Profiler - Drug resistance prediction</li> <li>SNP-sites - SNP extraction from alignments</li> <li>IQ-TREE - Phylogenetic tree construction</li> </ul>"},{"location":"modules/day7/#genome-assembly-tools","title":"Genome Assembly Tools","text":"<ul> <li>SPAdes - De novo genome assembly</li> <li>QUAST - Assembly quality assessment</li> <li>Prokka - Genome annotation</li> <li>BUSCO - Assembly completeness assessment</li> <li>Bandage - Assembly graph visualization</li> </ul>"},{"location":"modules/day7/#advanced-nextflow-features","title":"Advanced Nextflow Features","text":"<ul> <li>nf-core tools - Pipeline development framework</li> <li>nf-test - Testing framework</li> <li>Nextflow Tower - Pipeline monitoring</li> <li>Docker/Singularity - Container technologies</li> </ul>"},{"location":"modules/day7/#part-1-applied-genomics-mtb-analysis-pipeline","title":"Part 1: Applied Genomics - MTB Analysis Pipeline","text":""},{"location":"modules/day7/#the-global-tb-crisis-and-genomics-revolution","title":"The Global TB Crisis and Genomics Revolution","text":"<p>Tuberculosis kills more people than any other infectious disease caused by a single agent. In 2023, 10.6 million people fell ill with TB and 1.3 million died. Drug-resistant TB threatens to reverse decades of progress. This section equips you with cutting-edge genomic tools to combat this ancient disease using modern technology.</p>"},{"location":"modules/day7/#why-tb-genomics-matters","title":"Why TB Genomics Matters","text":"<p>Clinical Impact:</p> <ul> <li>Reduce diagnosis time from weeks to days</li> <li>Detect drug resistance before treatment failure</li> <li>Guide personalized treatment regimens</li> <li>Identify extensively drug-resistant (XDR) strains early</li> </ul> <p>Public Health Applications:</p> <ul> <li>Track transmission chains in real-time</li> <li>Identify super-spreaders and hotspots</li> <li>Monitor emergence of new resistance mutations</li> <li>Support contact tracing with genomic evidence</li> </ul>"},{"location":"modules/day7/#mtb-genomics-workflow-overview","title":"MTB Genomics Workflow Overview","text":"<pre><code>graph TD\n    A[Raw FASTQ reads] --&gt; B[Quality Control]\n    B --&gt; C[Read Trimming]\n    C --&gt; D[Reference Alignment]\n    D --&gt; E[Variant Calling]\n    E --&gt; F[Drug Resistance Prediction]\n    E --&gt; G[Lineage Typing]\n    E --&gt; H[Phylogenetic Analysis]\n    F --&gt; I[Clinical Report]\n    G --&gt; I\n    H --&gt; I</code></pre>"},{"location":"modules/day7/#exercise-1-mtb-quality-control-pipeline-30-minutes","title":"Exercise 1: MTB Quality Control Pipeline (30 minutes)","text":"<p>Objective: Build a QC pipeline for MTB sequencing data</p> <p>Create <code>mtb_qc.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Parameters\nparams.reads = \"data/mtb/*_R{1,2}.fastq.gz\"\nparams.outdir = \"results\"\nparams.reference = \"data/reference/H37Rv.fasta\"\n\n// Main workflow\nworkflow {\n    // Input channels\n    reads_ch = Channel.fromFilePairs(params.reads, checkIfExists: true)\n    reference_ch = Channel.fromPath(params.reference, checkIfExists: true)\n\n    // Quality control\n    FASTQC(reads_ch)\n\n    // Read trimming\n    TRIMMOMATIC(reads_ch)\n\n    // Post-trimming QC\n    FASTQC_TRIMMED(TRIMMOMATIC.out.trimmed)\n\n    // Aggregate reports\n    qc_files = FASTQC.out.zip.mix(FASTQC_TRIMMED.out.zip).collect()\n    MULTIQC(qc_files)\n}\n\n// Process definitions\nprocess FASTQC {\n    tag \"$sample_id\"\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n    publishDir \"${params.outdir}/fastqc_raw\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*_fastqc.{html,zip}\"), emit: reports\n    path \"*_fastqc.zip\", emit: zip\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n\nprocess TRIMMOMATIC {\n    tag \"$sample_id\"\n    container 'quay.io/biocontainers/trimmomatic:0.39--hdfd78af_2'\n    publishDir \"${params.outdir}/trimmed\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_R{1,2}_trimmed.fastq.gz\"), emit: trimmed\n    path \"${sample_id}_trimming_report.txt\", emit: report\n\n    script:\n    \"\"\"\n    trimmomatic PE \\\\\n        ${reads[0]} ${reads[1]} \\\\\n        ${sample_id}_R1_trimmed.fastq.gz ${sample_id}_R1_unpaired.fastq.gz \\\\\n        ${sample_id}_R2_trimmed.fastq.gz ${sample_id}_R2_unpaired.fastq.gz \\\\\n        ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 \\\\\n        LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36 \\\\\n        2&gt; ${sample_id}_trimming_report.txt\n    \"\"\"\n}\n\nprocess FASTQC_TRIMMED {\n    tag \"$sample_id\"\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n    publishDir \"${params.outdir}/fastqc_trimmed\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*_fastqc.{html,zip}\"), emit: reports\n    path \"*_fastqc.zip\", emit: zip\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    container 'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0'\n    publishDir \"${params.outdir}/multiqc\", mode: 'copy'\n\n    input:\n    path qc_files\n\n    output:\n    path \"multiqc_report.html\"\n    path \"multiqc_data\"\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day7/#exercise-2-mtb-variant-calling-pipeline-45-minutes","title":"Exercise 2: MTB Variant Calling Pipeline (45 minutes)","text":"<p>Objective: Build a complete MTB variant calling workflow</p> <p>Create <code>mtb_variant_calling.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Parameters\nparams.reads = \"data/mtb/*_R{1,2}.fastq.gz\"\nparams.reference = \"data/reference/H37Rv.fasta\"\nparams.outdir = \"results\"\n\nworkflow {\n    // Input channels\n    reads_ch = Channel.fromFilePairs(params.reads, checkIfExists: true)\n    reference_ch = Channel.fromPath(params.reference, checkIfExists: true)\n\n    // Index reference\n    BWA_INDEX(reference_ch)\n\n    // Align reads\n    BWA_MEM(reads_ch, BWA_INDEX.out.index)\n\n    // Call variants\n    BCFTOOLS_MPILEUP(BWA_MEM.out.bam, reference_ch)\n\n    // Drug resistance prediction\n    TB_PROFILER(reads_ch)\n}\n\nprocess BWA_INDEX {\n    container 'biocontainers/bwa:v0.7.17_cv1'\n\n    input:\n    path reference\n\n    output:\n    path \"reference.*\", emit: index\n\n    script:\n    \"\"\"\n    cp ${reference} reference.fasta\n    bwa index reference.fasta\n    \"\"\"\n}\n\nprocess BWA_MEM {\n    tag \"$sample_id\"\n    container 'biocontainers/bwa:v0.7.17_cv1'\n\n    input:\n    tuple val(sample_id), path(reads)\n    path index\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}.bam\"), emit: bam\n\n    script:\n    \"\"\"\n    bwa mem -t ${task.cpus} reference.fasta ${reads} | \\\\\n    samtools sort -@ ${task.cpus} -o ${sample_id}.bam -\n    samtools index ${sample_id}.bam\n    \"\"\"\n}\n\nprocess BCFTOOLS_MPILEUP {\n    tag \"$sample_id\"\n    container 'biocontainers/bcftools:v1.15.1_cv1'\n    publishDir \"${params.outdir}/variants\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(bam)\n    path reference\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}.vcf.gz\"), emit: vcf\n\n    script:\n    \"\"\"\n    bcftools mpileup -f ${reference} ${bam} | \\\\\n    bcftools call -mv -Oz -o ${sample_id}.vcf.gz\n    bcftools index ${sample_id}.vcf.gz\n    \"\"\"\n}\n\nprocess TB_PROFILER {\n    tag \"$sample_id\"\n    container 'quay.io/biocontainers/tb-profiler:4.4.0--pyhdfd78af_0'\n    publishDir \"${params.outdir}/tb_profiler\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"${sample_id}.*\", emit: results\n\n    script:\n    \"\"\"\n    tb-profiler profile -1 ${reads[0]} -2 ${reads[1]} -p ${sample_id} --txt\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day7/#part-2-genome-assembly-workflows","title":"Part 2: Genome Assembly Workflows","text":""},{"location":"modules/day7/#de-novo-assembly-pipeline-development","title":"De Novo Assembly Pipeline Development","text":"<p>Genome assembly is crucial for discovering new variants, understanding genome structure, and analyzing organisms without reference genomes.</p>"},{"location":"modules/day7/#exercise-3-bacterial-genome-assembly-pipeline-45-minutes","title":"Exercise 3: Bacterial Genome Assembly Pipeline (45 minutes)","text":"<p>Objective: Build a complete de novo assembly workflow</p> <p>Create <code>genome_assembly.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\nnextflow.enable.dsl=2\n\n// Process definitions\nprocess FASTQC {\n    tag \"$sample_id\"\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*_fastqc.{html,zip}\")\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n\nprocess TRIMMOMATIC {\n    tag \"$sample_id\"\n    container 'biocontainers/trimmomatic:v0.39-1'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*_paired.fastq.gz\")\n\n    script:\n    \"\"\"\n    trimmomatic PE ${reads[0]} ${reads[1]} \\\n        ${sample_id}_R1_paired.fastq.gz ${sample_id}_R1_unpaired.fastq.gz \\\n        ${sample_id}_R2_paired.fastq.gz ${sample_id}_R2_unpaired.fastq.gz \\\n        ILLUMINACLIP:adapters.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n    \"\"\"\n}\n\nprocess SPADES {\n    tag \"$sample_id\"\n    container 'biocontainers/spades:v3.15.3-1'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_assembly.fasta\")\n\n    script:\n    \"\"\"\n    spades.py --careful -1 ${reads[0]} -2 ${reads[1]} -o spades_output\n    cp spades_output/scaffolds.fasta ${sample_id}_assembly.fasta\n    \"\"\"\n}\n\nprocess QUAST {\n    tag \"$sample_id\"\n    container 'biocontainers/quast:v5.0.2-1'\n    publishDir \"${params.outdir}/quast\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(assembly)\n\n    output:\n    path \"${sample_id}_quast\"\n\n    script:\n    \"\"\"\n    quast.py ${assembly} -o ${sample_id}_quast\n    \"\"\"\n}\n\nprocess PROKKA {\n    tag \"$sample_id\"\n    container 'biocontainers/prokka:v1.14.6-1'\n    publishDir \"${params.outdir}/annotations\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(assembly)\n\n    output:\n    path \"${sample_id}_annotation\"\n\n    script:\n    \"\"\"\n    prokka ${assembly} --outdir ${sample_id}_annotation --prefix ${sample_id} \\\n        --kingdom Bacteria --cpus ${task.cpus}\n    \"\"\"\n}\n\n// Workflow definition\nworkflow {\n    // Input data\n    reads_ch = Channel.fromFilePairs(params.reads, checkIfExists: true)\n\n    // QC and trimming\n    FASTQC(reads_ch)\n    TRIMMOMATIC(reads_ch)\n\n    // Assembly\n    SPADES(TRIMMOMATIC.out)\n\n    // Quality assessment\n    QUAST(SPADES.out)\n\n    // Annotation\n    PROKKA(SPADES.out)\n}\n</code></pre>"},{"location":"modules/day7/#part-3-advanced-topics-optimization-deployment","title":"Part 3: Advanced Topics - Optimization &amp; Deployment","text":""},{"location":"modules/day7/#advanced-nextflow-features_1","title":"Advanced Nextflow Features","text":""},{"location":"modules/day7/#modules-and-subworkflows","title":"Modules and Subworkflows","text":"<p>Nextflow modules allow you to organize code into reusable components:</p> <pre><code>// modules/fastqc.nf\nprocess FASTQC {\n    tag \"$meta.id\"\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n</code></pre> <p>Include modules in your main workflow:</p> <pre><code>// main.nf\ninclude { FASTQC } from './modules/fastqc'\n\nworkflow {\n    reads_ch = Channel.fromFilePairs(params.reads)\n    FASTQC(reads_ch)\n}\n</code></pre>"},{"location":"modules/day7/#advanced-channel-operators","title":"Advanced Channel Operators","text":"<pre><code>workflow {\n    // Combine channels\n    reads_ch = Channel.fromFilePairs(params.reads)\n    reference_ch = Channel.fromPath(params.reference)\n\n    // Cross product - each read pair with each reference\n    combined_ch = reads_ch.combine(reference_ch)\n\n    // Join channels by key\n    metadata_ch = Channel.from([\n        ['sample1', 'lineage1'],\n        ['sample2', 'lineage2']\n    ])\n\n    reads_with_meta = reads_ch.join(metadata_ch)\n\n    // Filter channels\n    large_files = reads_ch.filter { sample, files -&gt;\n        files[0].size() &gt; 1000000\n    }\n\n    // Transform data\n    sample_names = reads_ch.map { sample, files -&gt; sample }\n}\n</code></pre>"},{"location":"modules/day7/#exercise-4-performance-optimization-30-minutes","title":"Exercise 4: Performance Optimization (30 minutes)","text":"<p>Objective: Optimize pipeline performance and resource usage</p> <p>Create <code>nextflow.config</code> with optimized settings:</p> <pre><code>// nextflow.config\nparams {\n    // Input parameters\n    reads = \"data/*_R{1,2}.fastq.gz\"\n    outdir = \"results\"\n\n    // Resource defaults\n    max_cpus = 16\n    max_memory = '64.GB'\n    max_time = '24.h'\n}\n\n// Profile configurations\nprofiles {\n    local {\n        process.executor = 'local'\n        process.cpus = 4\n        process.memory = '8.GB'\n        docker.enabled = true\n    }\n\n    slurm {\n        process.executor = 'slurm'\n        process.queue = 'batch'\n        process.cpus = 16\n        process.memory = '32.GB'\n        singularity.enabled = true\n    }\n\n    cloud {\n        process.executor = 'awsbatch'\n        aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n        aws.region = 'us-west-2'\n    }\n}\n\n// Process-specific resource allocation\nprocess {\n    // Default resources\n    cpus = 2\n    memory = 4.GB\n    time = '1.hour'\n\n    // Process-specific optimization\n    withName: SPADES {\n        cpus = { 8 * task.attempt }\n        memory = { 16.GB * task.attempt }\n        time = { 4.hour * task.attempt }\n        errorStrategy = 'retry'\n        maxRetries = 3\n    }\n\n    withName: BWA_MEM {\n        cpus = 8\n        memory = 16.GB\n        time = 2.hour\n    }\n\n    withName: FASTQC {\n        cpus = 1\n        memory = 2.GB\n        time = 30.min\n    }\n}\n</code></pre> <pre><code>// nextflow.config\nparams {\n    // Input parameters\n    reads = \"data/*_R{1,2}_001.fastq.gz\"\n    outdir = \"results\"\n\n    // Resource defaults\n    max_cpus = 16\n    max_memory = '64.GB'\n    max_time = '24.h'\n}\n\n// Profile configurations\nprofiles {\n    local {\n        process.executor = 'local'\n        process.cpus = 4\n        process.memory = '8.GB'\n        docker.enabled = true\n    }\n\n    slurm {\n        process.executor = 'slurm'\n        process.queue = 'batch'\n        process.cpus = 16\n        process.memory = '32.GB'\n        singularity.enabled = true\n    }\n\n    conda {\n        conda.enabled = true\n        process.conda = 'envs/microbial_genomics.yml'\n    }\n}\n\n// Process-specific configurations\nprocess {\n    withName: SPADES {\n        cpus = 8\n        memory = '16.GB'\n        time = '4.h'\n    }\n\n    withName: PROKKA {\n        cpus = 4\n        memory = '8.GB'\n        time = '2.h'\n    }\n}\n</code></pre>"},{"location":"modules/day7/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day7/#nextflow-language-features","title":"Nextflow Language Features","text":"<ul> <li>Processes: Encapsulated tasks with inputs, outputs, and script</li> <li>Channels: Asynchronous data streams connecting processes</li> <li>Operators: Transform and manipulate channel data</li> <li>Workflows: Orchestrate process execution and data flow</li> </ul>"},{"location":"modules/day7/#best-practices","title":"Best Practices","text":"<ul> <li>Modularity: Break complex workflows into reusable components</li> <li>Parameterization: Make pipelines configurable and flexible</li> <li>Error handling: Implement robust error recovery mechanisms</li> <li>Testing: Validate pipeline components and outputs</li> <li>Documentation: Clear usage instructions and examples</li> </ul>"},{"location":"modules/day7/#configuration-management","title":"Configuration Management","text":"<pre><code>// Environment-specific settings\nprofiles {\n    standard {\n        process.container = 'ubuntu:20.04'\n    }\n    docker {\n        docker.enabled = true\n        docker.runOptions = '-u $(id -u):$(id -g)'\n    }\n    singularity {\n        singularity.enabled = true\n        singularity.autoMounts = true\n    }\n}\n</code></pre>"},{"location":"modules/day7/#pipeline-development-workflow","title":"Pipeline Development Workflow","text":""},{"location":"modules/day7/#1-planning-phase","title":"1. Planning Phase","text":"<ul> <li>Define pipeline objectives and scope</li> <li>Identify required tools and dependencies</li> <li>Design modular architecture</li> <li>Plan testing and validation strategy</li> </ul>"},{"location":"modules/day7/#2-development-phase","title":"2. Development Phase","text":"<ul> <li>Create process definitions for each step</li> <li>Implement workflow logic and data flow</li> <li>Add configuration and parameterization</li> <li>Include error handling and logging</li> </ul>"},{"location":"modules/day7/#3-testing-phase","title":"3. Testing Phase","text":"<ul> <li>Test individual processes with sample data</li> <li>Validate complete workflow execution</li> <li>Performance testing and optimization</li> <li>Edge case and error condition testing</li> </ul>"},{"location":"modules/day7/#4-deployment-phase","title":"4. Deployment Phase","text":"<ul> <li>Containerize tools and dependencies</li> <li>Create user documentation</li> <li>Set up continuous integration</li> <li>Community sharing and feedback</li> </ul>"},{"location":"modules/day7/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day7/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Create simple Nextflow process for bioinformatics tool</li> <li>Develop multi-step pipeline with proper data flow</li> <li>Configure pipeline for different execution environments</li> <li>Test pipeline with provided datasets</li> </ul>"},{"location":"modules/day7/#group-exercise","title":"Group Exercise","text":"<ul> <li>Collaborate on pipeline feature development</li> <li>Review and optimize pipeline code</li> <li>Share experiences with different configuration profiles</li> <li>Troubleshoot common pipeline issues</li> </ul>"},{"location":"modules/day7/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day7/#resource-management","title":"Resource Management","text":"<pre><code>// Dynamic resource allocation\nprocess ASSEMBLY {\n    memory { 4.GB * task.attempt }\n    time { 2.h * task.attempt }\n    errorStrategy 'retry'\n    maxRetries 3\n\n    script:\n    \"\"\"\n    spades.py --memory ${task.memory.toGiga()} -t ${task.cpus} ...\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day7/#data-staging-issues","title":"Data Staging Issues","text":"<pre><code>// Proper input handling\nprocess ANALYSIS {\n    stageInMode 'copy'  // Ensure file availability\n    scratch true        // Use local scratch space\n\n    input:\n    path(large_file)\n\n    script:\n    \"\"\"\n    # Process large file locally\n    analysis_tool ${large_file}\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day7/#container-compatibility","title":"Container Compatibility","text":"<pre><code>// Container-specific configurations\nprocess {\n    withName: SPECIAL_TOOL {\n        container = 'custom/special-tool:v1.0'\n        containerOptions = '--user root'\n    }\n}\n</code></pre>"},{"location":"modules/day7/#resources","title":"Resources","text":""},{"location":"modules/day7/#documentation","title":"Documentation","text":"<ul> <li>Nextflow Documentation</li> <li>nf-core Website</li> <li>Nextflow Patterns</li> </ul>"},{"location":"modules/day7/#training-materials","title":"Training Materials","text":"<ul> <li>Nextflow Training</li> <li>nf-core Bytesize Talks</li> <li>Seqera Community Forum</li> </ul>"},{"location":"modules/day7/#development-tools","title":"Development Tools","text":"<ul> <li>nf-core tools</li> <li>Nextflow VS Code Extension</li> <li>nf-test Framework</li> </ul>"},{"location":"modules/day7/#looking-ahead","title":"Looking Ahead","text":"<p>Course Completion: You now have the skills to:</p> <ul> <li>Develop reproducible bioinformatics workflows for clinical and research applications</li> <li>Apply quality control and analysis methods to microbial genomics data</li> <li>Use version control for collaborative development and sharing</li> <li>Deploy pipelines across different platforms (local, HPC, cloud)</li> <li>Optimize performance and troubleshoot complex pipeline issues</li> </ul>"},{"location":"modules/day7/#next-steps-for-your-research","title":"Next Steps for Your Research","text":"<ul> <li>Apply to Your Data: Adapt these pipelines to your specific research questions</li> <li>Join the Community: Contribute to nf-core and share your pipelines</li> <li>Continue Learning: Explore advanced Nextflow features and cloud deployment</li> <li>Collaborate: Use version control and sharing practices for team projects</li> </ul>"},{"location":"modules/day7/#real-world-applications","title":"Real-world Applications","text":"<p>The skills you've learned enable you to:</p> <ul> <li>Clinical Genomics: Develop diagnostic pipelines for pathogen identification</li> <li>Outbreak Investigation: Build transmission analysis workflows</li> <li>Research Collaboration: Share reproducible analysis methods</li> <li>High-throughput Analysis: Scale pipelines for large datasets</li> <li>Multi-platform Deployment: Run pipelines from laptop to cloud</li> </ul> <p>Key Learning Outcome: You now have the expertise to develop, optimize, and deploy production-ready bioinformatics pipelines that can scale from laptop to cloud, enabling reproducible and collaborative genomics research that can make a real impact in clinical and public health applications.</p>"},{"location":"modules/day8/","title":"Day 8 - Comparative Genomics","text":""},{"location":"modules/day8/#day-8-comparative-genomics","title":"Day 8: Comparative Genomics","text":"<p>Date: September 10, 2025 Duration: 09:00-13:00 CAT Focus: Pan-genome analysis, phylogenetic inference, tree construction and visualization</p>"},{"location":"modules/day8/#overview","title":"Overview","text":"<p>Day 8 focuses on comparative genomics approaches for understanding microbial diversity and evolutionary relationships. We'll explore pangenome analysis to understand core and accessory gene content, and phylogenomic methods to infer evolutionary relationships from genomic data, including SNP-based phylogeny and tree visualization techniques.</p>"},{"location":"modules/day8/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 8, you will be able to:</p> <ul> <li>Understand pangenome concepts and perform core/accessory genome analysis</li> <li>Identify conserved and variable genomic regions across strains</li> <li>Construct phylogenetic trees from core genome SNPs</li> <li>Visualize and interpret phylogenomic relationships</li> <li>Apply comparative genomics to understand pathogen evolution</li> <li>Use tools like Roary, Panaroo, and IQ-TREE for comparative analysis</li> </ul>"},{"location":"modules/day8/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Pangenomics Arash Iranzadeh 10:30 Phylogenomics: Inferring evolutionary relationships from core SNPs Arash Iranzadeh 11:30 Break 12:00 Phylogenomics: Tree construction and visualisation Arash Iranzadeh"},{"location":"modules/day8/#key-topics","title":"Key Topics","text":"<ul> <li>Core vs accessory genome concepts</li> <li>Gene presence/absence analysis</li> <li>Population structure assessment</li> <li>Functional annotation of variable regions</li> </ul>"},{"location":"modules/day8/#2-phylogenetic-analysis","title":"2. Phylogenetic Analysis","text":"<ul> <li>Maximum likelihood methods</li> <li>Bootstrap support assessment</li> <li>Root placement and outgroup selection</li> <li>Molecular clock analysis</li> </ul>"},{"location":"modules/day8/#3-snp-based-analysis","title":"3. SNP-based Analysis","text":"<ul> <li>Core genome SNP calling</li> <li>Recombination detection and removal</li> <li>Distance matrix construction</li> <li>Transmission cluster identification</li> </ul>"},{"location":"modules/day8/#4-outbreak-investigation","title":"4. Outbreak Investigation","text":"<ul> <li>Epidemiological data integration</li> <li>Transmission network inference</li> <li>Source attribution methods</li> <li>Temporal analysis of spread</li> <li></li> </ul>"},{"location":"modules/day8/#1-advanced-pipeline-architecture","title":"1. Advanced Pipeline Architecture","text":"<ul> <li>Multi-sample processing strategies</li> <li>Conditional execution and branching</li> <li>Pipeline modularity and reusability</li> <li>Configuration management across environments</li> </ul>"},{"location":"modules/day8/#2-testing-and-validation","title":"2. Testing and Validation","text":"<ul> <li>Unit testing for individual processes</li> <li>Integration testing for complete workflows</li> <li>Continuous integration setup</li> <li>Regression testing strategies</li> </ul>"},{"location":"modules/day8/#3-performance-optimization","title":"3. Performance Optimization","text":"<ul> <li>Resource allocation strategies</li> <li>Parallelization patterns</li> <li>Caching and resume functionality</li> <li>Profile-based optimization</li> </ul>"},{"location":"modules/day8/#4-production-deployment","title":"4. Production Deployment","text":"<ul> <li>Environment-specific configurations</li> <li>Error handling and retry strategies</li> <li>Logging and monitoring</li> <li>Version control and release management</li> </ul>"},{"location":"modules/day8/#advanced-tools","title":"Advanced Tools","text":""},{"location":"modules/day8/#testing-frameworks","title":"Testing Frameworks","text":"<ul> <li>nf-test - Modern testing framework for Nextflow</li> <li>pytest-workflow - Python-based workflow testing</li> <li>Nextflow Tower - Pipeline monitoring and management</li> </ul>"},{"location":"modules/day8/#development-tools","title":"Development Tools","text":"<ul> <li>nf-core lint - Code quality checking</li> <li>pre-commit hooks - Automated code validation</li> <li>GitHub Actions - Continuous integration</li> <li>Nextflow plugins - Extended functionality</li> </ul>"},{"location":"modules/day8/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day8/#exercise-1-multi-sample-pipeline-75-minutes","title":"Exercise 1: Multi-Sample Pipeline (75 minutes)","text":"<p>Develop a comprehensive pipeline that processes multiple samples in parallel.</p> <pre><code>#!/usr/bin/env nextflow\n\nnextflow.enable.dsl=2\n\n// Parameter definitions with validation\nparams.input_dir = null\nparams.outdir = \"results\"\nparams.reference = null\nparams.min_quality = 20\nparams.threads = 4\n\n// Input validation\nif (!params.input_dir) {\n    error \"Please provide input directory with --input_dir\"\n}\nif (!params.reference) {\n    error \"Please provide reference genome with --reference\"\n}\n\n// Include modules from separate files\ninclude { FASTQC } from './modules/fastqc.nf'\ninclude { TRIMMOMATIC } from './modules/trimmomatic.nf'\ninclude { BWA_MEM } from './modules/bwa.nf'\ninclude { VARIANT_CALLING } from './modules/variants.nf'\ninclude { MULTIQC } from './modules/multiqc.nf'\n\nworkflow VARIANT_ANALYSIS {\n    take:\n    reads_ch\n    reference\n\n    main:\n    // Quality control\n    FASTQC(reads_ch)\n\n    // Read trimming\n    TRIMMOMATIC(reads_ch)\n\n    // Alignment\n    BWA_MEM(TRIMMOMATIC.out.trimmed, reference)\n\n    // Variant calling\n    VARIANT_CALLING(BWA_MEM.out.bam, reference)\n\n    // Aggregate QC\n    qc_files = FASTQC.out.html.mix(TRIMMOMATIC.out.log).collect()\n    MULTIQC(qc_files)\n\n    emit:\n    variants = VARIANT_CALLING.out.vcf\n    reports = MULTIQC.out.html\n}\n\n// Main workflow\nworkflow {\n    // Create channel from input directory\n    reads_ch = Channel\n        .fromFilePairs(\"${params.input_dir}/*_R{1,2}_001.fastq.gz\")\n        .ifEmpty { error \"No input files found in ${params.input_dir}\" }\n\n    // Load reference\n    reference_ch = Channel.fromPath(params.reference, checkIfExists: true)\n\n    // Run analysis\n    VARIANT_ANALYSIS(reads_ch, reference_ch)\n\n    // Output summary\n    VARIANT_ANALYSIS.out.variants.view { \"Variants called for: ${it[0]}\" }\n}\n\nworkflow.onComplete {\n    log.info \"\"\"\n    Pipeline completed at: ${workflow.complete}\n    Duration: ${workflow.duration}\n    Success: ${workflow.success}\n    Results: ${params.outdir}\n    \"\"\".stripIndent()\n}\n</code></pre>"},{"location":"modules/day8/#exercise-2-pipeline-testing-60-minutes","title":"Exercise 2: Pipeline Testing (60 minutes)","text":"<p>Implement comprehensive testing for the variant calling pipeline.</p> <pre><code># .github/workflows/test.yml\nname: Pipeline Testing\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        nextflow: ['23.04.0', 'latest']\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Setup Nextflow\n      uses: nf-core/setup-nextflow@v1\n      with:\n        version: ${{ matrix.nextflow }}\n\n    - name: Setup Test Data\n      run: |\n        mkdir test_data\n        wget -O test_data/sample_R1.fastq.gz https://example.com/test_R1.fastq.gz\n        wget -O test_data/sample_R2.fastq.gz https://example.com/test_R2.fastq.gz\n        wget -O test_data/reference.fasta https://example.com/reference.fasta\n\n    - name: Run Pipeline Tests\n      run: |\n        nextflow run main.nf \\\n          --input_dir test_data \\\n          --reference test_data/reference.fasta \\\n          --outdir test_results \\\n          -profile test,docker\n\n    - name: Validate Outputs\n      run: |\n        # Check if expected output files exist\n        test -f test_results/variants/*.vcf\n        test -f test_results/multiqc/multiqc_report.html\n\n        # Validate variant file format\n        bcftools view test_results/variants/*.vcf | head -20\n</code></pre>"},{"location":"modules/day8/#exercise-3-performance-optimization-45-minutes","title":"Exercise 3: Performance Optimization (45 minutes)","text":"<p>Optimize pipeline performance through profiling and resource tuning.</p> <pre><code>// Performance optimized configuration\nprocess {\n    // Default resources\n    cpus = 2\n    memory = 4.GB\n    time = '1.hour'\n\n    // Process-specific optimization\n    withName: BWA_MEM {\n        cpus = { 8 * task.attempt }\n        memory = { 16.GB * task.attempt }\n        time = { 4.hour * task.attempt }\n        errorStrategy = 'retry'\n        maxRetries = 3\n    }\n\n    withName: VARIANT_CALLING {\n        cpus = 4\n        memory = { 8.GB + (2.GB * task.attempt) }\n        time = { 2.hour * task.attempt }\n\n        // Use faster local storage when available\n        scratch = '/tmp'\n    }\n\n    withName: FASTQC {\n        // Lightweight process can use minimal resources\n        cpus = 1\n        memory = 2.GB\n        time = 30.min\n    }\n}\n\n// Profile-specific optimizations\nprofiles {\n    standard {\n        process.executor = 'local'\n        process.cpus = 2\n        process.memory = '4 GB'\n    }\n\n    hpc {\n        process.executor = 'slurm'\n        process.queue = 'compute'\n        process.clusterOptions = '--account=genomics --qos=normal'\n\n        // Optimize for cluster environment\n        process {\n            withName: BWA_MEM {\n                cpus = 16\n                memory = 32.GB\n                time = 2.hour\n            }\n        }\n    }\n\n    cloud {\n        process.executor = 'awsbatch'\n        aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n        aws.region = 'us-west-2'\n\n        // Cloud-specific optimizations\n        process {\n            withName: '.*' {\n                container = 'your-ecr-repo/pipeline:latest'\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"modules/day8/#advanced-concepts","title":"Advanced Concepts","text":""},{"location":"modules/day8/#error-handling-strategies","title":"Error Handling Strategies","text":"<pre><code>process ROBUST_ASSEMBLY {\n    errorStrategy 'retry'\n    maxRetries 3\n\n    // Dynamic resource allocation\n    memory { 8.GB * task.attempt }\n    cpus { 4 * task.attempt }\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_assembly.fasta\"), emit: assembly\n    tuple val(sample_id), path(\"${sample_id}_assembly.log\"), emit: log\n\n    script:\n    \"\"\"\n    # Log system information for debugging\n    echo \"Hostname: \\$(hostname)\" &gt; ${sample_id}_assembly.log\n    echo \"Memory: ${task.memory}\" &gt;&gt; ${sample_id}_assembly.log\n    echo \"CPUs: ${task.cpus}\" &gt;&gt; ${sample_id}_assembly.log\n    echo \"Attempt: ${task.attempt}\" &gt;&gt; ${sample_id}_assembly.log\n\n    # Run assembly with error checking\n    spades.py --careful -1 ${reads[0]} -2 ${reads[1]} \\\n        -o spades_out --threads ${task.cpus} --memory ${task.memory.toGiga()} \\\n        2&gt;&amp;1 | tee -a ${sample_id}_assembly.log\n\n    if [ ! -f spades_out/scaffolds.fasta ]; then\n        echo \"ERROR: Assembly failed\" &gt;&gt; ${sample_id}_assembly.log\n        exit 1\n    fi\n\n    cp spades_out/scaffolds.fasta ${sample_id}_assembly.fasta\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day8/#conditional-workflows","title":"Conditional Workflows","text":"<pre><code>workflow ADAPTIVE_ANALYSIS {\n    take:\n    samples\n\n    main:\n    // Initial QC\n    FASTQC(samples)\n\n    // Conditional trimming based on quality\n    samples\n        .join(FASTQC.out.stats)\n        .branch { sample_id, reads, qc_stats -&gt;\n            high_quality: qc_stats.mean_quality &gt; 30\n            needs_trimming: qc_stats.mean_quality &lt;= 30\n        }\n        .set { qc_branched }\n\n    // Process high quality samples directly\n    high_qual_samples = qc_branched.high_quality.map { sample_id, reads, stats -&gt; [sample_id, reads] }\n\n    // Trim lower quality samples\n    TRIMMOMATIC(qc_branched.needs_trimming.map { sample_id, reads, stats -&gt; [sample_id, reads] })\n\n    // Combine processed samples\n    all_samples = high_qual_samples.mix(TRIMMOMATIC.out.trimmed)\n\n    // Continue with assembly\n    SPADES(all_samples)\n\n    emit:\n    assemblies = SPADES.out.assembly\n}\n</code></pre>"},{"location":"modules/day8/#best-practices","title":"Best Practices","text":""},{"location":"modules/day8/#1-code-organization","title":"1. Code Organization","text":"<ul> <li>Use modules for reusable processes</li> <li>Implement clear naming conventions</li> <li>Document complex logic</li> <li>Version control all configurations</li> </ul>"},{"location":"modules/day8/#2-resource-management","title":"2. Resource Management","text":"<pre><code>// Dynamic resource allocation\nprocess {\n    withLabel: 'high_memory' {\n        memory = { 32.GB * task.attempt }\n        errorStrategy = 'retry'\n        maxRetries = 2\n    }\n\n    withLabel: 'cpu_intensive' {\n        cpus = { Math.min(16, task.attempt * 4) }\n        time = { 4.hour * task.attempt }\n    }\n}\n</code></pre>"},{"location":"modules/day8/#3-monitoring-and-debugging","title":"3. Monitoring and Debugging","text":"<pre><code>// Enable comprehensive reporting\ntrace {\n    enabled = true\n    file = \"${params.outdir}/trace.txt\"\n    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes,vol_ctxt,inv_ctxt'\n}\n\nreport {\n    enabled = true\n    file = \"${params.outdir}/report.html\"\n}\n\ntimeline {\n    enabled = true\n    file = \"${params.outdir}/timeline.html\"\n}\n\ndag {\n    enabled = true\n    file = \"${params.outdir}/dag.html\"\n}\n</code></pre>"},{"location":"modules/day8/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day8/#individual-projects","title":"Individual Projects","text":"<ul> <li>Optimize existing pipeline for specific use case</li> <li>Implement comprehensive error handling</li> <li>Create test suite for pipeline validation</li> <li>Deploy pipeline to different computing environment</li> </ul>"},{"location":"modules/day8/#group-collaboration","title":"Group Collaboration","text":"<ul> <li>Code review session for pipeline improvements</li> <li>Troubleshooting complex pipeline failures</li> <li>Sharing optimization strategies</li> <li>Planning production deployment</li> </ul>"},{"location":"modules/day8/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day8/#memory-management","title":"Memory Management","text":"<pre><code>// Handle large datasets efficiently\nprocess LARGE_DATA_PROCESSING {\n    memory { task.attempt &lt; 3 ? 16.GB : 32.GB }\n    time { 2.hour * task.attempt }\n    errorStrategy 'retry'\n    maxRetries 3\n\n    // Use streaming where possible\n    script:\n    \"\"\"\n    # Process data in chunks to manage memory\n    split -l 1000000 ${large_input} chunk_\n\n    for chunk in chunk_*; do\n        process_chunk.py \\$chunk &gt;&gt; results.txt\n        rm \\$chunk  # Clean up as we go\n    done\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day8/#workflow-resume-issues","title":"Workflow Resume Issues","text":"<pre><code># Best practices for resumable workflows\nnextflow run pipeline.nf -resume -with-report report.html\n\n# Clean resume when needed\nnextflow clean -f\nrm -rf work/\n</code></pre>"},{"location":"modules/day8/#container-compatibility","title":"Container Compatibility","text":"<pre><code>process {\n    withName: PROBLEMATIC_TOOL {\n        container = 'custom/fixed-tool:v2.0'\n        containerOptions = '--user root --privileged'\n    }\n}\n</code></pre>"},{"location":"modules/day8/#production-deployment","title":"Production Deployment","text":""},{"location":"modules/day8/#environment-setup","title":"Environment Setup","text":"<pre><code># production.config\nprocess {\n    executor = 'slurm'\n    queue = 'production'\n\n    // Production-level resource allocation\n    cpus = 16\n    memory = '64 GB'\n    time = '12 hours'\n\n    // Enhanced error handling\n    errorStrategy = 'terminate'  // Fail fast in production\n    maxRetries = 1\n}\n\n// Enable comprehensive logging\ntrace.enabled = true\nreport.enabled = true\ntimeline.enabled = true\n</code></pre>"},{"location":"modules/day8/#monitoring-setup","title":"Monitoring Setup","text":"<pre><code># Set up pipeline monitoring\nnextflow run pipeline.nf -with-tower -profile production\n\n# Custom monitoring hooks\nnextflow run pipeline.nf \\\n  --hook-url https://monitoring.example.com/webhook \\\n  --notify-on-completion \\\n  --notify-on-failure\n</code></pre>"},{"location":"modules/day8/#resources","title":"Resources","text":""},{"location":"modules/day8/#documentation","title":"Documentation","text":"<ul> <li>Nextflow Patterns</li> <li>nf-core Developer Guide</li> <li>Nextflow Tower Documentation</li> </ul>"},{"location":"modules/day8/#testing-tools","title":"Testing Tools","text":"<ul> <li>nf-test</li> <li>pytest-workflow</li> <li>Nextflow CI/CD Examples</li> </ul>"},{"location":"modules/day8/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Nextflow Performance Tips</li> <li>Resource Requirements Guide</li> </ul>"},{"location":"modules/day8/#looking-ahead","title":"Looking Ahead","text":"<p>Day 9 Preview: Bring Your Own Data session including: - Applying learned skills to participant datasets - Troubleshooting real-world challenges - Customizing pipelines for specific needs - Preparing for independent analysis</p> <p>Key Learning Outcome: Advanced Nextflow development enables creation of production-ready, scalable bioinformatics pipelines that can handle complex datasets across diverse computing environments while maintaining reproducibility and reliability.</p>"},{"location":"modules/day9/","title":"Day 9 - Bring your own data","text":""},{"location":"modules/day9/#day-9-bring-your-own-data","title":"Day 9: Bring your own data","text":"<p>Date: September 11, 2025 Duration: 09:00-13:00 CAT Focus: Mobile genetic elements in AMR, independent analysis of participant datasets</p>"},{"location":"modules/day9/#overview","title":"Overview","text":"<p>Day 9 begins with understanding the role of mobile genetic elements in AMR spread, then transitions to hands-on application of all techniques learned throughout the course. Participants will analyze their own datasets with guidance from trainers, troubleshoot real-world challenges, and develop customized analysis approaches for their specific research questions.</p>"},{"location":"modules/day9/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 9, you will be able to:</p> <ul> <li>Understand the role of plasmids, integrons, and transposons in AMR dissemination</li> <li>Apply learned bioinformatics techniques to your own research data</li> <li>Troubleshoot common issues encountered in real-world analyses</li> <li>Adapt standard protocols to meet specific research requirements</li> <li>Develop analysis strategies for novel research questions</li> <li>Integrate multiple analysis approaches into comprehensive workflows</li> <li>Plan sustainable bioinformatics practices for ongoing research</li> </ul>"},{"location":"modules/day9/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Role of plasmids, integrons, and transposons in AMR spread Ephifania Geza 10:30 Participants to analyse their own data All trainers 11:30 Break 12:00 Participants to analyse their own data All trainers"},{"location":"modules/day9/#session-structure","title":"Session Structure","text":""},{"location":"modules/day9/#mobile-genetic-elements-session-0900-1030","title":"Mobile Genetic Elements Session (09:00-10:30)","text":"<ul> <li>Understanding plasmids and their role in horizontal gene transfer</li> <li>Integrons and gene cassette systems</li> <li>Transposons and insertion sequences</li> <li>Tools for mobile element detection and analysis</li> </ul>"},{"location":"modules/day9/#opening-data-analysis-session-1030-1045","title":"Opening Data Analysis Session (10:30-10:45)","text":"<ul> <li>Brief recap of key course concepts</li> <li>Overview of available support and resources</li> <li>Formation of analysis groups based on data types</li> <li>Technical setup verification</li> </ul>"},{"location":"modules/day9/#individual-analysis-time-0915-1130","title":"Individual Analysis Time (09:15-11:30)","text":"<ul> <li>Independent work on participant datasets</li> <li>One-on-one consultation with trainers</li> <li>Peer collaboration and knowledge sharing</li> <li>Documentation of analysis approaches</li> </ul>"},{"location":"modules/day9/#progress-sharing-1145-1200","title":"Progress Sharing (11:45-12:00)","text":"<ul> <li>Brief presentations of initial findings</li> <li>Discussion of challenges encountered</li> <li>Sharing of successful analysis strategies</li> </ul>"},{"location":"modules/day9/#advanced-analysis-1200-1300","title":"Advanced Analysis (12:00-13:00)","text":"<ul> <li>Continued independent work</li> <li>Focus on complex analyses or troubleshooting</li> <li>Preparation for Day 10 presentations</li> <li>Final consultations with trainers</li> </ul>"},{"location":"modules/day9/#data-types-and-analysis-approaches","title":"Data Types and Analysis Approaches","text":""},{"location":"modules/day9/#genomic-data","title":"Genomic Data","text":"<p>Common analyses for participants with genomic datasets:</p> <ul> <li>Quality control and preprocessing</li> <li>FastQC assessment and interpretation</li> <li>Adapter trimming and quality filtering</li> <li> <p>Species identification and contamination detection</p> </li> <li> <p>Genome assembly and annotation</p> </li> <li>De novo assembly optimization</li> <li>Assembly quality assessment</li> <li> <p>Functional annotation and gene prediction</p> </li> <li> <p>Comparative genomics</p> </li> <li>MLST and serotyping</li> <li>Antimicrobial resistance gene detection</li> <li>Phylogenetic analysis and clustering</li> </ul>"},{"location":"modules/day9/#metagenomic-data","title":"Metagenomic Data","text":"<p>For participants with microbiome or metagenomic samples:</p> <ul> <li>Community profiling</li> <li>Taxonomic classification</li> <li>Abundance estimation and normalization</li> <li> <p>Diversity analysis (alpha and beta)</p> </li> <li> <p>Functional analysis</p> </li> <li>Pathway reconstruction</li> <li>Antimicrobial resistance profiling</li> <li> <p>Metabolic potential assessment</p> </li> <li> <p>Clinical applications</p> </li> <li>Pathogen detection in complex samples</li> <li>Co-infection analysis</li> <li>Treatment response monitoring</li> </ul>"},{"location":"modules/day9/#outbreak-investigation-data","title":"Outbreak Investigation Data","text":"<p>For epidemiological and outbreak datasets:</p> <ul> <li>Transmission analysis</li> <li>SNP-based clustering</li> <li>Phylogenetic reconstruction</li> <li> <p>Temporal and geographic analysis</p> </li> <li> <p>Resistance surveillance</p> </li> <li>Multi-drug resistance patterns</li> <li>Resistance gene distribution</li> <li>Treatment outcome correlations</li> </ul>"},{"location":"modules/day9/#technical-support-available","title":"Technical Support Available","text":""},{"location":"modules/day9/#computational-resources","title":"Computational Resources","text":"<ul> <li>Access to high-performance computing cluster</li> <li>Pre-installed bioinformatics software environments</li> <li>Container images for reproducible analysis</li> <li>Shared storage for large datasets</li> </ul>"},{"location":"modules/day9/#analysis-pipelines","title":"Analysis Pipelines","text":"<ul> <li>Nextflow workflows developed during the course</li> <li>Customizable analysis templates</li> <li>Pre-configured environment profiles</li> <li>Automated reporting tools</li> </ul>"},{"location":"modules/day9/#expert-guidance","title":"Expert Guidance","text":"<p>Trainer specializations available:</p> Trainer Expertise Areas Ephifania Geza Genomic surveillance, AMR analysis, metagenomics, clinical applications Arash Iranzadeh Phylogenomics, comparative genomics, outbreak investigation Sindiswa Lukhele Sequencing technologies, quality control, species identification Mamana Mbiyavanga Workflow development, HPC systems, pipeline optimization"},{"location":"modules/day9/#common-analysis-workflows","title":"Common Analysis Workflows","text":""},{"location":"modules/day9/#genomic-surveillance-workflow","title":"Genomic Surveillance Workflow","text":"<pre><code># 1. Initial data assessment\nfastqc raw_data/*.fastq.gz\nmultiqc fastqc_results/\n\n# 2. Species identification\nkraken2 --db minikraken2_v2 --paired sample_R1.fastq sample_R2.fastq\n\n# 3. Quality trimming\ntrimmomatic PE sample_R1.fastq sample_R2.fastq \\\n    sample_R1_trimmed.fastq sample_R1_unpaired.fastq \\\n    sample_R2_trimmed.fastq sample_R2_unpaired.fastq \\\n    SLIDINGWINDOW:4:20 MINLEN:50\n\n# 4. Assembly\nspades.py -1 sample_R1_trimmed.fastq -2 sample_R2_trimmed.fastq -o assembly/\n\n# 5. Assembly quality assessment\nquast.py assembly/scaffolds.fasta -o quast_results/\n\n# 6. Annotation\nprokka assembly/scaffolds.fasta --outdir annotation/ --prefix sample\n</code></pre>"},{"location":"modules/day9/#metagenomic-analysis-workflow","title":"Metagenomic Analysis Workflow","text":"<pre><code># 1. Host DNA removal (if applicable)\nkneaddata --input sample_R1.fastq --input sample_R2.fastq \\\n    --reference-db human_genome --output cleaned_data/\n\n# 2. Taxonomic profiling\nmetaphlan cleaned_data/sample_paired_1.fastq,cleaned_data/sample_paired_2.fastq \\\n    --bowtie2out sample.bowtie2.bz2 --nproc 4 --input_type fastq \\\n    --output_file sample_profile.txt\n\n# 3. Functional profiling\nhumann --input cleaned_data/sample_paired.fastq \\\n    --output functional_analysis/ --nucleotide-database chocophlan \\\n    --protein-database uniref90\n\n# 4. Diversity analysis in R\nRscript diversity_analysis.R sample_profile.txt metadata.csv\n</code></pre>"},{"location":"modules/day9/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"modules/day9/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"modules/day9/#low-quality-data","title":"Low-Quality Data","text":"<pre><code># Check read quality distribution\nfastqc *.fastq.gz\n\n# Aggressive quality trimming if needed\ntrimmomatic PE input_R1.fastq input_R2.fastq \\\n    output_R1.fastq unpaired_R1.fastq \\\n    output_R2.fastq unpaired_R2.fastq \\\n    SLIDINGWINDOW:4:25 LEADING:20 TRAILING:20 MINLEN:75\n\n# Consider different assembly strategies\n# For poor quality data, try more conservative parameters\nspades.py --careful --cov-cutoff auto -1 R1.fastq -2 R2.fastq -o assembly/\n</code></pre>"},{"location":"modules/day9/#contamination-issues","title":"Contamination Issues","text":"<pre><code># Check for contamination\nkraken2 --db standard --paired sample_R1.fastq sample_R2.fastq \\\n    --report contamination_report.txt\n\n# Remove contaminant sequences\nextract_kraken_reads.py -k sample.kraken -s1 sample_R1.fastq \\\n    -s2 sample_R2.fastq -o clean_R1.fastq -o2 clean_R2.fastq \\\n    --exclude --taxid 9606  # Exclude human reads\n</code></pre>"},{"location":"modules/day9/#assembly-problems","title":"Assembly Problems","text":"<pre><code># If SPAdes fails, try different assemblers\n# Unicycler for hybrid assembly\nunicycler -1 short_R1.fastq -2 short_R2.fastq -l long_reads.fastq -o assembly/\n\n# Or SKESA for quick assembly\nskesa --reads short_R1.fastq,short_R2.fastq --cores 8 &gt; assembly.fasta\n</code></pre>"},{"location":"modules/day9/#memoryresource-issues","title":"Memory/Resource Issues","text":"<pre><code># Monitor resource usage\nhtop\n\n# Reduce memory usage for large datasets\nspades.py --memory 16 -1 R1.fastq -2 R2.fastq -o assembly/\n\n# Use subsampling for initial testing\nseqtk sample -s100 input_R1.fastq 100000 &gt; subset_R1.fastq\nseqtk sample -s100 input_R2.fastq 100000 &gt; subset_R2.fastq\n</code></pre>"},{"location":"modules/day9/#analysis-documentation","title":"Analysis Documentation","text":""},{"location":"modules/day9/#laboratory-notebook-template","title":"Laboratory Notebook Template","text":"<pre><code># Analysis Log: [Your Dataset Name]\n**Date**: [Current Date]\n**Analyst**: [Your Name]\n**Data Source**: [Description of samples]\n\n## Objectives\n- Primary research question\n- Specific analyses planned\n- Expected outcomes\n\n## Data Description\n- Sample type and collection method\n- Sequencing platform and parameters\n- Data quality metrics\n\n## Analysis Steps\n### Step 1: Quality Control\n- Command used: `fastqc *.fastq.gz`\n- Results: [Summary of quality metrics]\n- Decision: [Any quality filtering applied]\n\n### Step 2: [Next Analysis]\n- Command: [Exact command used]\n- Parameters chosen: [Rationale for parameter selection]\n- Results: [Key findings]\n\n## Challenges Encountered\n- Issue: [Description of problem]\n- Solution attempted: [What was tried]\n- Outcome: [Whether resolved]\n\n## Key Findings\n- [Major results from analysis]\n- [Statistical summaries]\n- [Biological interpretations]\n\n## Next Steps\n- Additional analyses needed\n- Questions raised\n- Follow-up experiments\n</code></pre>"},{"location":"modules/day9/#resource-management","title":"Resource Management","text":""},{"location":"modules/day9/#data-organization","title":"Data Organization","text":"<pre><code># Recommended directory structure\nproject_name/\n\u251c\u2500\u2500 raw_data/          # Original sequencing files\n\u251c\u2500\u2500 quality_control/   # QC reports and cleaned data\n\u251c\u2500\u2500 analysis/          # Main analysis outputs\n\u251c\u2500\u2500 scripts/           # Custom scripts and commands\n\u251c\u2500\u2500 results/           # Final results and figures\n\u2514\u2500\u2500 documentation/     # Analysis logs and notes\n</code></pre>"},{"location":"modules/day9/#backup-strategies","title":"Backup Strategies","text":"<pre><code># Regular backup of important results\nrsync -av results/ backup_drive/project_results/\ntar -czf analysis_$(date +%Y%m%d).tar.gz analysis/\n\n# Version control for scripts\ngit init\ngit add scripts/\ngit commit -m \"Initial analysis scripts\"\n</code></pre>"},{"location":"modules/day9/#collaboration-guidelines","title":"Collaboration Guidelines","text":""},{"location":"modules/day9/#peer-support","title":"Peer Support","text":"<ul> <li>Form analysis groups based on similar data types</li> <li>Share successful parameter combinations</li> <li>Collaborate on troubleshooting challenging datasets</li> <li>Review each other's analysis approaches</li> </ul>"},{"location":"modules/day9/#trainer-consultation","title":"Trainer Consultation","text":"<ul> <li>Prepare specific questions about your data</li> <li>Document issues with exact error messages</li> <li>Have your analysis objectives clearly defined</li> <li>Be ready to explain your research context</li> </ul>"},{"location":"modules/day9/#assessment-and-preparation-for-day-10","title":"Assessment and Preparation for Day 10","text":""},{"location":"modules/day9/#presentation-preparation","title":"Presentation Preparation","text":"<p>Participants should prepare a 5-minute presentation covering:</p> <ol> <li>Research Question: What you aimed to investigate</li> <li>Data Overview: Type and source of your dataset</li> <li>Methods Applied: Which course techniques you used</li> <li>Key Results: Main findings from your analysis</li> <li>Challenges: Obstacles encountered and solutions found</li> <li>Future Directions: Next steps for your research</li> </ol>"},{"location":"modules/day9/#technical-documentation","title":"Technical Documentation","text":"<ul> <li>Save all commands used in a script file</li> <li>Document parameter choices and rationale</li> <li>Prepare summary statistics and key figures</li> <li>Note any analysis limitations or assumptions</li> </ul>"},{"location":"modules/day9/#success-metrics","title":"Success Metrics","text":"<p>By the end of Day 9, participants should have:</p> <ul> <li> Successfully processed their own dataset</li> <li> Applied at least 3 different analysis techniques from the course</li> <li> Documented their analysis workflow</li> <li> Identified key findings relevant to their research</li> <li> Prepared materials for Day 10 presentation</li> <li> Established ongoing analysis plan</li> </ul>"},{"location":"modules/day9/#resources-for-continued-learning","title":"Resources for Continued Learning","text":""},{"location":"modules/day9/#online-communities","title":"Online Communities","text":"<ul> <li>Bioinformatics Stack Exchange</li> <li>BioStars Forum</li> <li>Galaxy Community</li> </ul>"},{"location":"modules/day9/#software-documentation","title":"Software Documentation","text":"<ul> <li>Tool-specific manuals and tutorials</li> <li>GitHub repositories for pipeline development</li> <li>Container registries for reproducible environments</li> </ul>"},{"location":"modules/day9/#professional-development","title":"Professional Development","text":"<ul> <li>Local bioinformatics user groups</li> <li>International conferences and workshops</li> <li>Online course platforms for advanced topics</li> </ul>"},{"location":"modules/day9/#looking-ahead","title":"Looking Ahead","text":"<p>Day 10 Preview: Wrap-up session including: - Participant presentations of analysis results - Discussion of lessons learned and best practices - Information about ongoing support resources - Course completion and next steps planning</p> <p>Key Learning Outcome: Independent application of bioinformatics skills to real research data builds confidence and reveals the practical challenges and rewards of computational biology in actual research contexts.</p>"}]}