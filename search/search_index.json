{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#microbial-genomics-metagenomics-training","title":"Microbial Genomics &amp; Metagenomics Training","text":"<p>Welcome to the comprehensive training course on Microbial Genomics &amp; Metagenomics for Clinical and Public Health Applications.</p> <ul> <li> <p> Course Overview</p> <p>Learn core principles and practical applications of microbial genomics and metagenomics in clinical and public health contexts.</p> <p> Learn more</p> </li> <li> <p> 10-Day Program</p> <p>Intensive hands-on training covering everything from command line basics to advanced outbreak investigation.</p> <p> View schedule</p> </li> <li> <p> Real-World Data</p> <p>Work with actual clinical datasets including M. tuberculosis and V. cholerae.</p> <p> Explore datasets</p> </li> <li> <p> Practical Skills</p> <p>Master bioinformatics tools, workflows, and reproducible analysis techniques used in clinical microbiology.</p> <p> See objectives</p> </li> </ul>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<p>This course provides hands-on experience with:</p>"},{"location":"#pathogen-analysis","title":"Pathogen Analysis","text":"<ul> <li>Analyze genomic diversity and evolutionary relationships</li> <li>Investigate antimicrobial resistance (AMR) profiles</li> <li>Study mobile genetic elements (MGE) in resistance spread</li> </ul>"},{"location":"#metagenomics","title":"Metagenomics","text":"<ul> <li>Explore microbial communities in clinical and environmental samples</li> <li>Apply advanced sequencing analysis techniques</li> <li>Interpret complex metagenomic datasets</li> </ul>"},{"location":"#reproducible-workflows","title":"Reproducible Workflows","text":"<ul> <li>Use version control with Git</li> <li>Work with containerized environments</li> <li>Implement Nextflow pipelines for scalable analysis</li> </ul>"},{"location":"#data-interpretation","title":"Data Interpretation","text":"<ul> <li>Generate publication-ready visualizations</li> <li>Conduct epidemiological investigations</li> <li>Present findings effectively</li> </ul>"},{"location":"#course-highlights","title":"Course Highlights","text":"<p>Interactive Learning</p> <ul> <li>Hands-on exercises with real datasets</li> <li>Group discussions and case studies  </li> <li>Individual project presentations</li> <li>Expert guest speakers</li> </ul> <p>Technical Requirements</p> <ul> <li>Laptop (Linux/macOS preferred, Git Bash for Windows)</li> <li>Basic Linux command-line knowledge</li> <li>HPC access (recommended)</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to begin your journey in microbial genomics? Start with our setup guide:</p> <p>Get Started </p>"},{"location":"#course-information","title":"Course Information","text":"<p>| Duration | 10 days (September 1-12, 2025) | | Format | Hands-on workshops with lectures | | Level | Intermediate | | Prerequisites | Basic bioinformatics knowledge |</p>"},{"location":"#support","title":"Support","text":"<p>Need help? Check our troubleshooting guide or reach out to the course instructors.</p> <ul> <li> <p> Repository</p> <p>Access all course materials, datasets, and workflows on GitHub.</p> <p> Visit repository</p> </li> <li> <p> Documentation</p> <p>Comprehensive guides, references, and additional resources.</p> <p> Browse docs</p> </li> </ul>"},{"location":"datasets/","title":"Datasets","text":""},{"location":"datasets/#training-datasets","title":"Training Datasets","text":""},{"location":"datasets/#overview","title":"Overview","text":"<p>This course uses carefully curated datasets representing real-world scenarios in microbial genomics. All data has been quality-controlled and prepared for educational use, focusing on Mycobacterium tuberculosis and Vibrio cholerae collections.</p>"},{"location":"datasets/#dataset-categories","title":"Dataset Categories","text":""},{"location":"datasets/#1-primary-training-datasets","title":"1. Primary Training Datasets","text":""},{"location":"datasets/#mycobacterium-tuberculosis-collection","title":"Mycobacterium tuberculosis Collection","text":"<ul> <li>Sample Size: 20 clinical isolates</li> <li>Geographic Origin: Global collection (South Africa, India, UK, Peru)</li> <li>Drug Resistance: Mixed MDR, XDR, and drug-susceptible strains</li> <li>Lineages: Representatives from Lineages 1-4</li> <li>Sequencing: Illumina paired-end (2\u00d7150bp, 80-120x coverage)</li> <li>Size: ~2.5 GB</li> <li>Use Cases: Drug resistance analysis, lineage typing, phylogenetic analysis</li> </ul>"},{"location":"datasets/#vibrio-cholerae-outbreak-investigation","title":"Vibrio cholerae Outbreak Investigation","text":"<ul> <li>Sample Size: 15 outbreak isolates + 3 environmental samples</li> <li>Source: Simulated cholera outbreak (based on real data)</li> <li>Timeframe: 6-month epidemic period</li> <li>Geographic: Coastal urban setting</li> <li>Sequencing: Illumina paired-end (2\u00d7150bp, 60-100x coverage)</li> <li>Size: ~1.8 GB</li> <li>Use Cases: Outbreak tracking, source attribution, transmission analysis</li> </ul>"},{"location":"datasets/#2-reference-materials","title":"2. Reference Materials","text":""},{"location":"datasets/#reference-genomes","title":"Reference Genomes","text":"<ul> <li>High-quality reference assemblies for M. tuberculosis and V. cholerae</li> <li>Annotation files (GFF, GenBank formats)</li> <li>Resistance gene databases</li> <li>Size: ~500 MB</li> </ul>"},{"location":"datasets/#validation-datasets","title":"Validation Datasets","text":"<ul> <li>Known outbreak collections with confirmed epidemiological links</li> <li>Quality control standards</li> <li>Benchmark datasets for method comparison</li> <li>Size: ~1.5 GB</li> </ul>"},{"location":"datasets/#dataset-access","title":"Dataset Access","text":""},{"location":"datasets/#file-organization","title":"File Organization","text":"<pre><code>datasets/\n\u251c\u2500\u2500 genomics/\n\u2502   \u251c\u2500\u2500 mtb/                    # M. tuberculosis isolates\n\u2502   \u2514\u2500\u2500 vibrio/                 # V. cholerae outbreak\n\u251c\u2500\u2500 references/\n\u2502   \u251c\u2500\u2500 genomes/                # Reference assemblies\n\u2502   \u251c\u2500\u2500 databases/              # Resistance/virulence databases\n\u2502   \u2514\u2500\u2500 annotations/            # Gene annotations\n\u2514\u2500\u2500 validation/\n    \u251c\u2500\u2500 benchmarks/             # Method comparison datasets\n    \u2514\u2500\u2500 qc_standards/           # Quality control references\n</code></pre>"},{"location":"datasets/#download-instructions","title":"Download Instructions","text":""},{"location":"datasets/#during-course","title":"During Course","text":"<p>Data is pre-loaded on course HPC systems: </p><pre><code># Access course data directory\ncd /data/course/datasets/\n\n# Copy to your workspace\ncp -r /data/course/datasets/ ~/workspace/\n</code></pre><p></p>"},{"location":"datasets/#post-course-access","title":"Post-Course Access","text":"<p>Datasets remain available through: </p><pre><code># Clone dataset repository\ngit clone https://github.com/CIDRI-Africa/microbial-genomics-datasets.git\n\n# Download specific collections\nwget https://datasets.microbial-genomics.org/mtb_collection.tar.gz\n</code></pre><p></p>"},{"location":"datasets/#data-formats","title":"Data Formats","text":""},{"location":"datasets/#raw-sequencing-data","title":"Raw Sequencing Data","text":"<ul> <li>Format: FASTQ (compressed with gzip)</li> <li>Quality: Phred+33 encoding</li> <li>Naming: <code>SampleID_R1.fastq.gz</code>, <code>SampleID_R2.fastq.gz</code></li> </ul>"},{"location":"datasets/#processed-data","title":"Processed Data","text":"<ul> <li>Assemblies: FASTA format</li> <li>Annotations: GFF3, GenBank</li> <li>Alignments: SAM/BAM format</li> <li>Variants: VCF format</li> </ul>"},{"location":"datasets/#metadata","title":"Metadata","text":"<ul> <li>Sample Information: CSV/TSV format</li> <li>Study Design: Detailed README files</li> <li>Quality Metrics: MultiQC reports included</li> </ul>"},{"location":"datasets/#metadata-schema","title":"Metadata Schema","text":""},{"location":"datasets/#genomic-samples","title":"Genomic Samples","text":"Field Description Example sample_id Unique identifier MTB_001 species Organism name Mycobacterium tuberculosis collection_date Sample date 2023-01-15 location Geographic origin Cape Town, South Africa resistance_profile Known resistance INH-R, RIF-R sequencing_platform Technology Illumina MiSeq coverage_depth Average coverage 85x"},{"location":"datasets/#quality-control","title":"Quality Control","text":""},{"location":"datasets/#pre-processing-standards","title":"Pre-processing Standards","text":"<ul> <li>Quality Score: Minimum Q30 for 80% of bases</li> <li>Contamination: &lt;2% non-target DNA</li> <li>Coverage: Minimum 30x for genomic samples</li> <li>Assembly Quality: N50 &gt;50kb, &lt;200 contigs</li> </ul>"},{"location":"datasets/#validation-procedures","title":"Validation Procedures","text":"<ul> <li>Species confirmation by 16S rRNA or genome similarity</li> <li>Contamination screening with multiple tools</li> <li>Assembly quality assessment with standard metrics</li> <li>Metadata validation and consistency checking</li> </ul>"},{"location":"datasets/#ethical-considerations","title":"Ethical Considerations","text":""},{"location":"datasets/#data-privacy","title":"Data Privacy","text":"<ul> <li>All clinical data de-identified according to HIPAA standards</li> <li>Geographic information limited to city/region level</li> <li>No patient identifiers or medical record linkage possible</li> </ul>"},{"location":"datasets/#usage-rights","title":"Usage Rights","text":"<ul> <li>Educational use permitted under Creative Commons License</li> <li>Commercial use requires separate permission</li> <li>Attribution required for publications using these datasets</li> <li>Redistribution allowed with proper citation</li> </ul>"},{"location":"datasets/#responsible-use","title":"Responsible Use","text":"<ul> <li>Data should not be used to identify individuals</li> <li>Results should not be used for clinical decision-making</li> <li>Sharing outside course requires instructor approval</li> </ul>"},{"location":"datasets/#dataset-specific-notes","title":"Dataset-Specific Notes","text":""},{"location":"datasets/#m-tuberculosis-collection","title":"M. tuberculosis Collection","text":"<ul> <li>Lineage assignments based on SNP typing</li> <li>Drug resistance confirmed by phenotypic testing</li> <li>Geographic sampling represents global diversity</li> <li>Suitable for phylogeographic analysis</li> </ul>"},{"location":"datasets/#v-cholerae-outbreak","title":"V. cholerae Outbreak","text":"<ul> <li>Temporal sampling allows transmission inference</li> <li>Environmental samples included for source attribution</li> <li>Metadata includes case demographics and exposure history</li> <li>Excellent dataset for outbreak investigation training</li> </ul>"},{"location":"datasets/#troubleshooting","title":"Troubleshooting","text":""},{"location":"datasets/#common-issues","title":"Common Issues","text":""},{"location":"datasets/#file-access-problems","title":"File Access Problems","text":"<pre><code># Check file permissions\nls -la datasets/\n# Fix permissions if needed\nchmod -R 755 datasets/\n</code></pre>"},{"location":"datasets/#corrupted-downloads","title":"Corrupted Downloads","text":"<pre><code># Verify file integrity\nmd5sum -c checksums.md5\n# Re-download corrupted files\nwget -c https://datasets.url/file.tar.gz\n</code></pre>"},{"location":"datasets/#storage-space-issues","title":"Storage Space Issues","text":"<pre><code># Check available space\ndf -h\n# Compress unused files\ngzip *.fastq\n# Remove temporary files\nrm -rf temp/\n</code></pre>"},{"location":"datasets/#citation-information","title":"Citation Information","text":"<p>When using these datasets in publications, please cite:</p> <p>CIDRI-Africa Microbial Genomics Training Consortium. (2024).  Comprehensive training datasets for microbial genomics and metagenomics education.  Microbial Genomics Education, Dataset Repository.</p> <p>Individual dataset citations available in respective README files.</p>"},{"location":"datasets/#support","title":"Support","text":"<p>For dataset-related questions: - Technical Issues: Submit issue on GitHub repository - Scientific Questions: Contact course instructors - Access Problems: Email dataset-admin@cidri-africa.org</p>"},{"location":"datasets/#updates-and-versioning","title":"Updates and Versioning","text":"<ul> <li>Current Version: v2.1 (September 2025)</li> <li>Update Frequency: Annually or as needed</li> <li>Change Log: Available in repository documentation</li> <li>Notification: Users notified of major updates via email</li> </ul> <p>Remember: These datasets represent real scientific data and should be treated with appropriate care and respect for the original sample donors and research contexts.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"troubleshooting/#setup-and-installation-issues","title":"Setup and Installation Issues","text":""},{"location":"troubleshooting/#git-configuration-problems","title":"Git Configuration Problems","text":"<p>Problem: Git not configured properly </p><pre><code># Check current configuration\ngit config --list\n\n# Error: Please tell me who you are\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre><p></p> <p>Problem: SSH key authentication fails </p><pre><code># Generate new SSH key\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Add to SSH agent\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\n# Test connection\nssh -T git@github.com\n</code></pre><p></p>"},{"location":"troubleshooting/#permission-denied-errors","title":"Permission Denied Errors","text":"<p>Problem: Cannot execute scripts or access files </p><pre><code># Fix script permissions\nchmod +x script.sh\n\n# Fix directory permissions\nchmod 755 directory/\nchmod -R 755 directory/  # Recursive\n\n# Fix SSH key permissions\nchmod 600 ~/.ssh/id_ed25519\nchmod 644 ~/.ssh/id_ed25519.pub\n</code></pre><p></p>"},{"location":"troubleshooting/#data-analysis-issues","title":"Data Analysis Issues","text":""},{"location":"troubleshooting/#fastqc-problems","title":"FastQC Problems","text":"<p>Problem: FastQC fails to run </p><pre><code># Check Java installation\njava -version\n\n# Install Java if missing (Ubuntu/Debian)\nsudo apt install default-jdk\n\n# Run FastQC with memory limit\nfastqc --memory 4096 *.fastq.gz\n</code></pre><p></p> <p>Problem: Out of memory errors </p><pre><code># Increase memory allocation\nfastqc --memory 8192 file.fastq.gz\n\n# Process files individually\nfor file in *.fastq.gz; do\n    fastqc --memory 4096 \"$file\"\ndone\n</code></pre><p></p>"},{"location":"troubleshooting/#assembly-issues","title":"Assembly Issues","text":"<p>Problem: SPAdes assembly fails </p><pre><code># Check available memory\nfree -h\n\n# Run with memory limit\nspades.py --memory 32 -1 R1.fastq.gz -2 R2.fastq.gz -o output/\n\n# Try different k-mer sizes\nspades.py -k 21,33,55 -1 R1.fastq.gz -2 R2.fastq.gz -o output/\n</code></pre><p></p> <p>Problem: Poor assembly quality (high fragmentation) </p><pre><code># Check input data quality first\nfastqc input_files.fastq.gz\n\n# Try more aggressive trimming\ntrimmomatic PE input_R1.fastq.gz input_R2.fastq.gz \\\n    output_R1.fastq.gz output_R1_unpaired.fastq.gz \\\n    output_R2.fastq.gz output_R2_unpaired.fastq.gz \\\n    LEADING:10 TRAILING:10 SLIDINGWINDOW:4:20 MINLEN:50\n\n# Use careful mode in SPAdes\nspades.py --careful -1 trimmed_R1.fastq.gz -2 trimmed_R2.fastq.gz -o careful_assembly/\n</code></pre><p></p>"},{"location":"troubleshooting/#tool-installation-and-dependencies","title":"Tool Installation and Dependencies","text":""},{"location":"troubleshooting/#condamamba-issues","title":"Conda/Mamba Issues","text":"<p>Problem: Environment creation fails </p><pre><code># Update conda\nconda update conda\n\n# Clear package cache\nconda clean --all\n\n# Create environment with specific Python version\nconda create -n genomics python=3.9\n\n# Use mamba for faster solving\nmamba create -n genomics python=3.9\n</code></pre><p></p> <p>Problem: Package conflicts </p><pre><code># Create minimal environment first\nconda create -n clean_env python=3.9\n\n# Activate and install packages one by one\nconda activate clean_env\nconda install -c bioconda fastqc\nconda install -c bioconda spades\n</code></pre><p></p>"},{"location":"troubleshooting/#dockersingularity-issues","title":"Docker/Singularity Issues","text":"<p>Problem: Permission denied with Docker </p><pre><code># Add user to docker group\nsudo usermod -aG docker $USER\n\n# Log out and back in, then test\ndocker run hello-world\n</code></pre><p></p> <p>Problem: Singularity image won't run </p><pre><code># Pull image explicitly\nsingularity pull docker://biocontainers/fastqc:v0.11.9_cv8\n\n# Run with specific bind paths\nsingularity exec -B /data:/data image.sif fastqc --version\n\n# Check image integrity\nsingularity verify image.sif\n</code></pre><p></p>"},{"location":"troubleshooting/#hpc-and-remote-access-issues","title":"HPC and Remote Access Issues","text":""},{"location":"troubleshooting/#ssh-connection-problems","title":"SSH Connection Problems","text":"<p>Problem: Connection timed out </p><pre><code># Test basic connectivity\nping hostname\n\n# Try different port\nssh -p 2222 username@hostname\n\n# Use verbose mode for debugging\nssh -v username@hostname\n</code></pre><p></p> <p>Problem: Key exchange failed </p><pre><code># Generate compatible key\nssh-keygen -t rsa -b 4096\n\n# Specify key explicitly\nssh -i ~/.ssh/specific_key username@hostname\n\n# Check SSH config\ncat ~/.ssh/config\n</code></pre><p></p>"},{"location":"troubleshooting/#slurm-job-issues","title":"SLURM Job Issues","text":"<p>Problem: Job stuck in queue </p><pre><code># Check queue status\nsqueue -u $USER\n\n# Check job details\nscontrol show job JOBID\n\n# Check partition availability\nsinfo\n</code></pre><p></p> <p>Problem: Job fails with memory errors </p><pre><code># Check job output\ncat slurm-JOBID.out\n\n# Increase memory request\n#SBATCH --mem=32G\n\n# Use multiple cores if available\n#SBATCH --cpus-per-task=8\n</code></pre><p></p>"},{"location":"troubleshooting/#data-processing-errors","title":"Data Processing Errors","text":""},{"location":"troubleshooting/#file-format-issues","title":"File Format Issues","text":"<p>Problem: Unexpected file format </p><pre><code># Check file type\nfile filename\nhead filename\n\n# Convert line endings if needed\ndos2unix filename\n\n# Check compression\ngunzip -t file.gz\n</code></pre><p></p> <p>Problem: Corrupt or truncated files </p><pre><code># Check file integrity\nmd5sum file.fastq.gz\n# Compare with provided checksum\n\n# Test gzip integrity\ngunzip -t file.fastq.gz\n\n# Repair if possible (may lose data)\ngzip -d file.fastq.gz\ngzip file.fastq\n</code></pre><p></p>"},{"location":"troubleshooting/#large-file-handling","title":"Large File Handling","text":"<p>Problem: Running out of disk space </p><pre><code># Check disk usage\ndf -h\ndu -sh directory/\n\n# Clean up temporary files\nrm -rf temp/\nrm *.tmp\n\n# Compress large files\ngzip *.fastq\ntar -czf archive.tar.gz directory/\n</code></pre><p></p> <p>Problem: Processing very large files </p><pre><code># Process in chunks\nsplit -l 4000000 large_file.fastq chunk_\n# Process each chunk separately\n\n# Use streaming where possible\nzcat file.fastq.gz | head -n 1000000 | tool\n\n# Use efficient tools\nseqtk sample file.fastq.gz 10000 &gt; sample.fastq\n</code></pre><p></p>"},{"location":"troubleshooting/#analysis-and-interpretation-issues","title":"Analysis and Interpretation Issues","text":""},{"location":"troubleshooting/#resistance-gene-detection","title":"Resistance Gene Detection","text":"<p>Problem: No resistance genes found (expected some) </p><pre><code># Check assembly quality\nquast.py assembly.fasta\n\n# Try multiple databases\nabricate --db resfinder assembly.fasta\nabricate --db card assembly.fasta\nabricate --db argannot assembly.fasta\n\n# Reduce stringency\nabricate --minid 80 --mincov 60 assembly.fasta\n</code></pre><p></p> <p>Problem: Too many false positives </p><pre><code># Increase stringency\nabricate --minid 95 --mincov 90 assembly.fasta\n\n# Verify hits manually\nblast -query resistance_gene.fasta -subject assembly.fasta\n\n# Check for truncated genes\nabricate --mincov 95 assembly.fasta\n</code></pre><p></p>"},{"location":"troubleshooting/#phylogenetic-analysis","title":"Phylogenetic Analysis","text":"<p>Problem: Tree looks wrong or unrealistic </p><pre><code># Check sequence alignment quality\naliview alignment.fasta\n\n# Remove problematic sequences\nseqtk subseq sequences.fasta good_ids.txt &gt; clean.fasta\n\n# Try different tree method\nFastTree -nt alignment.fasta &gt; tree.newick\niqtree -s alignment.fasta -m TEST\n</code></pre><p></p> <p>Problem: Low bootstrap support </p><pre><code># Increase bootstrap replicates\niqtree -s alignment.fasta -bb 1000\n\n# Check for recombination\ngubbins alignment.fasta\n\n# Use only core SNPs\nsnp-sites -c alignment.fasta &gt; core_snps.fasta\n</code></pre><p></p>"},{"location":"troubleshooting/#performance-and-resource-issues","title":"Performance and Resource Issues","text":""},{"location":"troubleshooting/#memory-management","title":"Memory Management","text":"<p>Problem: Out of memory errors </p><pre><code># Check memory usage\nfree -h\ntop\n\n# Limit memory usage\nulimit -v 8000000  # Limit to ~8GB\n\n# Use memory-efficient tools\nminimap2 instead of BWA-MEM for large references\n</code></pre><p></p> <p>Problem: Process running too slowly </p><pre><code># Use multiple cores\ntool -t 8 input output\n\n# Optimize I/O\n# Use local storage instead of network drives\ncp data /tmp/\ncd /tmp/\n# Run analysis\ncp results back/to/network/storage\n</code></pre><p></p>"},{"location":"troubleshooting/#storage-management","title":"Storage Management","text":"<p>Problem: Quota exceeded </p><pre><code># Find large files\nfind . -size +100M -ls\n\n# Clean up intermediate files\nrm *.sam  # Keep only BAM files\nrm temp_*\n\n# Compress old data\ntar -czf old_analysis.tar.gz old_directory/\nrm -rf old_directory/\n</code></pre><p></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#before-asking-for-help","title":"Before Asking for Help","text":"<ol> <li>Check error messages carefully - Often contain specific solutions</li> <li>Search documentation - Tool manuals usually have troubleshooting sections</li> <li>Try simple test cases - Use small datasets to isolate problems</li> <li>Check system resources - Memory, disk space, permissions</li> </ol>"},{"location":"troubleshooting/#how-to-ask-for-help","title":"How to Ask for Help","text":""},{"location":"troubleshooting/#include-essential-information","title":"Include Essential Information","text":"<ul> <li>Exact error message (copy-paste, don't retype)</li> <li>Command that failed (exact command with parameters)</li> <li>System information (OS, tool versions)</li> <li>Input file details (size, format, sample content)</li> </ul>"},{"location":"troubleshooting/#good-help-request-example","title":"Good Help Request Example","text":"<pre><code>Subject: SPAdes assembly fails with error code 1\n\nI'm running SPAdes on paired-end M. tuberculosis data:\nCommand: spades.py -1 sample_R1.fastq.gz -2 sample_R2.fastq.gz -o spades_out/\n\nError message:\n\"== Error ==  system call for: ['/usr/bin/python3', '/opt/spades/bin/spades_init.py'] finished abnormally, err code: 1\"\n\nSystem: Ubuntu 20.04, SPAdes v3.15.3\nInput files: 2x150bp Illumina, ~50x coverage, 2.3GB total\nAvailable memory: 32GB\nDisk space: 500GB free\n\nI've tried with --careful flag and different k-mer sizes but get the same error.\n</code></pre>"},{"location":"troubleshooting/#support-resources","title":"Support Resources","text":""},{"location":"troubleshooting/#course-support","title":"Course Support","text":"<ul> <li>Instructors: Available during course hours</li> <li>Slack Channel: <code>#troubleshooting</code></li> <li>Office Hours: Daily 17:00-18:00 (course week)</li> <li>Peer Support: Encouraged among participants</li> </ul>"},{"location":"troubleshooting/#online-resources","title":"Online Resources","text":"<ul> <li>Biostars: General bioinformatics Q&amp;A</li> <li>Stack Overflow: Programming and command line issues</li> <li>Tool Documentation: Always check official documentation</li> <li>Galaxy Training: Alternative tutorials and explanations</li> </ul>"},{"location":"troubleshooting/#emergency-contacts","title":"Emergency Contacts","text":"<ul> <li>Technical Issues: tech-support@course.org</li> <li>Data Access Problems: data-admin@course.org</li> <li>General Questions: instructors@course.org</li> </ul>"},{"location":"troubleshooting/#prevention-tips","title":"Prevention Tips","text":""},{"location":"troubleshooting/#best-practices","title":"Best Practices","text":"<ol> <li>Test with small datasets first</li> <li>Keep detailed logs of commands</li> <li>Use version control for scripts</li> <li>Regular backups of important results</li> <li>Document your workflow steps</li> </ol>"},{"location":"troubleshooting/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>Running analysis without checking input quality</li> <li>Using inappropriate parameters for your data type</li> <li>Ignoring error messages and logs</li> <li>Not checking intermediate results</li> <li>Working in directories with spaces in names</li> <li>Not backing up important data</li> </ul> <p>Remember: Most bioinformatics problems have been encountered before. Don't hesitate to search online and ask for help - the community is generally very supportive!</p>"},{"location":"course/objectives/","title":"Learning Objectives","text":""},{"location":"course/objectives/#learning-objectives","title":"Learning Objectives","text":"<p>After completing this workshop, participants will be able to:</p>"},{"location":"course/objectives/#core-competencies","title":"Core Competencies","text":""},{"location":"course/objectives/#1-pathogen-genomics-principles","title":"1. Pathogen Genomics Principles","text":"<ul> <li>Understand the principles of pathogen genomics and its role in infectious disease surveillance and control</li> <li>Explain how genomic data complements traditional epidemiological approaches</li> <li>Describe the impact of genomic surveillance on public health decision-making</li> </ul>"},{"location":"course/objectives/#2-data-generation-workflow-days-1-2","title":"2. Data Generation Workflow (Days 1-2)","text":"<ul> <li>Describe the complete workflow of genomic data generation, from sample collection to sequence analysis</li> <li>Evaluate different sequencing technologies and their applications (Day 1)</li> <li>Design appropriate sampling strategies for genomic surveillance</li> <li>Navigate high-performance computing systems (Day 2)</li> </ul>"},{"location":"course/objectives/#3-bioinformatics-analysis-days-2-3","title":"3. Bioinformatics Analysis (Days 2-3)","text":"<ul> <li>Apply basic bioinformatics tools to analyze pathogen genomic data</li> <li>Perform quality control, assembly, and annotation of microbial genomes (Days 2-3)</li> <li>Execute MLST typing and serotyping for strain characterization (Day 3)</li> <li>Implement reproducible analysis workflows using command-line tools</li> </ul>"},{"location":"course/objectives/#4-epidemiological-applications-day-4","title":"4. Epidemiological Applications (Day 4)","text":"<ul> <li>Interpret genomic data for epidemiological insights, including outbreak detection and tracking</li> <li>Construct and interpret phylogenetic trees for transmission analysis (Day 4)</li> <li>Perform pangenome analysis and comparative genomics (Day 4)</li> <li>Integrate genomic and epidemiological data for outbreak investigation</li> </ul>"},{"location":"course/objectives/#5-antimicrobial-resistance-day-3","title":"5. Antimicrobial Resistance (Day 3)","text":"<ul> <li>Understand the role of genomics in identifying antimicrobial resistance mechanisms</li> <li>Detect AMR genes and mutations in genomic data (Day 3)</li> <li>Analyze mobile genetic elements and resistance spread (Day 3)</li> <li>Predict resistance phenotypes from genomic data</li> </ul>"},{"location":"course/objectives/#6-metagenomics-days-5-6","title":"6. Metagenomics (Days 5-6)","text":"<ul> <li>Apply metagenomic approaches to study microbial communities (Day 5)</li> <li>Analyze microbiome composition and diversity metrics (Day 5)</li> <li>Identify pathogens in complex microbial communities (Day 6)</li> <li>Detect co-infections and community shifts (Day 6)</li> </ul>"},{"location":"course/objectives/#7-advanced-analysis-days-6-8","title":"7. Advanced Analysis (Days 6-8)","text":"<ul> <li>Perform variant calling and mutation analysis (Day 6)</li> <li>Establish genotype-phenotype correlations (Day 6)</li> <li>Develop reproducible Nextflow pipelines (Days 7-8)</li> <li>Optimize workflow performance and testing (Day 8)</li> </ul>"},{"location":"course/objectives/#8-data-management-and-reproducibility-days-7-8","title":"8. Data Management and Reproducibility (Days 7-8)","text":"<ul> <li>Implement version control for research projects</li> <li>Create reproducible analysis pipelines using workflow managers (Days 7-8)</li> <li>Apply best practices for data management and sharing</li> </ul>"},{"location":"course/objectives/#9-critical-analysis-and-communication-days-9-10","title":"9. Critical Analysis and Communication (Days 9-10)","text":"<ul> <li>Apply learned skills to real research data (Day 9)</li> <li>Troubleshoot analysis challenges independently (Day 9)</li> <li>Evaluate the quality and reliability of genomic analyses</li> <li>Communicate genomic findings to diverse audiences</li> <li>Present research findings effectively (Day 10)</li> </ul>"},{"location":"course/objectives/#practical-skills","title":"Practical Skills","text":"<p>Participants will gain hands-on experience with:</p> <ul> <li>Unix/Linux command-line interface</li> <li>Git version control</li> <li>Docker containerization</li> <li>Nextflow workflow management</li> <li>Popular bioinformatics tools and databases</li> <li>Data visualization and interpretation</li> <li>Scientific presentation and communication</li> </ul>"},{"location":"course/objectives/#assessment-criteria","title":"Assessment Criteria","text":"<p>Progress will be evaluated through:</p> <ul> <li>Completion of hands-on exercises</li> <li>Quality of individual analysis projects</li> <li>Participation in group discussions</li> <li>Final project presentation</li> <li>Understanding demonstrated in Q&amp;A sessions</li> </ul>"},{"location":"course/overview/","title":"Overview","text":""},{"location":"course/overview/#course-overview","title":"Course Overview","text":""},{"location":"course/overview/#about-the-course","title":"About the Course","text":"<p>This comprehensive 10-day training course introduces participants to the core principles and practical applications of microbial genomics and metagenomics in clinical and public health contexts.</p>"},{"location":"course/overview/#course-schedule-september-1-12-2025","title":"Course Schedule (September 1-12, 2025)","text":"<p>The training covers 10 days of intensive hands-on learning:</p>"},{"location":"course/overview/#week-1-foundations","title":"Week 1: Foundations","text":"<ul> <li>Day 1: Course welcome, genomic surveillance, sequencing technologies, PubMLST</li> <li>Day 2: HPC introduction, command line, quality control, species identification  </li> <li>Day 3: Genome assembly, annotation, MLST, serotyping, AMR detection</li> <li>Day 4: Comparative genomics, pangenomics, phylogenomics</li> <li>Day 5: Metagenomic profiling and microbiome analysis</li> </ul>"},{"location":"course/overview/#week-2-advanced-applications","title":"Week 2: Advanced Applications","text":"<ul> <li>Day 6: Co-infection detection, variant calling, genotype-phenotype correlation</li> <li>Day 7: Nextflow workflow development and nf-core</li> <li>Day 8: Advanced pipeline development and optimization</li> <li>Day 9: Bring-your-own-data analysis session</li> <li>Day 10: Final presentations and course wrap-up</li> </ul>"},{"location":"course/overview/#target-pathogens","title":"Target Pathogens","text":"<p>Using real-world datasets, participants will analyze:</p> <ul> <li>Mycobacterium tuberculosis - TB genomics and drug resistance analysis</li> <li>Vibrio cholerae - Outbreak investigation and epidemiological analysis</li> </ul>"},{"location":"course/overview/#key-focus-areas","title":"Key Focus Areas","text":""},{"location":"course/overview/#genomic-analysis","title":"Genomic Analysis","text":"<ul> <li>Analyze genomic diversity and evolutionary relationships</li> <li>Investigate antimicrobial resistance (AMR) profiles</li> <li>Study mobile genetic elements (MGE) in resistance spread</li> </ul>"},{"location":"course/overview/#metagenomics","title":"Metagenomics","text":"<ul> <li>Explore microbial communities in clinical and environmental samples</li> <li>Apply advanced sequencing analysis techniques</li> <li>Interpret complex metagenomic datasets</li> </ul>"},{"location":"course/overview/#reproducible-workflows","title":"Reproducible Workflows","text":"<ul> <li>Master version control with Git</li> <li>Work with containerized environments (Docker/Singularity)</li> <li>Implement Nextflow pipelines for scalable analysis</li> </ul>"},{"location":"course/overview/#course-format","title":"Course Format","text":"<ul> <li>Interactive workshops with hands-on exercises</li> <li>Real-world datasets from clinical and surveillance studies</li> <li>Expert instruction from experienced genomics researchers</li> <li>Group projects and individual presentations</li> <li>Bring-your-own-data analysis sessions</li> </ul>"},{"location":"course/overview/#expected-outcomes","title":"Expected Outcomes","text":"<p>By the end of this course, participants will be equipped with practical skills to conduct genomic surveillance, investigate outbreaks, and contribute to public health decision-making through genomic analysis.</p>"},{"location":"course/prerequisites/","title":"Prerequisites","text":""},{"location":"course/prerequisites/#prerequisites-and-requirements","title":"Prerequisites and Requirements","text":""},{"location":"course/prerequisites/#technical-prerequisites","title":"Technical Prerequisites","text":""},{"location":"course/prerequisites/#essential-requirements","title":"Essential Requirements","text":""},{"location":"course/prerequisites/#1-computing-equipment","title":"1. Computing Equipment","text":"<ul> <li>Laptop with at least 8GB RAM (16GB recommended)</li> <li>Operating System: Linux or macOS preferred</li> <li>Windows users must install Git Bash before the course</li> <li>Reliable internet connection for downloading datasets and software</li> </ul>"},{"location":"course/prerequisites/#2-basic-technical-knowledge","title":"2. Basic Technical Knowledge","text":"<ul> <li>Basic understanding of Linux command-line tools</li> <li>File navigation (cd, ls, pwd)</li> <li>File manipulation (cp, mv, rm, mkdir)</li> <li>Text viewing (cat, less, head, tail)</li> <li>Familiarity with text editors (nano, vim, or similar)</li> <li>Basic understanding of file systems and permissions</li> </ul>"},{"location":"course/prerequisites/#3-scientific-background","title":"3. Scientific Background","text":"<ul> <li>Undergraduate-level biology or microbiology</li> <li>Basic understanding of genetics and molecular biology</li> <li>Familiarity with concepts of:</li> <li>DNA sequencing</li> <li>Bacterial genetics</li> <li>Infectious diseases</li> <li>Public health surveillance</li> </ul>"},{"location":"course/prerequisites/#recommended-not-required","title":"Recommended (Not Required)","text":""},{"location":"course/prerequisites/#1-programming-experience","title":"1. Programming Experience","text":"<ul> <li>Basic scripting in Python, R, or shell scripting</li> <li>Experience with data analysis workflows</li> <li>Version control with Git (we'll cover this in Day 1)</li> </ul>"},{"location":"course/prerequisites/#2-bioinformatics-background","title":"2. Bioinformatics Background","text":"<ul> <li>Previous experience with sequence analysis tools</li> <li>Understanding of genomic file formats (FASTA, FASTQ, VCF)</li> <li>Familiarity with biological databases</li> </ul>"},{"location":"course/prerequisites/#3-high-performance-computing","title":"3. High-Performance Computing","text":"<ul> <li>Access to HPC resources (recommended but not essential)</li> <li>Experience with job schedulers (SLURM, PBS)</li> </ul>"},{"location":"course/prerequisites/#software-requirements","title":"Software Requirements","text":""},{"location":"course/prerequisites/#pre-course-installation","title":"Pre-course Installation","text":"<p>Please install the following software before the course begins:</p>"},{"location":"course/prerequisites/#1-git-all-operating-systems","title":"1. Git (All Operating Systems)","text":"<ul> <li>Linux: <code>sudo apt install git</code> or equivalent</li> <li>macOS: Install Xcode Command Line Tools or use Homebrew</li> <li>Windows: Download and install Git Bash from git-scm.com</li> </ul>"},{"location":"course/prerequisites/#2-text-editor","title":"2. Text Editor","text":"<p>Choose one of: - VS Code (recommended for beginners) - Sublime Text - Atom - Vim/Emacs (for advanced users)</p>"},{"location":"course/prerequisites/#3-ssh-client","title":"3. SSH Client","text":"<ul> <li>Linux/macOS: Built-in terminal</li> <li>Windows: Use Git Bash or Windows Subsystem for Linux (WSL)</li> </ul>"},{"location":"course/prerequisites/#course-provided-software","title":"Course-Provided Software","text":"<p>The following will be provided during the course:</p> <ul> <li>Docker/Singularity containers with pre-installed bioinformatics tools</li> <li>Nextflow for workflow management</li> <li>Access to high-performance computing resources</li> <li>Pre-configured analysis environments</li> </ul>"},{"location":"course/prerequisites/#data-requirements","title":"Data Requirements","text":""},{"location":"course/prerequisites/#practice-datasets","title":"Practice Datasets","text":"<p>We will provide: - Curated training datasets for all major pathogens - Quality-controlled sequence data in standard formats - Reference genomes and databases - Example analysis outputs for comparison</p>"},{"location":"course/prerequisites/#bring-your-own-data-optional","title":"Bring Your Own Data (Optional)","text":"<p>If you have your own datasets: - Raw sequencing data (FASTQ format) - Associated metadata (sample information, collection details) - Ethical approval for data sharing (if applicable) - Data should be &lt;100GB total for practical analysis</p>"},{"location":"course/prerequisites/#pre-course-preparation","title":"Pre-course Preparation","text":""},{"location":"course/prerequisites/#1-complete-the-setup-guide","title":"1. Complete the Setup Guide","text":"<p>Follow our detailed Setup Guide to prepare your computing environment.</p>"},{"location":"course/prerequisites/#2-review-basic-concepts","title":"2. Review Basic Concepts","text":"<p>Refresh your knowledge of: - Basic microbiology and infectious diseases - DNA sequencing principles - File formats in bioinformatics</p>"},{"location":"course/prerequisites/#3-practice-command-line","title":"3. Practice Command Line","text":"<p>If you're new to command line: - Complete an online tutorial (e.g., \"Command Line Crash Course\") - Practice basic file operations - Get comfortable with navigating directories</p>"},{"location":"course/prerequisites/#4-test-your-setup","title":"4. Test Your Setup","text":"<ul> <li>Verify Git installation: <code>git --version</code></li> <li>Test SSH connectivity (instructions will be provided)</li> <li>Ensure you can create and edit text files</li> </ul>"},{"location":"course/prerequisites/#accessibility-and-support","title":"Accessibility and Support","text":""},{"location":"course/prerequisites/#technical-support","title":"Technical Support","text":"<ul> <li>Pre-course: Contact instructors for setup assistance</li> <li>During course: Dedicated technical support available</li> <li>Post-course: Community forum for ongoing questions</li> </ul>"},{"location":"course/prerequisites/#accommodations","title":"Accommodations","text":"<p>Please inform instructors of any: - Accessibility requirements - Dietary restrictions (for course meals) - Special technical needs</p>"},{"location":"course/prerequisites/#language-support","title":"Language Support","text":"<ul> <li>Course materials are in English</li> <li>Instructors can provide support in multiple languages</li> <li>Translated resources available for key concepts</li> </ul>"},{"location":"course/prerequisites/#questions","title":"Questions?","text":"<p>If you have questions about prerequisites or setup:</p> <ol> <li>Check the Troubleshooting Guide</li> <li>Contact the course coordinators</li> <li>Join our pre-course Q&amp;A sessions (dates TBD)</li> </ol>"},{"location":"course/schedule/","title":"Schedule","text":""},{"location":"course/schedule/#course-schedule","title":"Course Schedule","text":""},{"location":"course/schedule/#overview","title":"Overview","text":"<p>The 10-day intensive training course covers fundamental to advanced topics in microbial genomics and metagenomics. Each day combines theoretical presentations with hands-on practical exercises.</p> <p>!!! info \"Course Information\"</p> <ul> <li>Duration: 10 days (September 1-12, 2025)</li> <li>Time: 09:00-13:00 CAT daily</li> <li>Venue: MAC room, level 2, Health Science UCT, Barnard Fuller Building</li> <li>Address: Anzio Rd, Observatory, Cape Town, 7935</li> <li>Format: Hands-on workshops with lectures</li> <li>Break: 11:00/11:30 AM coffee break</li> </ul> <p>!!! note \"Participant Requirements\"  - Participants are required to bring their own data to perform the analysis. If a participant does not have data, it will be made available to them.</p>"},{"location":"course/schedule/#day-1-welcome-to-the-course","title":"Day 1: Welcome to the Course","text":"<p>Date: September 1, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Introductions All 09:10 Overview of clinical pathogens and genomic surveillance Slides Ephifania Geza 09:40 Overview of sequencing technologies and data types Sindiswa Lukhele 10:00 Setting up and exploring PubMLST Sindiswa Lukhele 11:00 Break 11:30 Introduction to command line interface Practical Arash Iranzadeh <p>Key Learning Outcomes: Introduction to genomic surveillance, sequencing technologies, command line basics</p>"},{"location":"course/schedule/#day-2-introduction-to-commandline-and-guest-talk","title":"Day 2: Introduction to Commandline and Guest Talk","text":"<p>Date: September 2, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Introduction to command line interface Practical Arash Iranzadeh 11:30 Break 12:00 Guest talk: MtB and co-infection Bio. Presentation Bethlehem Adnew <p>Key Learning Outcomes: Command line proficiency, HPC fundamentals</p>"},{"location":"course/schedule/#day-3-accelerating-bioinformatics-hpc-qc-and-species-identification-essentials","title":"Day 3: Accelerating Bioinformatics: HPC, QC, and Species Identification Essentials","text":"<p>Date: September 3, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Introduction High Performance Computing (HPC) \u2013 Ilifu Notes \u2022 Practical 1 \u2022 Practical 2 Mamana Mbiyavanga 11:00 Break 12:00 Quality checking and control, as well as species identification Practical Arash Iranzadeh <p>Key Learning Outcomes: HPC fundamentals, Quality control fundamentals</p>"},{"location":"course/schedule/#day-4-genome-assembly-essentials-qc-identification-and-annotation","title":"Day 4: Genome Assembly Essentials: QC, Identification, and Annotation","text":"<p>Date: September 4, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Recap: Quality checking and control, and species identification Practical Arash Iranzadeh 10:30 Genome assembly, quality assessment Notes. Practical Ephifania Geza 11:00 Break 11:30 Genome assembly, quality assessment: Continuation Notes. Practical Ephifania Geza <p>Key Learning Outcomes: Quality control, Genome assembly, assessment</p>"},{"location":"course/schedule/#day-5-tracking-threats-genomic-detection-of-amr-virulence-and-plasmid-mobility","title":"Day 5: Tracking Threats: Genomic Detection of AMR, Virulence, and Plasmid Mobility","text":"<p>Date: September 5, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Recap: Genome assembly, quality assessment Notes. Practical 10:00 Genome annotation and Multi-Locus Sequence Typing Notes Arash Iranzadeh 11:30 Break 12:00 Antimicrobial Resistance gene detection and resistance prediction Ephifania Geza <p>Key Learning Outcomes: Genome quality and functional gene annotation fundamentals, AMR and virulence factors and plasmid detection</p>"},{"location":"course/schedule/#day-6-nextflow-pipeline-development","title":"Day 6: Nextflow Pipeline Development","text":"<p>Date: September 8, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Reproducible workflows with Nextflow and nf-core Mamana Mbiyavanga 10:30 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga 11:30 Break 12:00 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga <p>Key Learning Outcomes: Workflow reproducibility, Nextflow basics, pipeline development</p>"},{"location":"course/schedule/#day-7-advanced-nextflow-version-control-with-github-real-genomics-applications","title":"Day 7: Advanced Nextflow, Version Control with GitHub &amp; Real Genomics Applications","text":"<p>Date: September 9, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Git and GitHub for Pipeline Version Control Mamana Mbiyavanga 09:45 MTB Analysis Pipeline Development Mamana Mbiyavanga 10:30 Break 10:45 Genome Assembly Workflows Mamana Mbiyavanga 11:30 Advanced Nextflow Features &amp; Optimization Mamana Mbiyavanga 12:15 Pipeline Deployment Strategies Mamana Mbiyavanga 13:00 End <p>Key Learning Outcomes: - Professional version control with Git/GitHub for collaborative pipeline development - Real-world MTB genomics workflows for clinical applications - Complete genome assembly pipelines from raw reads to annotated genomes - Advanced Nextflow features including modules, subworkflows, and optimization - Multi-platform deployment strategies (local, HPC, cloud environments)</p>"},{"location":"course/schedule/#day-8-metagenomic-profiling","title":"Day 8: Metagenomic profiling","text":"<p>Date: September 10, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Metagenomic sequencing Ephifania Geza 09:30 Quality control in metagenomic data Ephifania Geza 10:30 Microbiome profiling and diversity metrics (R or QIIME2) Ephifania Geza 11:30 Break 12:00 Microbiome profiling and diversity metrics (R or QIIME2) Ephifania Geza 13:00 Role of plasmids, integrons, and transposons in AMR spread Ephifania Geza <p>Key Learning Outcomes: Metagenomic sequencing principles, microbiome analysis, diversity metrics</p>"},{"location":"course/schedule/#day-9-comparative-genomics","title":"Day 9: Comparative Genomics","text":"<p>Date: September 11, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Pangenomics Arash Iranzadeh 10:30 Phylogenomics: Inferring evolutionary relationships from core SNPs Arash Iranzadeh 11:30 Break 12:00 Phylogenomics: Tree construction and visualisation Arash Iranzadeh <p>Key Learning Outcomes: Pan-genome analysis, phylogenetic inference, tree construction and visualization</p>"},{"location":"course/schedule/#day-10-wrap-up-session","title":"Day 10: Wrap-up session","text":"<p>Date: September 12, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Participant presentations 11:15 Short talks NGS-Academy/AfriGen-D/eLwazi ODSP 11:40 End of the course <p>Key Learning Outcomes: Applying learned skills to real data, Scientific presentation skills, course completion</p>"},{"location":"course/schedule/#trainers","title":"Trainers","text":""},{"location":"course/schedule/#core-training-team","title":"Core Training Team","text":"Trainer Role Expertise Ephifania Geza Lead Instructor Genomic surveillance, AMR analysis, metagenomics Arash Iranzadeh Technical Instructor Command line, QC, assembly, phylogenomics Sindiswa Lukhele Technical Instructor Sequencing technologies, PubMLST Mamana Mbiyavanga HPC/Workflow Specialist High-performance computing, Nextflow pipelines"},{"location":"course/schedule/#guest-speaker","title":"Guest Speaker","text":"Speaker Topic Date Bethlehem Adnew MtB and co-infection September 2, 2025 <p>This comprehensive training provides participants with both theoretical knowledge and practical skills needed for microbial genomics analysis in clinical and public health settings.</p>"},{"location":"course/setup/","title":"Setup","text":""},{"location":"course/setup/#setup-guide","title":"Setup Guide","text":""},{"location":"course/setup/#before-you-arrive","title":"Before You Arrive","text":"<p>Complete these setup steps at least one week before the course begins.</p>"},{"location":"course/setup/#1-system-requirements","title":"1. System Requirements","text":""},{"location":"course/setup/#minimum-specifications","title":"Minimum Specifications","text":"<ul> <li>RAM: 8GB (16GB recommended)</li> <li>Storage: 50GB free space</li> <li>OS: Linux, macOS, or Windows 10/11</li> <li>Internet: Stable broadband connection</li> </ul>"},{"location":"course/setup/#operating-system-setup","title":"Operating System Setup","text":"Linux (Ubuntu/Debian)macOSWindows <pre><code># Update package list\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install essential tools\nsudo apt install -y git curl wget build-essential\n</code></pre> <pre><code># Install Homebrew (if not already installed)\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install essential tools\nbrew install git curl wget\n</code></pre> <ol> <li>Install Git Bash: Download from git-scm.com</li> <li>Enable WSL2 (optional but recommended):    <pre><code>wsl --install\n</code></pre></li> <li>Install Windows Terminal from Microsoft Store</li> </ol>"},{"location":"course/setup/#2-git-configuration","title":"2. Git Configuration","text":"<p>Configure Git with your information:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\ngit config --global init.defaultBranch main\n</code></pre> <p>Verify configuration: </p><pre><code>git config --list\n</code></pre><p></p>"},{"location":"course/setup/#3-ssh-setup","title":"3. SSH Setup","text":"<p>Generate SSH key for secure connections:</p> <pre><code># Generate SSH key pair\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Start SSH agent\neval \"$(ssh-agent -s)\"\n\n# Add key to agent\nssh-add ~/.ssh/id_ed25519\n\n# Display public key (copy this for later use)\ncat ~/.ssh/id_ed25519.pub\n</code></pre>"},{"location":"course/setup/#4-text-editor","title":"4. Text Editor","text":"<p>Choose and install a text editor:</p> VS Code (Recommended)Sublime TextCommand Line Editors <ul> <li>Download from code.visualstudio.com</li> <li>Install useful extensions:</li> <li>Python</li> <li>Markdown All in One</li> <li>GitLens</li> </ul> <ul> <li>Download from sublimetext.com</li> <li>Consider Sublime Merge for Git integration</li> </ul> <pre><code># For vim users\nsudo apt install vim  # Linux\nbrew install vim      # macOS\n\n# For nano users (usually pre-installed)\nwhich nano\n</code></pre>"},{"location":"course/setup/#5-test-your-setup","title":"5. Test Your Setup","text":""},{"location":"course/setup/#basic-command-line-test","title":"Basic Command Line Test","text":"<pre><code># Check versions\ngit --version\ncurl --version\nwget --version\n\n# Test directory operations\nmkdir test_setup\ncd test_setup\necho \"Hello World\" &gt; test.txt\ncat test.txt\ncd ..\nrm -rf test_setup\n</code></pre>"},{"location":"course/setup/#git-test","title":"Git Test","text":"<pre><code># Clone a test repository\ngit clone https://github.com/CIDRI-Africa/microbial-genomics-training.git\ncd microbial-genomics-training\nls -la\ncd ..\nrm -rf microbial-genomics-training\n</code></pre>"},{"location":"course/setup/#6-course-specific-setup","title":"6. Course-Specific Setup","text":""},{"location":"course/setup/#hpc-access-if-provided","title":"HPC Access (If Provided)","text":"<p>You will receive: - SSH connection details - Username and login instructions - VPN setup (if required)</p>"},{"location":"course/setup/#dockersingularity-optional","title":"Docker/Singularity (Optional)","text":"<p>For local analysis (advanced users):</p> DockerSingularity <pre><code># Linux\nsudo apt install docker.io\nsudo usermod -aG docker $USER\n\n# macOS\n# Download Docker Desktop from docker.com\n\n# Test installation\ndocker --version\ndocker run hello-world\n</code></pre> <pre><code># Linux (Ubuntu/Debian)\nsudo apt install singularity-container\n\n# Test installation\nsingularity --version\n</code></pre>"},{"location":"course/setup/#7-download-course-materials","title":"7. Download Course Materials","text":"<p>One week before the course:</p> <pre><code># Create course directory\nmkdir -p ~/microbial-genomics-course\ncd ~/microbial-genomics-course\n\n# Clone course repository\ngit clone https://github.com/CIDRI-Africa/microbial-genomics-training.git\n\n# Check repository contents\ncd microbial-genomics-training\nls -la\n</code></pre>"},{"location":"course/setup/#8-pre-course-data-download","title":"8. Pre-course Data Download","text":"<p>Large datasets will be provided via: - Shared storage on HPC systems - Cloud storage links (Google Drive/Dropbox) - Direct download scripts (provided before course)</p>"},{"location":"course/setup/#9-connectivity-test","title":"9. Connectivity Test","text":""},{"location":"course/setup/#ssh-connection-test","title":"SSH Connection Test","text":"<pre><code># Test SSH connectivity (replace with provided details)\nssh -T username@hostname\n\n# If successful, you should see a welcome message\n</code></pre>"},{"location":"course/setup/#internet-speed-test","title":"Internet Speed Test","text":"<p>Ensure you have adequate bandwidth: - Minimum: 10 Mbps download - Recommended: 50+ Mbps download - Upload: 5+ Mbps for video calls</p>"},{"location":"course/setup/#10-backup-and-recovery","title":"10. Backup and Recovery","text":""},{"location":"course/setup/#create-backups","title":"Create Backups","text":"<pre><code># Backup SSH keys\ncp ~/.ssh/id_ed25519* ~/backup_location/\n\n# Backup Git configuration\ngit config --list &gt; ~/git_config_backup.txt\n</code></pre>"},{"location":"course/setup/#recovery-commands","title":"Recovery Commands","text":"<p>Keep these handy in case of issues: </p><pre><code># Reset Git configuration\ngit config --global --unset-all user.name\ngit config --global --unset-all user.email\n\n# Regenerate SSH keys\nrm ~/.ssh/id_ed25519*\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n</code></pre><p></p>"},{"location":"course/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"course/setup/#common-issues","title":"Common Issues","text":""},{"location":"course/setup/#git-issues","title":"Git Issues","text":"<pre><code># Fix permission issues (Linux/macOS)\nsudo chown -R $USER ~/.ssh/\n\n# Reset SSH agent\neval \"$(ssh-agent -k)\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n</code></pre>"},{"location":"course/setup/#network-issues","title":"Network Issues","text":"<pre><code># Test connectivity\nping google.com\ncurl -I https://github.com\n\n# Check proxy settings (if behind firewall)\necho $http_proxy\necho $https_proxy\n</code></pre>"},{"location":"course/setup/#permission-issues-linux","title":"Permission Issues (Linux)","text":"<pre><code># Fix common permission problems\nsudo chown -R $USER:$USER ~/\nchmod 755 ~/.ssh\nchmod 600 ~/.ssh/id_ed25519\nchmod 644 ~/.ssh/id_ed25519.pub\n</code></pre>"},{"location":"course/setup/#getting-help","title":"Getting Help","text":""},{"location":"course/setup/#before-the-course","title":"Before the Course","text":"<ul> <li>Email: instructors@microbial-genomics-training.org</li> <li>Slack: Join our pre-course channel</li> <li>Office Hours: Weekly setup sessions (schedule TBD)</li> </ul>"},{"location":"course/setup/#documentation","title":"Documentation","text":"<ul> <li>Command Line Tutorial</li> <li>Git Tutorial</li> <li>SSH Tutorial</li> </ul>"},{"location":"course/setup/#final-checklist","title":"Final Checklist","text":"<p>Before the course starts, verify:</p> <ul> <li> Git installed and configured</li> <li> SSH key generated and working</li> <li> Text editor installed and functional</li> <li> Course repository cloned</li> <li> HPC access tested (if provided)</li> <li> Internet connection stable</li> <li> Backup of important configurations created</li> </ul>"},{"location":"course/setup/#day-1-preview","title":"Day 1 Preview","text":"<p>On the first day (September 1, 2025), we'll: 1. Course introductions and welcome session 2. Overview of clinical pathogens and genomic surveillance 3. Sequencing technologies and data types overview 4. Hands-on PubMLST database exploration 5. Command line interface basics and R environment setup</p> <p>Come prepared with your laptop and the software installed above. Don't worry if you encounter setup problems \u2013 we'll troubleshoot together during the session!</p>"},{"location":"day2/hpc-ilifu-training/","title":"HPC and ILIFU Training Materials","text":""},{"location":"day2/hpc-ilifu-training/#hpc-and-ilifu-training-materials","title":"HPC and ILIFU Training Materials","text":""},{"location":"day2/hpc-ilifu-training/#getting-started","title":"Getting Started","text":"<p>This document provides an overview of HPC concepts and ILIFU infrastructure. </p> <p>For hands-on practice:</p> <ul> <li>SLURM Jobs: See High Performance Computing with SLURM: Practical Tutorial for step-by-step SLURM exercises</li> <li>Unix Commands: See Unix Commands for Pathogen Genomics - Practical Tutorial for genomics command-line basics</li> </ul> <p>Quick Setup (if needed for examples below): </p><pre><code>mkdir -p ~/hpc_practice &amp;&amp; cd ~/hpc_practice\ncp -r /cbio/training/courses/2025/micmet-genomics/sample-data/* .\n</code></pre><p></p>"},{"location":"day2/hpc-ilifu-training/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to High Performance Computing (HPC)</li> <li>ILIFU Infrastructure Overview</li> <li>Getting Started with ILIFU</li> <li>SLURM Job Scheduling</li> <li>Resource Allocation and Management</li> <li>Best Practices</li> <li>Practical Examples</li> <li>Troubleshooting</li> </ol>"},{"location":"day2/hpc-ilifu-training/#introduction-to-hpc","title":"Introduction to HPC","text":""},{"location":"day2/hpc-ilifu-training/#what-is-high-performance-computing","title":"What is High Performance Computing?","text":"<p>High Performance Computing (HPC) is the use of powerful computers with multiple processors working in parallel to solve complex computational problems that require significant processing power, memory, or time.</p> <p>Key Characteristics:</p> <ul> <li>Parallel processing: Multiple CPUs/cores work simultaneously on the same problem</li> <li>Cluster architecture: Hundreds or thousands of interconnected compute nodes</li> <li>High memory capacity: Large RAM for data-intensive computations</li> <li>Fast storage systems: High-speed file systems for handling large datasets</li> <li>Job scheduling: Queue management systems to optimize resource allocation</li> <li>Specialized hardware: GPUs, high-speed interconnects (InfiniBand), and custom processors</li> </ul>"},{"location":"day2/hpc-ilifu-training/#why-use-hpc","title":"Why Use HPC?","text":"<ul> <li>Speed: Complete computations faster than desktop computers</li> <li>Scale: Handle larger datasets and more complex problems</li> <li>Efficiency: Optimize resource utilization</li> <li>Cost-effective: Share expensive hardware among researchers</li> </ul>"},{"location":"day2/hpc-ilifu-training/#traditional-computing-vs-hpc","title":"Traditional Computing vs HPC","text":"<pre><code>Desktop Computer          HPC Cluster\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   1 CPU      \u2502   vs    \u2502Node1\u2502Node2\u2502Node3\u2502\n\u2502   8GB RAM    \u2502         \u2502 32  \u2502 64  \u2502128  \u2502\n\u2502   1TB Disk   \u2502         \u2502cores\u2502cores\u2502cores\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>HPC = Many computers working together</p> Traditional Computing HPC Cluster Single processor Hundreds of processors Limited memory (8-32GB) Massive shared memory (TB) Local storage (TB) Distributed storage (PB) Individual use Shared resources Desktop/Laptop Specialized data centers"},{"location":"day2/hpc-ilifu-training/#why-do-we-need-hpc","title":"Why Do We Need HPC?","text":""},{"location":"day2/hpc-ilifu-training/#real-world-problems-that-need-hpc","title":"Real-world Problems That Need HPC","text":""},{"location":"day2/hpc-ilifu-training/#astronomy-processing-telescope-data","title":"\ud83d\udd2c Astronomy: Processing telescope data","text":"<ul> <li>Data volume: 1-2 TB per night from modern telescopes</li> <li>Local machine: 3-4 weeks to process one night's data</li> <li>HPC cluster: 2-3 hours with parallel processing</li> <li>Example: MeerKAT telescope generates 2.5 TB/hour during observations</li> </ul>"},{"location":"day2/hpc-ilifu-training/#bioinformatics-genome-assembly","title":"\ud83e\uddec Bioinformatics: Genome assembly","text":"<ul> <li>Data size: 100-300 GB of raw sequencing reads</li> <li>Local machine (8GB RAM): Often fails due to memory limits</li> <li>Local machine (32GB RAM): 2-3 weeks for bacterial genome</li> <li>HPC cluster: 4-6 hours with 256GB RAM</li> <li>Example: Human genome assembly needs ~1TB RAM, impossible on most desktops</li> </ul>"},{"location":"day2/hpc-ilifu-training/#climate-modeling-weather-simulations","title":"\ud83c\udf21\ufe0f Climate Modeling: Weather simulations","text":"<ul> <li>Computation: Millions of grid points \u00d7 thousands of time steps</li> <li>Local machine: 6-8 months for regional model (if it runs at all)</li> <li>HPC cluster: 12-24 hours on 100+ cores</li> <li>Example: 10km resolution global model needs 10,000+ CPU hours</li> </ul>"},{"location":"day2/hpc-ilifu-training/#machine-learning-training-deep-neural-networks","title":"\ud83e\uddee Machine Learning: Training deep neural networks","text":"<ul> <li>Model size: GPT-3 has 175 billion parameters</li> <li>Local machine (single GPU): 355 years to train</li> <li>HPC cluster (1000 GPUs): 34 days</li> <li>Example: Training ResNet-50 on ImageNet: 2 weeks (laptop) \u2192 1 hour (8 GPUs)</li> </ul>"},{"location":"day2/hpc-ilifu-training/#pathogen-genomics-outbreak-analysis","title":"\ud83e\udda0 Pathogen Genomics: Outbreak analysis","text":"<ul> <li>Dataset: 1000 M. tuberculosis genomes for outbreak investigation</li> <li>Local machine tasks and times:</li> <li>Quality control: 50 hours (3 min/sample)</li> <li>Read alignment: 167 hours (10 min/sample)  </li> <li>Variant calling: 83 hours (5 min/sample)</li> <li>Phylogenetic tree: 48-72 hours</li> <li>Total: ~15 days of continuous processing</li> <li>HPC cluster: </li> <li>All samples in parallel: 4-6 hours total</li> <li>Tree construction on high-memory node: 2-3 hours</li> <li>Real example: COVID-19 surveillance processing 10,000 genomes weekly - impossible without HPC</li> </ul>"},{"location":"day2/hpc-ilifu-training/#additional-pathogen-genomics-use-cases","title":"Additional Pathogen Genomics Use Cases","text":""},{"location":"day2/hpc-ilifu-training/#bacterial-genome-assembly-illumina-nanopore","title":"\ud83e\uddec Bacterial Genome Assembly (Illumina + Nanopore)","text":"<ul> <li>Dataset: Hybrid assembly of 50 bacterial isolates</li> <li>Computational requirements:</li> <li>RAM: 16-32GB per genome</li> <li>CPU: 8-16 cores optimal per assembly</li> <li>Local machine (16GB RAM, 4 cores):</li> <li>One genome at a time only</li> <li>Per genome: 3-4 hours</li> <li>Total time: 150-200 hours (6-8 days)</li> <li>Risk of crashes with large genomes</li> <li>HPC cluster (256GB RAM, 32 cores/node):</li> <li>Process 8 genomes simultaneously per node</li> <li>Use 7 nodes for all 50 genomes</li> <li>Total time: 3-4 hours</li> <li>Speedup: 50x faster</li> </ul>"},{"location":"day2/hpc-ilifu-training/#amr-gene-detection-across-multiple-species","title":"\ud83d\udc8a AMR Gene Detection Across Multiple Species","text":"<ul> <li>Dataset: 5000 bacterial genomes from hospital surveillance</li> <li>Tools: AMRFinder, CARD-RGI, ResFinder</li> <li>Computational requirements:</li> <li>Database size: 2-5GB per tool</li> <li>RAM: 4-8GB per genome</li> <li>CPU time: 5-10 minutes per genome per tool</li> <li>Local machine (8 cores):</li> <li>Sequential processing: 5000 \u00d7 3 tools \u00d7 7.5 min = 1875 hours (78 days)</li> <li>Database loading overhead adds 20% more time</li> <li>HPC cluster (100 nodes, 32 cores each):</li> <li>Parallel processing across nodes</li> <li>Shared database in memory</li> <li>Total time: 6-8 hours</li> <li>Speedup: 230x faster</li> </ul>"},{"location":"day2/hpc-ilifu-training/#phylogeographic-analysis-of-cholera-outbreak","title":"\ud83c\udf0d Phylogeographic Analysis of Cholera Outbreak","text":"<ul> <li>Dataset: 2000 V. cholerae genomes from Haiti outbreak</li> <li>Computational requirements:</li> <li>Alignment: 100GB RAM for reference-based</li> <li>SNP calling: 4GB per genome</li> <li>Tree building (RAxML-NG): 64-128GB RAM</li> <li>BEAST analysis: 32GB RAM, 1000+ hours CPU time</li> <li>Local machine attempts:</li> <li>Alignment: Often fails (out of memory)</li> <li>If successful: 48 hours</li> <li>SNP calling: 133 hours (4 min/genome)</li> <li>RAxML tree: Fails on most laptops (needs &gt;64GB RAM)</li> <li>BEAST: 6-8 weeks for proper MCMC convergence</li> <li>HPC cluster:</li> <li>Alignment: 2 hours on high-memory node</li> <li>SNP calling: 2 hours (parallel)</li> <li>RAxML: 4-6 hours on 64 cores</li> <li>BEAST: 48 hours on 32 cores</li> <li>Total: 2-3 days vs 2-3 months</li> </ul>"},{"location":"day2/hpc-ilifu-training/#real-time-nanopore-sequencing-analysis","title":"\ud83d\udd2c Real-time Nanopore Sequencing Analysis","text":"<ul> <li>Scenario: Meningitis outbreak, need results in &lt;24 hours</li> <li>Data flow: 20 samples, 5GB data/sample, arriving over 12 hours</li> <li>Pipeline: Basecalling \u2192 QC \u2192 Assembly \u2192 Typing \u2192 AMR</li> <li>Local machine challenges:</li> <li>Can't keep up with data generation</li> <li>Basecalling alone: 2 hours/sample (40 hours total)</li> <li>Sequential processing: Miss the 24-hour deadline</li> <li>HPC solution:</li> <li>Real-time processing as data arrives</li> <li>GPU nodes for basecalling: 10 min/sample</li> <li>Parallel assembly and analysis</li> <li>Results available within 2-3 hours of sequencing</li> <li>Clinical impact: Treatment decisions in same day</li> </ul>"},{"location":"day2/hpc-ilifu-training/#computational-requirements-comparison-table","title":"Computational Requirements Comparison Table","text":"Task Local Machine HPC Cluster Speedup 100 TB genomes QC 8GB RAM, 5 hours 256GB RAM, 10 min 30x 1000 genome alignment 16GB RAM, 7 days 32GB/node \u00d7 50, 3 hours 56x Phylogenetic tree (5000 taxa) Often fails (&gt;64GB needed) 512GB RAM, 6 hours \u221e Pan-genome analysis (500 genomes) 32GB RAM, 2 weeks 256GB RAM, 8 hours 42x GWAS (10,000 samples) Impossible (&lt;1TB RAM) 1TB RAM node, 24 hours \u221e Metagenomic assembly 64GB RAM, 3 days 512GB RAM, 4 hours 18x"},{"location":"day2/hpc-ilifu-training/#why-these-tasks-fail-on-local-machines","title":"Why These Tasks Fail on Local Machines","text":"<ol> <li>Memory Walls:</li> <li>De novo assembly: Needs 100-1000x coverage data in RAM</li> <li>Tree building: O(n\u00b2) memory for distance matrices</li> <li> <p>Pan-genome: Stores all genomes simultaneously</p> </li> <li> <p>Time Constraints:</p> </li> <li>Outbreak response: Need results in hours, not weeks</li> <li>Grant deadlines: Can't wait months for analysis</li> <li> <p>Iterative analysis: Need to test multiple parameters</p> </li> <li> <p>Data Volume:</p> </li> <li>Modern sequencer: 100-500GB per run</li> <li>Surveillance programs: 100s of genomes weekly</li> <li>Can't even store data on laptop (typical: 256GB-1TB SSD)</li> </ol>"},{"location":"day2/hpc-ilifu-training/#ilifu-infrastructure","title":"ILIFU Infrastructure","text":""},{"location":"day2/hpc-ilifu-training/#what-is-ilifu","title":"What is ILIFU?","text":"<ul> <li>Inter-University Institute for Data Intensive Astronomy</li> <li>South African national research data facility</li> <li>Supports astronomy, bioinformatics, and other data-intensive sciences</li> <li>Located at University of Cape Town and University of the Western Cape</li> </ul>"},{"location":"day2/hpc-ilifu-training/#ilifu-services","title":"ILIFU Services","text":"<ol> <li>Compute Cluster: High-performance computing resources</li> <li>Storage: Large-scale data storage solutions</li> <li>Cloud Services: Virtualized computing environments</li> <li>Data Transfer: High-speed data movement capabilities</li> <li>Support: Technical assistance and training</li> </ol>"},{"location":"day2/hpc-ilifu-training/#ilifu-cluster-architecture","title":"ILIFU Cluster Architecture","text":"<p>ILIFU (Inter-University Institute for Data Intensive Astronomy) is a cloud computing infrastructure designed for data-intensive research in astronomy, bioinformatics, and other computational sciences. The facility operates on an OpenStack platform with containerized workloads using Singularity and job scheduling through SLURM.</p> <pre><code>graph TB\n    subgraph \"ILIFU Infrastructure\"\n        subgraph \"Cloud Platform\"\n            OS[OpenStack Cloud&lt;br/&gt;Infrastructure-as-a-Service]\n        end\n\n        subgraph \"Compute Resources\"\n            CN[Compute Nodes&lt;br/&gt;Max: 96 CPUs per job&lt;br/&gt;Max: 1500 GB RAM per job]\n        end\n\n        subgraph \"Container Platform\"\n            SP[Singularity Containers&lt;br/&gt;HPC-optimized&lt;br/&gt;Rootless execution]\n        end\n\n        subgraph \"Job Scheduler\"\n            SL[SLURM Workload Manager&lt;br/&gt;Max runtime: 336 hours]\n        end\n    end\n\n    subgraph \"Research Domains\"\n        AST[Astronomy&lt;br/&gt;MeerKAT, SKA]\n        BIO[Bioinformatics&lt;br/&gt;Genomics, Metagenomics]\n        DS[Data Science&lt;br/&gt;ML/AI Research]\n    end\n\n    OS --&gt; CN\n    CN --&gt; SP\n    SP --&gt; SL\n\n    AST --&gt; SL\n    BIO --&gt; SL\n    DS --&gt; SL\n\n    style OS fill:#e1f5fe\n    style CN fill:#e8f5e9\n    style SP fill:#fff3e0\n    style SL fill:#f3e5f5</code></pre> <p>Figure: ILIFU cloud infrastructure architecture supporting multiple research domains</p>"},{"location":"day2/hpc-ilifu-training/#resource-specifications","title":"Resource Specifications","text":"<p>Based on ILIFU's cloud infrastructure configuration:</p>"},{"location":"day2/hpc-ilifu-training/#maximum-job-resources","title":"Maximum Job Resources","text":"<ul> <li>CPUs: Up to 96 cores per job</li> <li>Memory: Up to 1500 GB (1.5 TB) RAM per job</li> <li>Runtime: Maximum 336 hours (14 days) per job</li> <li>Storage: Distributed file systems for large-scale data</li> </ul>"},{"location":"day2/hpc-ilifu-training/#key-features","title":"Key Features","text":"<ul> <li>OpenStack Platform: Provides flexible cloud computing resources</li> <li>Singularity Containers: Enables reproducible, portable workflows</li> <li>SLURM Scheduler: Manages resource allocation and job queuing</li> <li>Multi-domain Support: Serves astronomy, bioinformatics, and data science communities</li> </ul>"},{"location":"day2/hpc-ilifu-training/#access-methods","title":"Access Methods","text":"<ul> <li>SSH access to login nodes</li> <li>Jupyter notebooks for interactive computing</li> <li>Web-based interfaces for specific services</li> <li>API access for programmatic interaction</li> </ul>"},{"location":"day2/hpc-ilifu-training/#infrastructure-components","title":"Infrastructure Components","text":"Component Description Purpose OpenStack Cloud computing platform Infrastructure management and virtualization SLURM Workload manager Job scheduling and resource allocation Singularity Container platform Application deployment and portability CephFS Distributed storage High-performance shared file system Login Nodes Access points User entry and job submission Compute Nodes Processing units Actual computation execution <p>Note: ILIFU operates as a cloud infrastructure rather than a traditional fixed HPC cluster, allowing dynamic resource allocation based on user requirements. Specific hardware configurations may vary as resources are allocated on-demand through the OpenStack platform</p>"},{"location":"day2/hpc-ilifu-training/#getting-started-with-ilifu","title":"Getting Started with ILIFU","text":""},{"location":"day2/hpc-ilifu-training/#account-setup","title":"Account Setup","text":"<ol> <li>Request Access: Apply through your institution</li> <li>SSH Keys: Generate and register SSH key pairs</li> <li>VPN: Configure institutional VPN if required</li> <li>Initial Login: Connect to login nodes</li> </ol>"},{"location":"day2/hpc-ilifu-training/#basic-commands","title":"Basic Commands","text":"<pre><code># Login to ILIFU\nssh username@training.ilifu.ac.za\n\n# Check your home directory\nls -la ~\n\n# Check available modules\nmodule avail\n\n# Load a module\nmodule load python/3.12.3  # Or use system python3\n</code></pre> <p>\ud83d\udca1 Next Steps: After logging in, follow the hands-on exercises in High Performance Computing with SLURM: Practical Tutorial</p>"},{"location":"day2/hpc-ilifu-training/#file-system-layout","title":"File System Layout","text":"<pre><code>/home/username/          # Your home directory (limited space)\n/scratch/username/       # Temporary fast storage\n/data/project/          # Shared project data\n/software/              # Installed software\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#data-management","title":"Data Management","text":"<ul> <li>Home Directory: Small, backed up, permanent</li> <li>Scratch Space: Large, fast, temporary (auto-cleaned)</li> <li>Project Directories: Shared, persistent, for collaboration</li> </ul>"},{"location":"day2/hpc-ilifu-training/#slurm-basics","title":"SLURM Basics","text":"<p>\ud83d\udcda For detailed SLURM tutorials and exercises, see: High Performance Computing with SLURM: Practical Tutorial</p>"},{"location":"day2/hpc-ilifu-training/#what-is-slurm","title":"What is SLURM?","text":"<pre><code>graph TB\n    subgraph \"Users\"\n        U1[User 1]\n        U2[User 2]\n        U3[User 3]\n    end\n\n    subgraph \"Login Nodes\"\n        LN[Login Node&lt;br/&gt;- SSH Access&lt;br/&gt;- Job Submission&lt;br/&gt;- File Editing]\n    end\n\n    subgraph \"SLURM Controller\"\n        SC[SLURM Scheduler&lt;br/&gt;- Resource Allocation&lt;br/&gt;- Job Queuing&lt;br/&gt;- Priority Management]\n        DB[(Accounting&lt;br/&gt;Database)]\n    end\n\n    subgraph \"Compute Nodes\"\n        CN1[Compute Node 1&lt;br/&gt;CPUs: 32&lt;br/&gt;RAM: 128GB]\n        CN2[Compute Node 2&lt;br/&gt;CPUs: 32&lt;br/&gt;RAM: 128GB]\n        CN3[Compute Node 3&lt;br/&gt;CPUs: 32&lt;br/&gt;RAM: 128GB]\n        CNN[... More Nodes]\n    end\n\n    subgraph \"Storage\"\n        FS[Shared Filesystem&lt;br/&gt;/home&lt;br/&gt;/scratch&lt;br/&gt;/data]\n    end\n\n    U1 --&gt; LN\n    U2 --&gt; LN\n    U3 --&gt; LN\n\n    LN --&gt;|sbatch/srun| SC\n    SC --&gt; DB\n    SC --&gt;|Allocates| CN1\n    SC --&gt;|Allocates| CN2\n    SC --&gt;|Allocates| CN3\n    SC --&gt;|Allocates| CNN\n\n    CN1 --&gt; FS\n    CN2 --&gt; FS\n    CN3 --&gt; FS\n    CNN --&gt; FS\n    LN --&gt; FS\n\n    style U1 fill:#e1f5fe\n    style U2 fill:#e1f5fe\n    style U3 fill:#e1f5fe\n    style LN fill:#fff3e0\n    style SC fill:#f3e5f5\n    style DB fill:#f3e5f5\n    style CN1 fill:#e8f5e9\n    style CN2 fill:#e8f5e9\n    style CN3 fill:#e8f5e9\n    style CNN fill:#e8f5e9\n    style FS fill:#fce4ec</code></pre> <p>Figure: HPC cluster architecture showing the relationship between users, login nodes, SLURM scheduler, compute nodes, and shared storage</p>"},{"location":"day2/hpc-ilifu-training/#about-slurm","title":"About SLURM","text":"<p>Simple Linux Utility for Resource Management (SLURM) is a job scheduling and cluster management tool that:</p> <ul> <li>Job scheduler: Allocates compute resources efficiently among users</li> <li>Resource manager: Controls access to CPUs, memory, and other resources  </li> <li>Workload manager: Manages job queues and priorities based on fairness policies</li> <li>Framework components: Login nodes for access, compute nodes for execution, scheduler for coordination, and accounting database for tracking</li> </ul>"},{"location":"day2/hpc-ilifu-training/#key-slurm-concepts","title":"Key SLURM Concepts","text":"<ul> <li>Job: A computational task submitted to the cluster</li> <li>Partition: Group of nodes with similar characteristics</li> <li>Queue: Collection of jobs waiting for resources</li> <li>Node: Individual compute server</li> <li>Core/CPU: Processing unit within a node</li> </ul>"},{"location":"day2/hpc-ilifu-training/#basic-slurm-commands","title":"Basic SLURM Commands","text":"<pre><code># Submit a job\nsbatch job_script.sh\n\n# Check job status\nsqueue -u username\n\n# Cancel a job\nscancel job_id\n\n# Check node information\nsinfo\n\n# Check your job history\nsacct -u username\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-script-template","title":"Job Script Template","text":"<p>To create a job script, use the nano text editor:</p> <pre><code># Open nano editor to create your script\nnano my_job.sh\n</code></pre> <p>Then copy and paste the following template:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=my_job\n#SBATCH --partition=Main\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8GB\n#SBATCH --time=01:00:00\n#SBATCH --output=output_%j.log\n#SBATCH --error=error_%j.log\n\n# Load modules\nmodule load python/3.12.3  # Or use system python3\n\n# Run your program\npython my_script.py\n</code></pre> <p>To save and exit nano: - Press <code>Ctrl+X</code> to exit - Press <code>Y</code> to confirm save - Press <code>Enter</code> to accept the filename</p>"},{"location":"day2/hpc-ilifu-training/#slurm-directives-explained","title":"SLURM Directives Explained","text":"<ul> <li><code>--job-name</code>: Human-readable job name</li> <li><code>--partition</code>: Which partition to use</li> <li><code>--nodes</code>: Number of nodes required</li> <li><code>--ntasks-per-node</code>: Tasks per node</li> <li><code>--cpus-per-task</code>: CPUs per task</li> <li><code>--mem</code>: Memory requirement</li> <li><code>--time</code>: Maximum runtime</li> <li><code>--output/--error</code>: Log file locations</li> </ul>"},{"location":"day2/hpc-ilifu-training/#resource-management","title":"Resource Management","text":""},{"location":"day2/hpc-ilifu-training/#understanding-resources","title":"Understanding Resources","text":"<ul> <li>CPU Cores: Processing units</li> <li>Memory (RAM): Working memory</li> <li>GPU: Graphics processing units</li> <li>Storage: Disk space</li> <li>Network: Data transfer bandwidth</li> </ul>"},{"location":"day2/hpc-ilifu-training/#resource-allocation-strategies","title":"Resource Allocation Strategies","text":"<pre><code># CPU-intensive job\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=32GB\n\n# Memory-intensive job\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=64GB\n\n# GPU job\n#SBATCH --gres=gpu:1\n#SBATCH --partition=GPU\n\n# Parallel job\n#SBATCH --nodes=2\n#SBATCH --ntasks=32\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#monitoring-resource-usage","title":"Monitoring Resource Usage","text":"<pre><code># Check job efficiency\nseff job_id\n\n# Real-time job monitoring\nsstat job_id\n\n# Detailed job information\nscontrol show job job_id\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#best-practices","title":"Best Practices","text":""},{"location":"day2/hpc-ilifu-training/#job-submission","title":"Job Submission","text":"<ul> <li>Test small first: Start with short test runs</li> <li>Use checkpoints: Save progress regularly</li> <li>Estimate resources: Don't over-request</li> <li>Use appropriate partitions: Match job to partition</li> <li>Clean up: Remove temporary files</li> </ul>"},{"location":"day2/hpc-ilifu-training/#code-optimization","title":"Code Optimization","text":"<pre><code># Use parallel processing\n#SBATCH --cpus-per-task=8\n\n# In Python\nfrom multiprocessing import Pool\nwith Pool(8) as pool:\n    results = pool.map(my_function, data)\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#data-management_1","title":"Data Management","text":"<ul> <li>Use scratch space for temporary files</li> <li>Compress data when possible</li> <li>Clean up regularly</li> <li>Use appropriate file formats</li> </ul>"},{"location":"day2/hpc-ilifu-training/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":"<ul> <li>Requesting too many resources</li> <li>Running jobs on login nodes</li> <li>Not using version control</li> <li>Ignoring error messages</li> <li>Not testing scripts locally first</li> </ul>"},{"location":"day2/hpc-ilifu-training/#practical-examples","title":"Practical Examples","text":"<p>\ud83d\udcdd Complete Step-by-Step Tutorials: For detailed, hands-on SLURM exercises with explanations, see High Performance Computing with SLURM: Practical Tutorial</p>"},{"location":"day2/hpc-ilifu-training/#example-1-python-data-analysis","title":"Example 1: Python Data Analysis","text":"<p>To create this script:</p> <pre><code># Open nano editor\nnano data_analysis.sh\n\n# Copy and paste the script below, then:\n# Press Ctrl+X to exit\n# Press Y to save\n# Press Enter to confirm filename\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=data_analysis\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16GB\n#SBATCH --time=02:00:00\n#SBATCH --output=analysis_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n# Install with: pip install pandas numpy matplotlib\n\npython data_analysis.py input.csv\n</code></pre> <p>Submit with: <code>sbatch data_analysis.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#example-2-r-statistical-analysis","title":"Example 2: R Statistical Analysis","text":"<p>Create the script with nano:</p> <pre><code>nano r_stats.sh\n# Paste the script below, save with Ctrl+X, Y, Enter\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=r_stats\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8GB\n#SBATCH --time=01:30:00\n\nmodule load R/4.4.1  # Check available version\n\nRscript statistical_analysis.R\n</code></pre> <p>Submit with: <code>sbatch r_stats.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#example-3-gpu-machine-learning","title":"Example 3: GPU Machine Learning","text":"<p>Create the script:</p> <pre><code>nano ml_training.sh\n# Paste content, save with Ctrl+X, Y, Enter\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=ml_training\n#SBATCH --partition=GPU\n#SBATCH --gres=gpu:1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32GB\n#SBATCH --time=04:00:00\n\n# module load cuda  # Check if GPU/CUDA is available\nmodule load python/3.12.3  # Or use system python3\n\npython train_model.py\n</code></pre> <p>Submit with: <code>sbatch ml_training.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#example-4-array-jobs","title":"Example 4: Array Jobs","text":"<p>Create the array job script:</p> <pre><code>nano array_job.sh\n# Paste content, save with Ctrl+X, Y, Enter\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=array_job\n#SBATCH --partition=Main\n#SBATCH --array=1-100\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4GB\n#SBATCH --time=00:30:00\n\n# Process different files based on array index\ninput_file=\"data_${SLURM_ARRAY_TASK_ID}.txt\"\noutput_file=\"result_${SLURM_ARRAY_TASK_ID}.txt\"\n\npython process_data.py $input_file $output_file\n</code></pre> <p>Submit with: <code>sbatch array_job.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"day2/hpc-ilifu-training/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"day2/hpc-ilifu-training/#job-wont-start","title":"Job Won't Start","text":"<pre><code># Check partition limits\nscontrol show partition\n\n# Check job details\nscontrol show job job_id\n\n# Check node availability\nsinfo -N\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#out-of-memory-errors","title":"Out of Memory Errors","text":"<pre><code># Check memory usage\nsstat -j job_id --format=AveCPU,AvePages,AveRSS,AveVMSize\n\n# Increase memory request\n#SBATCH --mem=32GB\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-timeouts","title":"Job Timeouts","text":"<pre><code># Check time limits\nscontrol show partition\n\n# Increase time limit\n#SBATCH --time=04:00:00\n\n# Use checkpointing for long jobs\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#module-issues","title":"Module Issues","text":"<pre><code># List available modules\nmodule avail\n\n# Check module conflicts\nmodule list\n\n# Purge and reload\nmodule purge\nmodule load python/3.12.3  # Or use system python3\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Check ILIFU docs</li> <li>Help Desk: Submit support tickets</li> <li>Community: Ask on forums or Slack</li> <li>Training: Attend workshops</li> <li>Practical Tutorials: Work through High Performance Computing with SLURM: Practical Tutorial</li> </ul>"},{"location":"day2/hpc-ilifu-training/#quick-reference","title":"Quick Reference","text":""},{"location":"day2/hpc-ilifu-training/#essential-slurm-commands","title":"Essential SLURM Commands","text":"Command Purpose Example Output <code>sbatch script.sh</code> Submit job <code>Submitted batch job 10</code> <code>squeue -u $USER</code> Check your jobs Shows running/pending jobs <code>scancel job_id</code> Cancel job Terminates specified job <code>sinfo</code> Node information Shows partition and node status <code>sacct -j job_id</code> Job accounting Shows job completion details <code>seff job_id</code> Job efficiency Shows resource utilization"},{"location":"day2/hpc-ilifu-training/#example-command-outputs","title":"Example Command Outputs","text":""},{"location":"day2/hpc-ilifu-training/#checking-partition-information","title":"Checking Partition Information","text":"<pre><code>$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\ntraining*    up 14-00:00:0      7   idle compute-1-sep2025,compute-2-sep2025,compute-3-sep2025,compute-4-sep2025,compute-5-sep2025,compute-6-sep2025,compute-7-sep2025\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-submission-and-status","title":"Job Submission and Status","text":"<pre><code>$ sbatch hello.sh\nSubmitted batch job 10\n\n$ squeue -u mamana\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                10  training    hello   mamana  R       0:01      1 compute-1-sep2025\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-efficiency-report","title":"Job Efficiency Report","text":"<pre><code>$ seff 10\nJob ID: 10\nCluster: training\nUser/Group: mamana/training\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 1\nCPU Utilized: 00:00:00\nCPU Efficiency: 0.00% of 00:00:01 core-walltime\nJob Wall-clock time: 00:00:01\nMemory Utilized: 4.80 MB\nMemory Efficiency: 0.48% of 1.00 GB\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#common-sbatch-directives","title":"Common SBATCH Directives","text":"Directive Purpose Example <code>--job-name</code> Job name <code>my_analysis</code> <code>--partition</code> Partition <code>Main</code>, <code>GPU</code> <code>--cpus-per-task</code> CPU cores <code>4</code> <code>--mem</code> Memory <code>16GB</code> <code>--time</code> Runtime limit <code>02:00:00</code> <code>--gres</code> GPU resources <code>gpu:1</code>"},{"location":"day2/hpc-ilifu-training/#file-transfer","title":"File Transfer","text":"<pre><code># Upload data\nscp local_file.txt username@training.ilifu.ac.za:~/\n\n# Download results\nscp username@training.ilifu.ac.za:~/results.txt ./\n\n# Sync directories\nrsync -av local_dir/ username@training.ilifu.ac.za:~/remote_dir/\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#additional-resources","title":"Additional Resources","text":"<ul> <li>ILIFU Documentation: https://docs.ilifu.ac.za</li> <li>SLURM Documentation: https://slurm.schedmd.com/documentation.html</li> <li>HPC Best Practices: Various online resources</li> <li>Training Materials: Regular workshops and tutorials</li> </ul>"},{"location":"day2/slurm-practical-tutorial/","title":"High Performance Computing with SLURM: Practical Tutorial","text":""},{"location":"day2/slurm-practical-tutorial/#high-performance-computing-with-slurm-practical-tutorial","title":"High Performance Computing with SLURM: Practical Tutorial","text":""},{"location":"day2/slurm-practical-tutorial/#getting-started-setup-instructions","title":"Getting Started - Setup Instructions","text":"<p>Before starting the exercises, you need to set up your working environment and copy the sample data files to your home directory. Follow these steps:</p>"},{"location":"day2/slurm-practical-tutorial/#step-1-create-your-working-directory","title":"Step 1: Create Your Working Directory","text":"<pre><code># The mkdir command creates a new directory\n# The -p flag creates parent directories if they don't exist\nmkdir -p ~/hpc_practice\n\n# Change to your new working directory\n# The ~ symbol represents your home directory\ncd ~/hpc_practice\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-copy-sample-data-files","title":"Step 2: Copy Sample Data Files","text":"<pre><code># Copy all sample data from the shared course directory to your current directory\n# The -r flag means \"recursive\" - it copies directories and their contents\n# The * wildcard matches all files in the source directory\n# The . (dot) means \"current directory\" (where you are now)\ncp -r /cbio/training/courses/2025/micmet-genomics/sample-data/* .\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-verify-your-setup","title":"Step 3: Verify Your Setup","text":"<pre><code># List all files in your directory to confirm they copied correctly\n# The -l flag shows detailed information (permissions, size, date)\n# The -a flag shows all files including hidden ones (starting with .)\nls -la\n\n# You should see these files:\n# - sample.fastq.gz    : Compressed DNA sequencing data (gzipped FASTQ format)\n# - sample1.fastq      : Uncompressed sequencing reads for practice\n# - sample2.fastq      : Another set of sequencing reads\n# - reference.fasta    : Reference genome sequence for alignment exercises\n# - data.txt          : Tab-delimited data for text processing examples\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#what-these-files-contain","title":"What These Files Contain","text":"<ul> <li>FASTQ files: Contain DNA sequences and quality scores from sequencing machines</li> <li>FASTA files: Contain reference sequences without quality scores</li> <li>Text files: Contain structured data for analysis practice</li> </ul> <p>Now you're ready to start the exercises!</p>"},{"location":"day2/slurm-practical-tutorial/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites - Unix Commands</li> <li>Getting Started - Your First Job</li> <li>Basic Job Templates</li> <li>Python and Bash Examples</li> <li>Advanced Job Types</li> <li>Resource Optimization</li> <li>Troubleshooting Examples</li> </ol>"},{"location":"day2/slurm-practical-tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"day2/slurm-practical-tutorial/#essential-unix-commands-for-hpc","title":"Essential Unix Commands for HPC","text":"<p>Before submitting SLURM jobs, master these Unix commands for pathogen genomics:</p> <pre><code># Navigate and organize\nmkdir -p project/{data,results,scripts}\ncd project\npwd\n\n# Inspect FASTQ files\nzcat sample.fastq.gz | head -20\nzcat sample.fastq.gz | wc -l | awk '{print $1/4}'  # Count reads\n\n# Search and filter\ngrep \"^&gt;\" reference.fasta  # Find FASTA headers\ngrep -c \"PASS\" variants.vcf  # Count PASS variants\n\n# Process text\nawk '{print $1, $2}' data.txt\nsed 's/old/new/g' file.txt\n</code></pre> <p>\ud83d\udcda Full Unix guide: See Unix Commands for Pathogen Genomics - Practical Tutorial for comprehensive examples and exercises.</p>"},{"location":"day2/slurm-practical-tutorial/#tutorial-your-first-slurm-jobs-step-by-step","title":"Tutorial: Your First SLURM Jobs - Step by Step","text":""},{"location":"day2/slurm-practical-tutorial/#tutorial-overview","title":"Tutorial Overview","text":"<p>In this hands-on tutorial, you'll learn to:</p> <ol> <li>Write and submit your first SLURM job</li> <li>Monitor job status and view outputs</li> <li>Run Python scripts on HPC</li> <li>Process genomics data with SLURM</li> <li>Handle errors and optimize resources</li> </ol> <p>Time needed: 30-45 minutes Prerequisites: Basic Unix commands (covered above)</p>"},{"location":"day2/slurm-practical-tutorial/#tutorial-1-hello-world-on-hpc","title":"Tutorial 1: Hello World on HPC","text":""},{"location":"day2/slurm-practical-tutorial/#step-1-write-your-first-job-script","title":"Step 1: Write Your First Job Script","text":"<p>Create a simple SLURM job that prints a greeting:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=hello\n#SBATCH --time=00:05:00\n\necho \"Hello from HPC!\"\necho \"This job ran on node: $(hostname)\"\necho \"Current time: $(date)\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-save-the-script","title":"Step 2: Save the Script","text":"<pre><code># Use nano editor to create the file\nnano hello.sh\n\n# Paste the script above, then:\n# Press Ctrl+X to exit\n# Press Y to save\n# Press Enter to confirm filename\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-submit-your-job","title":"Step 3: Submit Your Job","text":"<pre><code># Submit the job to SLURM\nsbatch hello.sh\n</code></pre> <p>You'll see: <code>Submitted batch job 12345</code> (your job ID will differ)</p>"},{"location":"day2/slurm-practical-tutorial/#step-4-monitor-your-job","title":"Step 4: Monitor Your Job","text":"<pre><code># Check if your job is running\nsqueue -u $USER\n\n# You'll see something like:\n# JOBID PARTITION     NAME     USER ST       TIME  NODES\n# 12345      Main    hello  yourname  R       0:01      1\n# ST column: PD=Pending, R=Running, CG=Completing\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-5-view-the-output","title":"Step 5: View the Output","text":"<pre><code># Once job completes (status disappears from squeue)\n# View the output file (replace 12345 with your job ID)\ncat slurm-12345.out\n</code></pre> <p>Example run:</p> <pre><code>$ sbatch hello.sh\nSubmitted batch job 10\n\n$ squeue -u $USER\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                10  training    hello   mamana  R       0:01      1 compute-1-sep2025\n\n$ cat slurm-10.out\nHello from HPC!\nThis job ran on node: compute-1-sep2025\nCurrent time: Mon Sep 1 23:57:07 SAST 2025\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#tutorial-2-running-python-on-hpc","title":"Tutorial 2: Running Python on HPC","text":""},{"location":"day2/slurm-practical-tutorial/#step-1-create-a-python-job-script","title":"Step 1: Create a Python Job Script","text":"<p>Let's run Python code on the cluster:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=python_hello\n#SBATCH --time=00:10:00\n#SBATCH --mem=1GB\n\n# Load Python (or use system python3 if modules not available)\nmodule load python/3.12.3  # Or use system python3 || echo \"Using system Python\"\n\n# Run your Python script\npython3 &lt;&lt; 'EOF'\nprint(\"Hello from Python on HPC!\")\nimport os\nprint(f\"Running on: {os.uname().nodename}\")\n\n# Simple calculation\nresult = sum(range(1000))\nprint(f\"Sum of 0-999 = {result}\")\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-submit-and-monitor","title":"Step 2: Submit and Monitor","text":"<pre><code># Save the script\nnano python_job.sh\n# (paste script, save with Ctrl+X, Y, Enter)\n\n# Submit the job\nsbatch python_job.sh\n\n# Watch it run (updates every 2 seconds)\nwatch -n 2 squeue -u $USER\n# Press Ctrl+C to stop watching\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-check-the-output","title":"Step 3: Check the Output","text":"<pre><code># Find your output file\nls -lt slurm-*.out | head -5\n\n# View the results\ncat slurm-[JOBID].out\n</code></pre> <p>Expected output:</p> <pre><code>Hello from Python on HPC!\nRunning on: compute-1-sep2025\nSum of 0-999 = 499500\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"Problem Solution \"Module not found\" Use <code>python3</code> instead of loading module \"Python: command not found\" Check with <code>which python3</code> Job stays pending too long Check resources with <code>sinfo</code>"},{"location":"day2/slurm-practical-tutorial/#tutorial-3-real-genomics-analysis","title":"Tutorial 3: Real Genomics Analysis","text":""},{"location":"day2/slurm-practical-tutorial/#objective","title":"Objective","text":"<p>Process FASTQ files using SLURM, simulating a real bioinformatics pipeline.</p>"},{"location":"day2/slurm-practical-tutorial/#step-1-create-the-analysis-script","title":"Step 1: Create the Analysis Script","text":"<p>This script demonstrates a typical genomics workflow:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=fastq_analysis\n#SBATCH --time=00:05:00\n#SBATCH --mem=2GB\n#SBATCH --cpus-per-task=2\n\necho \"=== FASTQ Analysis Pipeline Starting ===\"\necho \"Job ID: $SLURM_JOB_ID\"\necho \"Node: $(hostname)\"\necho \"Start time: $(date)\"\n\n# Create sample FASTQ files for analysis\necho \"Creating sample FASTQ files...\"\ncat &gt; sample1.fastq &lt;&lt; 'EOF'\n@SEQ_1\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65\n@SEQ_2\nACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGT\n+\nBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\nEOF\n\ncat &gt; sample2.fastq &lt;&lt; 'EOF'\n@SEQ_3\nTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n+\nIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n@SEQ_4\nCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n+\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nEOF\n\necho \"Step 1: Initial file validation...\"\nsleep 45  # Simulate file checking and validation\n\necho \"Step 2: Sequence counting and basic stats...\"\nfor file in sample*.fastq; do\n    echo \"Processing $file...\"\n    sequences=$(wc -l &lt; \"$file\")\n    sequences=$((sequences / 4))\n    echo \"  Found $sequences sequences\"\n\n    # Simulate per-file analysis time\n    echo \"  Analyzing sequence lengths...\"\n    sleep 25  # Processing time per file\n\n    avg_length=60\n    echo \"  Average sequence length: ${avg_length}bp\"\ndone\n\necho \"Step 3: Quality score analysis...\"\necho \"Analyzing quality scores across all sequences...\"\nsleep 60  # Simulate quality analysis\n\necho \"Step 4: Generating contamination check...\"\necho \"Checking for adapter sequences and contaminants...\"\nsleep 45  # Simulate contamination screening\n\necho \"Step 5: Creating final summary report...\"\ntotal_sequences=0\nfor file in sample*.fastq; do\n    seqs=$(wc -l &lt; \"$file\")\n    seqs=$((seqs / 4))\n    total_sequences=$((total_sequences + seqs))\ndone\n\necho \"Step 6: Finalizing results and cleanup...\"\nsleep 20  # Final processing and cleanup\n\necho \"=== Analysis Complete ===\"\necho \"Total sequences analyzed: $total_sequences\"\necho \"Analysis completed at: $(date)\"\necho \"Total runtime: ~4 minutes\"\n\n# Create a summary file\ncat &gt; analysis_summary.txt &lt;&lt; EOF\nFASTQ Analysis Summary\n=====================\nTotal files processed: 2\nTotal sequences: $total_sequences\nAverage sequence length: 60bp\nQuality check: PASSED\nContamination check: CLEAN\nAnalysis date: $(date)\nEOF\n\necho \"Summary report saved to: analysis_summary.txt\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-submit-and-monitor-the-job","title":"Step 2: Submit and Monitor the Job","text":"<pre><code># Save the script\nnano fastq_analysis.sh\n\n# Submit the job\nsbatch fastq_analysis.sh\n# Note your job ID (e.g., \"Submitted batch job 12347\")\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-monitor-job-progress-in-real-time","title":"Step 3: Monitor Job Progress in Real-Time","text":"<p>Open multiple terminal windows to watch different aspects:</p> <p>Terminal 1: Submit and monitor queue</p> <pre><code># Submit the job\nsbatch fastq_analysis.sh\nSubmitted batch job 15\n\n# Watch it in the queue (run multiple times)\n# Watch the queue (repeat every 10 seconds)\nsqueue -u $USER\n# Status codes: PD=Pending, R=Running, CG=Completing\n</code></pre> <p>Terminal 2: Watch live output</p> <pre><code># Once job starts running (status = R), watch the output\ntail -f slurm-15.out\n# Press Ctrl+C to stop watching\n</code></pre> <p>Terminal 3: Check job details</p> <pre><code># Get detailed job information\nscontrol show job 15\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-4-understanding-job-states","title":"Step 4: Understanding Job States","text":"<p>During the 4-minute runtime, you'll observe these states:</p> Time Status What's Happening 0:00-0:05 PD (Pending) Job waiting for resources 0:05-4:00 R (Running) Job executing on compute node 4:00+ - (Completed) Job finished, no longer in queue <p>Timeline of analysis steps:</p> <ul> <li>0:00-0:45 - File validation</li> <li>0:45-1:35 - Sequence counting (sample1.fastq)</li> <li>1:35-2:25 - Sequence counting (sample2.fastq)  </li> <li>2:25-3:25 - Quality score analysis</li> <li>3:25-4:10 - Contamination screening</li> <li>4:10-4:30 - Final report generation</li> </ul> <p>Learning opportunity: This 4-minute window allows everyone to:</p> <ul> <li>Practice using <code>squeue</code> to monitor jobs multiple times</li> <li>See job state transitions and timing in real-time  </li> <li>Understand queue system behavior with sufficient time for discussion</li> <li>Watch live output with <code>tail -f</code> to see analysis progress</li> <li>Check intermediate results and final efficiency reports</li> </ul> <p>\ud83d\udca1 Training Tip: Have participants submit this job, then use the 4-minute window to demonstrate:</p> <ul> <li>Refreshing <code>squeue -u $USER</code> every 30 seconds to track progress</li> <li>Using <code>scontrol show job JOBID</code> for detailed job information</li> <li>Explaining what PENDING vs RUNNING states mean</li> <li>Demonstrating <code>tail -f slurm-JOBID.out</code> to watch live step-by-step output</li> <li>Discussing resource allocation while job runs</li> <li>Explaining the difference between walltime and CPU time</li> </ul> <p>Expected final output files:</p> <ul> <li><code>slurm-JOBID.out</code> - Complete log of all analysis steps</li> <li><code>analysis_summary.txt</code> - Final summary report</li> <li><code>sample1.fastq</code> &amp; <code>sample2.fastq</code> - Generated test data files</li> </ul> <p>Sample log output:</p> <pre><code>=== FASTQ Analysis Pipeline Starting ===\nJob ID: 15\nNode: compute-2-sep2025\nStart time: Mon Sep 2 10:15:23 SAST 2025\nCreating sample FASTQ files...\nStep 1: Initial file validation...\nStep 2: Sequence counting and basic stats...\nProcessing sample1.fastq...\n  Found 2 sequences\n  Analyzing sequence lengths...\n  Average sequence length: 60bp\nProcessing sample2.fastq...\n  Found 2 sequences\n  Analyzing sequence lengths...\n  Average sequence length: 60bp\nStep 3: Quality score analysis...\nAnalyzing quality scores across all sequences...\nStep 4: Generating contamination check...\nChecking for adapter sequences and contaminants...\nStep 5: Creating final summary report...\nStep 6: Finalizing results and cleanup...\n=== Analysis Complete ===\nTotal sequences analyzed: 4\nAnalysis completed at: Mon Sep 2 10:19:45 SAST 2025\nTotal runtime: ~4 minutes\nSummary report saved to: analysis_summary.txt\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#practice-exercises","title":"Practice Exercises","text":""},{"location":"day2/slurm-practical-tutorial/#exercise-1-modify-and-submit-a-job","title":"Exercise 1: Modify and Submit a Job","text":"<p>Task: Modify the hello.sh script to include your name and the current date.</p> <pre><code># Step 1: Edit the script\nnano hello.sh\n\n# Step 2: Add these lines:\necho \"Submitted by: [YOUR NAME]\"\necho \"Analysis date: $(date +%Y-%m-%d)\"\n\n# Step 3: Submit and check\nsbatch hello.sh\nsqueue -u $USER\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#exercise-2-resource-monitoring","title":"Exercise 2: Resource Monitoring","text":"<p>Task: Create a job that uses specific resources and monitor them.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=resource_test\n#SBATCH --time=00:02:00\n#SBATCH --mem=500MB\n#SBATCH --cpus-per-task=2\n\necho \"Allocated CPUs: $SLURM_CPUS_PER_TASK\"\necho \"Allocated Memory: $SLURM_MEM_PER_NODE MB\"\necho \"Running on node: $(hostname)\"\n\n# Use the allocated CPUs\nstress --cpu $SLURM_CPUS_PER_TASK --timeout 30s\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#exercise-3-array-jobs","title":"Exercise 3: Array Jobs","text":"<p>Task: Process multiple files in parallel using array jobs.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=array_demo\n#SBATCH --array=1-3\n#SBATCH --time=00:05:00\n\necho \"Processing file number: $SLURM_ARRAY_TASK_ID\"\n# Your processing command here\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#basic-templates","title":"Basic Templates","text":""},{"location":"day2/slurm-practical-tutorial/#1-standard-job-template","title":"1. Standard Job Template","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=my_job          # Give your job a name\n#SBATCH --time=01:00:00            # Max runtime (1 hour)\n#SBATCH --mem=4GB                  # Memory needed\n#SBATCH --output=output_%j.log     # Output file (%j = job ID)\n\n# Load software you need\nmodule load python/3.12.3  # Or use system python3\n\n# Run your command\necho \"Job started on $(hostname) at $(date)\"\npython my_script.py\necho \"Job completed at $(date)\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-multi-core-parallel-job","title":"2. Multi-core Parallel Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=parallel_job\n#SBATCH --partition=Main\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16GB\n#SBATCH --time=02:00:00\n#SBATCH --output=parallel_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n\n# Use all available cores\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\necho \"Using $SLURM_CPUS_PER_TASK CPU cores\"\npython parallel_script.py\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#python-and-bash-examples","title":"Python and Bash Examples","text":""},{"location":"day2/slurm-practical-tutorial/#python-jobs","title":"Python Jobs","text":""},{"location":"day2/slurm-practical-tutorial/#basic-python-analysis","title":"Basic Python Analysis","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=python_analysis\n#SBATCH --time=01:30:00\n#SBATCH --mem=8GB\n#SBATCH --cpus-per-task=4\n\n# Load Python module\nmodule load python/3.12.3  # Or use system python3\n\n# Run your analysis\npython genome_analysis.py sample_data.fasta\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#python-with-virtual-environment","title":"Python with Virtual Environment","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=python_venv\n#SBATCH --time=02:00:00\n#SBATCH --mem=16GB\n\nmodule load python/3.12.3  # Or use system python3\n\n# Create and activate virtual environment\npython -m venv pathogen_env\nsource pathogen_env/bin/activate\n\n# Install bioinformatics packages\npip install biopython pandas numpy matplotlib\n\n# Run pathogen analysis\npython pathogen_analysis.py\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#pathogen-genomics-snp-analysis","title":"Pathogen Genomics - SNP Analysis","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=snp_analysis\n#SBATCH --time=04:00:00\n#SBATCH --mem=32GB\n#SBATCH --cpus-per-task=8\n\nmodule load python/3.12.3  # Or use system python3\n\n# Python script for SNP analysis\npython &lt;&lt; 'EOF'\nimport pandas as pd\nfrom multiprocessing import Pool\nimport os\n\ndef analyze_sample(vcf_file):\n    \"\"\"Analyze SNPs in a VCF file\"\"\"\n    print(f\"Processing {vcf_file}\")\n\n    # Count SNPs (simplified example)\n    with open(vcf_file, 'r') as f:\n        snp_count = sum(1 for line in f if not line.startswith('#'))\n\n    return vcf_file, snp_count\n\n# Get all VCF files\nvcf_files = [f for f in os.listdir('.') if f.endswith('.vcf')]\n\n# Use all available CPU cores\nwith Pool(int(os.environ['SLURM_CPUS_PER_TASK'])) as pool:\n    results = pool.map(analyze_sample, vcf_files)\n\n# Save results\nresults_df = pd.DataFrame(results, columns=['Sample', 'SNP_Count'])\nresults_df.to_csv('snp_analysis_results.csv', index=False)\nprint(f\"Analyzed {len(vcf_files)} samples\")\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#bashshell-script-jobs","title":"Bash/Shell Script Jobs","text":""},{"location":"day2/slurm-practical-tutorial/#basic-fastq-processing","title":"Basic FASTQ Processing","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=fastq_processing\n#SBATCH --time=01:00:00\n#SBATCH --mem=4GB\n\n# Process multiple FASTQ files\nfor file in *.fastq; do\n    echo \"Processing $file...\"\n\n    # Count sequences (FASTQ has 4 lines per sequence)\n    sequences=$(wc -l &lt; \"$file\")\n    sequences=$((sequences / 4))\n\n    # Get basic stats\n    echo \"File: $file - Sequences: $sequences\"\n\n    # Count reads with quality scores above threshold\n    good_reads=$(awk 'NR%4==0 &amp;&amp; length($0)&gt;20' \"$file\" | wc -l)\n    echo \"High quality reads: $good_reads\"\ndone\n\necho \"Processing complete!\"\n</code></pre> <p>Expected output:</p> <pre><code>Processing sample1.fastq...\nFile: sample1.fastq - Sequences: 3\nHigh quality reads: 3\nProcessing sample2.fastq...\nFile: sample2.fastq - Sequences: 2\nHigh quality reads: 2\nProcessing complete!\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#pathogen-genomics-pipeline","title":"Pathogen Genomics Pipeline","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=pathogen_pipeline\n#SBATCH --time=06:00:00\n#SBATCH --mem=64GB\n#SBATCH --cpus-per-task=16\n\n# Load bioinformatics tools\nmodule load fastqc/0.12.1  # Check available version with 'module avail'\n# module load trimmomatic  # Install if needed\nmodule load bwa/github  # Check available version\nmodule load samtools/1.22.1\nmodule load bcftools/1.22\n\n# Sample information\nSAMPLE=\"pathogen_sample\"\nREFERENCE=\"reference_genome.fasta\"\n\necho \"=== Pathogen Genomics Pipeline Starting ===\"\necho \"Sample: $SAMPLE\"\necho \"Reference: $REFERENCE\"\necho \"CPUs: $SLURM_CPUS_PER_TASK\"\n\n# Step 1: Quality control\necho \"Step 1: Running FastQC...\"\nmkdir -p qc_reports\nfastqc \"${SAMPLE}_R1.fastq\" \"${SAMPLE}_R2.fastq\" -o qc_reports/\n\n# Step 2: Trim low-quality reads and adapters\necho \"Step 2: Trimming reads...\"\ntrimmomatic PE -threads $SLURM_CPUS_PER_TASK \\\n    \"${SAMPLE}_R1.fastq\" \"${SAMPLE}_R2.fastq\" \\\n    \"${SAMPLE}_R1_trimmed.fastq\" \"${SAMPLE}_R1_unpaired.fastq\" \\\n    \"${SAMPLE}_R2_trimmed.fastq\" \"${SAMPLE}_R2_unpaired.fastq\" \\\n    ILLUMINACLIP:adapters.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n\n# Step 3: Align reads to reference genome\necho \"Step 3: Aligning to reference genome...\"\nbwa mem -t $SLURM_CPUS_PER_TASK \"$REFERENCE\" \\\n    \"${SAMPLE}_R1_trimmed.fastq\" \"${SAMPLE}_R2_trimmed.fastq\" | \\\n    samtools sort -@ $SLURM_CPUS_PER_TASK -o \"${SAMPLE}_sorted.bam\"\n\n# Step 4: Index BAM file\necho \"Step 4: Indexing BAM file...\"\nsamtools index \"${SAMPLE}_sorted.bam\"\n\n# Step 5: Variant calling\necho \"Step 5: Calling variants...\"\nbcftools mpileup -f \"$REFERENCE\" \"${SAMPLE}_sorted.bam\" | \\\n    bcftools call -mv -Oz -o \"${SAMPLE}_variants.vcf.gz\"\n\n# Step 6: Index VCF and get basic stats\necho \"Step 6: Processing variants...\"\nbcftools index \"${SAMPLE}_variants.vcf.gz\"\nbcftools stats \"${SAMPLE}_variants.vcf.gz\" &gt; \"${SAMPLE}_variant_stats.txt\"\n\n# Summary\necho \"=== Pipeline Summary ===\"\necho \"Alignment stats:\"\nsamtools flagstat \"${SAMPLE}_sorted.bam\"\n\necho \"Variant counts:\"\nbcftools view -H \"${SAMPLE}_variants.vcf.gz\" | wc -l\n\necho \"=== Pathogen Genomics Pipeline Complete ===\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#multi-sample-outbreak-analysis","title":"Multi-Sample Outbreak Analysis","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=outbreak_analysis\n#SBATCH --time=08:00:00\n#SBATCH --mem=128GB\n#SBATCH --cpus-per-task=32\n\n# Load required modules\nmodule load python/3.12.3  # Or use system python3\nmodule load iqtree/2.2.0\nmodule load mafft/7.490\n\necho \"=== Multi-Sample Outbreak Analysis ===\"\n\n# Step 1: Concatenate all consensus sequences\necho \"Step 1: Preparing sequences for phylogenetic analysis...\"\ncat *.consensus.fasta &gt; all_samples.fasta\n\n# Step 2: Multiple sequence alignment\necho \"Step 2: Performing multiple sequence alignment...\"\nmafft --auto --thread $SLURM_CPUS_PER_TASK all_samples.fasta &gt; aligned_sequences.fasta\n\n# Step 3: Build phylogenetic tree\necho \"Step 3: Building phylogenetic tree...\"\niqtree2 -s aligned_sequences.fasta -nt $SLURM_CPUS_PER_TASK -bb 1000\n\n# Step 4: Calculate pairwise distances\necho \"Step 4: Calculating genetic distances...\"\npython &lt;&lt; 'EOF'\nfrom Bio import AlignIO\nfrom Bio.Phylo.TreeConstruction import DistanceCalculator\nimport pandas as pd\n\n# Read alignment\nalignment = AlignIO.read(\"aligned_sequences.fasta\", \"fasta\")\n\n# Calculate distances\ncalculator = DistanceCalculator('identity')\ndistance_matrix = calculator.get_distance(alignment)\n\n# Convert to DataFrame for easier handling\nsamples = [record.id for record in alignment]\ndist_df = pd.DataFrame(distance_matrix.matrix, \n                      index=samples, \n                      columns=samples)\n\n# Save distance matrix\ndist_df.to_csv('genetic_distances.csv')\n\n# Find closely related samples (distance &lt; 0.001)\nclose_pairs = []\nfor i, sample1 in enumerate(samples):\n    for j, sample2 in enumerate(samples[i+1:], i+1):\n        distance = distance_matrix.matrix[i][j]\n        if distance &lt; 0.001:  # Very similar sequences\n            close_pairs.append([sample1, sample2, distance])\n\nif close_pairs:\n    close_df = pd.DataFrame(close_pairs, \n                           columns=['Sample1', 'Sample2', 'Distance'])\n    close_df.to_csv('potential_transmission_links.csv', index=False)\n    print(f\"Found {len(close_pairs)} potential transmission links\")\nelse:\n    print(\"No closely related samples found\")\nEOF\n\necho \"=== Outbreak Analysis Complete ===\"\necho \"Results:\"\necho \"- Phylogenetic tree: aligned_sequences.fasta.treefile\"\necho \"- Genetic distances: genetic_distances.csv\"\necho \"- Potential links: potential_transmission_links.csv\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#advanced-job-types","title":"Advanced Job Types","text":""},{"location":"day2/slurm-practical-tutorial/#1-array-jobs","title":"1. Array Jobs","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=array_processing\n#SBATCH --partition=Main\n#SBATCH --array=1-100%10        # 100 jobs, max 10 concurrent\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4GB\n#SBATCH --time=00:30:00\n#SBATCH --output=array_%A_%a.log\n\nmodule load python/3.12.3  # Or use system python3\n\n# Use array task ID to process different files\nINPUT_FILE=\"input_${SLURM_ARRAY_TASK_ID}.txt\"\nOUTPUT_FILE=\"output_${SLURM_ARRAY_TASK_ID}.txt\"\n\necho \"Processing $INPUT_FILE on $(hostname)\"\npython process_file.py $INPUT_FILE $OUTPUT_FILE\n\necho \"Task $SLURM_ARRAY_TASK_ID completed\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-job-dependencies","title":"2. Job Dependencies","text":"<pre><code>#!/bin/bash\n# Submit first job\nJOB1=$(sbatch --parsable preprocess.sh)\n\n# Submit second job that depends on first\nJOB2=$(sbatch --parsable --dependency=afterok:$JOB1 analysis.sh)\n\n# Submit final job that depends on second\nsbatch --dependency=afterok:$JOB2 postprocess.sh\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#3-multi-node-mpi-job","title":"3. Multi-node MPI Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=mpi_job\n#SBATCH --partition=Main\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=16\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=2GB\n#SBATCH --time=04:00:00\n\n# module load openmpi  # Check if MPI is available\n\n# Total tasks = nodes * ntasks-per-node = 4 * 16 = 64\necho \"Running on $SLURM_NNODES nodes with $SLURM_NTASKS total tasks\"\n\nmpirun ./my_mpi_program input.dat\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#4-interactive-job","title":"4. Interactive Job","text":"<pre><code>graph TB\n    subgraph \"Interactive Jobs\"\n        I1[User logs in] --&gt; I2[Request interactive session&lt;br/&gt;sinteractive/srun --pty]\n        I2 --&gt; I3[Wait for resources]\n        I3 --&gt; I4[Get shell on compute node]\n        I4 --&gt; I5[Run commands interactively]\n        I5 --&gt; I6[See output in real-time]\n        I6 --&gt; I7[Exit when done]\n\n        style I4 fill:#e8f5e9\n        style I5 fill:#e8f5e9\n        style I6 fill:#e8f5e9\n    end\n\n    subgraph \"Batch Jobs\"\n        B1[User logs in] --&gt; B2[Write job script]\n        B2 --&gt; B3[Submit with sbatch]\n        B3 --&gt; B4[Job queued]\n        B4 --&gt; B5[Job runs automatically]\n        B5 --&gt; B6[Output to files]\n        B6 --&gt; B7[Check results later]\n\n        style B5 fill:#e1f5fe\n        style B6 fill:#e1f5fe\n    end\n\n    subgraph \"When to Use\"\n        UI[Interactive: Development,&lt;br/&gt;Testing, Debugging]\n        UB[Batch: Production runs,&lt;br/&gt;Long jobs, Multiple jobs]\n    end\n\n    I7 --&gt; UI\n    B7 --&gt; UB</code></pre> <p>Figure: Comparison between interactive and batch job workflows in SLURM</p> <pre><code># Request interactive session using sinteractive (ILIFU-specific)\nsinteractive -c 1 --time 03:00                    # 1 CPU for 3 hours (default)\nsinteractive -c 5 --time 5-00:00                 # 5 CPUs for 5 days (maximum)\n\n# Alternative: Use srun for interactive session\nsrun --partition=Main --cpus-per-task=4 --mem=8GB --time=02:00:00 --pty bash\n\n# Once in interactive session:\nmodule load python/3.12.3  # Or use system python3\npython  # Start interactive Python\n</code></pre> <p>Note: Resources on the Devel partition are shared (CPU and memory). For dedicated resources, use <code>srun</code> on the Main partition.</p>"},{"location":"day2/slurm-practical-tutorial/#5-jupyter-notebook-on-compute-node","title":"5. Jupyter Notebook on Compute Node","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=jupyter\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16GB\n#SBATCH --time=04:00:00\n#SBATCH --output=jupyter_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n\n# Install jupyter if needed\npip install --user jupyter\n\n# Get node info\nNODE=$(hostname -s)\nPORT=8888\n\necho \"Starting Jupyter notebook on node $NODE, port $PORT\"\necho \"SSH tunnel command:\"\necho \"ssh -N -L ${PORT}:${NODE}:${PORT} ${USER}@training.ilifu.ac.za\"\n\n# Start Jupyter\njupyter notebook --no-browser --port=$PORT --ip=0.0.0.0\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#resource-optimization","title":"Resource Optimization","text":""},{"location":"day2/slurm-practical-tutorial/#1-memory-optimization-examples","title":"1. Memory Optimization Examples","text":""},{"location":"day2/slurm-practical-tutorial/#low-memory-job","title":"Low Memory Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=low_mem\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=2GB           # Conservative memory request\n#SBATCH --time=01:00:00\n\nmodule load python/3.12.3  # Or use system python3\n\n# Process data in chunks to save memory\npython &lt;&lt; 'EOF'\nimport pandas as pd\n\n# Read in chunks instead of loading entire file\nchunk_size = 10000\nresults = []\n\nfor chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n    # Process chunk\n    processed = chunk.groupby('category').sum()\n    results.append(processed)\n\n# Combine results\nfinal_result = pd.concat(results)\nfinal_result.to_csv('output.csv')\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#memory-intensive-job","title":"Memory-intensive Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=high_mem\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=64GB          # High memory for large datasets\n#SBATCH --time=04:00:00\n\nmodule load python/3.12.3  # Or use system python3\n\n# Load large dataset into memory\npython &lt;&lt; 'EOF'\nimport pandas as pd\nimport numpy as np\n\n# Load entire large dataset\ndf = pd.read_csv('very_large_file.csv')\nprint(f\"Loaded dataset with shape: {df.shape}\")\n\n# Memory-intensive operations\ncorrelation_matrix = df.corr()\ncorrelation_matrix.to_csv('correlations.csv')\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-time-optimization","title":"2. Time Optimization","text":""},{"location":"day2/slurm-practical-tutorial/#checkpointing-example","title":"Checkpointing Example","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=checkpointed_job\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16GB\n#SBATCH --time=02:00:00\n#SBATCH --output=checkpoint_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n\npython &lt;&lt; 'EOF'\nimport pickle\nimport os\nimport time\n\ncheckpoint_file = 'checkpoint.pkl'\n\n# Try to load previous state\nif os.path.exists(checkpoint_file):\n    with open(checkpoint_file, 'rb') as f:\n        state = pickle.load(f)\n    start_iteration = state['iteration']\n    results = state['results']\n    print(f\"Resuming from iteration {start_iteration}\")\nelse:\n    start_iteration = 0\n    results = []\n    print(\"Starting from scratch\")\n\n# Main computation loop\nfor i in range(start_iteration, 1000):\n    # Simulate some work\n    time.sleep(1)\n    result = i ** 2\n    results.append(result)\n\n    # Save checkpoint every 100 iterations\n    if i % 100 == 0:\n        state = {'iteration': i + 1, 'results': results}\n        with open(checkpoint_file, 'wb') as f:\n            pickle.dump(state, f)\n        print(f\"Checkpoint saved at iteration {i}\")\n\nprint(\"Computation completed\")\n\n# Clean up checkpoint file\nif os.path.exists(checkpoint_file):\n    os.remove(checkpoint_file)\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#troubleshooting-examples","title":"Troubleshooting Examples","text":""},{"location":"day2/slurm-practical-tutorial/#1-debug-job-failures","title":"1. Debug Job Failures","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=debug_job\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4GB\n#SBATCH --time=00:15:00\n#SBATCH --output=debug_%j.log\n#SBATCH --error=debug_%j.err\n\n# Enable debugging\nset -e  # Exit on any error\nset -x  # Print commands as they execute\n\necho \"=== Environment Information ===\"\necho \"Node: $(hostname)\"\necho \"Date: $(date)\"\necho \"Working directory: $(pwd)\"\necho \"User: $(whoami)\"\necho \"SLURM Job ID: $SLURM_JOB_ID\"\necho \"SLURM CPUs: $SLURM_CPUS_PER_TASK\"\n\necho \"=== Module Information ===\"\nmodule list\n\necho \"=== Python Information ===\"\nmodule load python/3.12.3  # Or use system python3\nwhich python\npython --version\n\necho \"=== Running Script ===\"\npython my_script.py 2&gt;&amp;1 | tee python_output.log\n\necho \"=== Job Completed ===\"\necho \"Exit code: $?\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-memory-usage-monitoring","title":"2. Memory Usage Monitoring","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=memory_monitor\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8GB\n#SBATCH --time=01:00:00\n\n# Function to monitor memory usage\nmonitor_memory() {\n    while true; do\n        echo \"$(date): Memory usage: $(free -h | grep '^Mem' | awk '{print $3}')\"\n        sleep 30\n    done\n}\n\n# Start memory monitoring in background\nmonitor_memory &amp;\nMONITOR_PID=$!\n\n# Load modules and run main task\nmodule load python/3.12.3  # Or use system python3\npython memory_intensive_script.py\n\n# Stop monitoring\nkill $MONITOR_PID\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#3-file-permission-issues","title":"3. File Permission Issues","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=file_check\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=2GB\n#SBATCH --time=00:10:00\n\necho \"=== File System Checks ===\"\n\n# Check input files exist and are readable\nINPUT_FILES=(\"input1.txt\" \"input2.txt\" \"config.json\")\n\nfor file in \"${INPUT_FILES[@]}\"; do\n    if [[ -f \"$file\" ]]; then\n        if [[ -r \"$file\" ]]; then\n            echo \"\u2713 $file exists and is readable\"\n        else\n            echo \"\u2717 $file exists but is not readable\"\n            ls -l \"$file\"\n            exit 1\n        fi\n    else\n        echo \"\u2717 $file does not exist\"\n        exit 1\n    fi\ndone\n\n# Check output directory is writable\nOUTPUT_DIR=\"results\"\nif [[ ! -d \"$OUTPUT_DIR\" ]]; then\n    mkdir -p \"$OUTPUT_DIR\" || {\n        echo \"\u2717 Cannot create output directory $OUTPUT_DIR\"\n        exit 1\n    }\nfi\n\nif [[ -w \"$OUTPUT_DIR\" ]]; then\n    echo \"\u2713 Output directory $OUTPUT_DIR is writable\"\nelse\n    echo \"\u2717 Output directory $OUTPUT_DIR is not writable\"\n    ls -ld \"$OUTPUT_DIR\"\n    exit 1\nfi\n\necho \"All file checks passed!\"\n\n# Proceed with actual work\npython main_script.py\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#job-submission-scripts","title":"Job Submission Scripts","text":""},{"location":"day2/slurm-practical-tutorial/#batch-submit-multiple-jobs","title":"Batch Submit Multiple Jobs","text":"<pre><code>#!/bin/bash\n# submit_multiple.sh - Submit multiple related jobs\n\n# Array of input files\nINPUT_FILES=(data1.txt data2.txt data3.txt data4.txt)\n\n# Submit a job for each input file\nfor i in \"${!INPUT_FILES[@]}\"; do\n    input_file=\"${INPUT_FILES[$i]}\"\n    job_name=\"process_$(basename $input_file .txt)\"\n\n    echo \"Submitting job for $input_file\"\n\n    sbatch --job-name=\"$job_name\" \\\n           --output=\"${job_name}_%j.log\" \\\n           --export=INPUT_FILE=\"$input_file\" \\\n           process_template.sh\n\n    sleep 1  # Brief pause between submissions\ndone\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#template-with-environment-variables","title":"Template with Environment Variables","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=templated_job\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8GB\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.log  # %x = job name, %j = job id\n\n# Use environment variables passed from submission script\necho \"Processing file: $INPUT_FILE\"\necho \"Output directory: $OUTPUT_DIR\"\necho \"Parameters: $PARAMS\"\n\nmodule load python/3.12.3  # Or use system python3\n\n# Use the variables in your script\npython analysis.py \\\n    --input \"$INPUT_FILE\" \\\n    --output \"$OUTPUT_DIR\" \\\n    --params \"$PARAMS\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#performance-testing-template","title":"Performance Testing Template","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=performance_test\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16GB\n#SBATCH --time=01:00:00\n#SBATCH --output=perf_%j.log\n\necho \"=== Performance Test Started ===\"\necho \"Job ID: $SLURM_JOB_ID\"\necho \"Node: $(hostname)\"\necho \"CPUs allocated: $SLURM_CPUS_PER_TASK\"\necho \"Memory allocated: ${SLURM_MEM_PER_NODE}MB\"\necho \"Start time: $(date)\"\n\n# Record resource usage\necho \"=== Initial Resource Usage ===\"\nfree -h\ndf -h $HOME\ndf -h /scratch/$USER\n\nmodule load python/3.12.3  # Or use system python3\n\n# Time the main computation\necho \"=== Starting Main Computation ===\"\nstart_time=$(date +%s)\n\npython performance_test_script.py\n\nend_time=$(date +%s)\nruntime=$((end_time - start_time))\n\necho \"=== Performance Summary ===\"\necho \"Runtime: ${runtime} seconds\"\necho \"End time: $(date)\"\n\n# Check final resource usage\necho \"=== Final Resource Usage ===\"\nfree -h\n\necho \"=== Performance Test Completed ===\"\n</code></pre> <p>This comprehensive set of SLURM examples covers most common use cases and provides templates that can be adapted for specific needs. Each example includes comments explaining the key parameters and concepts.</p>"},{"location":"day2/unix-commands-pathogen-examples/","title":"Unix Commands for Pathogen Genomics - Practical Tutorial","text":""},{"location":"day2/unix-commands-pathogen-examples/#unix-commands-for-pathogen-genomics-practical-tutorial","title":"Unix Commands for Pathogen Genomics - Practical Tutorial","text":"<p>Adapted from Microbial-Genomics practice scripts</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-overview","title":"Tutorial Overview","text":""},{"location":"day2/unix-commands-pathogen-examples/#what-youll-learn","title":"What You'll Learn","text":"<p>This hands-on tutorial will teach you essential Unix commands for pathogen genomics analysis. By the end, you'll be able to: - Navigate and organize genomics project directories - Process FASTQ and FASTA files efficiently - Extract meaningful information from sequencing data - Build simple analysis pipelines - Prepare data for HPC analysis</p>"},{"location":"day2/unix-commands-pathogen-examples/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic terminal/command line access</li> <li>No prior Unix experience required</li> <li>Access to the training server</li> </ul>"},{"location":"day2/unix-commands-pathogen-examples/#time-required","title":"Time Required","text":"<ul> <li>Complete tutorial: 2-3 hours</li> <li>Quick essentials: 45 minutes</li> </ul>"},{"location":"day2/unix-commands-pathogen-examples/#learning-path","title":"Learning Path","text":"<ol> <li>Setup \u2192 2. Basic Navigation \u2192 3. File Operations \u2192 4. Data Processing \u2192 5. Pipeline Building</li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#setup-instructions","title":"Setup Instructions","text":"<p>Before starting the Unix command exercises, you need to prepare your workspace with sample genomics data. Here's how:</p>"},{"location":"day2/unix-commands-pathogen-examples/#step-1-create-your-practice-directory","title":"Step 1: Create Your Practice Directory","text":"<pre><code># Create a new directory for your practice exercises\n# mkdir = \"make directory\"\n# -p = create parent directories if needed (won't error if directory exists)\n# ~ = shortcut for your home directory (/home/username)\nmkdir -p ~/hpc_practice\n\n# Navigate into your new directory\n# cd = \"change directory\"\ncd ~/hpc_practice\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-copy-sample-genomics-data","title":"Step 2: Copy Sample Genomics Data","text":"<pre><code># Copy all sample files from the shared course folder to your current location\n# cp = \"copy\" command\n# -r = \"recursive\" - copy directories and all their contents\n# * = wildcard that matches all files\n# . = current directory (destination)\ncp -r /cbio/training/courses/2025/micmet-genomics/sample-data/* .\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-verify-your-files","title":"Step 3: Verify Your Files","text":"<pre><code># List all files with details to confirm the copy was successful\n# ls = \"list\" command\n# -l = long format (shows permissions, size, dates)\n# -a = show all files (including hidden files starting with .)\nls -la\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#understanding-your-sample-files","title":"Understanding Your Sample Files","text":"<p>The following files are now in your directory for practice:</p> File Description Used For <code>sample.fastq.gz</code> Compressed DNA sequencing reads Learning <code>zcat</code>, <code>gunzip</code>, file compression <code>sample1.fastq</code> Uncompressed sequencing data (3 reads) Practicing <code>grep</code>, <code>wc</code>, sequence counting <code>sample2.fastq</code> Another FASTQ file (2 reads) Array processing, file comparisons <code>reference.fasta</code> Reference genome sequences Learning <code>grep</code> with FASTA headers, sequence extraction <code>data.txt</code> Tab-delimited sample metadata Practicing <code>awk</code>, <code>sed</code>, <code>cut</code>, <code>sort</code> commands <p>File Formats Explained: - FASTQ: Contains sequences + quality scores (4 lines per read) - FASTA: Contains sequences only (2 lines per sequence: header + sequence) - GZ: Gzip compressed file (saves space, common in genomics)</p>"},{"location":"day2/unix-commands-pathogen-examples/#quick-command-reference-with-detailed-explanations","title":"Quick Command Reference with Detailed Explanations","text":""},{"location":"day2/unix-commands-pathogen-examples/#essential-commands-for-genomics-analysis","title":"Essential Commands for Genomics Analysis","text":"<p>Before diving into detailed modules, here's a quick reference of the most commonly used commands in pathogen genomics, with detailed explanations of what each component does:</p>"},{"location":"day2/unix-commands-pathogen-examples/#navigate-and-organize","title":"Navigate and Organize","text":"<pre><code># Create nested directories for a genomics project\nmkdir -p project/{data,results,scripts}\n# Explanation:\n# mkdir = make directory command\n# -p = create parent directories as needed (won't error if they exist)\n# project/ = main project folder\n# {data,results,scripts} = brace expansion creates 3 subdirectories at once\n#   - data/ for raw sequencing files\n#   - results/ for analysis outputs\n#   - scripts/ for your analysis code\n\n# Navigate to your project directory\ncd project\n# cd = change directory\n# project = destination directory (relative path from current location)\n\n# Show current working directory\npwd\n# pwd = print working directory\n# Returns absolute path like: /home/username/hpc_practice/project\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#inspect-fastq-files","title":"Inspect FASTQ Files","text":"<pre><code># View compressed FASTQ file content\nzcat sample.fastq.gz | head -20\n# Explanation:\n# zcat = view compressed file without extracting (z = gzip, cat = concatenate)\n# sample.fastq.gz = compressed FASTQ file (common in genomics to save space)\n# | = pipe operator, sends output to next command\n# head -20 = show first 20 lines (5 complete reads since FASTQ uses 4 lines/read)\n\n# Count number of reads in FASTQ file\nzcat sample.fastq.gz | wc -l | awk '{print $1/4}'\n# Explanation:\n# zcat sample.fastq.gz = decompress and output file content\n# wc -l = word count with -l flag counts lines\n# | = pipe the line count to awk\n# awk '{print $1/4}' = divide line count by 4 (FASTQ has 4 lines per read)\n#   - $1 = first field (the line count)\n#   - /4 = division to get read count\n# Example: 400 lines / 4 = 100 reads\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#search-and-filter","title":"Search and Filter","text":"<pre><code># Find all FASTA headers in reference genome\ngrep \"^&gt;\" reference.fasta\n# Explanation:\n# grep = global regular expression print (searches for patterns)\n# \"^&gt;\" = pattern to search for\n#   - ^ = start of line anchor (line must begin with &gt;)\n#   - &gt; = literal \"&gt;\" character (FASTA headers start with &gt;)\n# reference.fasta = file to search in\n# Output: Shows all sequence headers like \"&gt;chr1\", \"&gt;gene_ABC123\"\n\n# Count high-quality variants\ngrep -c \"PASS\" variants.vcf\n# Explanation:\n# grep = search command\n# -c = count matching lines instead of showing them\n# \"PASS\" = quality filter status in VCF files\n# variants.vcf = variant call format file\n# Returns: Number like \"1234\" (count of variants passing quality filters)\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#process-text-data","title":"Process Text Data","text":"<pre><code># Extract specific columns from data\nawk '{print $1, $2}' data.txt\n# Explanation:\n# awk = powerful text processing tool\n# '{print $1, $2}' = awk program\n#   - {} = action block\n#   - print = output command\n#   - $1 = first column/field\n#   - $2 = second column/field\n#   - , = adds space between fields in output\n# data.txt = input file\n# Example input:  \"Sample1 100 resistant\"\n# Example output: \"Sample1 100\"\n\n# Replace text in files\nsed 's/old/new/g' file.txt\n# Explanation:\n# sed = stream editor for text transformation\n# 's/old/new/g' = substitution command\n#   - s = substitute command\n#   - /old/ = pattern to find\n#   - /new/ = replacement text\n#   - g = global flag (replace all occurrences, not just first)\n# file.txt = input file\n# Example: Changes \"old_sample_name\" to \"new_sample_name\" throughout file\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#combined-pipeline-examples","title":"Combined Pipeline Examples","text":""},{"location":"day2/unix-commands-pathogen-examples/#example-1-quick-fastq-quality-check","title":"Example 1: Quick FASTQ Quality Check","text":"<pre><code># Count reads and check quality score distribution\nzcat sample.fastq.gz | \\\n  awk 'NR%4==0' | \\\n  cut -c1-10 | \\\n  sort | \\\n  uniq -c | \\\n  sort -rn\n\n# Line-by-line explanation:\n# zcat sample.fastq.gz = decompress FASTQ\n# awk 'NR%4==0' = get every 4th line (quality scores)\n#   - NR = line number\n#   - %4==0 = divisible by 4 (4th, 8th, 12th lines...)\n# cut -c1-10 = first 10 characters of quality string\n# sort = alphabetically sort quality patterns\n# uniq -c = count unique patterns\n# sort -rn = sort by count, highest first\n#   - -r = reverse order\n#   - -n = numerical sort\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#example-2-extract-high-quality-reads","title":"Example 2: Extract High-Quality Reads","text":"<pre><code># Get read IDs with average quality &gt; 30\nzcat sample.fastq.gz | \\\n  paste - - - - | \\\n  awk '{if(length($4) &gt; 0) print $1, length($4)}' | \\\n  grep \"^@\"\n\n# Explanation:\n# paste - - - - = combine every 4 lines into 1 tab-delimited line\n# awk = process the combined lines\n# $1 = read ID, $4 = quality string\n# grep \"^@\" = filter for valid read IDs\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#pro-tips-for-these-commands","title":"Pro Tips for These Commands","text":"<ol> <li> <p>Always preview before processing:    </p><pre><code>zcat file.gz | head -20  # Check format first\n</code></pre><p></p> </li> <li> <p>Count before and after filtering:    </p><pre><code># Before\ngrep -c \"^@\" input.fastq\n# After filtering\ngrep -c \"^@\" filtered.fastq\n</code></pre><p></p> </li> <li> <p>Use quotes for patterns with special characters:    </p><pre><code>grep \"^&gt;\" file.fasta     # Correct\ngrep ^&gt; file.fasta        # May fail - shell interprets &gt;\n</code></pre><p></p> </li> <li> <p>Combine commands efficiently:    </p><pre><code># Instead of creating intermediate files:\nzcat file.gz &gt; temp.txt\ngrep \"pattern\" temp.txt &gt; result.txt\n\n# Use pipes:\nzcat file.gz | grep \"pattern\" &gt; result.txt\n</code></pre><p></p> </li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#module-1-directory-organization-for-genomics-projects","title":"Module 1: Directory Organization for Genomics Projects","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives","title":"Learning Objectives","text":"<p>\u2713 Create organized project directories \u2713 Navigate between directories efficiently \u2713 Understand genomics project structure</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-11-creating-your-first-project-structure","title":"Tutorial 1.1: Creating Your First Project Structure","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-start-with-a-simple-structure","title":"Step 1: Start with a Simple Structure","text":"<pre><code># Create your main project directory\nmkdir my_first_project\n\n# Enter the directory\ncd my_first_project\n\n# Check where you are\npwd\n# Output: /home/username/hpc_practice/my_first_project\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-add-subdirectories","title":"Step 2: Add Subdirectories","text":"<pre><code># Create data directories\nmkdir data\nmkdir results\nmkdir scripts\n\n# List what you created\nls\n# Output: data  results  scripts\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-create-a-complex-structure","title":"Step 3: Create a Complex Structure","text":"<pre><code># Use -p to create nested directories\nmkdir -p data/{raw_reads,reference_genomes,metadata}\nmkdir -p results/{qc,alignment,variants,phylogeny}\nmkdir -p scripts logs tmp\n\n# View the structure\nls -la\n# The -la flags show: l=long format, a=all files\n</code></pre> <p>Try It Yourself: </p><pre><code># Exercise: Create this structure\n# project/\n#   \u251c\u2500\u2500 input/\n#   \u2502   \u251c\u2500\u2500 sequences/\n#   \u2502   \u2514\u2500\u2500 references/\n#   \u2514\u2500\u2500 output/\n#       \u251c\u2500\u2500 aligned/\n#       \u2514\u2500\u2500 reports/\n\n# Solution:\nmkdir -p project/{input/{sequences,references},output/{aligned,reports}}\n</code></pre><p></p> <p>Real-world application: </p><pre><code># Set up M. tuberculosis outbreak analysis\nmkdir -p mtb_outbreak_2025/{data,results,scripts,logs}\ncd mtb_outbreak_2025\nmkdir -p data/{fastq,references,clinical_metadata}\nmkdir -p results/{fastqc,trimming,bwa_alignment,vcf_files,phylogenetic_tree}\n</code></pre><p></p>"},{"location":"day2/unix-commands-pathogen-examples/#module-2-file-management-for-sequencing-data","title":"Module 2: File Management for Sequencing Data","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_1","title":"Learning Objectives","text":"<p>\u2713 Create and edit text files \u2713 Copy and rename sequencing files \u2713 Organize data systematically</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-21-working-with-sample-lists","title":"Tutorial 2.1: Working with Sample Lists","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-create-a-sample-list","title":"Step 1: Create a Sample List","text":"<pre><code># First, ensure you're in the right place\npwd\ncd ~/hpc_practice\n\n# Create an empty file\ntouch sample_list.txt\n\n# Check it was created\nls -la sample_list.txt\n# Output: -rw-r--r-- 1 user group 0 Sep 2 10:00 sample_list.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-add-content-to-the-file","title":"Step 2: Add Content to the File","text":"<pre><code># Method 1: Using echo (for single lines)\necho \"Sample_001\" &gt; sample_list.txt\necho \"Sample_002\" &gt;&gt; sample_list.txt  # &gt;&gt; appends, &gt; overwrites!\n\n# Method 2: Using nano editor (recommended for multiple lines)\nnano sample_list.txt\n# Type or paste the following content:\n# MTB_sample_001\n# MTB_sample_002\n# MTB_sample_003\n# MTB_sample_004\n# Then save with: Ctrl+X, Y, Enter\n\n# View what you created\ncat sample_list.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-count-and-verify","title":"Step 3: Count and Verify","text":"<pre><code># Count lines in file\nwc -l sample_list.txt\n# Output: 4 sample_list.txt\n\n# Count words\nwc -w sample_list.txt\n# Output: 4 sample_list.txt\n\n# Get full statistics\nwc sample_list.txt\n# Output: 4  4  58 sample_list.txt\n#        (lines, words, characters)\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-22-organizing-sequencing-files","title":"Tutorial 2.2: Organizing Sequencing Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-copy-files-safely","title":"Step 1: Copy Files Safely","text":"<pre><code># Copy sample files to practice with\ncp sample*.fastq .\n\n# List files before renaming\nls sample*.fastq\n# Output: sample1.fastq  sample2.fastq\n\n# Create a backup first (always!)\nmkdir backups\ncp sample*.fastq backups/\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-rename-files-systematically","title":"Step 2: Rename Files Systematically","text":"<pre><code># Rename a single file\nmv sample1.fastq patient001_reads.fastq\n\n# Batch rename using a loop\nfor file in sample*.fastq; do\n    # Extract the number from filename\n    num=$(echo $file | grep -o '[0-9]\\+')\n    # Create new name\n    newname=\"patient_${num}_sequences.fastq\"\n    echo \"Renaming $file to $newname\"\n    mv \"$file\" \"$newname\"\ndone\n\n# Verify the renaming\nls *.fastq\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#practice-exercise","title":"Practice Exercise:","text":"<pre><code># Exercise: Create copies with dates\n# Copy sample.fastq.gz to sample_20250902.fastq.gz\n\n# Solution:\ndate_stamp=$(date +%Y%m%d)\ncp sample.fastq.gz sample_${date_stamp}.fastq.gz\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#module-3-viewing-and-inspecting-genomics-files","title":"Module 3: Viewing and Inspecting Genomics Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_2","title":"Learning Objectives","text":"<p>\u2713 View compressed and uncompressed files \u2713 Count sequences in FASTQ/FASTA files \u2713 Extract specific parts of files</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-31-working-with-fastq-files","title":"Tutorial 3.1: Working with FASTQ Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-view-compressed-files","title":"Step 1: View Compressed Files","text":"<pre><code># View first 4 lines (1 complete read) of compressed file\nzcat sample.fastq.gz | head -4\n# Output:\n# @SEQ_001\n# ACGTACGTACGTACGTACGTACGTACGTACGTACGT\n# +\n# IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n\n# View first 2 reads (8 lines)\nzcat sample.fastq.gz | head -8\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-count-sequences","title":"Step 2: Count Sequences","text":"<pre><code># Count total lines\nzcat sample.fastq.gz | wc -l\n# Output: 12  (for 3 reads)\n\n# Count number of reads (FASTQ has 4 lines per read)\nzcat sample.fastq.gz | wc -l | awk '{print $1/4}'\n# Output: 3\n\n# Alternative: Count sequence headers\nzcat sample.fastq.gz | grep -c \"^@\"\n# Output: 3\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-view-with-line-numbers","title":"Step 3: View with Line Numbers","text":"<pre><code># Add line numbers to output\nzcat sample.fastq.gz | head -8 | cat -n\n# Output:\n#      1    @SEQ_001\n#      2    ACGTACGTACGTACGTACGTACGTACGTACGTACGT\n#      3    +\n#      4    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n#      5    @SEQ_002\n#      6    TGCATGCATGCATGCATGCATGCATGCATGCATGCA\n#      7    +\n#      8    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-32-working-with-fasta-files","title":"Tutorial 3.2: Working with FASTA Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-view-fasta-headers","title":"Step 1: View FASTA Headers","text":"<pre><code># View first line (header) of FASTA\nhead -1 reference.fasta\n# Output: &gt;Sequence_1 gene=ABC123\n\n# View all headers in file\ngrep \"^&gt;\" reference.fasta\n# Output:\n# &gt;Sequence_1 gene=ABC123\n# &gt;Sequence_2 gene=DEF456\n\n# Count sequences\ngrep -c \"^&gt;\" reference.fasta\n# Output: 2\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-extract-sequences","title":"Step 2: Extract Sequences","text":"<pre><code># View first 2 lines (header + sequence start)\nhead -2 reference.fasta\n\n# Skip header, view sequence only\ntail -n +2 reference.fasta | head -1\n# Output: ACGTACGTACGTACGTACGTACGTACGTACGTACGT\n\n# Get sequence length\ntail -n +2 reference.fasta | head -1 | wc -c\n# Output: 37 (includes newline)\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#practice-exercise_1","title":"Practice Exercise:","text":"<pre><code># Exercise: Count total bases in all sequences\n# Hint: Remove headers first\n\n# Solution:\ngrep -v \"^&gt;\" reference.fasta | tr -d '\\n' | wc -c\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#module-4-searching-and-filtering-genomics-data","title":"Module 4: Searching and Filtering Genomics Data","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_3","title":"Learning Objectives","text":"<p>\u2713 Search for patterns in files \u2713 Filter genomics data \u2713 Use regular expressions</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-41-basic-pattern-searching","title":"Tutorial 4.1: Basic Pattern Searching","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-simple-searches","title":"Step 1: Simple Searches","text":"<pre><code># Search for a word in a file\ngrep \"resistant\" data.txt\n# Output: Sample1 100 resistant\n#         Sample3 150 resistant\n\n# Case-insensitive search (-i flag)\ngrep -i \"SAMPLE\" data.txt\n# Finds: Sample1, Sample2, etc.\n\n# Count matches (-c flag)\ngrep -c \"resistant\" data.txt\n# Output: 2\n\n# Show line numbers (-n flag)\ngrep -n \"resistant\" data.txt\n# Output: 1:Sample1 100 resistant\n#         3:Sample3 150 resistant\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-search-in-genomics-files","title":"Step 2: Search in Genomics Files","text":"<pre><code># Find sequence headers in FASTA\ngrep \"^&gt;\" reference.fasta\n# ^ means \"start of line\"\n\n# Find adapter sequences in FASTQ\ngrep \"AGATCGGAAGAG\" sample1.fastq\n\n# Search compressed files\nzcat sample.fastq.gz | grep \"ACGT\"\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-42-advanced-pattern-matching","title":"Tutorial 4.2: Advanced Pattern Matching","text":""},{"location":"day2/unix-commands-pathogen-examples/#using-regular-expressions","title":"Using Regular Expressions","text":"<pre><code># Find lines with numbers\ngrep '[0-9]' data.txt\n# [0-9] matches any digit\n\n# Find lines ending with specific pattern\ngrep 'resistant$' data.txt\n# $ means \"end of line\"\n\n# Extract only the matching part (-o flag)\necho \"Sample123\" | grep -o '[0-9]\\+'\n# Output: 123\n# \\+ means \"one or more\"\n\n# Multiple patterns (OR)\ngrep -E 'resistant|sensitive' data.txt\n# -E enables extended regex\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#practice-exercise_2","title":"Practice Exercise:","text":"<pre><code># Exercise: Find all samples with values &gt; 150\n# Hint: Use awk instead of grep for numeric comparisons\n\n# Solution:\nawk '$2 &gt; 150' data.txt\n# Output: Sample2 200 sensitive\n#         Sample4 300 sensitive\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#module-5-text-processing-for-genomics","title":"Module 5: Text Processing for Genomics","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_4","title":"Learning Objectives","text":"<p>\u2713 Extract specific columns from files \u2713 Perform calculations on data \u2713 Transform text efficiently</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-51-using-awk-for-data-processing","title":"Tutorial 5.1: Using awk for Data Processing","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-extract-columns","title":"Step 1: Extract Columns","text":"<pre><code># Print specific columns (1st and 2nd)\nawk '{print $1, $2}' data.txt\n# Output: Sample1 100\n#         Sample2 200\n#         Sample3 150\n#         Sample4 300\n\n# Print with custom separator\nawk '{print $1 \",\" $2}' data.txt\n# Output: Sample1,100\n\n# Add text to output\nawk '{print \"Sample:\" $1 \" Value:\" $2}' data.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-perform-calculations","title":"Step 2: Perform Calculations","text":"<pre><code># Sum values in column 2\nawk '{sum+=$2} END {print \"Total:\", sum}' data.txt\n# Output: Total: 750\n\n# Calculate average\nawk '{sum+=$2; count++} END {print \"Average:\", sum/count}' data.txt\n# Output: Average: 187.5\n\n# Filter based on value\nawk '$2 &gt; 150 {print $0}' data.txt\n# Output: Sample2 200 sensitive\n#         Sample4 300 sensitive\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-52-using-sed-for-text-manipulation","title":"Tutorial 5.2: Using sed for Text Manipulation","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-basic-substitutions","title":"Step 1: Basic Substitutions","text":"<pre><code># Replace text (s = substitute)\necho \"Sample1\" | sed 's/1/A/'\n# Output: SampleA\n\n# Global replacement (g = global)\necho \"Sample111\" | sed 's/1/A/g'\n# Output: SampleAAA\n\n# Replace in file and save\nsed 's/Sample/Patient/g' data.txt &gt; patients.txt\n\n# Edit file in place (careful!)\nsed -i.bak 's/Sample/Patient/g' data.txt\n# Creates data.txt.bak as backup\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-advanced-manipulations","title":"Step 2: Advanced Manipulations","text":"<pre><code># Delete lines containing pattern\nsed '/sensitive/d' data.txt\n# Removes lines with \"sensitive\"\n\n# Add text to beginning of lines\nsed 's/^/PREFIX_/' data.txt\n# Adds PREFIX_ to each line start\n\n# Convert spaces to tabs\nsed 's/ /\\t/g' data.txt &gt; data.tsv\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#6-file-permissions-and-management","title":"6. File Permissions and Management","text":""},{"location":"day2/unix-commands-pathogen-examples/#managing-permissions","title":"Managing Permissions","text":"<pre><code># Make scripts executable\nchmod +x scripts/analysis_pipeline.sh\n\n# Protect raw data from accidental modification\nchmod 444 data/raw_reads/*.fastq.gz\n\n# Set directory permissions\nchmod 755 results/\n\n# Check permissions\nls -la data/raw_reads/\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#7-sorting-and-unique-operations","title":"7. Sorting and Unique Operations","text":""},{"location":"day2/unix-commands-pathogen-examples/#processing-sample-lists","title":"Processing Sample Lists","text":"<pre><code># Sort sample names\nsort data/sample_list.txt\n\n# Sort numerically by coverage\nsort -k2 -n coverage_stats.txt\n\n# Get unique mutations\ncut -f1,2 variants.txt | sort | uniq\n\n# Count occurrences of each mutation\ncut -f3 mutations.txt | sort | uniq -c | sort -rn\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#8-pipelines-and-redirection","title":"8. Pipelines and Redirection","text":""},{"location":"day2/unix-commands-pathogen-examples/#creating-simple-analysis-pipelines","title":"Creating Simple Analysis Pipelines","text":"<pre><code># Count reads per sample and save to file\nfor file in data/raw_reads/*.fastq.gz; do\n    sample=$(basename $file .fastq.gz)\n    count=$(zcat $file | wc -l | awk '{print $1/4}')\n    echo -e \"$sample\\t$count\"\ndone &gt; results/qc/read_counts.txt\n\n# Extract and count specific genes\ngrep \"^&gt;\" reference.fasta | cut -d' ' -f1 | sed 's/&gt;//' | sort | uniq -c &gt; gene_counts.txt\n\n# Process multiple VCF files\nfor vcf in results/variants/*.vcf; do\n    sample=$(basename $vcf .vcf)\n    pass_count=$(grep -c \"PASS\" $vcf)\n    total_count=$(grep -v \"^#\" $vcf | wc -l)\n    echo -e \"$sample\\t$total_count\\t$pass_count\"\ndone &gt; results/variant_summary.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#9-practical-slurm-integration","title":"9. Practical SLURM Integration","text":""},{"location":"day2/unix-commands-pathogen-examples/#preparing-files-for-hpc-analysis","title":"Preparing Files for HPC Analysis","text":"<p>To create a SLURM job script:</p> <pre><code># Open nano to create the script\nnano prep_pathogen_data.sh\n</code></pre> <p>Copy and paste the following content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=prep_pathogen_data\n#SBATCH --time=00:30:00\n#SBATCH --mem=4GB\n\n# Create directory structure\nmkdir -p ${SLURM_JOB_ID}_analysis/{data,results,logs}\n\n# Copy and organize files\ncp /shared/data/*.fastq.gz ${SLURM_JOB_ID}_analysis/data/\n\n# Generate file list for processing\nls ${SLURM_JOB_ID}_analysis/data/*.fastq.gz &gt; file_list.txt\n\n# Count and verify files\necho \"Total files to process: $(wc -l &lt; file_list.txt)\"\n\n# Create metadata file\nfor file in ${SLURM_JOB_ID}_analysis/data/*.fastq.gz; do\n    size=$(du -h $file | cut -f1)\n    reads=$(zcat $file | wc -l | awk '{print $1/4}')\n    echo \"$(basename $file)\\t$size\\t$reads\"\ndone &gt; ${SLURM_JOB_ID}_analysis/data/file_metadata.tsv\n\necho \"Preparation complete. Ready for analysis.\"\n</code></pre> <p>Save the file with: <code>Ctrl+X</code>, then <code>Y</code>, then <code>Enter</code></p> <p>Submit the job with: <code>sbatch prep_pathogen_data.sh</code></p>"},{"location":"day2/unix-commands-pathogen-examples/#hands-on-exercise-complete-pathogen-analysis-workflow","title":"Hands-On Exercise: Complete Pathogen Analysis Workflow","text":""},{"location":"day2/unix-commands-pathogen-examples/#exercise-overview","title":"Exercise Overview","text":"<p>Build a complete analysis pipeline step-by-step, applying all the skills you've learned.</p>"},{"location":"day2/unix-commands-pathogen-examples/#part-1-setup-and-data-preparation","title":"Part 1: Setup and Data Preparation","text":"<pre><code># Step 1: Create project structure\nmkdir -p pathogen_practice/{data,results,scripts}\ncd pathogen_practice\npwd  # Verify you're in the right place\n\n# Step 2: Create sample metadata\nnano data/samples.txt\n# Paste the following content:\n# Mtb_patient_001_resistant\n# Mtb_patient_002_susceptible  \n# Mtb_patient_003_resistant\n# Salmonella_outbreak_001\n# Salmonella_outbreak_002\n# Save with: Ctrl+X, Y, Enter\n\n# Verify the file was created\ncat data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#part-2-data-analysis-tasks","title":"Part 2: Data Analysis Tasks","text":""},{"location":"day2/unix-commands-pathogen-examples/#task-1-find-resistant-samples","title":"Task 1: Find Resistant Samples","text":"<pre><code># Use grep to find resistant samples\ngrep \"resistant\" data/samples.txt\n\n# Save results to file\ngrep \"resistant\" data/samples.txt &gt; results/resistant_samples.txt\n\n# Count how many\ngrep -c \"resistant\" data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#task-2-count-by-pathogen-type","title":"Task 2: Count by Pathogen Type","text":"<pre><code># Count MTB samples\ngrep -c \"Mtb\" data/samples.txt\n\n# Count Salmonella samples\ngrep -c \"Salmonella\" data/samples.txt\n\n# Save counts\necho \"MTB samples: $(grep -c 'Mtb' data/samples.txt)\" &gt; results/pathogen_counts.txt\necho \"Salmonella samples: $(grep -c 'Salmonella' data/samples.txt)\" &gt;&gt; results/pathogen_counts.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#task-3-generate-summary-report","title":"Task 3: Generate Summary Report","text":"<pre><code># Create a comprehensive summary using nano\nnano results/summary_report.txt\n\n# Type the following content (replace the values with actual counts):\n# === Pathogen Analysis Summary ===\n# Date: [current date]\n# Total samples: 5\n# Resistant samples: 2\n# Susceptible samples: 1\n# MTB samples: 3\n# Salmonella samples: 2\n# =================================\n# Save with: Ctrl+X, Y, Enter\n\n# Or use echo commands to generate it automatically:\necho \"=== Pathogen Analysis Summary ===\" &gt; results/summary_report.txt\necho \"Date: $(date)\" &gt;&gt; results/summary_report.txt\necho \"Total samples: $(wc -l &lt; data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"Resistant samples: $(grep -c \"resistant\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"Susceptible samples: $(grep -c \"susceptible\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"MTB samples: $(grep -c \"Mtb\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"Salmonella samples: $(grep -c \"Salmonella\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"=================================\" &gt;&gt; results/summary_report.txt\n\n# Display the report\ncat results/summary_report.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#part-3-challenge-exercises","title":"Part 3: Challenge Exercises","text":""},{"location":"day2/unix-commands-pathogen-examples/#challenge-1-extract-sample-ids","title":"Challenge 1: Extract Sample IDs","text":"<pre><code># Extract just the patient IDs (hint: use cut or awk)\n# Try it yourself first!\n\n# Solution:\ncut -d'_' -f2,3 data/samples.txt\n# Or using awk:\nawk -F'_' '{print $2\"_\"$3}' data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#challenge-2-sort-and-count-unique-pathogens","title":"Challenge 2: Sort and Count Unique Pathogens","text":"<pre><code># Extract pathogen names and count occurrences\n# Try it yourself first!\n\n# Solution:\ncut -d'_' -f1 data/samples.txt | sort | uniq -c\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#challenge-3-create-a-pipeline","title":"Challenge 3: Create a Pipeline","text":"<pre><code># Find all resistant MTB samples in one command\n# Try it yourself first!\n\n# Solution:\ngrep \"Mtb\" data/samples.txt | grep \"resistant\"\n# Or more elegantly:\ngrep \"Mtb.*resistant\" data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tips-for-pathogen-genomics-unix-usage","title":"Tips for Pathogen Genomics Unix Usage","text":"<ol> <li>Always work with copies of raw sequencing data</li> <li>Use meaningful file names with sample IDs and dates</li> <li>Document your commands in scripts for reproducibility</li> <li>Check file integrity after transfers (md5sum)</li> <li>Compress large files to save space (gzip/bgzip)</li> <li>Use screen or tmux for long-running processes</li> <li>Regular backups of analysis results</li> <li>Version control for scripts (git)</li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#common-file-formats-in-pathogen-genomics","title":"Common File Formats in Pathogen Genomics","text":"Extension Format View Command Description <code>.fastq.gz</code> Compressed FASTQ <code>zcat file.fastq.gz \\| head</code> Raw sequencing reads <code>.fasta</code> FASTA <code>cat file.fasta</code> Reference genomes <code>.sam/.bam</code> SAM/BAM <code>samtools view file.bam \\| head</code> Alignments <code>.vcf</code> VCF <code>cat file.vcf</code> Variant calls <code>.gff/.gtf</code> GFF/GTF <code>cat file.gff</code> Gene annotations <code>.newick/.tree</code> Newick <code>cat file.tree</code> Phylogenetic trees"},{"location":"day2/unix-commands-pathogen-examples/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"day2/unix-commands-pathogen-examples/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"day2/unix-commands-pathogen-examples/#issue-1-permission-denied","title":"Issue 1: \"Permission denied\"","text":"<pre><code># Problem: Can't access or modify a file\n# Solution: Check permissions\nls -la filename\n\n# Fix: Change permissions if you own the file\nchmod u+rw filename\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-2-no-such-file-or-directory","title":"Issue 2: \"No such file or directory\"","text":"<pre><code># Problem: File path is wrong\n# Solution: Check your current location\npwd\n\n# List files to verify\nls -la\n\n# Use absolute paths to be sure\n/full/path/to/file\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-3-command-not-found","title":"Issue 3: \"Command not found\"","text":"<pre><code># Problem: Tool not installed or not in PATH\n# Solution: Check if command exists\nwhich command_name\n\n# Load module if available\nmodule avail\nmodule load tool_name\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-4-file-is-empty-or-corrupted","title":"Issue 4: File is empty or corrupted","text":"<pre><code># Check file size\nls -lh filename\n\n# Check file type\nfile filename\n\n# For compressed files, test integrity\ngzip -t file.gz\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-5-out-of-disk-space","title":"Issue 5: Out of disk space","text":"<pre><code># Check available space\ndf -h\n\n# Find large files\ndu -sh * | sort -h\n\n# Clean up temporary files\nrm -rf tmp/*\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#best-practices-to-avoid-issues","title":"Best Practices to Avoid Issues","text":"<ol> <li> <p>Always backup before modifying </p><pre><code>cp important_file important_file.backup\n</code></pre><p></p> </li> <li> <p>Use tab completion to avoid typos    </p><pre><code>cat sam[TAB]  # Completes to sample.fastq\n</code></pre><p></p> </li> <li> <p>Preview commands with echo first </p><pre><code>echo mv *.fastq backup/  # See what would happen\nmv *.fastq backup/       # Then run for real\n</code></pre><p></p> </li> <li> <p>Check file contents before processing </p><pre><code>head -5 file.txt  # Preview first 5 lines\nwc -l file.txt    # Count total lines\n</code></pre><p></p> </li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#quick-reference-card","title":"Quick Reference Card","text":""},{"location":"day2/unix-commands-pathogen-examples/#essential-commands-summary","title":"Essential Commands Summary","text":"Task Command Example List files <code>ls -la</code> <code>ls -la *.fastq</code> Change directory <code>cd</code> <code>cd ~/hpc_practice</code> Create directory <code>mkdir -p</code> <code>mkdir -p data/reads</code> Copy files <code>cp -r</code> <code>cp sample.fastq backup/</code> Move/rename <code>mv</code> <code>mv old.txt new.txt</code> View compressed <code>zcat</code> <code>zcat file.gz \\| head</code> Count lines <code>wc -l</code> <code>wc -l sample.txt</code> Search text <code>grep</code> <code>grep \"pattern\" file</code> Extract columns <code>awk</code> <code>awk '{print $1}' file</code> Replace text <code>sed</code> <code>sed 's/old/new/g' file</code> Sort data <code>sort</code> <code>sort -n numbers.txt</code> Get unique <code>uniq</code> <code>sort file \\| uniq</code>"},{"location":"day2/unix-commands-pathogen-examples/#next-steps","title":"Next Steps","text":"<p>After mastering these Unix commands, you're ready to: 1. Submit SLURM jobs - See High Performance Computing with SLURM: Practical Tutorial 2. Learn HPC concepts - See HPC and ILIFU Training Materials 3. Build analysis pipelines with Nextflow 4. Perform quality control with FastQC 5. Align reads with BWA 6. Call variants with SAMtools/BCFtools</p> <p>Remember: Unix commands are the foundation of all bioinformatics pipelines!</p>"},{"location":"day3/genome_assembly_notes/","title":"Objectives","text":""},{"location":"day3/genome_assembly_notes/#objectives","title":"Objectives","text":"<ol> <li>To perform De novo genome assembly and reference-based genome assembly</li> <li>Assess assembly quality using quality metrics</li> </ol>"},{"location":"day3/genome_assembly_notes/#genome-assembly","title":"Genome assembly","text":"<p>Genome assembly involves reconstructing a genome from a set of fragmented DNA sequences (reads) obtained from sequencing technologies. </p> <p>It aims to piece together the reads to create a continuous sequence that represents the genome of an organism.</p>"},{"location":"day3/genome_assembly_notes/#key-concepts","title":"Key Concepts","text":"<ul> <li>Reads: Fragmented sequences of DNA obtained from sequencing technologies.</li> <li>Contigs: Continuous sequences formed by overlapping reads.</li> <li>Scaffolds: Ordered and oriented sets of contigs, sometimes with gaps, which represent larger regions of the genome.</li> <li>Coverage: The average number of times each base in the genome is sequenced, which affects the accuracy of the assembly.</li> <li>Assembly can be:<ul> <li>De novo assembly - the construction of the genome from scratch without a reference.</li> <li>Reference-guided assembly - uses a known reference genome to guide the assembly of the new genome.</li> </ul> </li> </ul>"},{"location":"day3/genome_assembly_notes/#steps-in-genome-assembly","title":"Steps in Genome Assembly","text":"<ol> <li>Preprocessing (quality control, adapter sequences and low-quality bases filtering with FASTQC, TRIMMOMATIC/FASTP/BBMap).</li> <li>Assembly</li> <li>Scaffolding (involves use of long-range information from paired-end or mate-pair reads to order and orient contigs into scaffolds (e.g., SSPACE).</li> <li>Gap Filling (use additional reads or assembly techniques to close gaps within scaffolds (e.g., GapCloser).</li> <li>Polishing (correcting sequencing errors and improving the accuracy of the assembly, e.g., Pilon).</li> <li>Evaluation (Assembly Metrics including N50 (length of the contig such that 50% of the total assembly length is in contigs of this length or longer); genome completeness; and accuracy, e.g., QUAST).</li> </ol>"},{"location":"day3/genome_assembly_notes/#genome-assembly-algorithms","title":"Genome Assembly Algorithms","text":""},{"location":"day3/genome_assembly_notes/#overlap-layout-consensus-olc","title":"Overlap Layout Consensus (OLC)","text":"<ul> <li>Suitable for long-read sequencing (e.g., PacBio, Oxford Nanopore)</li> <li>Determines overlap between reads</li> <li>Arranges reads based on ovelaps</li> <li>Resolves conflict to build the final sequences</li> <li>Handles long reads and complex regions well</li> <li>Computationally intensive</li> </ul>"},{"location":"day3/genome_assembly_notes/#de-bruijn-graph-dbg","title":"De Bruijn Graph (DBG)","text":"<ul> <li>Best for short-read sequencing (e.g., illumina).</li> <li>uses small overlapping sequences (k-mers) to build a graph, nodes: k-mers, and edges: overlaps.</li> <li>Fast and memory efficient for large datasets</li> <li>Struggles with repetitive regions</li> </ul>"},{"location":"day3/genome_assembly_notes/#hybrid-methods","title":"Hybrid Methods","text":"<ul> <li>Combine DBG and OLC strengths to improve assembly quality, especially for error-prone long reads.\u200b</li> </ul>"},{"location":"day3/genome_assembly_notes/#assembler-classification-by-read-type","title":"Assembler Classification by Read Type","text":""},{"location":"day3/genome_assembly_notes/#short-read-assemblers","title":"Short-Read Assemblers\u200b","text":"<ul> <li>Short-read assemblers process high volumes of short sequences using DBG algorithms</li> <li>Ideal for technologies like Illumina.\u200b</li> <li>SPAdes (widely used for small genomes)</li> <li>Velvet (older though useful)</li> <li>SOAPdenovo (short-read assembler for larger genomes),</li> <li>ABySS (large genomes).</li> </ul>"},{"location":"day3/genome_assembly_notes/#long-read-assemblers","title":"Long-Read Assemblers\u200b","text":"<ul> <li>Long-read assemblers handle fewer, longer sequences using OLC algorithms</li> <li>Can span entire genes and repetitive regions</li> <li>Optimized for platforms like PacBio and ONT.\u200b</li> <li>Higher error rates (can be corrected by short reads or polishing)</li> <li>Canu, Celera, Flye</li> </ul>"},{"location":"day3/genome_assembly_notes/#hybrid-assemblers","title":"Hybrid Assemblers\u200b","text":"<ul> <li>Combine short and long reads, integrating DBG and OLC methods for improved accuracy and contiguity.\u200b</li> <li>Balances accuracy (short reads) and completeness (long reads, resolving repeats and structural variants)</li> <li>Requires careful data integration</li> <li>Unicycler, MaSuRCA</li> </ul>"},{"location":"day3/genome_assembly_notes/#reference-guided-assembly","title":"Reference-guided Assembly","text":"<ul> <li>Works well given a closely related reference genome</li> <li>Aligns reads to a reference genome and fills gaps</li> <li>Faster and less computationally demanding</li> <li>May miss novel sequences or structural variations</li> </ul>"},{"location":"day3/genome_assembly_notes/#note","title":"NOTE","text":"Assembly Strategy Subtypes Read Type Compatibility Examples Notes De novo assembly DBG, OLC, Hybrid Short, long, hybrid Velvet, SPAdes, Canu, Flye, MaSuRCA No reference genome used Reference-guided assembly Mapping-based Short, long BWA, Bowtie2, Novoalign, Minimap2 Aligns reads to a known reference genome"},{"location":"day3/genome_assembly_notes/#assembler-selection-factors","title":"Assembler Selection Factors\u200b","text":"<p>Choosing an assembler depends on\" - Sequencing technology, - Project goals, and - Available computational resources.\u200b</p>"},{"location":"day3/genome_assembly_notes/#best-practices-for-genome-assembly","title":"Best Practices for Genome Assembly","text":"<ul> <li>Start with high-quality, high-coverage sequencing data to improve the accuracy of the assembly.</li> <li>Try multiple assemblers and compare results, as different tools may perform better for different data types.</li> <li>Use iterative rounds of assembly, scaffolding, and polishing to gradually improve the assembly.</li> <li>Validate the final assembly using independent data, such as long-read sequencing or optical mapping.</li> <li>Keep detailed records of all parameters and steps used in the assembly process for reproducibility.</li> </ul>"},{"location":"day3/practical_genome_assmbly/","title":"De novo Genome Assembly Practical","text":""},{"location":"day3/practical_genome_assmbly/#de-novo-genome-assembly-practical","title":"De novo Genome Assembly Practical","text":"<p>Before setting up you need to know the current workig directory  </p><pre><code># Check current working directory\npwd\n\n# List the contents of the working diresctory\nls\n\n# Create relevant output directories. -p so that it creates parent dir if it doesn't exist\nmkdir -p /users/${USER}/results/02_assembly/tb\nmkdir -p /users/${USER}/results/02_assembly/vc\nmkdir -p /users/${USER}/results/03_quality/tb\nmkdir -p /users/${USER}/results/03_quality/vc\n</code></pre><p></p> <p>Delete /users/${USER}/results/02_assembly and /users/user24/results/03_quality and create thenm using one line code and not four as above</p> <pre><code>mkdir /users/${USER}/results/02_assembly/tb /users/user24/results/02_assembly/vc \\\n  /users/${USER}/results/03_quality/tb /users/user24/results/03_quality/vc\n\n# Request for an interactive node\nsrun --cpus-per-tasks=32 --mem=128g --time 5:00:00 --job-name \"ephie\" --pty /bin/bash\n\n# # Clear all modules\nmodule purge\n\n# Load the required modules\nmodule load spades/4.2.0\n\n## How can I get the help documentation?\nspades.py --help\n\n# Lets run spades with a test dataset\n/software/bio/spades/4.2.0/bin/spades.py  --test  --careful\n\n# Run spades with test data but with more details on the reads\nspades.py  -1 /software/bio/spades/4.2.0/share/spades/test_dataset/ecoli_1K_1.fq.gz \\\n    -2 /software/bio/spades/4.2.0/share/spades/test_dataset/ecoli_1K_2.fq.gz \\\n  --careful -o results/02_assembly-test\n\n## Subset the data\nhead -5 tb_IDs &gt; 02_tb_IDs\nhead -5 vc_IDs &gt; 02_vc_IDs\n\n# Perform denovo assembly for all TB samples\nmkdir -p /users/${USER}/scripts\nnano /users/${USER}/scripts/02_assembly.sh\n\n#!/bin/bash\n#SBATCH --job-name='02_assembly'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=120GB\n#SBATCH --output=/users/${USER}/logs/02_assembly-stdout.txt\n#SBATCH --error=/users/${USER}/logs/02_assembly-stdout.txt\n#SBATCH --time=12:00:00\n\nfor SAMPLE in $(cat tb_IDs); do\n  echo \"[TB|SPADES] $SAMPLE\"\n  spades.py -1 /users/${USER}/results/trimmed_trimmomatic/tb/${SAMPLE}_1.fastq.gz \\\n    -2 /users/${USER}/results/trimmed_trimmomatic/tb/${SAMPLE}_2.fastq.gz \\\n    --careful \\\n    -o /users/${USER}/results/02_assembly/tb/${SAMPLE}\ndone\n\n## Save\n# Ctrl + O\n\n# CLOSE\n# ctrl + x\n\nsbatch /users/${USER}/scripts/02_assembly.sh\n</code></pre>"},{"location":"day3/practical_genome_assmbly/#notes","title":"Notes","text":"<ul> <li>-1                file with forward paired-end reads</li> <li>-2                file with reverse paired-end reads</li> <li>--careful                   will reduce number of mismatches and short indels</li> <li>-o                          path to output file (directory shouldn't be existing)</li> </ul>"},{"location":"day3/practical_genome_assmbly/#quality-assessment","title":"Quality Assessment","text":"<p>We need assess the quality of the contigs so that we are confident that they are of good quality.</p>"},{"location":"day4/genome_assembly/","title":"Objectives","text":""},{"location":"day4/genome_assembly/#objectives","title":"Objectives","text":"<ol> <li>To perform De novo genome assembly and reference-based genome assembly</li> <li>Assess assembly quality using quality metrics</li> </ol>"},{"location":"day4/genome_assembly/#target-organisms","title":"Target Organisms","text":""},{"location":"day4/genome_assembly/#mycobacterium-tuberculosis","title":"Mycobacterium tuberculosis","text":"<ul> <li>Genome size: ~4.4 Mb</li> <li>Characteristics: High GC content ~65.6%, complex secondary structures</li> <li>Genes ~4,000 protein-coding genes</li> <li>Clinical relevance: Major global pathogen, drug resistance concerns</li> <li>Assembly challenges: Repetitive sequences, PE/PPE gene families</li> </ul>"},{"location":"day4/genome_assembly/#vibrio-cholerae","title":"Vibrio cholerae","text":"<ul> <li>Characteristics: Dual chromosome structure</li> <li>Genome size: ~4.0 Mb (two chromosomes: ~3.0 Mb + ~1.1 Mb)</li> <li>GC content: ~47.7%</li> <li>Genes: ~3,800 protein-coding genes</li> <li>Clinical relevance: Cholera pandemics, epidemic strain tracking</li> <li>Assembly challenges: Chromosome separation, mobile genetic elements</li> </ul>"},{"location":"day4/genome_assembly/#genome-assembly","title":"Genome assembly","text":"<p>Genome assembly involves reconstructing a genome from a set of fragmented DNA sequences (reads) obtained from sequencing technologies. </p> <p>It aims to piece together the reads to create a continuous sequence that represents the genome of an organism.</p>"},{"location":"day4/genome_assembly/#key-concepts","title":"Key Concepts","text":"<ul> <li>Reads: Fragmented sequences of DNA obtained from sequencing technologies.</li> <li>Contigs: Continuous sequences formed by overlapping reads.</li> <li>Scaffolds: Ordered and oriented sets of contigs, sometimes with gaps, which represent larger regions of the genome.</li> <li>Coverage: The average number of times each base in the genome is sequenced, which affects the accuracy of the assembly.</li> <li>Assembly can be:<ul> <li>De novo assembly - the construction of the genome from scratch without a reference.</li> <li>Reference-guided assembly - uses a known reference genome to guide the assembly of the new genome.</li> </ul> </li> </ul>"},{"location":"day4/genome_assembly/#steps-in-genome-assembly","title":"Steps in Genome Assembly","text":"<ol> <li>Preprocessing (quality control, adapter sequences and low-quality bases filtering with FASTQC, TRIMMOMATIC/FASTP/BBMap).</li> <li>Assembly</li> <li>Scaffolding (involves use of long-range information from paired-end or mate-pair reads to order and orient contigs into scaffolds (e.g., SSPACE).</li> <li>Gap Filling (use additional reads or assembly techniques to close gaps within scaffolds (e.g., GapCloser).</li> <li>Polishing (correcting sequencing errors and improving the accuracy of the assembly, e.g., Pilon).</li> <li>Evaluation (Assembly Metrics including N50 (length of the contig such that 50% of the total assembly length is in contigs of this length or longer); genome completeness; and accuracy, e.g., QUAST).</li> </ol>"},{"location":"day4/genome_assembly/#genome-assembly-algorithms","title":"Genome Assembly Algorithms","text":""},{"location":"day4/genome_assembly/#overlap-layout-consensus-olc","title":"Overlap Layout Consensus (OLC)","text":"<ul> <li>Suitable for long-read sequencing (e.g., PacBio, Oxford Nanopore)</li> <li>Determines overlap between reads</li> <li>Arranges reads based on ovelaps</li> <li>Resolves conflict to build the final sequences</li> <li>Handles long reads and complex regions well</li> <li>Computationally intensive</li> </ul>"},{"location":"day4/genome_assembly/#de-bruijn-graph-dbg","title":"De Bruijn Graph (DBG)","text":"<ul> <li>Best for short-read sequencing (e.g., illumina).</li> <li>Uses small overlapping sequences (k-mers) to build a graph, nodes: k-mers, and edges: overlaps.</li> <li>Fast and memory efficient for large datasets</li> <li>Struggles with repetitive regions</li> </ul>"},{"location":"day4/genome_assembly/#hybrid-methods","title":"Hybrid Methods","text":"<ul> <li>Combine DBG and OLC strengths to improve assembly quality, especially for error-prone long reads.\u200b</li> </ul>"},{"location":"day4/genome_assembly/#assembler-classification-by-read-type","title":"Assembler Classification by Read Type","text":""},{"location":"day4/genome_assembly/#short-read-assemblers","title":"Short-Read Assemblers\u200b","text":"<ul> <li>Short-read assemblers process high volumes of short sequences using DBG algorithms</li> <li>Ideal for technologies like Illumina.\u200b</li> <li>SPAdes (widely used for small genomes)</li> <li>Velvet (older though useful)</li> <li>SOAPdenovo (short-read assembler for larger genomes),</li> <li>ABySS (large genomes).</li> </ul>"},{"location":"day4/genome_assembly/#long-read-assemblers","title":"Long-Read Assemblers\u200b","text":"<ul> <li>Long-read assemblers handle fewer, longer sequences using OLC algorithms</li> <li>Can span entire genes and repetitive regions</li> <li>Optimized for platforms like PacBio and ONT.\u200b</li> <li>Higher error rates (can be corrected by short reads or polishing)</li> </ul>"},{"location":"day4/genome_assembly/#examples-of-long-read-assemblers-for-bacterial-genomes","title":"Examples of Long-Read Assemblers for Bacterial Genomes","text":"<ol> <li> <p>FLYE (Recommended for most bacterial genomes)</p> </li> <li> <p>Excellent for PacBio and Oxford Nanopore</p> </li> <li>Good repeat resolution</li> <li> <p>Usage: flye --nano-raw reads.fastq --out-dir output --genome-size 4.5m</p> </li> <li> <p>Canu (High accuracy, slower)</p> </li> <li> <p>Gold standard for accuracy</p> </li> <li>Requires significant computational resources</li> <li> <p>Usage: canu -p prefix -d output genomeSize=4.5m -nanopore reads.fastq</p> </li> <li> <p>Unicycler (Hybrid approach)</p> </li> <li> <p>Combines short and long reads</p> </li> <li>Excellent for complete genomes</li> <li> <p>Usage: unicycler -1 short_R1.fq -2 short_R2.fq -l long_reads.fq -o output</p> </li> <li> <p>Raven (Fast, lightweight)</p> </li> <li> <p>Quick assemblies for preliminary analysis</p> </li> <li>Good for large datasets</li> <li> <p>Usage: raven reads.fastq &gt; assembly.fasta</p> </li> <li> <p>NextDenovo (High accuracy for Nanopore)</p> </li> <li> <p>Specialized for Oxford Nanopore data</p> </li> <li>Good error correction</li> <li>Usage: nextDenovo config.txt</li> </ol> <p>Polishing Tools:</p> <ul> <li>Medaka (Nanopore): medaka_consensus -i reads.fastq -d assembly.fasta -o polished</li> <li>Pilon (with short reads): pilon --genome assembly.fasta --frags mapped_reads.bam</li> <li>Racon: racon reads.fastq mappings.paf assembly.fasta</li> </ul>"},{"location":"day4/genome_assembly/#hybrid-assemblers","title":"Hybrid Assemblers\u200b","text":"<ul> <li>Combine short and long reads, integrating DBG and OLC methods for improved accuracy and contiguity.\u200b</li> <li>Balances accuracy (short reads) and completeness (long reads, resolving repeats and structural variants)</li> <li>Requires careful data integration</li> <li>Unicycler, MaSuRCA</li> </ul>"},{"location":"day4/genome_assembly/#reference-guided-assembly","title":"Reference-guided Assembly","text":"<ul> <li>Works well given a closely related reference genome</li> <li>Aligns reads to a reference genome and fills gaps</li> <li>Faster and less computationally demanding</li> <li>May miss novel sequences or structural variations</li> </ul>"},{"location":"day4/genome_assembly/#note","title":"NOTE","text":"Assembly Strategy Subtypes Read Type Compatibility Examples Notes De novo assembly DBG, OLC, Hybrid Short, long, hybrid Velvet, SPAdes, Canu, Flye, MaSuRCA, UniCycler No reference genome used Reference-guided assembly Mapping-based Short, long BWA, Bowtie2, Novoalign, Minimap2 Aligns reads to a known reference genome"},{"location":"day4/genome_assembly/#assembler-selection-factors","title":"Assembler Selection Factors\u200b","text":"<p>Choosing an assembler depends on\" - Sequencing technology, - Project goals, and - Available computational resources.\u200b</p>"},{"location":"day4/genome_assembly/#best-practices-for-genome-assembly","title":"Best Practices for Genome Assembly","text":"<ul> <li>Start with high-quality, high-coverage sequencing data to improve the accuracy of the assembly.</li> <li>Try multiple assemblers and compare results, as different tools may perform better for different data types.</li> <li>Use iterative rounds of assembly, scaffolding, and polishing to gradually improve the assembly.</li> <li>Validate the final assembly using independent data, such as long-read sequencing or optical mapping.</li> <li>Keep detailed records of all parameters and steps used in the assembly process for reproducibility.</li> </ul>"},{"location":"day4/practical_assembly/","title":"De novo Genome Assembly Practical","text":""},{"location":"day4/practical_assembly/#de-novo-genome-assembly-practical","title":"De novo Genome Assembly Practical","text":"<p>Before setting up you need to know the current workig directory  </p><pre><code># Check current working directory\npwd\n# List the contents of the working diresctory\nls\n# Define output dir\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n# Create relevant output directories. -p so that it creates parent dir if it doesn't exist\nmkdir -p ${outspades}\"tb\"\nmkdir -p ${outspades}\"vc\"\nmkdir -p ${outquast}\"tb\"\nmkdir -p ${outquast}\"vc\"\n</code></pre><p></p> <p>Delete ${outspades} and ${outquast} and create them using one line of code and not four as above</p> <p></p><pre><code># Request for an interactive node (Resources)\n## Use this command to retrieve the previously used \"srun\" command\nhistory | grep \"srun\"\nsrun --cpus-per-tasks=16 --mem=128g --time 3:00:00 --job-name \"${USER}-assembly\" --pty /bin/bash\n\n# # Clear all modules\nmodule purge\n# Load the required modules\nmodule load spades/4.2.0\n## How can I get the help documentation?\nspades.py --help | less\n# Lets run spades with a test dataset\n/software/bio/spades/4.2.0/bin/spades.py  --test  --careful\n</code></pre> We  will use trimmed reads and file with IDs that we created during the initial data cleaning process. Check where the ID files are  and CHANGE DIR to where these are. Use \"cd /path/to/tb_IDs\"<p></p> <pre><code>## Subset the data\nhead -2 /data/users/${USER}/data_analysis/tb_IDs &gt; /data/users/${USER}/02_tb_IDs\nhead -2 /data/users/${USER}/data_analysis/vc_IDs &gt; /data/users/${USER}/02_vc_IDs\n# Define dirs\nindir=\"/data/users/${USER}/data_analysis/trimmed_trimmomatic/\"\noutdir=\"/data/users/${USER}/data_analysis/assembly-test/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outspades}tb ${outspades}vc ${outquast}tb ${outquast}vc\n\ncd /data/users/${USER}/\n# Run SPAdes for M. tuberculosis\necho \"Starting M. tuberculosis assembly at $(date)\"\n\nfor SAMPLE in $(cat 02_tb_IDs)\ndo\n  echo \"[TB|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}tb/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}tb/${SAMPLE}_2.fastq.gz \\\n    --careful \\\n    --cov-cutoff auto \\\n    -t 16 \\\n    -o ${outspades}tb/${SAMPLE}\ndone\n</code></pre>"},{"location":"day4/practical_assembly/#notes","title":"Notes","text":"<ul> <li>-1                file with forward paired-end reads</li> <li>-2                file with reverse paired-end reads</li> <li>--careful                   error and mismatch correction</li> <li>-o                          path to output file (directory shouldn't be existing)</li> <li>--cov-cutoff auto           Automatic coverage cutoff (usiful mostly for high GC genomes)</li> </ul> <pre><code># Perform denovo assembly for all TB ands VC samples\nmkdir -p /users/${USER}/scripts /users/${USER}/logs\n\n## Now let's write our submission script\nnano /users/${USER}/scripts/assembly_01.sh\n\n#!/bin/bash\n#SBATCH --job-name='assembly-${USER}'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=128GB\n#SBATCH --output=/users/${USER}/logs/assembly_01-stdout.txt\n#SBATCH --error=/users/${USER}/logs/assembly_01-stdout.txt\n#SBATCH --time=12:00:00\n\n# Define dir\nindir=\"/data/users/${USER}/data_analysis/trimmed_trimmomatic/\"\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\n# Change into the DIR with \"tb_IDs\"\ncd /data/users/${USER}/data_analysis/\n# Run SPAdes for M. tuberculosis\necho \"Starting M. tuberculosis assembly at $(date)\"\nfor SAMPLE in $(cat tb_IDs); do\n  echo \"[TB|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}tb/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}tb/${SAMPLE}_2.fastq.gz \\\n    --careful --cov-cutoff auto -t 16 \\\n    -o ${outspades}tb/${SAMPLE}\ndone\necho \"M. tuberculosis assembly completed at $(date)\"\n\nfor SAMPLE in $(cat vc_IDs); do\n  echo \"[VC|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}vc/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}vc/${SAMPLE}_2.fastq.gz \\\n    --careful -t 16 \\\n    -o ${outspades}vc/${SAMPLE}\ndone\n## SAVE THIS IN NANO BY\n# Ctrl + O\n\n# CLOSE THE FILE\n# ctrl + x\n</code></pre> Before submission of your script, verify if all the input directory and files exist. <pre><code>## SUBMIT THE ASSEMBLY SCRIPT\nsbatch /users/${USER}/scripts/assembly_01.sh\n</code></pre>"},{"location":"day4/practical_assembly/#genome-assembly-quality","title":"Genome Assembly Quality","text":"<p>Assess the quality of your assemblies to be confident of your downstream analysis. </p>"},{"location":"day4/practical_assembly/#common-assembly-quality-metrics","title":"Common Assembly Quality Metrics","text":"<ul> <li>Expected genome size</li> <li>Expected GC content %</li> <li>Number of contigs</li> <li>N50 and L50</li> </ul>"},{"location":"day4/practical_assembly/#common-assembly-issues-and-solutions","title":"Common Assembly Issues and Solutions","text":"<p>===================================</p> \ud83d\udea8 Issue \ud83d\udd0d Possible Cause \ud83d\udee0\ufe0f Solution High number of contigs (&gt;100) \u2022 Low coverage (&lt;30x)  \u2022 Poor quality reads  \u2022 Highly repetitive genome  \u2022 Contamination \u2022 Increase sequencing depth  \u2022 Better read trimming/filtering  \u2022 Use hybrid assembly with long reads  \u2022 Check contamination with Kraken2 Low N50 (&lt;50kb for bacteria) \u2022 Repetitive sequences  \u2022 Low coverage  \u2022 Assembly parameter issues \u2022 Adjust k-mer sizes in SPAdes  \u2022 Use <code>--careful</code> flag  \u2022 Try different assemblers (Unicycler, SKESA) Assembly size much larger than expected \u2022 Contamination  \u2022 Duplication in assembly  \u2022 Presence of plasmids \u2022 Perform contamination screening  \u2022 Check duplication ratio in QUAST  \u2022 Separate plasmid sequences High number of hypothetical proteins (&gt;50%) \u2022 Novel organism/strain  \u2022 Poor annotation database match  \u2022 Assembly quality issues \u2022 Use specialized databases (RefSeq, UniProt)  \u2022 Manual curation of key genes  \u2022 Functional annotation with KEGG/COG"},{"location":"day4/practical_assembly/#running-quast-tool","title":"Running QUAST Tool","text":"<pre><code># Run QUAST for a subset of M. tuberculosis dataset\necho \"Running QUAST analysis for M. tuberculosis...\"\n\nindir=\"/data/users/${USER}/data_analysis/trimmed_trimmomatic/\"\noutdir=\"/data/users/${USER}/data_analysis/assembly-test/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outdir}\"02_quast/\"\n# Remove modules in your env\nmodules purge\n# Load module\nmodule load quast\n# Use the subset of our data\ncd /data/users/${USER}/\nfor SAMPLE in $(cat 02_tb_IDs)\ndo\n  echo \"[TB|SPADES] $SAMPLE\"\n  quast.py ${outspades}tb/${SAMPLE}/contigs.fasta \\\n    #-r /data/TB_H37Rv.fasta -g /data/TB_H37rv.gff \\\n    -o ${outquast}tb/${SAMPLE} \\\n    --threads 4 --min-contig 200 \\\n    --labels \"MTB_Assembly\"\ndone\n</code></pre>"},{"location":"day4/practical_assembly/#quast-submission-script-for-all-samples","title":"Quast Submission script for all samples","text":"<pre><code># Assess all assembly results for all TB ands VC samples\n# In case you didn't create these above\nmkdir -p /users/${USER}/scripts /users/${USER}/logs\n\n## Now let's write our submission script\nnano /users/${USER}/scripts/assembly_02.sh\n\n#!/bin/bash\n#SBATCH --job-name='quast-${USER}'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=128GB\n#SBATCH --output=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --error=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --time=12:00:00\n\n# Define dir\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outdir}\"01_spades\" ${outdir}\"02_quast\"\n# Run QUAST for a subset of M. tuberculosis dataset\necho \"Running QUAST analysis for M. tuberculosis...\"\n# Remove modules in your env\nmodules purge\n# Load module\nmodule load quast\n# Use the sample IDs for our data\ncd /data/users/${USER}/data_analysis\nfor SAMPLE in $(cat tb_IDs)\ndo\n  echo \"[TB|QUAST] $SAMPLE\"\n  quast.py ${outspades}tb/${SAMPLE}/contigs.fasta \\\n    #-r /data/TB_H37Rv.fasta -g /data/TB_H37rv.gff \\\n    -o ${outquast}tb/${SAMPLE} \\\n    --threads 4 --min-contig 200 \\\n    --labels \"MTB_Assembly\"\ndone\n\nfor SAMPLE in $(cat vc_IDs); do\n  echo \"[VC|QUAST] $SAMPLE\"\n  quast.py ${outspades}vc/${SAMPLE}/contigs.fasta \\\n    #-r /data/TB_H37Rv.fasta -g /data/TB_H37rv.gff \\\n    -o ${outquast}tb/${SAMPLE} \\\n    --threads 4 --min-contig 200 \\\n    --labels \"MTB_Assembly\"\ndone\n## SAVE THIS IN NANO BY\n# Ctrl + O\n\n# CLOSE THE FILE\n# ctrl + x\n</code></pre> <pre><code># Assess all assembly results for all TB ands VC samples\n# In case you didn't create these above\nmkdir -p /users/${USER}/scripts /users/${USER}/logs\n\n## Now let's write our submission script\nnano /users/${USER}/scripts/assembly_02.sh\n\n#!/bin/bash\n#SBATCH --job-name='quast-${USER}'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=128GB\n#SBATCH --output=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --error=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --time=12:00:00\n\n# Define dir\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outdir}\"02_quast/tb\" ${outdir}\"02_quast/vc\"\n# Run Prokka for M. tuberculosis\necho \"Starting M. tuberculosis annotation at $(date)\"\nfor SAMPLE in $(cat tb_IDs); do\n  echo \"[TB|SPADES] $SAMPLE\"\n  prokka ${outspades}tb/${SAMPLE}/contigs.fasta \\\n       --outdir ${outquast}/${SAMPLE} \\\n       --cpus 8 --genus Mycobacterium\ndone\necho \"M. tuberculosis annotation completed at $(date)\"\n\nfor SAMPLE in $(cat vc_IDs); do\n  echo \"[VC|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}vc/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}vc/${SAMPLE}_2.fastq.gz \\\n    --careful -t 8 \\\n    -o ${outspades}vc/${SAMPLE}\ndone\n## SAVE THIS IN NANO BY\n# Ctrl + O\n\n# CLOSE THE FILE\n# ctrl + x\n</code></pre> <pre><code>## Clean-up\nrm -rf /data/users/${USER}/data_analysis/assembly-test/\n</code></pre>"},{"location":"day5/amr_prediction/","title":"Amr prediction","text":""},{"location":"day5/amr_prediction/#antimicrobial-resistance","title":"Antimicrobial Resistance","text":""},{"location":"modules/day1/","title":"Day 1 - Welcome to the Course!","text":""},{"location":"modules/day1/#day-1-welcome-to-the-course","title":"Day 1: Welcome to the Course!","text":"<p>Date: September 1, 2025 Duration: 09:00-13:00 CAT Focus: Course introduction, genomic surveillance overview, sequencing technologies</p>"},{"location":"modules/day1/#overview","title":"Overview","text":"<p>Welcome to the Microbial Genomics &amp; Metagenomics Training Course! Day 1 introduces the course, provides essential background on genomic surveillance, covers sequencing technologies, and introduces key databases and tools used throughout the training.</p>"},{"location":"modules/day1/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 1, you will be able to:</p> <ul> <li>Understand the role of genomic surveillance in public health</li> <li>Recognize different sequencing technologies and their applications</li> <li>Navigate and use PubMLST database resources</li> <li>Perform basic command line operations</li> <li>Set up and configure analysis environments</li> </ul>"},{"location":"modules/day1/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Introductions All 09:10 Overview of clinical pathogens and genomic surveillance Slides Ephifania Geza 09:40 Overview of sequencing technologies and data types Sindiswa Lukhele 10:00 Setting up and exploring PubMLST Sindiswa Lukhele 11:00 Break 11:30 Introduction to command line interface Practical Arash Iranzadeh"},{"location":"modules/day1/#key-topics","title":"Key Topics","text":""},{"location":"modules/day1/#1-course-introduction-and-participant-introductions","title":"1. Course Introduction and Participant Introductions","text":"<ul> <li>Welcome and course overview</li> <li>Trainer and participant introductions</li> <li>Course objectives and structure</li> <li>Training schedule and logistics</li> </ul>"},{"location":"modules/day1/#2-clinical-pathogens-and-genomic-surveillance","title":"2. Clinical Pathogens and Genomic Surveillance","text":"<ul> <li>Role of genomics in infectious disease surveillance</li> <li>Applications in outbreak investigation</li> <li>Antimicrobial resistance monitoring</li> <li>Integration with epidemiological data</li> </ul>"},{"location":"modules/day1/#3-sequencing-technologies-and-data-types","title":"3. Sequencing Technologies and Data Types","text":"<ul> <li>Next-generation sequencing platforms</li> <li>Illumina, Oxford Nanopore, PacBio comparison</li> <li>Short-read vs long-read technologies</li> <li>Data quality considerations and file formats</li> </ul>"},{"location":"modules/day1/#4-pubmlst-database-system","title":"4. PubMLST Database System","text":"<ul> <li>Multi-locus sequence typing (MLST) concepts</li> <li>Database navigation and search functions</li> <li>Species-specific typing schemes</li> <li>Data submission and retrieval</li> </ul>"},{"location":"modules/day1/#5-command-line-interface-basics","title":"5. Command Line Interface Basics","text":"<ul> <li>Introduction to Unix/Linux command line</li> <li>Git Bash setup for Windows users</li> <li>Basic file operations and navigation</li> <li>Introduction to R statistical environment</li> </ul>"},{"location":"modules/day1/#tools-and-resources","title":"Tools and Resources","text":""},{"location":"modules/day1/#databases-explored","title":"Databases Explored","text":"<ul> <li>PubMLST - Public databases for molecular typing</li> <li>Pathogen databases - Species-specific resources</li> <li>MLST schemes - Standardized typing protocols</li> </ul>"},{"location":"modules/day1/#software-introduced","title":"Software Introduced","text":"<ul> <li>Git Bash - Command line interface for Windows</li> <li>R/RStudio - Statistical computing environment</li> <li>Web browsers - For database navigation</li> <li>Terminal applications - Command line access</li> </ul>"},{"location":"modules/day1/#hands-on-activities","title":"Hands-on Activities","text":""},{"location":"modules/day1/#exercise-1-pubmlst-exploration-30-minutes","title":"Exercise 1: PubMLST Exploration (30 minutes)","text":"<p>Navigate the PubMLST website and explore available databases for different pathogens.</p>"},{"location":"modules/day1/#exercise-2-basic-command-line-operations-45-minutes","title":"Exercise 2: Basic Command Line Operations (45 minutes)","text":"<p>Practice essential Unix commands and file system navigation.</p>"},{"location":"modules/day1/#exercise-3-r-environment-setup-30-minutes","title":"Exercise 3: R Environment Setup (30 minutes)","text":"<p>Install and configure R/RStudio for data analysis.</p>"},{"location":"modules/day1/#exercise-4-database-search-practice-15-minutes","title":"Exercise 4: Database Search Practice (15 minutes)","text":"<p>Search for MLST data for specific bacterial isolates.</p>"},{"location":"modules/day1/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day1/#genomic-surveillance-applications","title":"Genomic Surveillance Applications","text":"<ul> <li>Outbreak investigation: Tracking transmission patterns</li> <li>Antimicrobial resistance: Monitoring resistance emergence</li> <li>Epidemiological studies: Population structure analysis</li> <li>Public health response: Informing control measures</li> </ul>"},{"location":"modules/day1/#sequencing-technology-comparison","title":"Sequencing Technology Comparison","text":"Platform Read Length Accuracy Throughput Cost Best For Illumina 150-300 bp &gt;99% High Low Routine surveillance Oxford Nanopore 1-100 kb ~95% Medium Medium Structural variants PacBio 10-25 kb &gt;99% Medium High Complete genomes"},{"location":"modules/day1/#mlst-fundamentals","title":"MLST Fundamentals","text":"<ul> <li>Housekeeping genes: Conserved sequences for typing</li> <li>Allelic profiles: Unique combinations define sequence types</li> <li>Population structure: Understanding strain relationships</li> <li>Standardization: Reproducible typing across laboratories</li> </ul>"},{"location":"modules/day1/#resources","title":"Resources","text":""},{"location":"modules/day1/#essential-websites","title":"Essential Websites","text":"<ul> <li>PubMLST - Public databases for molecular typing</li> <li>Pathogen Watch - Genomic surveillance platform</li> <li>NCBI SRA - Sequence Read Archive</li> </ul>"},{"location":"modules/day1/#documentation","title":"Documentation","text":"<ul> <li>Git Bash User Guide</li> <li>R Project Documentation</li> <li>PubMLST User Guide</li> </ul>"},{"location":"modules/day1/#training-materials","title":"Training Materials","text":"<ul> <li>Command line cheat sheets</li> <li>MLST database tutorials</li> <li>Sequencing technology overviews</li> </ul>"},{"location":"modules/day1/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day1/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Navigate PubMLST interface successfully</li> <li>Execute basic command line operations</li> <li>Identify appropriate sequencing platforms for different applications</li> <li>Understand MLST typing principles</li> </ul>"},{"location":"modules/day1/#group-discussion","title":"Group Discussion","text":"<ul> <li>Share experiences with different pathogens</li> <li>Discuss genomic surveillance challenges in different settings</li> <li>Compare sequencing technology applications</li> <li>Explore database search strategies</li> </ul>"},{"location":"modules/day1/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day1/#command-line-anxiety","title":"Command Line Anxiety","text":"<p>Many participants are new to command line interfaces. We provide: - Patient, step-by-step instruction - Plenty of practice time - Peer support and collaboration - Reference materials for later use</p>"},{"location":"modules/day1/#technical-setup-issues","title":"Technical Setup Issues","text":"<pre><code># Common Git Bash issues on Windows\n# Check if Git Bash is properly installed\ngit --version\n\n# Verify R installation\nR --version\n</code></pre>"},{"location":"modules/day1/#database-navigation","title":"Database Navigation","text":"<ul> <li>Start with simple searches</li> <li>Use guided examples</li> <li>Practice with known organisms</li> <li>Build confidence gradually</li> </ul>"},{"location":"modules/day1/#looking-ahead","title":"Looking Ahead","text":"<p>Day 2 Preview: Introduction to Command Line, HPC, &amp; Quality Control including: - High Performance Computing (HPC) introduction - Advanced command line operations - Quality checking and control methods - Species identification techniques - Guest talk on M. tuberculosis and co-infection</p> <p>Key Learning Outcome: Day 1 establishes the foundational knowledge of genomic surveillance principles, sequencing technologies, and essential database resources that underpin all subsequent training activities.</p>"},{"location":"modules/day10/","title":"Day 10 - Wrap-up session","text":""},{"location":"modules/day10/#day-10-wrap-up-session","title":"Day 10: Wrap-up session","text":"<p>Date: September 12, 2025 Duration: 09:00-11:40 CAT Focus: Course conclusion, presentations, and future directions</p>"},{"location":"modules/day10/#overview","title":"Overview","text":"<p>The final day of the microbial genomics training course brings together all learning experiences through participant presentations, showcases ongoing research initiatives, and provides guidance for continued professional development in computational biology.</p>"},{"location":"modules/day10/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 10, you will be able to:</p> <ul> <li>Present bioinformatics analysis results effectively to scientific audiences</li> <li>Demonstrate mastery of key concepts covered throughout the course</li> <li>Identify resources for continued learning and professional development</li> <li>Connect with ongoing research initiatives and training opportunities</li> <li>Plan next steps for implementing learned skills in your research</li> <li>Build professional networks within the computational biology community</li> </ul>"},{"location":"modules/day10/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Participant presentations 11:15 Short talks NGS-Academy/AfriGen-D/eLwazi ODSP 11:40 End of the course [Resources]"},{"location":"modules/day10/#presentation-session-0900-1115","title":"Presentation Session (09:00-11:15)","text":""},{"location":"modules/day10/#presentation-guidelines","title":"Presentation Guidelines","text":"<p>Each participant will deliver a 5-minute presentation covering their Day 9 analysis work:</p>"},{"location":"modules/day10/#presentation-structure","title":"Presentation Structure","text":"<ol> <li>Introduction (1 minute)</li> <li>Research question or objective</li> <li>Brief background context</li> <li> <p>Dataset description</p> </li> <li> <p>Methods (1.5 minutes)</p> </li> <li>Analysis workflow overview</li> <li>Key tools and techniques used</li> <li> <p>Parameter choices and rationale</p> </li> <li> <p>Results (2 minutes)</p> </li> <li>Major findings from analysis</li> <li>Key figures or summary statistics</li> <li> <p>Interpretation of results</p> </li> <li> <p>Challenges &amp; Solutions (30 seconds)</p> </li> <li>Main obstacles encountered</li> <li>How they were addressed</li> <li>Lessons learned</li> </ol>"},{"location":"modules/day10/#technical-requirements","title":"Technical Requirements","text":"<ul> <li>Format: PDF slides or live demonstration</li> <li>Time limit: Strictly enforced 5 minutes</li> <li>Q&amp;A: 2-3 minutes for questions after each presentation</li> <li>Backup: Have presentation files ready on USB drive</li> </ul>"},{"location":"modules/day10/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>Presentations will be assessed on:</p> <ul> <li>Scientific rigor: Appropriate methods and interpretation</li> <li>Technical competence: Correct use of bioinformatics tools</li> <li>Communication clarity: Clear explanation of complex concepts</li> <li>Problem-solving: Evidence of troubleshooting and adaptation</li> <li>Course integration: Application of multiple course concepts</li> </ul>"},{"location":"modules/day10/#sample-presentation-topics","title":"Sample Presentation Topics","text":"<p>Based on participant data types, presentations may cover:</p>"},{"location":"modules/day10/#genomic-analysis-examples","title":"Genomic Analysis Examples","text":"<ul> <li>\"Antimicrobial resistance profiling in Mycobacterium tuberculosis isolates\"</li> <li>\"Phylogenetic analysis of Mycobacterium tuberculosis outbreak strains\"</li> <li>\"Comparative genomics of Vibrio cholerae from environmental samples\"</li> <li>\"Transmission analysis of Vibrio cholerae epidemic strains\"</li> </ul>"},{"location":"modules/day10/#methodological-examples","title":"Methodological Examples","text":"<ul> <li>\"Optimizing assembly parameters for low-coverage genomes\"</li> <li>\"Developing Nextflow pipeline for routine surveillance\"</li> <li>\"Quality control strategies for degraded DNA samples\"</li> </ul>"},{"location":"modules/day10/#presentation-order","title":"Presentation Order","text":"<p>Presentations will be grouped by analysis type to facilitate discussion:</p> <ol> <li>Genomic Surveillance (9:00-9:45)</li> <li>Methodological &amp; Pipeline Development (9:45-11:15)</li> </ol>"},{"location":"modules/day10/#special-presentations-1115-1140","title":"Special Presentations (11:15-11:40)","text":""},{"location":"modules/day10/#ngs-academy-initiative","title":"NGS-Academy Initiative","text":"<p>Overview of next-generation sequencing training programs - Advanced training opportunities - Certification pathways - Research collaboration opportunities - International exchange programs</p>"},{"location":"modules/day10/#afrigen-d-project","title":"AfriGen-D Project","text":"<p>African Genome Diversity Project updates - Current research initiatives - Collaboration opportunities for participants - Data sharing and analysis platforms - Future funding opportunities</p>"},{"location":"modules/day10/#elwazi-odsp-open-data-science-platform","title":"eLwazi ODSP (Open Data Science Platform)","text":"<p>Data science infrastructure and resources - Platform capabilities and access - Training modules and resources - Research project support - Community building initiatives</p>"},{"location":"modules/day10/#course-completion","title":"Course Completion","text":""},{"location":"modules/day10/#certificate-requirements","title":"Certificate Requirements","text":"<p>To receive course completion certificate, participants must:</p> <ul> <li> Attend at least 8 out of 10 training days</li> <li> Complete all hands-on exercises</li> <li> Submit Day 9 analysis documentation</li> <li> Deliver Day 10 presentation</li> <li> Participate in course evaluation</li> </ul>"},{"location":"modules/day10/#skills-assessment-summary","title":"Skills Assessment Summary","text":"<p>By course completion, participants will have demonstrated:</p>"},{"location":"modules/day10/#technical-skills","title":"Technical Skills","text":"<ul> <li>Command line proficiency: Navigation, file management, and tool execution</li> <li>Quality control: Assessment and improvement of sequencing data</li> <li>Genome assembly: De novo assembly and quality assessment</li> <li>Annotation: Functional and structural genome annotation</li> <li>Phylogenetics: Tree construction and interpretation</li> <li>Workflow development: Nextflow pipeline creation and optimization</li> </ul>"},{"location":"modules/day10/#analytical-skills","title":"Analytical Skills","text":"<ul> <li>Data interpretation: Drawing biological conclusions from computational results</li> <li>Method selection: Choosing appropriate tools for specific analyses</li> <li>Parameter optimization: Adjusting analysis parameters for data characteristics</li> <li>Quality assessment: Evaluating reliability of computational results</li> <li>Troubleshooting: Diagnosing and solving technical problems</li> </ul>"},{"location":"modules/day10/#professional-skills","title":"Professional Skills","text":"<ul> <li>Documentation: Maintaining analysis logs and reproducible workflows</li> <li>Presentation: Communicating results to scientific audiences</li> <li>Collaboration: Working effectively in computational research teams</li> <li>Continuous learning: Accessing resources for ongoing skill development</li> </ul>"},{"location":"modules/day10/#post-course-resources","title":"Post-Course Resources","text":""},{"location":"modules/day10/#immediate-support-next-3-months","title":"Immediate Support (Next 3 months)","text":"<ul> <li>Email support: Continued access to trainer expertise</li> <li>Online forum: Participant discussion platform</li> <li>Monthly virtual meetups: Progress sharing and troubleshooting</li> <li>Resource sharing: Access to course materials and datasets</li> </ul>"},{"location":"modules/day10/#long-term-development","title":"Long-term Development","text":"<ul> <li>Advanced training: Information about specialized workshops</li> <li>Research collaboration: Connections to ongoing projects</li> <li>Professional networks: Links to regional and international communities</li> <li>Career opportunities: Job postings and fellowship announcements</li> </ul>"},{"location":"modules/day10/#recommended-next-steps","title":"Recommended Next Steps","text":""},{"location":"modules/day10/#for-beginners","title":"For Beginners","text":"<ol> <li>Practice with additional datasets</li> <li>Complete online tutorials for specific tools</li> <li>Join local bioinformatics user groups</li> <li>Consider formal coursework in computational biology</li> </ol>"},{"location":"modules/day10/#for-intermediate-users","title":"For Intermediate Users","text":"<ol> <li>Develop specialized analysis pipelines</li> <li>Contribute to open-source bioinformatics projects</li> <li>Attend specialized conferences and workshops</li> <li>Mentor others in computational skills</li> </ol>"},{"location":"modules/day10/#for-advanced-users","title":"for Advanced Users","text":"<ol> <li>Lead research projects using learned techniques</li> <li>Develop novel analytical methods</li> <li>Teach and train others in the community</li> <li>Collaborate on large-scale genomics initiatives</li> </ol>"},{"location":"modules/day10/#course-evaluation","title":"Course Evaluation","text":""},{"location":"modules/day10/#feedback-categories","title":"Feedback Categories","text":""},{"location":"modules/day10/#content-assessment","title":"Content Assessment","text":"<ul> <li>Relevance to research needs</li> <li>Appropriate level of technical detail</li> <li>Balance of theory and practical application</li> <li>Currency of methods and tools</li> </ul>"},{"location":"modules/day10/#delivery-evaluation","title":"Delivery Evaluation","text":"<ul> <li>Trainer expertise and communication</li> <li>Hands-on exercise quality</li> <li>Technical support adequacy</li> <li>Course pacing and organization</li> </ul>"},{"location":"modules/day10/#impact-measurement","title":"Impact Measurement","text":"<ul> <li>Confidence in using bioinformatics tools</li> <li>Likelihood of applying learned skills</li> <li>Interest in advanced training</li> <li>Recommendations to colleagues</li> </ul>"},{"location":"modules/day10/#improvement-suggestions","title":"Improvement Suggestions","text":"<p>Participants are encouraged to provide specific suggestions for: - Additional topics to cover - Alternative teaching methods - Better integration of concepts - Enhanced practical exercises - Improved course materials</p>"},{"location":"modules/day10/#networking-and-community-building","title":"Networking and Community Building","text":""},{"location":"modules/day10/#contact-information-exchange","title":"Contact Information Exchange","text":"<ul> <li>WhatsApp group for ongoing communication</li> <li>LinkedIn professional network connections</li> <li>GitHub collaboration on analysis projects</li> <li>Research ResearchGate connections</li> </ul>"},{"location":"modules/day10/#regional-initiatives","title":"Regional Initiatives","text":"<ul> <li>South African Bioinformatics Society: Local meetings and conferences</li> <li>H3ABioNet: Pan-African bioinformatics network</li> <li>ISCB Regional Student Groups: International student connections</li> <li>Local university partnerships: Ongoing collaboration opportunities</li> </ul>"},{"location":"modules/day10/#final-remarks","title":"Final Remarks","text":""},{"location":"modules/day10/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Bioinformatics is a journey: Continuous learning and adaptation required</li> <li>Community matters: Collaboration accelerates learning and research</li> <li>Documentation is crucial: Reproducible research starts with good record-keeping</li> <li>Practice makes perfect: Regular use of skills prevents deterioration</li> <li>Stay current: Field evolves rapidly, ongoing education essential</li> </ol>"},{"location":"modules/day10/#words-of-encouragement","title":"Words of Encouragement","text":"<p>The computational biology field welcomes contributors from diverse backgrounds. Your unique perspective, combined with the skills learned in this course, positions you to make meaningful contributions to understanding microbial genomics and its applications to human health.</p>"},{"location":"modules/day10/#course-legacy","title":"Course Legacy","text":"<p>This training represents an investment in the future of computational biology in Africa. As you apply these skills in your research and career, you become part of a growing network of scientists advancing genomic medicine and public health through computational approaches.</p>"},{"location":"modules/day10/#acknowledgments","title":"Acknowledgments","text":""},{"location":"modules/day10/#training-team-recognition","title":"Training Team Recognition","text":"<p>Special thanks to the course instructors: - Ephifania Geza: Lead instructor and course coordinator - Arash Iranzadeh: Technical instruction and phylogenomics expertise - Sindiswa Lukhele: Sequencing technologies and quality control - Mamana Mbiyavanga: HPC systems and workflow development - Bethlehem Adnew: Guest expertise on tuberculosis genomics</p>"},{"location":"modules/day10/#institutional-support","title":"Institutional Support","text":"<ul> <li>University of Cape Town Computational Biology Division</li> <li>CIDRI-Africa research infrastructure</li> <li>High-performance computing facility access</li> <li>Guest lecture coordination and logistics</li> </ul>"},{"location":"modules/day10/#community-contributions","title":"Community Contributions","text":"<ul> <li>Dataset providers and research collaborators</li> <li>Open-source software developers</li> <li>International training program partnerships</li> <li>Participant engagement and peer learning</li> </ul>"},{"location":"modules/day10/#contact-information","title":"Contact Information","text":""},{"location":"modules/day10/#immediate-questions","title":"Immediate Questions","text":"<p>Course Coordinator: Ephifania Geza Email: ephifania.geza@uct.ac.za</p>"},{"location":"modules/day10/#technical-support","title":"Technical Support","text":"<p>HPC and Workflows: Mamana Mbiyavanga Email: mamana.mbiyavanga@uct.ac.za</p>"},{"location":"modules/day10/#general-inquiries","title":"General Inquiries","text":"<p>Training Program: microbial-genomics-training@uct.ac.za</p>"},{"location":"modules/day10/#follow-up-resources","title":"Follow-up Resources","text":"<ul> <li>Course Materials: GitHub repository access maintained</li> <li>Discussion Forum: Access links provided via email</li> <li>Newsletter: Quarterly updates on opportunities and resources</li> </ul> <p>Final Learning Outcome: Completion of this intensive training program provides participants with both the technical skills and professional network needed to pursue independent research in microbial genomics, contributing to advances in infectious disease understanding, antimicrobial resistance surveillance, and public health genomics.</p>"},{"location":"modules/day2/","title":"Day 2 - Introduction to Commandline, High Performance Computing, & Quality Control","text":""},{"location":"modules/day2/#day-2-introduction-to-commandline","title":"Day 2: Introduction to Commandline","text":"<p>Date: September 2, 2025 Duration: 09:00-13:00 CAT Focus: Command line proficiency, M. tuberculosis genomics</p>"},{"location":"modules/day2/#overview","title":"Overview","text":"<p>Day 2 focuses on building strong command line skills essential for bioinformatics work. This day provides the computational foundation needed for all subsequent genomic analyses in the course.</p>"},{"location":"modules/day2/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 2, you will be able to:</p> <ul> <li>Master essential Unix/Linux command line operations for bioinformatics workflows</li> <li>Understand M. tuberculosis genomics and co-infection patterns</li> </ul>"},{"location":"modules/day2/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Introduction to command line interface Practical Arash Iranzadeh 11:30 Break 12:00 Guest talk: MtB and co-infection Speaker Bio Bethlehem Adnew"},{"location":"modules/day2/#key-topics","title":"Key Topics","text":""},{"location":"modules/day2/#1-command-line-interface-fundamentals","title":"1. Command Line Interface Fundamentals","text":"<ul> <li>Unix/Linux file system navigation</li> <li>Essential commands for bioinformatics (grep, awk, sed)</li> <li>File manipulation and text processing</li> <li>Pipes and command chaining</li> <li>Working with compressed files (gzip, tar)</li> <li>Shell scripting basics for automation</li> </ul>"},{"location":"modules/day2/#2-m-tuberculosis-and-co-infection","title":"2. M. tuberculosis and Co-infection","text":"<ul> <li>TB genomics and strain typing</li> <li>Co-infection patterns and detection</li> <li>Clinical implications</li> <li>Molecular epidemiology approaches</li> <li>Drug resistance mechanisms</li> <li>Public health applications</li> </ul>"},{"location":"modules/day2/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day2/#command-line-tools","title":"Command Line Tools","text":"<ul> <li>Bash shell - Command line interface and scripting</li> <li>GNU coreutils - Essential Unix utilities (ls, cd, grep, etc.)</li> <li>Text processing - awk, sed, cut, sort, uniq</li> <li>File compression - gzip, tar, zip</li> <li>tmux/screen - Terminal session management</li> <li>rsync/scp - File transfer and synchronization</li> </ul>"},{"location":"modules/day2/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day2/#exercise-1-command-line-fundamentals-90-minutes","title":"Exercise 1: Command Line Fundamentals (90 minutes)","text":"<p>Master essential Unix commands for bioinformatics through practical exercises.</p> <pre><code># Navigate file systems and manipulate files\ncd ~/data\nls -la\nmkdir analysis_output\n\n# Process text files with Unix tools\ngrep \"^&gt;\" sequences.fasta | wc -l  # Count sequences\ncat sample.fastq | head -20         # View file contents\n\n# Work with compressed files\ngzip large_file.txt\ngunzip -c compressed.gz | head\n\n# Use pipes and redirection\ncat data.txt | sort | uniq &gt; unique_values.txt\n</code></pre>"},{"location":"modules/day2/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day2/#command-line-essentials","title":"Command Line Essentials","text":"<ul> <li>File system navigation: Understanding directory structure and paths</li> <li>Text processing: Using grep, sed, awk for data manipulation</li> <li>Pipes and redirection: Chaining commands for complex operations</li> <li>Shell scripting: Automating repetitive tasks</li> <li>Regular expressions: Pattern matching in bioinformatics data</li> </ul>"},{"location":"modules/day2/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day2/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Successfully connect to Ilifu HPC system</li> <li>Navigate Unix file system and manipulate files</li> <li>Complete command line exercises for pathogen genomics data</li> </ul>"},{"location":"modules/day2/#group-discussion","title":"Group Discussion","text":"<ul> <li>Share command line tips and tricks</li> <li>Discuss HPC resource management strategies</li> <li>Troubleshoot connection and job submission issues</li> <li>Compare different approaches to batch processing</li> </ul>"},{"location":"modules/day2/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day2/#command-line-challenges","title":"Command Line Challenges","text":"<pre><code># Permission denied errors\nchmod +x script.sh    # Make script executable\nls -la                # Check file permissions\n\n# Path issues\necho $PATH            # Check current PATH\nexport PATH=$PATH:/new/path  # Add to PATH\n</code></pre>"},{"location":"modules/day2/#command-line-resources","title":"Command Line Resources","text":"<ul> <li>Unix for Bioinformatics</li> <li>Bash Scripting Guide</li> <li>Command Line for Genomics</li> </ul>"},{"location":"modules/day2/#m-tuberculosis-resources","title":"M. tuberculosis Resources","text":"<ul> <li>TB-Profiler</li> <li>ReSeqTB</li> <li>TBDB</li> </ul>"},{"location":"modules/day2/#guest-lecture-mtb-and-co-infection","title":"Guest Lecture: MtB and Co-infection","text":""},{"location":"modules/day2/#speaker-bethlehem-adnew","title":"Speaker: Bethlehem Adnew","text":""},{"location":"modules/day2/#key-topics-covered","title":"Key Topics Covered","text":"<ul> <li>M. tuberculosis genomics: Strain diversity and typing methods</li> <li>Co-infection dynamics: TB-HIV and other respiratory pathogens</li> <li>Diagnostic challenges: Molecular detection in complex samples</li> <li>Treatment implications: Drug resistance in co-infected patients</li> <li>Epidemiological insights: Transmission patterns and control strategies</li> </ul>"},{"location":"modules/day2/#interactive-discussion-points","title":"Interactive Discussion Points","text":"<ul> <li>Current challenges in TB diagnosis</li> <li>Role of genomics in outbreak investigation</li> <li>Future directions in TB research</li> <li>Integration of genomic and clinical data</li> </ul>"},{"location":"modules/day2/#looking-ahead","title":"Looking Ahead","text":"<p>Day 3 Preview:  - Command line proficiency, - HPC fundamentals - Quality checking and control with FastQC - Species identification using Kraken2</p> <p>Key Learning Outcome: Mastery of command line operations and HPC infrastructure usage provides the essential computational foundation for all subsequent genomic analyses in the course. </p>"},{"location":"modules/day3/","title":"Day 3 - Genomic Characterization","text":""},{"location":"modules/day3/#day-3-accelerating-bioinformatics-hpc-qc-and-species-identification-essentials","title":"Day 3: Accelerating Bioinformatics: HPC, QC, and Species Identification Essentials","text":"<p>Date: September 3, 2025 Duration: 09:00-13:00 CAT Focus: HPC infrastructure, quality control, species identification</p>"},{"location":"modules/day3/#overview","title":"Overview","text":"<p>Day 3 continues building computational skills with an introduction to the Ilifu HPC infrastructure, then introduces essential genomic characterization techniques including quality control, species identification. These foundational skills are critical for all downstream genomic analyses.</p>"},{"location":"modules/day3/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 3, you will be able to:</p> <ul> <li>Connect to and navigate the Ilifu high-performance computing cluster</li> <li>Submit and manage jobs using the SLURM scheduler</li> <li>Understand resource allocation and job queue management on HPC systems</li> <li>Perform quality checking and control on sequencing data using FastQC</li> <li>Identify species from genomic data using Kraken2 and other tools</li> </ul>"},{"location":"modules/day3/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Introduction High Performance Computing (HPC) \u2013 Ilifu Notes \u2022 Practical 1 \u2022 Practical 2 Mamana Mbiyavanga 11:00 Break 11:30 Quality checking and control, as well as species identification Practical Arash Iranzadeh"},{"location":"modules/day3/#key-topics","title":"Key Topics","text":""},{"location":"modules/day3/#1-qc-and-species-identification-essentials","title":"1. QC and Species Identification Essentials","text":"<ul> <li>Quality checking, and adapter and low read quality filtering</li> <li>Contamination detection and removal</li> <li>Species identification or confirmation</li> </ul>"},{"location":"modules/day3/#2-high-performance-computing-hpc-ilifu-infrastructure","title":"2. High Performance Computing (HPC) - Ilifu Infrastructure","text":"<ul> <li>Introduction to cluster computing concepts</li> <li>Ilifu cluster architecture and capabilities</li> <li>SSH connections and authentication</li> <li>SLURM job scheduling system</li> <li>Resource allocation (CPU, memory, time)</li> <li>Module system for software management</li> </ul>"},{"location":"modules/day3/#3-slurm-job-management","title":"3. SLURM Job Management","text":"<ul> <li>Writing and submitting batch scripts</li> <li>Interactive vs batch jobs</li> <li>Job monitoring and queue management</li> <li>Resource specification and optimization</li> <li>Output and error file handling</li> <li>Best practices for efficient HPC usage</li> </ul>"},{"location":"modules/day3/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day3/#hpc-environment","title":"HPC Environment","text":"<ul> <li>Ilifu cluster - High-performance computing infrastructure</li> <li>SLURM - Job scheduling system</li> <li>Module system - Software environment management</li> <li>SSH clients - Remote connection tools</li> </ul>"},{"location":"modules/day3/#quality-control-tools","title":"Quality Control Tools","text":"<ul> <li>FASTQC - Quality checking</li> <li>MULTIQC - Quality checking and amalgation of reports</li> <li>Trimmomatic - Filter adapters and low quality reads</li> <li>Fastp -  Filter adapters and low quality reads</li> <li>KRAKEN2 - Species Identification</li> </ul>"},{"location":"modules/day3/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day3/#exercise-2-ilifu-hpc-connection-and-setup-30-minutes","title":"Exercise 2: Ilifu HPC Connection and Setup (30 minutes)","text":"<p>Connect to the Ilifu cluster and set up your working environment.</p> <pre><code># Connect to Ilifu via SSH\nssh username@slurm.ilifu.ac.za\n\n# Explore the HPC environment\npwd                    # Check current directory\nmodule avail          # List available software\nmodule load python    # Load software module\n\n# Check cluster resources\nsinfo                 # View cluster partitions\nsqueue               # Check job queue\n</code></pre>"},{"location":"modules/day3/#exercise-3-slurm-job-submission-60-minutes","title":"Exercise 3: SLURM Job Submission (60 minutes)","text":"<p>Learn to submit and manage jobs on the HPC cluster.</p> <pre><code># Create a simple batch script\ncat &gt; my_first_job.sh &lt;&lt; 'EOF'\n#!/bin/bash\n#SBATCH --job-name=test_job\n#SBATCH --time=00:10:00\n#SBATCH --mem=1G\n#SBATCH --cpus-per-task=1\n\necho \"Hello from HPC!\"\necho \"Running on node: $HOSTNAME\"\ndate\nEOF\n\n# Submit the job\nsbatch my_first_job.sh\n\n# Monitor job progress\nsqueue -u $USER\n</code></pre>"},{"location":"modules/day3/#exercise-1-species-identification-60-minutes","title":"Exercise 1: Species Identification (60 minutes)","text":"<p>Species identification and contamination screening</p>"},{"location":"modules/day3/#contamination-screening","title":"Contamination screening","text":"<p>kraken2 --db minikraken2_v2 assembly_output/scaffolds.fasta --report contamination_check.txt </p><pre><code>## Key Concepts\n\n### HPC Computing Principles\n- **Cluster architecture**: Login nodes vs compute nodes\n- **Job scheduling**: SLURM queue management and priority\n- **Resource allocation**: CPU, memory, and time specifications\n- **Module system**: Managing software environments\n- **Parallel processing**: Utilizing multiple cores efficiently\n\n### SLURM Job Management\n| Component | Description | Example |\n|-----------|-------------|---------|\n| Partition | Compute resource group | `main`, `gpu`, `bigmem` |\n| Job State | Current job status | `PD` (pending), `R` (running) |\n| Resources | CPU/Memory/Time | `--cpus-per-task=4 --mem=8G` |\n| Output | Job results and logs | `slurm-jobid.out` |\n\n## Assessment Activities\n\n### Individual Analysis\n- Write and submit a SLURM batch script\n- Monitor job status and retrieve results\n- Complete genome assembly workflow\n- Perform quality assessment and interpretation\n\n### HPC Connection Issues\n```bash\n# SSH key problems\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nssh-copy-id username@slurm.ilifu.ac.za\n\n# Module loading issues\nmodule avail    # List available modules\nmodule list     # Show loaded modules\nmodule purge    # Clear all modules\n</code></pre><p></p>"},{"location":"modules/day3/#slurm-job-troubleshooting","title":"SLURM Job Troubleshooting","text":"<pre><code># Job stuck in pending\nscontrol show job &lt;jobid&gt;  # Check job details\nsqueue -j &lt;jobid&gt;          # Check specific job\n\n# Resource issues\nsacct -j &lt;jobid&gt; --format=JobID,State,ExitCode,MaxRSS,Elapsed\n</code></pre>"},{"location":"modules/day3/#resources","title":"Resources","text":""},{"location":"modules/day3/#hpc-documentation","title":"HPC Documentation","text":"<ul> <li>Ilifu User Guide</li> <li>SLURM Quick Start</li> <li>SSH Key Management</li> </ul>"},{"location":"modules/day3/#assembly-issues","title":"Assembly Issues","text":"<pre><code># Low coverage assemblies\nspades.py --careful --cov-cutoff 5 -1 R1.fastq -2 R2.fastq -o low_cov_assembly/\n\n# Contamination removal\n# Remove contaminant contigs based on taxonomy\nseqtk subseq scaffolds.fasta clean_contigs.txt &gt; clean_assembly.fasta\n</code></pre>"},{"location":"modules/day3/#clinical-applications","title":"Clinical Applications","text":""},{"location":"modules/day3/#routine-surveillance","title":"Routine Surveillance","text":"<ul> <li>Mantaining data quality control standards</li> <li>Rapid species identification and typing</li> </ul>"},{"location":"modules/day3/#looking-ahead","title":"Looking Ahead","text":"<p>Day 4 Preview:  - Genome assembly and - Genome quality assessment - Genome annotation with Prokka</p> <p>Key Learning Outcome: Quality genomes to increase our confidence in our characterization capabilities essential for clinical genomics and public health surveillance.</p>"},{"location":"modules/day4/","title":"Day 4 - Genomic Characterization","text":""},{"location":"modules/day4/#day-4-genome-assembly-essentials-qc-identification-and-assembly","title":"Day 4: Genome Assembly Essentials: QC, Identification, and assembly","text":"<p>Date: September 4, 2025 Duration: 09:00-13:00 CAT Focus: Genome assembly and assessment</p>"},{"location":"modules/day4/#overview","title":"Overview","text":"<p>Day 4 builds on Day 3 by starting with a recap on quality control, followed by genome assembly and assessment of assemblies to ensure.</p>"},{"location":"modules/day4/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 4, you will be able to:</p> <ul> <li>Execute de novo genome assembly using SPAdes or other assemblers</li> <li>Assess assembly quality using QUAST and other metrics</li> </ul>"},{"location":"modules/day4/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Recap: Quality checking and control, and species identification Practical Arash Iranzadeh 10:30 Genome assembly, quality assessment Notes. Ephifania Geza 11:00 Break 11:30 Genome assembly, quality assessment: Continuation Practical Ephifania Geza"},{"location":"modules/day4/#key-topics","title":"Key Topics","text":""},{"location":"modules/day4/#1-genome-assembly-and-assessing-contigs","title":"1. Genome assembly and assessing contigs","text":"<ul> <li>De novo assembly algorithms and approaches</li> <li>Short-read vs long-read assembly strategies</li> <li>Assembly quality metrics and interpretation</li> <li>Assembly polishing and gap filling</li> </ul>"},{"location":"modules/day4/#datasets-used","title":"Datasets Used","text":""},{"location":"modules/day4/#v-cholerae-outbreak-collection","title":"V. cholerae Outbreak Collection","text":"<ul> <li>Source: Multi-country cholera outbreak (2019)</li> <li>Samples: 25 clinical isolates + environmental samples</li> <li>Timespan: 8-month outbreak period</li> <li>Geographic: Three countries, coastal regions</li> <li>Epidemiological data: Case demographics, travel history</li> </ul>"},{"location":"modules/day4/#tools-introduced","title":"Tools Introduced","text":""},{"location":"modules/day4/#pangenome-analysis","title":"Pangenome Analysis","text":"<ul> <li>SPAdes - De novo genome assembler</li> <li>Unicycler - Hybrid assembly pipeline</li> <li>Flye - Long-read assembly</li> <li>QUAST - Assembly quality assessment</li> </ul>"},{"location":"modules/day4/#annotation-tools","title":"Annotation Tools","text":"<ul> <li>Prokka - Automated prokaryotic annotation</li> <li>RAST - Rapid Annotation using Subsystem Technology</li> <li>NCBI PGAP - Prokaryotic Genome Annotation Pipeline</li> <li> <p>Bakta - Rapid bacterial genome annotation</p> </li> <li> <p>Panaroo - Pangenome pipeline</p> </li> <li>Roary - Rapid large-scale prokaryote pangenome analysis</li> <li>PPanGGOLiN - Depicting microbial diversity via pangenomes</li> </ul>"},{"location":"modules/day4/#phylogenetic-analysis","title":"Phylogenetic Analysis","text":"<ul> <li>IQ-TREE - Maximum likelihood phylogenetic inference</li> <li>RAxML - Randomized Axelerated Maximum Likelihood</li> <li>FastTree - Approximately maximum-likelihood trees</li> </ul>"},{"location":"modules/day4/#snp-analysis","title":"SNP Analysis","text":"<ul> <li>Snippy - Rapid haploid variant calling</li> <li>ParSNP - Rapid core genome SNP typing</li> <li>Gubbins - Recombination detection in bacterial genomes</li> </ul>"},{"location":"modules/day4/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day4/#exercise-1-genome-assembly-and-quality-assessment-60-minutes","title":"Exercise 1: Genome Assembly and Quality Assessment (60 minutes)","text":"<p>Assemble bacterial genomes and evaluate assembly quality.</p> <pre><code># De novo assembly with SPAdes\nspades.py --careful -1 sample_R1.fastq.gz -2 sample_R2.fastq.gz -o assembly_output/\n\n# Assembly quality assessment\nquast.py assembly_output/scaffolds.fasta -o quast_results/\n\n# Check assembly statistics\nassembly-stats assembly_output/scaffolds.fasta\n\n\n### Exercise 1: Pangenome Analysis (60 minutes)\nAnalyze the core and accessory genome of *V. cholerae* outbreak strains.\n\n```bash\n# Run Panaroo pangenome analysis\npanaroo -i *.gff -o panaroo_output --clean-mode strict\n\n# Visualize results\npython3 scripts/visualize_pangenome.py panaroo_output/\n</code></pre>"},{"location":"modules/day4/#exercise-2-phylogenetic-tree-construction-60-minutes","title":"Exercise 2: Phylogenetic Tree Construction (60 minutes)","text":"<p>Build maximum likelihood trees from core genome alignments.</p> <pre><code># Generate core genome alignment\nsnippy-core --ref reference.gbk snippy_output/*\n\n# Build phylogenetic tree\niqtree -s core_alignment.aln -m GTR+G -bb 1000 -nt 4\n\n# Visualize tree\nfigtree core_alignment.aln.treefile\n</code></pre>"},{"location":"modules/day4/#exercise-3-outbreak-investigation-45-minutes","title":"Exercise 3: Outbreak Investigation (45 minutes)","text":"<p>Integrate genomic and epidemiological data to investigate transmission.</p> <pre><code># Calculate pairwise SNP distances\nsnp-dists core_alignment.aln &gt; snp_distances.tsv\n\n# Identify transmission clusters\ncluster_analysis.py --snp-threshold 10 --epi-data metadata.csv\n</code></pre>"},{"location":"modules/day4/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day4/#assembly-quality-metrics","title":"Assembly Quality Metrics","text":"Metric Good Assembly Poor Assembly Action N50 &gt;50 kb &lt;10 kb Optimize parameters Contigs &lt;100 &gt;500 Check contamination Genome size Expected \u00b110% &gt;20% difference Review input data Coverage &gt;50x &lt;20x Sequence more"},{"location":"modules/day4/#pangenome-structure","title":"Pangenome Structure","text":"<ul> <li>Core genome: Genes present in all strains (housekeeping functions)</li> <li>Accessory genome: Variable genes (adaptation, virulence, resistance)</li> <li>Singleton genes: Present in single strain only</li> <li>Shell genes: Present in several but not all strains</li> </ul>"},{"location":"modules/day4/#phylogenetic-inference","title":"Phylogenetic Inference","text":"<ul> <li>Substitution models: GTR, HKY, JC69 for nucleotide evolution</li> <li>Rate heterogeneity: Gamma distribution for variable sites</li> <li>Bootstrap support: Statistical confidence in tree topology</li> <li>Branch lengths: Evolutionary distance (substitutions per site)</li> </ul>"},{"location":"modules/day4/#snp-thresholds-for-outbreak-investigation","title":"SNP Thresholds for Outbreak Investigation","text":"Pathogen SNP Threshold Timeframe Context M. tuberculosis 0-5 SNPs Recent transmission Same household S. Typhi 0-20 SNPs Outbreak cluster Weeks to months V. cholerae 0-10 SNPs Epidemic spread Days to weeks E. coli O157 0-15 SNPs Foodborne outbreak Days"},{"location":"modules/day4/#advanced-topics","title":"Advanced Topics","text":""},{"location":"modules/day4/#molecular-dating","title":"Molecular Dating","text":"<ul> <li>Tip dating: Using collection dates for molecular clock</li> <li>Bayesian methods: BEAST, MrBayes for time-resolved phylogenies</li> <li>Substitution rates: Pathogen-specific evolutionary rates</li> </ul>"},{"location":"modules/day4/#network-analysis","title":"Network Analysis","text":"<ul> <li>Minimum spanning trees: Alternative to bifurcating phylogenies</li> <li>Median-joining networks: Visualization of reticulate evolution</li> <li>Transmission networks: Direct transmission inference</li> </ul>"},{"location":"modules/day4/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day4/#individual-analysis","title":"Individual Analysis","text":"<ul> <li>Generate pangenome summary statistics</li> <li>Construct phylogenetic tree with bootstrap support</li> <li>Calculate SNP distances between outbreak isolates</li> <li>Identify transmission clusters based on genomic data</li> </ul>"},{"location":"modules/day4/#group-discussion","title":"Group Discussion","text":"<ul> <li>Compare assembly strategies and results</li> </ul>"},{"location":"modules/day4/#common-challenges","title":"Common Challenges","text":"<ul> <li>Interpret pangenome diversity in outbreak context</li> <li>Evaluate phylogenetic support for transmission hypotheses</li> <li>Discuss integration of genomic and epidemiological evidence</li> <li>Consider limitations of genomic outbreak investigation</li> </ul>"},{"location":"modules/day4/#common-challenges_1","title":"Common Challenges","text":""},{"location":"modules/day4/#low-phylogenetic-resolution","title":"Low Phylogenetic Resolution","text":"<pre><code># Try different substitution models\niqtree -s alignment.aln -m MFP -bb 1000  # Model selection\n\n# Use more informative sites only\niqtree -s alignment.aln -m GTR+G -bb 1000 --rate\n</code></pre>"},{"location":"modules/day4/#recombination-issues","title":"Recombination Issues","text":"<pre><code># Detect and remove recombinant regions\ngubbins alignment.aln\n\n# Use recombination-free alignment\niqtree -s alignment.filtered_polymorphic_sites.fasta\n</code></pre>"},{"location":"modules/day4/#missing-epidemiological-links","title":"Missing Epidemiological Links","text":"<pre><code># Lower SNP threshold for exploration\ncluster_analysis.py --snp-threshold 20 --epi-data metadata.csv\n\n# Consider longer transmission chains\ntransmission_chains.py --max-generations 3\n</code></pre>"},{"location":"modules/day4/#resources","title":"Resources","text":""},{"location":"modules/day4/#resources_1","title":"Resources","text":""},{"location":"modules/day4/#assembly-resources","title":"Assembly Resources","text":"<ul> <li>SPAdes Manual</li> <li>QUAST Documentation</li> <li>Assembly Best Practices</li> </ul>"},{"location":"modules/day4/#key-publications","title":"Key Publications","text":"<ul> <li>Page et al. (2015). Roary: rapid large-scale prokaryote pangenome analysis</li> <li>Tonkin-Hill et al. (2020). Producing polished prokaryotic pangenomes</li> <li>Croucher et al. (2015). Rapid phylogenetic analysis of bacterial genomes</li> </ul>"},{"location":"modules/day4/#software-documentation","title":"Software Documentation","text":"<ul> <li>IQ-TREE Manual</li> <li>Panaroo Documentation</li> <li>Snippy Manual</li> </ul>"},{"location":"modules/day4/#online-resources","title":"Online Resources","text":"<ul> <li>Microreact - Visualization platform</li> <li>iTOL - Interactive tree visualization</li> <li>FigTree - Tree viewing software</li> </ul>"},{"location":"modules/day4/#looking-ahead","title":"Looking Ahead","text":"<p>Day 5 Preview: Metagenomics analysis including: - Microbiome profiling from clinical samples - Pathogen detection in complex communities - Functional analysis of metagenomes - Association with host health outcomes</p>"},{"location":"modules/day4/#homework-optional","title":"Homework (Optional)","text":"<ol> <li>Analyze pangenome diversity in additional pathogen datasets</li> <li>Experiment with different phylogenetic methods and compare results</li> <li>Read case studies of genomic outbreak investigations</li> <li>Practice tree interpretation with published examples</li> </ol> <p>Key Takeaway: Genomic analysis provides unprecedented resolution for understanding pathogen evolution and transmission, but interpretation requires careful integration with epidemiological context and understanding of method limitations.</p>"},{"location":"modules/day5/","title":"Day 5 - Nextflow Pipeline Development","text":""},{"location":"modules/day5/#day-5-nextflow-pipeline-development","title":"Day 5: Nextflow Pipeline Development","text":"<p>Date: September 5, 2025 Duration: 09:00-13:00 CAT Focus: Workflow reproducibility, Nextflow basics, pipeline development</p>"},{"location":"modules/day5/#overview","title":"Overview","text":"<p>Day 5 introduces Nextflow, a powerful workflow management system for creating reproducible and scalable bioinformatics pipelines. We'll explore the fundamentals of Nextflow, the nf-core community standards, and begin developing a pipeline for genomic analysis including QC, assembly, quality assessment, and annotation.</p>"},{"location":"modules/day5/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 5, you will be able to:</p> <ul> <li>Understand the principles of reproducible computational workflows</li> <li>Write basic Nextflow scripts with processes and channels</li> <li>Utilize nf-core tools and community pipelines</li> <li>Design workflow architecture for genomic analysis</li> <li>Implement data flow using Nextflow channels</li> <li>Begin developing a pipeline for QC, assembly, and annotation</li> </ul>"},{"location":"modules/day5/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Reproducible workflows with Nextflow and nf-core Mamana Mbiyavanga 10:30 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga 11:30 Break 12:00 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga"},{"location":"modules/day5/#key-topics","title":"Key Topics","text":""},{"location":"modules/day5/#1-introduction-to-workflow-management","title":"1. Introduction to Workflow Management","text":"<ul> <li>Challenges in bioinformatics reproducibility</li> <li>Benefits of workflow management systems</li> <li>Nextflow vs other workflow systems (Snakemake, CWL, WDL)</li> <li>Container technologies (Docker, Singularity)</li> </ul>"},{"location":"modules/day5/#2-nextflow-fundamentals","title":"2. Nextflow Fundamentals","text":"<ul> <li>Nextflow architecture and concepts</li> <li>Processes, channels, and operators</li> <li>Configuration files and profiles</li> <li>Resource management and executors</li> <li>Error handling and resume capabilities</li> </ul>"},{"location":"modules/day5/#3-nf-core-community-and-standards","title":"3. nf-core Community and Standards","text":"<ul> <li>nf-core pipeline structure</li> <li>Community guidelines and best practices</li> <li>Using nf-core tools</li> <li>Available nf-core pipelines for genomics</li> <li>Contributing to nf-core</li> </ul>"},{"location":"modules/day5/#4-building-a-genomic-analysis-pipeline","title":"4. Building a Genomic Analysis Pipeline","text":"<ul> <li>Pipeline design and planning</li> <li>Implementing QC processes (FastQC, MultiQC)</li> <li>Assembly process integration (SPAdes)</li> <li>Quality assessment steps (QUAST)</li> <li>Annotation process (Prokka)</li> </ul>"},{"location":"modules/day5/#5-nextflow-scripting","title":"5. Nextflow Scripting","text":"<ul> <li>Writing process definitions</li> <li>Channel operations and data flow</li> <li>Parameter handling</li> <li>Conditional execution</li> <li>Module organization</li> </ul>"},{"location":"modules/day5/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day5/#workflow-management","title":"Workflow Management","text":"<ul> <li>Nextflow - Workflow orchestration system</li> <li>nf-core tools - Pipeline development framework</li> <li>Tower - Workflow monitoring platform</li> </ul>"},{"location":"modules/day5/#containerization","title":"Containerization","text":"<ul> <li>Docker - Container platform</li> <li>Singularity - HPC-friendly containers</li> <li>Conda - Package management</li> </ul>"},{"location":"modules/day5/#pipeline-components","title":"Pipeline Components","text":"<ul> <li>FastQC - Read quality control</li> <li>MultiQC - Aggregate reporting</li> <li>SPAdes - Genome assembly</li> <li>QUAST - Assembly assessment</li> <li>Prokka - Genome annotation</li> </ul>"},{"location":"modules/day5/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day5/#exercise-1-first-nextflow-script-30-minutes","title":"Exercise 1: First Nextflow Script (30 minutes)","text":"<p>Create and run a simple Nextflow pipeline.</p> <pre><code>#!/usr/bin/env nextflow\n\n// Define parameters\nparams.input = \"data/*.fastq\"\nparams.outdir = \"results\"\n\n// Create a channel from input files\nChannel\n    .fromPath(params.input)\n    .set { fastq_ch }\n\n// Define a process\nprocess countReads {\n    input:\n    path fastq from fastq_ch\n\n    output:\n    path \"*.count\" into counts_ch\n\n    script:\n    \"\"\"\n    echo \"Processing ${fastq}\"\n    wc -l ${fastq} &gt; ${fastq.baseName}.count\n    \"\"\"\n}\n\n// View the results\ncounts_ch.view()\n</code></pre>"},{"location":"modules/day5/#exercise-2-building-a-qc-pipeline-60-minutes","title":"Exercise 2: Building a QC Pipeline (60 minutes)","text":"<p>Implement quality control with FastQC and MultiQC.</p> <pre><code>process fastqc {\n    container 'biocontainers/fastqc:v0.11.9'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -t ${task.cpus} ${reads}\n    \"\"\"\n}\n\nprocess multiqc {\n    publishDir params.outdir, mode: 'copy'\n    container 'ewels/multiqc:latest'\n\n    input:\n    path '*' from fastqc_results.collect()\n\n    output:\n    path 'multiqc_report.html'\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day5/#exercise-3-integrating-assembly-90-minutes","title":"Exercise 3: Integrating Assembly (90 minutes)","text":"<p>Add genome assembly to the pipeline.</p> <pre><code>process spades_assembly {\n    container 'staphb/spades:latest'\n    cpus 4\n    memory '8 GB'\n\n    input:\n    tuple val(sample_id), path(reads1), path(reads2)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_contigs.fasta\")\n\n    script:\n    \"\"\"\n    spades.py \\\n        -1 ${reads1} \\\n        -2 ${reads2} \\\n        -o spades_output \\\n        -t ${task.cpus} \\\n        --careful\n\n    cp spades_output/contigs.fasta ${sample_id}_contigs.fasta\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day5/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day5/#workflow-principles","title":"Workflow Principles","text":"<ul> <li>Reproducibility: Same input \u2192 same output</li> <li>Portability: Run anywhere (laptop, HPC, cloud)</li> <li>Scalability: Handle any data volume</li> <li>Resumability: Restart from failure points</li> </ul>"},{"location":"modules/day5/#nextflow-components","title":"Nextflow Components","text":"Component Description Example Process Computational step <code>process fastqc { ... }</code> Channel Data flow connection <code>Channel.fromPath()</code> Operator Channel transformation <code>.map()</code>, <code>.filter()</code> Directive Process configuration <code>cpus 4</code>"},{"location":"modules/day5/#best-practices","title":"Best Practices","text":"<ol> <li>Use containers: Ensure environment reproducibility</li> <li>Parameterize everything: Make pipelines flexible</li> <li>Version control: Track pipeline changes</li> <li>Document thoroughly: Help users and future self</li> <li>Test incrementally: Build and test step by step</li> </ol>"},{"location":"modules/day5/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day5/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Create a basic Nextflow script with at least 2 processes</li> <li>Successfully run a pipeline with test data</li> <li>Modify pipeline parameters and observe changes</li> <li>Debug a pipeline with intentional errors</li> <li>Document pipeline usage</li> </ul>"},{"location":"modules/day5/#group-discussion","title":"Group Discussion","text":"<ul> <li>Compare Nextflow with traditional shell scripting</li> <li>Discuss reproducibility challenges and solutions</li> <li>Share pipeline design strategies</li> <li>Explore nf-core pipeline catalog</li> </ul>"},{"location":"modules/day5/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day5/#installation-issues","title":"Installation Issues","text":"<pre><code># Install Nextflow\ncurl -s https://get.nextflow.io | bash\n./nextflow run hello\n\n# Set up environment\nexport PATH=$PATH:$PWD\nexport NXF_VER=23.10.0\n</code></pre>"},{"location":"modules/day5/#channel-operations","title":"Channel Operations","text":"<pre><code>// Common channel patterns\nChannel\n    .fromFilePairs(params.reads)\n    .ifEmpty { error \"No read files found!\" }\n    .set { read_pairs_ch }\n\n// Combining channels\nfastqc_ch\n    .join(assembly_ch)\n    .map { sample, qc, assembly -&gt; \n        [sample, qc, assembly]\n    }\n</code></pre>"},{"location":"modules/day5/#resource-management","title":"Resource Management","text":"<pre><code>process memory_intensive {\n    memory { 2.GB * task.attempt }\n    maxRetries 3\n    errorStrategy 'retry'\n\n    script:\n    \"\"\"\n    # Your command here\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day5/#resources","title":"Resources","text":""},{"location":"modules/day5/#documentation","title":"Documentation","text":"<ul> <li>Nextflow Documentation</li> <li>nf-core Website</li> <li>Nextflow Training</li> </ul>"},{"location":"modules/day5/#tutorials","title":"Tutorials","text":"<ul> <li>Nextflow Tutorial</li> <li>nf-core Tutorials</li> <li>Seqera Labs Training</li> </ul>"},{"location":"modules/day5/#community","title":"Community","text":"<ul> <li>Nextflow Slack</li> <li>nf-core Slack</li> <li>GitHub Discussions</li> </ul>"},{"location":"modules/day5/#looking-ahead","title":"Looking Ahead","text":"<p>Day 6 Preview: Nextflow Pipeline Development - Continue building the genomic analysis pipeline - Advanced Nextflow features and optimization - Pipeline testing and validation - Deployment strategies</p> <p>Key Learning Outcome: Understanding workflow management principles and gaining hands-on experience with Nextflow enables creation of reproducible, scalable bioinformatics pipelines essential for modern genomic analysis.</p>"},{"location":"modules/day6/","title":"Day 6 - Nextflow Pipeline Development & Version Control with GitHub","text":""},{"location":"modules/day6/#day-6-nextflow-foundations-core-concepts","title":"Day 6: Nextflow Foundations &amp; Core Concepts","text":"<p>Date: September 8, 2025 Duration: 09:00-13:00 CAT Focus: Introduction to workflow management, Nextflow fundamentals, and first pipelines</p>"},{"location":"modules/day6/#learning-philosophy-see-it-understand-it-try-it-build-it-master-it","title":"Learning Philosophy: See it \u2192 Understand it \u2192 Try it \u2192 Build it \u2192 Master it","text":"<p>This module follows a proven learning approach designed specifically for beginners:</p> <ul> <li>See it: Visual diagrams and examples show you what workflows look like</li> <li>Understand it: Clear explanations of why workflow management matters</li> <li>Try it: Simple exercises to practice basic concepts</li> <li>Build it: Create your own working pipeline step by step</li> <li>Master it: Apply skills to real genomics problems with confidence</li> </ul> <p>Every section builds on the previous one, ensuring you develop solid foundations before moving to more complex topics.</p>"},{"location":"modules/day6/#overview","title":"Overview","text":"<p>Day 6 introduces participants to workflow management systems and Nextflow fundamentals. This comprehensive session covers the theoretical foundations of reproducible workflows, core Nextflow concepts, and hands-on development of basic pipelines. Participants will understand why workflow management is crucial for bioinformatics and gain practical experience with Nextflow's core components.</p>"},{"location":"modules/day6/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 6, you will be able to:</p> <ul> <li>Understand the challenges in bioinformatics reproducibility and benefits of workflow management systems</li> <li>Explain Nextflow's core features and architecture</li> <li>Identify the main components of a Nextflow script (processes, channels, workflows)</li> <li>Write and execute basic Nextflow processes and workflows</li> <li>Use channels to manage data flow between processes</li> <li>Configure Nextflow for different execution environments</li> <li>Debug common Nextflow issues and understand error messages</li> <li>Apply best practices for pipeline development</li> </ul>"},{"location":"modules/day6/#schedule","title":"Schedule","text":"Time (CAT) Topic Duration Trainer 09:00 Part 1: The Challenge of Complex Genomics Analyses 45 min Mamana Mbiyavanga 09:45 Workflow Management Systems Comparison &amp; Nextflow Introduction 45 min Mamana Mbiyavanga 10:30 Break 15 min 10:45 Part 2: Nextflow Architecture and Core Concepts 45 min Mamana Mbiyavanga 11:30 Part 3: Hands-on Exercises (Installation, First Scripts, Channels) 90 min Mamana Mbiyavanga 13:00 End"},{"location":"modules/day6/#key-topics","title":"Key Topics","text":""},{"location":"modules/day6/#1-foundation-review-30-minutes","title":"1. Foundation Review (30 minutes)","text":"<ul> <li>Command line proficiency check</li> <li>Basic software installation and environment setup</li> <li>Development workspace organization</li> </ul>"},{"location":"modules/day6/#2-introduction-to-workflow-management-45-minutes","title":"2. Introduction to Workflow Management (45 minutes)","text":"<ul> <li>The challenge of complex genomics analyses</li> <li>Problems with traditional scripting approaches</li> <li>Benefits of workflow management systems</li> <li>Nextflow vs other systems (Snakemake, CWL, WDL)</li> <li>Reproducibility, portability, and scalability</li> </ul>"},{"location":"modules/day6/#3-nextflow-core-concepts-75-minutes","title":"3. Nextflow Core Concepts (75 minutes)","text":"<ul> <li>Nextflow architecture and execution model</li> <li>Processes: encapsulated tasks with inputs, outputs, and scripts</li> <li>Channels: asynchronous data streams connecting processes</li> <li>Workflows: orchestrating process execution and data flow</li> <li>The work directory structure and caching mechanism</li> <li>Executors and execution platforms</li> </ul>"},{"location":"modules/day6/#4-hands-on-pipeline-development-75-minutes","title":"4. Hands-on Pipeline Development (75 minutes)","text":"<ul> <li>Writing your first Nextflow process</li> <li>Creating channels and managing data flow</li> <li>Building a simple QC workflow</li> <li>Testing and debugging pipelines</li> <li>Understanding the work directory</li> </ul>"},{"location":"modules/day6/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day6/#core-requirements","title":"Core Requirements","text":"<ul> <li>Nextflow (version 20.10.0 or later) - Workflow orchestration system</li> <li>Java (version 11 or later) - Required for Nextflow execution</li> <li>Text editor - VS Code with Nextflow extension recommended</li> <li>Command line access - Terminal or command prompt for running Nextflow commands</li> </ul>"},{"location":"modules/day6/#bioinformatics-tools","title":"Bioinformatics Tools","text":"<ul> <li>FastQC - Read quality control assessment</li> <li>MultiQC - Aggregate quality control reports</li> <li>Trimmomatic - Read trimming and filtering</li> <li>SPAdes - Genome assembly (for later exercises)</li> </ul>"},{"location":"modules/day6/#development-environment","title":"Development Environment","text":"<ul> <li>Terminal/Command line - For running Nextflow commands</li> <li>Text editor - For writing pipeline scripts</li> </ul>"},{"location":"modules/day6/#foundation-review-30-minutes","title":"Foundation Review (30 minutes)","text":"<p>Before diving into workflow management, let's ensure everyone has the essential foundation skills needed for this module.</p>"},{"location":"modules/day6/#command-line-proficiency-check","title":"Command Line Proficiency Check","text":"<p>Let's quickly verify your command line skills with some essential operations:</p> \ud83d\udd27 Quick Command Line Assessment  **Test your skills with these commands:**  <pre><code># Navigation and file operations\npwd                          # Where am I?\nls -la                      # List files with details\ncd /path/to/data           # Change directory\nmkdir analysis_results     # Create directory\ncp file1.txt backup/       # Copy files\nmv old_name.txt new_name.txt  # Rename/move files\n\n# File content examination\nhead -n 10 data.fastq      # First 10 lines\ntail -n 5 logfile.txt      # Last 5 lines\nwc -l sequences.fasta      # Count lines\ngrep \"&gt;\" sequences.fasta   # Find FASTA headers\n\n# Process management\nps aux                     # List running processes\ntop                        # Monitor system resources\nkill -9 [PID]             # Terminate process\nnohup command &amp;            # Run in background\n</code></pre>  **Expected competency:** You should be comfortable with basic file operations, text processing, and process management."},{"location":"modules/day6/#software-installation-overview","title":"Software Installation Overview","text":"<p>For Day 6, we'll focus on basic software installation and environment setup. Container technologies will be covered in Day 7 as part of advanced deployment strategies.</p>"},{"location":"modules/day6/#using-the-module-system","title":"Using the Module System","text":"\ud83d\udce6 Loading Required Software  **All tools are pre-installed and available through the module system. No installation required!**  **Step 1: Check if module system is available** <pre><code># Test if module command works\nmodule --version\n\n# If you get \"command not found\", see troubleshooting below\n</code></pre>  **Step 2: Check available modules** <pre><code># List all available modules\nmodule avail\n\n# Search for specific tools\nmodule avail nextflow\nmodule avail java\nmodule avail fastqc\n</code></pre>  **Step 3: Load required modules** <pre><code># Load Java 17 (required for Nextflow)\nmodule load java/openjdk-17.0.2\n\n# Load Nextflow\nmodule load nextflow/25.04.6\n\n# Load bioinformatics tools for exercises\nmodule load fastqc/0.12.1\nmodule load trimmomatic/0.39\nmodule load multiqc/1.22.3\n</code></pre>  **Step 4: Verify loaded modules** <pre><code># Check what modules are currently loaded\nmodule list\n\n# Test that tools are working\nnextflow -version\njava -version\nfastqc --version\n</code></pre>  **Step 5: Module management** <pre><code># Unload a specific module\nmodule unload fastqc/0.12.1\n\n# Unload all modules\nmodule purge\n\n# Create a convenient setup script\ncat &gt; setup_modules.sh &lt;&lt; 'EOF'\n#!/bin/bash\nmodule load java/openjdk-17.0.2 nextflow/25.04.6 fastqc/0.12.1 trimmomatic/0.39 multiqc/1.22.3\necho \"Modules loaded successfully!\"\nmodule list\nEOF\n\nchmod +x setup_modules.sh\n</code></pre>  **Troubleshooting: If module command is not found** <pre><code># Only if you get \"module: command not found\", try:\nsource /opt/lmod/8.7/lmod/lmod/init/bash\n\n# Then retry the module commands above\nmodule --version\n</code></pre>"},{"location":"modules/day6/#development-environment-setup","title":"Development Environment Setup","text":"<p>Let's ensure your environment is ready for Nextflow development:</p>"},{"location":"modules/day6/#module-environment-verification","title":"Module Environment Verification","text":"\u2705 Environment Verification  **Complete verification workflow:** <pre><code># Step 1: Test module system\nmodule --version\n# Should show: Modules based on Lua: Version 8.7\n\n# Step 2: Load all required modules with specific versions\nmodule load java/openjdk-17.0.2 nextflow/25.04.6 fastqc/0.12.1 trimmomatic/0.39 multiqc/1.22.3\n\n# Step 3: Verify Java (required for Nextflow)\njava -version\n# Should show: openjdk version \"17.0.2\"\n\n# Step 4: Verify Nextflow\nnextflow -version\n# Should show: nextflow version 25.04.6\n\n# Step 5: Verify bioinformatics tools\nfastqc --version\n# Should show: FastQC v0.12.1\n\ntrimmomatic -version\n# Should show: 0.39\n\nmultiqc --version\n# Should show: multiqc, version 1.22.3\n\n# Step 6: Check all loaded modules\nmodule list\n# Should show all 5 loaded modules\n</code></pre>  **If module command is not found:** <pre><code># Initialize module system (only if needed)\nsource /opt/lmod/8.7/lmod/lmod/init/bash\n\n# Then retry the verification steps above\nmodule --version\n</code></pre>  **If modules are not available:** <pre><code># Search for modules with different names\nmodule avail 2&gt;&amp;1 | grep -i nextflow\nmodule avail 2&gt;&amp;1 | grep -i java\n\n# Contact system administrator if modules are missing\n</code></pre>  **Quick Setup Script:** <pre><code># Create a one-command setup (handles module initialization if needed)\ncat &gt; ~/setup_day6.sh &lt;&lt; 'EOF'\n#!/bin/bash\n\n# Test if module command works\nif ! command -v module &gt;/dev/null 2&gt;&amp;1; then\n    echo \"Initializing module system...\"\n    source /opt/lmod/8.7/lmod/lmod/init/bash\nfi\n\n# Load required modules\nmodule load java/openjdk-17.0.2 nextflow/25.04.6 fastqc/0.12.1 trimmomatic/0.39 multiqc/1.22.3\necho \"All modules loaded successfully!\"\nmodule list\nEOF\n\nchmod +x ~/setup_day6.sh\n\n# Use it anytime with:\nsource ~/setup_day6.sh\n</code></pre>"},{"location":"modules/day6/#workspace-organization","title":"Workspace Organization","text":"<p>Create a well-organized workspace for today's exercises:</p> <pre><code># Create main working directory\nmkdir ~/nextflow-training\ncd ~/nextflow-training\n\n# Create subdirectories\nmkdir -p {scripts,results,configs}\n\n# Check available real data\nls -la /data/Dataset_Mt_Vc/\necho \"Real genomic data available in /data/Dataset_Mt_Vc/\"\n</code></pre> \ud83d\udca1 Pro Tip: Development Best Practices  **Recommended setup:**  - Use a dedicated directory for each project - Keep data, scripts, and results separate - Use meaningful file names and directory structure - Document your workflow with README files - Use version control (we'll cover this in Day 7!)"},{"location":"modules/day6/#part-1-the-challenge-of-complex-genomics-analyses","title":"Part 1: The Challenge of Complex Genomics Analyses","text":""},{"location":"modules/day6/#why-workflow-management-matters","title":"Why Workflow Management Matters","text":"<p>Consider analyzing 100 bacterial genomes without workflow management:</p> <pre><code># Manual approach - tedious and error-prone\nfor sample in sample1 sample2 sample3 ... sample100; do\n    fastqc ${sample}_R1.fastq ${sample}_R2.fastq\n    if [ $? -ne 0 ]; then echo \"FastQC failed\"; exit 1; fi\n\n    trimmomatic PE ${sample}_R1.fastq ${sample}_R2.fastq \\\n        ${sample}_R1_trimmed.fastq ${sample}_R1_unpaired.fastq \\\n        ${sample}_R2_trimmed.fastq ${sample}_R2_unpaired.fastq \\\n        SLIDINGWINDOW:4:20\n    if [ $? -ne 0 ]; then echo \"Trimming failed\"; exit 1; fi\n\n    spades.py -1 ${sample}_R1_trimmed.fastq -2 ${sample}_R2_trimmed.fastq \\\n        -o ${sample}_assembly\n    if [ $? -ne 0 ]; then echo \"Assembly failed\"; exit 1; fi\n\n    # What if step 3 fails for sample 67?\n    # How do you restart from where it failed?\n    # How do you run samples in parallel efficiently?\n    # How do you ensure reproducibility across different systems?\ndone\n</code></pre>"},{"location":"modules/day6/#why-this-approach-is-tedious-and-error-prone","title":"Why This Approach is \"Tedious and Error-Prone\"","text":"<p>Major Problems with Traditional Shell Scripting:</p> <ol> <li> <p>No Parallelization</p> <ul> <li>Processes samples sequentially (one after another)</li> <li>Wastes computational resources on multi-core systems</li> <li>Takes unnecessarily long time</li> </ul> </li> <li> <p>Poor Error Recovery &amp; Resumability</p> <ul> <li>If one sample fails, entire pipeline stops</li> <li>No way to resume from failure point</li> <li>Must restart from beginning</li> <li>Manual error checking is verbose and error-prone</li> </ul> </li> <li> <p>Resource Management Issues</p> <ul> <li>No control over CPU/memory usage</li> <li>Can overwhelm system or underutilize resources</li> <li>No queue management for HPC systems</li> <li>No automatic optimization of resource allocation</li> </ul> </li> <li> <p>Lack of Reproducibility</p> <ul> <li>Hard to track software versions</li> <li>Environment dependencies not managed</li> <li>Difficult to share and reproduce results across different systems</li> <li>Software installation and version conflicts</li> </ul> </li> <li> <p>Poor Scalability</p> <ul> <li>Doesn't scale well from laptop to HPC to cloud</li> <li>No automatic adaptation to different computing environments</li> <li>Limited ability to handle varying data volumes</li> </ul> </li> <li> <p>Maintenance Nightmare</p> <ul> <li>Adding new steps requires modifying the entire script</li> <li>Parameter changes need manual editing throughout</li> <li>No modular design for reusable components</li> <li>Difficult to test individual components</li> </ul> </li> <li> <p>No Progress Tracking</p> <ul> <li>Can't easily see which samples completed</li> <li>No reporting or logging mechanisms</li> <li>Difficult to debug failures</li> <li>No visibility into pipeline performance</li> </ul> </li> </ol>"},{"location":"modules/day6/#the-workflow-management-solution","title":"The Workflow Management Solution","text":""},{"location":"modules/day6/#overview-of-workflow-management-systems","title":"Overview of Workflow Management Systems","text":"<p>Workflow management systems (WMS) are specialized programming languages and frameworks designed specifically to address the challenges of complex, multi-step computational pipelines. They provide a higher-level abstraction that automatically handles the tedious and error-prone aspects of traditional shell scripting.</p>"},{"location":"modules/day6/#how-workflow-management-systems-solve-traditional-problems","title":"How Workflow Management Systems Solve Traditional Problems:","text":"<ul> <li> <p>Automatic Parallelization</p> <ul> <li>Analyze task dependencies and run independent steps simultaneously</li> <li>Efficiently utilize all available CPU cores and computing nodes</li> <li>Scale from single machines to massive HPC clusters and cloud environments</li> </ul> </li> <li> <p>Built-in Error Recovery</p> <ul> <li>Automatic retry mechanisms for failed tasks</li> <li>Resume functionality to restart from failure points</li> <li>Intelligent caching to avoid re-running successful steps</li> </ul> </li> <li> <p>Resource Management</p> <ul> <li>Automatic CPU and memory allocation based on task requirements</li> <li>Integration with job schedulers (SLURM, PBS, SGE)</li> <li>Dynamic scaling in cloud environments</li> </ul> </li> <li> <p>Reproducibility by Design</p> <ul> <li>Container integration (Docker, Singularity) for consistent environments</li> <li>Version tracking for all software dependencies</li> <li>Portable execution across different computing platforms</li> </ul> </li> <li> <p>Progress Monitoring</p> <ul> <li>Real-time pipeline execution tracking</li> <li>Detailed logging and reporting</li> <li>Performance metrics and resource usage statistics</li> </ul> </li> <li> <p>Modular Architecture</p> <ul> <li>Reusable workflow components</li> <li>Easy parameter configuration</li> <li>Clean separation of logic and execution</li> </ul> </li> </ul>"},{"location":"modules/day6/#comparison-of-popular-workflow-languages","title":"Comparison of Popular Workflow Languages","text":"<p>The bioinformatics community has developed several powerful workflow management systems, each with unique strengths and design philosophies:</p>"},{"location":"modules/day6/#1-nextflow","title":"1. Nextflow","text":"<ul> <li>Language Base: Groovy (JVM-based)</li> <li>Philosophy: Dataflow programming with reactive streams</li> <li>Strengths: Excellent parallelization, cloud-native, strong container support</li> <li>Community: Large bioinformatics community, nf-core ecosystem</li> </ul>"},{"location":"modules/day6/#2-snakemake","title":"2. Snakemake","text":"<ul> <li>Language Base: Python</li> <li>Philosophy: Rule-based workflow definition inspired by GNU Make</li> <li>Strengths: Pythonic syntax, excellent for Python developers, strong academic adoption</li> <li>Community: Very active in computational biology and data science</li> </ul>"},{"location":"modules/day6/#3-common-workflow-language-cwl","title":"3. Common Workflow Language (CWL)","text":"<ul> <li>Language Base: YAML/JSON</li> <li>Philosophy: Vendor-neutral, standards-based approach</li> <li>Strengths: Platform independence, strong metadata support, scientific reproducibility focus</li> <li>Community: Broad industry and academic support across multiple domains</li> </ul>"},{"location":"modules/day6/#4-workflow-description-language-wdl","title":"4. Workflow Description Language (WDL)","text":"<ul> <li>Language Base: Custom domain-specific language</li> <li>Philosophy: Human-readable workflow descriptions with strong typing</li> <li>Strengths: Excellent cloud integration, strong at Broad Institute and genomics centers</li> <li>Community: Strong in genomics, particularly for large-scale sequencing projects</li> </ul>"},{"location":"modules/day6/#feature-comparison-table","title":"Feature Comparison Table","text":"Feature Nextflow Snakemake CWL WDL Syntax Base Groovy Python YAML/JSON Custom DSL Learning Curve Moderate Easy (for Python users) Steep Moderate Parallelization Excellent (automatic) Excellent Good Excellent Container Support Native (Docker/Singularity) Native Native Native Cloud Integration Excellent (AWS, GCP, Azure) Good Good Excellent HPC Support Excellent (SLURM, PBS, etc.) Excellent Good Good Resume Capability Excellent Excellent Limited Good Community Size Large (bioinformatics) Large (data science) Medium Medium Package Ecosystem nf-core (500+ pipelines) Snakemake Wrappers Limited Limited Debugging Tools Good (Tower, reports) Excellent Limited Good Best Use Cases Multi-omics, clinical pipelines Data analysis, research Standards compliance Large-scale genomics Industry Adoption High (pharma, biotech) High (academia) Growing High (genomics centers)"},{"location":"modules/day6/#simple-code-examples","title":"Simple Code Examples","text":"<p>Let's see how the same basic task - running FastQC on multiple samples - would be implemented in different workflow languages:</p>"},{"location":"modules/day6/#traditional-shell-script-for-comparison","title":"Traditional Shell Script (for comparison)","text":"<pre><code># Manual approach - sequential processing\nfor sample in sample1 sample2 sample3; do\n    fastqc ${sample}_R1.fastq ${sample}_R2.fastq -o results/\n    if [ $? -ne 0 ]; then echo \"FastQC failed for $sample\"; exit 1; fi\ndone\n</code></pre>"},{"location":"modules/day6/#nextflow-implementation","title":"Nextflow Implementation","text":"<pre><code>#!/usr/bin/env nextflow\n\n// Define input channel\nChannel\n    .fromFilePairs(\"data/*_{R1,R2}.fastq\")\n    .set { read_pairs_ch }\n\n// FastQC process\nprocess fastqc {\n    container 'biocontainers/fastqc:v0.11.9'\n    publishDir 'results/', mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\"\n\n    script:\n    \"\"\"\n    fastqc ${reads} -t ${task.cpus}\n    \"\"\"\n}\n\n// Run the workflow\nworkflow {\n    fastqc(read_pairs_ch)\n}\n</code></pre>"},{"location":"modules/day6/#snakemake-implementation","title":"Snakemake Implementation","text":"<pre><code># Snakefile\nSAMPLES = [\"sample1\", \"sample2\", \"sample3\"]\n\nrule all:\n    input:\n        expand(\"results/{sample}_{read}_fastqc.html\",\n               sample=SAMPLES, read=[\"R1\", \"R2\"])\n\nrule fastqc:\n    input:\n        \"data/{sample}_{read}.fastq\"\n    output:\n        html=\"results/{sample}_{read}_fastqc.html\",\n        zip=\"results/{sample}_{read}_fastqc.zip\"\n    container:\n        \"docker://biocontainers/fastqc:v0.11.9\"\n    shell:\n        \"fastqc {input} -o results/\"\n</code></pre>"},{"location":"modules/day6/#cwl-implementation","title":"CWL Implementation","text":"<pre><code># fastqc-workflow.cwl\ncwlVersion: v1.2\nclass: Workflow\n\ninputs:\n  fastq_files:\n    type: File[]\n\noutputs:\n  fastqc_reports:\n    type: File[]\n    outputSource: fastqc/html_report\n\nsteps:\n  fastqc:\n    run: fastqc-tool.cwl\n    scatter: fastq_file\n    in:\n      fastq_file: fastq_files\n    out: [html_report, zip_report]\n\n# fastqc-tool.cwl\ncwlVersion: v1.2\nclass: CommandLineTool\n\nbaseCommand: fastqc\n\ninputs:\n  fastq_file:\n    type: File\n    inputBinding:\n      position: 1\n\noutputs:\n  html_report:\n    type: File\n    outputBinding:\n      glob: \"*_fastqc.html\"\n  zip_report:\n    type: File\n    outputBinding:\n      glob: \"*_fastqc.zip\"\n\nrequirements:\n  DockerRequirement:\n    dockerPull: biocontainers/fastqc:v0.11.9\n</code></pre>"},{"location":"modules/day6/#key-differences-in-syntax","title":"Key Differences in Syntax:","text":"<ul> <li>Nextflow: Uses Groovy syntax with channels for data flow, processes define computational steps</li> <li>Snakemake: Python-based with rules that define input/output relationships, uses wildcards for pattern matching</li> <li>CWL: YAML-based with explicit input/output definitions, requires separate tool and workflow files</li> <li>WDL: Custom syntax with strong typing, task-based approach with explicit variable declarations</li> </ul>"},{"location":"modules/day6/#why-nextflow-for-this-course","title":"Why Nextflow for This Course","text":"<p>This course focuses on Nextflow for several compelling reasons that make it particularly well-suited for microbial genomics workflows:</p>"},{"location":"modules/day6/#1-bioinformatics-community-adoption","title":"1. Bioinformatics Community Adoption","text":"<ul> <li>nf-core ecosystem: Over 500 community-curated pipelines specifically for bioinformatics</li> <li>Industry standard: Widely adopted by pharmaceutical companies, biotech firms, and genomics centers</li> <li>Active development: Strong community support with regular updates and improvements</li> </ul>"},{"location":"modules/day6/#2-excellent-parallelization-for-genomics","title":"2. Excellent Parallelization for Genomics","text":"<ul> <li>Automatic scaling: Seamlessly scales from single samples to thousands of genomes</li> <li>Dataflow programming: Natural fit for genomics pipelines with complex dependencies</li> <li>Resource optimization: Intelligent task scheduling maximizes computational efficiency</li> </ul>"},{"location":"modules/day6/#3-clinical-and-production-ready","title":"3. Clinical and Production Ready","text":"<ul> <li>Robust error handling: Critical for clinical pipelines where reliability is essential</li> <li>Comprehensive logging: Detailed audit trails required for regulatory compliance</li> <li>Resume capability: Minimizes computational waste in long-running genomic analyses</li> </ul>"},{"location":"modules/day6/#4-multi-platform-flexibility","title":"4. Multi-Platform Flexibility","text":"<ul> <li>HPC integration: Native support for SLURM, PBS, and other job schedulers common in genomics</li> <li>Cloud-native: Excellent support for AWS, Google Cloud, and Azure for scalable genomics</li> <li>Container support: Seamless Docker and Singularity integration for reproducible environments</li> </ul>"},{"location":"modules/day6/#5-microbial-genomics-specific-advantages","title":"5. Microbial Genomics Specific Advantages","text":"<ul> <li>Pathogen surveillance pipelines: Many nf-core pipelines designed for bacterial genomics</li> <li>AMR analysis workflows: Established patterns for antimicrobial resistance detection</li> <li>Outbreak investigation: Scalable phylogenetic analysis capabilities</li> <li>Metagenomics support: Robust handling of complex metagenomic datasets</li> </ul>"},{"location":"modules/day6/#6-learning-and-career-benefits","title":"6. Learning and Career Benefits","text":"<ul> <li>Industry relevance: Skills directly transferable to genomics industry positions</li> <li>Growing demand: Increasing adoption means more job opportunities</li> <li>Comprehensive ecosystem: Learning Nextflow provides access to hundreds of ready-to-use pipelines</li> </ul> <p>The combination of these factors makes Nextflow an ideal choice for training the next generation of microbial genomics researchers and practitioners. Its balance of power, usability, and industry adoption ensures that skills learned in this course will be immediately applicable in real-world genomics applications.</p>"},{"location":"modules/day6/#visual-guide-understanding-workflow-management","title":"Visual Guide: Understanding Workflow Management","text":""},{"location":"modules/day6/#the-big-picture-traditional-vs-modern-approaches","title":"The Big Picture: Traditional vs Modern Approaches","text":"<p>To understand why workflow management systems like Nextflow are revolutionary, let's visualize the time difference:</p>"},{"location":"modules/day6/#traditional-shell-scripting-the-slow-way","title":"Traditional Shell Scripting - The Slow Way","text":"<pre><code>graph TD\n    A1[Sample 1] --&gt; B1[FastQC - 5 min]\n    B1 --&gt; C1[Trimming - 10 min]\n    C1 --&gt; D1[Assembly - 30 min]\n    D1 --&gt; E1[Annotation - 15 min]\n    E1 --&gt; F1[\u2713 Done - 60 min total]\n\n    F1 --&gt; A2[Sample 2]\n    A2 --&gt; B2[FastQC - 5 min]\n    B2 --&gt; C2[Trimming - 10 min]\n    C2 --&gt; D2[Assembly - 30 min]\n    D2 --&gt; E2[Annotation - 15 min]\n    E2 --&gt; F2[\u2713 Done - 120 min total]\n\n    F2 --&gt; A3[Sample 3]\n    A3 --&gt; B3[FastQC - 5 min]\n    B3 --&gt; C3[Trimming - 10 min]\n    C3 --&gt; D3[Assembly - 30 min]\n    D3 --&gt; E3[Annotation - 15 min]\n    E3 --&gt; F3[\u2713 All Done - 180 min total]\n\n    style A1 fill:#ffcccc\n    style A2 fill:#ffcccc\n    style A3 fill:#ffcccc\n    style F3 fill:#ff9999</code></pre> <p>Problems with traditional approach:</p> <ul> <li>Sequential processing: Must wait for each sample to finish completely</li> <li>Wasted resources: Only uses one CPU core at a time</li> <li>Total time: 180 minutes (3 hours) for 3 samples</li> <li>Scaling nightmare: 100 samples = 100 hours!</li> </ul>"},{"location":"modules/day6/#nextflow-the-fast-way","title":"Nextflow - The Fast Way","text":"<pre><code>graph TD\n    A4[Sample 1] --&gt; B4[FastQC - 5 min]\n    A5[Sample 2] --&gt; B5[FastQC - 5 min]\n    A6[Sample 3] --&gt; B6[FastQC - 5 min]\n\n    B4 --&gt; C4[Trimming - 10 min]\n    B5 --&gt; C5[Trimming - 10 min]\n    B6 --&gt; C6[Trimming - 10 min]\n\n    C4 --&gt; D4[Assembly - 30 min]\n    C5 --&gt; D5[Assembly - 30 min]\n    C6 --&gt; D6[Assembly - 30 min]\n\n    D4 --&gt; E4[Annotation - 15 min]\n    D5 --&gt; E5[Annotation - 15 min]\n    D6 --&gt; E6[Annotation - 15 min]\n\n    E4 --&gt; F4[\u2713 All Done - 60 min total]\n    E5 --&gt; F5[3x FASTER!]\n    E6 --&gt; F6[Same time as 1 sample]\n\n    style A4 fill:#ccffcc\n    style A5 fill:#ccffcc\n    style A6 fill:#ccffcc\n    style F4 fill:#99ff99\n    style F5 fill:#99ff99\n    style F6 fill:#99ff99</code></pre> <p>Benefits of Nextflow approach: - Parallel processing: All samples start simultaneously - Efficient resource use: Uses all available CPU cores - Total time: 60 minutes (1 hour) for 3 samples - Amazing scaling: 100 samples still = ~1 hour!</p>"},{"location":"modules/day6/#the-dramatic-difference","title":"The Dramatic Difference\ud83e\uddee Interactive Time Calculator","text":"Approach 3 Samples 10 Samples 100 Samples Traditional 3 hours 10 hours 100 hours Nextflow 1 hour 1 hour 1 hour Speed Gain 3x faster 10x faster 100x faster <p>Real-world impact: The more samples you have, the more dramatic the time savings become!</p> <p>See how much time Nextflow can save you with your own data:</p> Number of samples: 10 Time per sample (minutes): 60 \ud83d\udc0c Traditional Approach <p>Total time: 10 hours</p> <p>Sequential processing</p> \u26a1 Nextflow Approach <p>Total time: 1 hour</p> <p>Parallel processing</p>          Time saved: 9 hours (10x faster)"},{"location":"modules/day6/#nextflow-fundamentals","title":"Nextflow Fundamentals","text":"<p>Before diving into practical exercises, let's understand the core concepts that make Nextflow powerful.</p>"},{"location":"modules/day6/#what-is-nextflow","title":"What is Nextflow?","text":"<p>Nextflow is a workflow management system that comprises both a runtime environment and a domain-specific language (DSL). It's designed specifically to manage computational data-analysis workflows in bioinformatics and other scientific fields.</p>"},{"location":"modules/day6/#core-nextflow-features","title":"Core Nextflow Features","text":"<pre><code>graph LR\n    A[Fast Prototyping] --&gt; B[Simple Syntax]\n    C[Reproducibility] --&gt; D[Containers &amp; Conda]\n    E[Portability] --&gt; F[Run Anywhere]\n    G[Parallelism] --&gt; H[Automatic Scaling]\n    I[Checkpoints] --&gt; J[Resume from Failures]\n\n    style A fill:#e1f5fe\n    style C fill:#e8f5e8\n    style E fill:#fff3e0\n    style G fill:#f3e5f5\n    style I fill:#fce4ec</code></pre> <p>1. Fast Prototyping - Simple syntax that lets you reuse existing scripts and tools - Quick to write and test new workflows</p> <p>2. Reproducibility - Built-in support for Docker, Singularity, and Conda - Consistent execution environments across platforms - Same results every time, on any platform</p> <p>3. Portability &amp; Interoperability - Write once, run anywhere (laptop, HPC cluster, cloud) - Separates workflow logic from execution environment</p> <p>4. Simple Parallelism - Based on dataflow programming model - Automatically runs independent tasks in parallel</p> <p>5. Continuous Checkpoints - Tracks all intermediate results automatically - Resume from the last successful step if something fails</p>"},{"location":"modules/day6/#the-three-building-blocks","title":"The Three Building Blocks","text":"<p>Every Nextflow workflow has three main components:</p>"},{"location":"modules/day6/#1-processes-what-to-do","title":"1. Processes - What to do","text":"<pre><code>process FASTQC {\n    input:\n    path reads\n\n    output:\n    path \"*_fastqc.html\"\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day6/#2-channels-how-data-flows","title":"2. Channels - How data flows","text":"<pre><code>// Create a channel from files\nChannel\n    .fromPath(\"data/*.fastq\")\n    .set { reads_ch }\n</code></pre>"},{"location":"modules/day6/#3-workflows-how-it-all-connects","title":"3. Workflows - How it all connects","text":"<pre><code>workflow {\n    FASTQC(reads_ch)\n}\n</code></pre>"},{"location":"modules/day6/#understanding-processes-channels-and-workflows","title":"Understanding Processes, Channels, and Workflows","text":""},{"location":"modules/day6/#processes-in-detail","title":"Processes in Detail","text":"<p>A process describes a task to be run. Think of it as a recipe that tells Nextflow: - What inputs it needs - What outputs it produces - What commands to run</p> <pre><code>process COUNT_READS {\n    // Process directives (optional)\n    tag \"$sample_id\"           // Label for this task\n    publishDir \"results/\"      // Where to save outputs\n\n    input:\n    tuple val(sample_id), path(reads)  // What this process needs\n\n    output:\n    path \"${sample_id}.count\"          // What this process creates\n\n    script:\n    \"\"\"\n    echo \"Counting reads in ${sample_id}\"\n    wc -l ${reads} &gt; ${sample_id}.count\n    \"\"\"\n}\n</code></pre> <p>Key Points:</p> <ul> <li>Each process runs independently (cannot talk to other processes)</li> <li>If you have 3 input files, Nextflow automatically creates 3 separate tasks</li> <li>Tasks can run in parallel if resources are available</li> </ul>"},{"location":"modules/day6/#channels-in-detail","title":"Channels in Detail","text":"<p>Channels are like conveyor belts that move data between processes. They're asynchronous queues that connect processes together.</p> <pre><code>// Different ways to create channels\n\n// From files matching a pattern\nChannel.fromPath(\"data/*.fastq\")\n\n// From pairs of files (R1/R2)\nChannel.fromFilePairs(\"data/*_{R1,R2}.fastq\")\n\n// From a list of values\nChannel.from(['sample1', 'sample2', 'sample3'])\n\n// From a CSV file\nChannel.fromPath(\"samples.csv\")\n    .splitCsv(header: true)\n</code></pre> <p>Channel Flow Example:</p> <pre><code>graph LR\n    A[Input Files] --&gt; B[Channel]\n    B --&gt; C[Process 1]\n    C --&gt; D[Output Channel]\n    D --&gt; E[Process 2]\n    E --&gt; F[Final Results]\n\n    style B fill:#e1f5fe\n    style D fill:#e1f5fe</code></pre>"},{"location":"modules/day6/#workflows-in-detail","title":"Workflows in Detail","text":"<p>The workflow section defines how processes connect together. It's like the assembly line instructions.</p> <pre><code>workflow {\n    // Create input channel\n    reads_ch = Channel.fromPath(\"data/*.fastq\")\n\n    // Run processes in order\n    FASTQC(reads_ch)\n    COUNT_READS(reads_ch)\n\n    // Use output from one process as input to another\n    TRIMMING(reads_ch)\n    ASSEMBLY(TRIMMING.out)\n}\n</code></pre>"},{"location":"modules/day6/#how-nextflow-executes-your-workflow","title":"How Nextflow Executes Your Workflow","text":"<p>When you run a Nextflow script, here's what happens:</p> <ol> <li>Parse the script: Nextflow reads your workflow definition</li> <li>Create the execution graph: Figures out which processes depend on which</li> <li>Submit tasks: Sends individual tasks to the executor (local computer, cluster, cloud)</li> <li>Monitor progress: Tracks which tasks complete successfully</li> <li>Handle failures: Retries failed tasks or stops gracefully</li> <li>Collect results: Gathers outputs in the specified locations</li> </ol> <pre><code>graph TD\n    A[Nextflow Script] --&gt; B[Parse &amp; Plan]\n    B --&gt; C[Submit Tasks]\n    C --&gt; D[Monitor Execution]\n    D --&gt; E{All Tasks Done?}\n    E --&gt;|No| F[Handle Failures]\n    F --&gt; C\n    E --&gt;|Yes| G[Collect Results]\n\n    style A fill:#e1f5fe\n    style G fill:#c8e6c9</code></pre>"},{"location":"modules/day6/#your-first-nextflow-script","title":"Your First Nextflow Script","text":"<p>Let's look at a complete, simple example that counts lines in a file:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Parameters (can be changed when running)\nparams.input = \"data/sample.fastq\"\n\n// Create input channel\ninput_ch = Channel.fromPath(params.input)\n\n// Main workflow\nworkflow {\n    NUM_LINES(input_ch)\n    NUM_LINES.out.view()  // Print results to screen\n}\n\n// Process definition\nprocess NUM_LINES {\n    input:\n    path read\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo \"Processing: ${read}\"\n    wc -l ${read}\n    \"\"\"\n}\n</code></pre> <p>Run the Nextflow script: </p><pre><code>nextflow run count_lines.nf\n</code></pre><p></p> Expected output <pre><code>N E X T F L O W  ~  version 23.10.0\nLaunching `count_lines.nf` [amazing_euler] - revision: a1b2c3d4\nexecutor &gt;  local (1)\n[a1/b2c3d4] process &gt; NUM_LINES (1) [100%] 1 of 1 \u2714\nProcessing: sample.fastq\n1000\n</code></pre> <p>What this output means: - Line 1: Nextflow version information - Line 2: Script name and unique run identifier - Line 3: Executor type (local computer) - Line 4: Process execution status with unique task ID - Line 5-6: Your script's actual output</p>"},{"location":"modules/day6/#workflow-execution-and-executors","title":"Workflow Execution and Executors","text":"<p>One of Nextflow's most powerful features is that it separates what your workflow does from where it runs.</p>"},{"location":"modules/day6/#executors-where-your-workflow-runs","title":"Executors: Where Your Workflow Runs","text":"<pre><code>graph TD\n    A[Your Nextflow Script] --&gt; B{Choose Executor}\n    B --&gt; C[Local Computer]\n    B --&gt; D[SLURM Cluster]\n    B --&gt; E[AWS Cloud]\n    B --&gt; F[Google Cloud]\n    B --&gt; G[Azure Cloud]\n\n    C --&gt; H[Same Workflow Code]\n    D --&gt; H\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    style A fill:#e1f5fe\n    style H fill:#c8e6c9</code></pre> <p>Available Executors:</p> <ul> <li>Local: Your laptop/desktop (default, great for testing)</li> <li>SLURM: High-performance computing clusters</li> <li>PBS/Torque: Another type of cluster scheduler</li> <li>AWS Batch: Amazon cloud computing</li> <li>Google Cloud: Google's cloud platform</li> <li>Kubernetes: Container orchestration platform</li> </ul>"},{"location":"modules/day6/#how-to-choose-execution-platform","title":"How to Choose Execution Platform","text":"<p>You don't change your workflow code! Instead, you use configuration:</p> <p>For local execution (default): </p><pre><code>nextflow run my_pipeline.nf\n</code></pre><p></p> <p>For SLURM cluster: </p><pre><code>nextflow run my_pipeline.nf -profile slurm\n</code></pre><p></p> <p>For AWS cloud: </p><pre><code>nextflow run my_pipeline.nf -profile aws\n</code></pre><p></p>"},{"location":"modules/day6/#resource-management","title":"Resource Management","text":"<p>Nextflow automatically handles:</p> <ul> <li>CPU allocation: How many cores each task gets</li> <li>Memory management: How much RAM each task needs</li> <li>Queue submission: Sending jobs to cluster schedulers</li> <li>Error handling: Retrying failed tasks</li> <li>File staging: Moving data between storage systems</li> </ul>"},{"location":"modules/day6/#quick-recap-key-concepts","title":"Quick Recap: Key Concepts","text":"<p>Before we start coding, let's make sure you understand these essential concepts:</p> Workflow Management System (WfMS) A computational platform for setting up, executing, and monitoring workflows Process A task definition that specifies inputs, outputs, and commands to run Channel An asynchronous queue that passes data between processes Workflow The section that defines how processes connect together Executor The system that actually runs your tasks (local, cluster, cloud) Task A single instance of a process running with specific input data Parallelization Running multiple tasks simultaneously to save time"},{"location":"modules/day6/#understanding-nextflow-output-organization","title":"Understanding Nextflow Output Organization","text":"<p>Before diving into exercises, it's essential to understand how Nextflow organizes its outputs. This knowledge will help you navigate results and debug issues effectively.</p>"},{"location":"modules/day6/#nextflow-directory-structure","title":"Nextflow Directory Structure\ud83d\udcc1 Interactive Folder Explorer","text":"<p>When you run a Nextflow pipeline, several directories are automatically created:</p> <pre><code>graph TD\n    A[Your Project Directory] --&gt; B[work/]\n    A --&gt; C[results/]\n    A --&gt; D[.nextflow/]\n    A --&gt; E[.nextflow.log]\n    A --&gt; F[timeline.html]\n    A --&gt; G[report.html]\n\n    B --&gt; H[Task Directories]\n    H --&gt; I[a1/b2c3d4.../]\n    I --&gt; J[.command.sh]\n    I --&gt; K[.command.log]\n    I --&gt; L[.command.err]\n    I --&gt; M[Input Files]\n    I --&gt; N[Output Files]\n\n    C --&gt; O[Published Results]\n    C --&gt; P[fastqc/]\n    C --&gt; Q[assembly/]\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#e8f5e8\n    style D fill:#f3e5f5</code></pre> <p>Click on folders to explore Nextflow's directory structure:</p>              \ud83d\udcc1 nextflow-training/ (your project directory)                  \ud83d\udcc1 work/ (temporary task files)                      \ud83d\udcc1 a1/b2c3d4e5f6.../ (individual task directory) \ud83d\udcc4 .command.sh (the actual command run) \ud83d\udcc4 .command.log (stdout from command) \ud83d\udcc4 .command.err (stderr from command) \ud83d\udcc4 .command.out (captured output) \ud83d\udcc4 .exitcode (exit status) \ud83d\udcc4 sample1_R1.fastq (input files - symlinks) \ud83d\udcc4 sample1_fastqc.html (output files)                  \ud83d\udcc1 results/ (published outputs) \ud83d\udcc1 fastqc/ (quality control reports) \ud83d\udcc1 assembly/ (genome assemblies) \ud83d\udcc1 annotation/ (gene annotations)                  \ud83d\udcc1 .nextflow/ (Nextflow cache and metadata) \ud83d\udcc1 cache/ (pipeline cache) \ud83d\udcc1 history (run history) \ud83d\udcc4 pid (process ID file) \ud83d\udcc4 .nextflow.log (main log file) \ud83d\udcc4 timeline.html (execution timeline) \ud83d\udcc4 report.html (execution report) \ud83d\udcc4 hello.nf (your pipeline script)"},{"location":"modules/day6/#practical-navigation-commands","title":"Practical Navigation Commands","text":"<p>Here are essential commands for exploring Nextflow outputs:</p> <p>Check overall structure: </p><pre><code>tree -L 2\n</code></pre><p></p> Expected output <pre><code>.\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 sample1_R1.fastq\n\u2502   \u2514\u2500\u2500 sample1_R2.fastq\n\u251c\u2500\u2500 hello.nf\n\u251c\u2500\u2500 results/\n\u2502   \u2514\u2500\u2500 fastqc/\n\u251c\u2500\u2500 work/\n\u2502   \u251c\u2500\u2500 a1/\n\u2502   \u251c\u2500\u2500 b2/\n\u2502   \u2514\u2500\u2500 c3/\n\u251c\u2500\u2500 .nextflow/\n\u251c\u2500\u2500 .nextflow.log\n\u2514\u2500\u2500 timeline.html\n</code></pre> <p>Find the most recent task directory: </p><pre><code>find work/ -name \"*.exitcode\" -exec dirname {} \\; | head -1\n</code></pre><p></p> <p>Check task execution details: </p><pre><code># Navigate to a task directory (use actual path from above)\ncd work/a1/b2c3d4e5f6...\n\n# See what command was run\ncat .command.sh\n\n# Check if it succeeded\ncat .exitcode  # 0 = success, non-zero = error\n\n# View any error messages\ncat .command.err\n</code></pre><p></p> <p>Monitor pipeline progress: </p><pre><code># Watch log in real-time\ntail -f .nextflow.log\n\n# Check execution summary\nnextflow log\n</code></pre><p></p> Example nextflow log output <pre><code>TIMESTAMP            DURATION  RUN NAME         STATUS   REVISION ID  SESSION ID                            COMMAND\n2024-01-15 10:30:15  2m 15s    clever_volta     OK       a1b2c3d4     12345678-1234-1234-1234-123456789012  nextflow run hello.nf\n2024-01-15 10:25:30  45s       sad_einstein     ERR      e5f6g7h8     87654321-4321-4321-4321-210987654321  nextflow run broken.nf\n</code></pre>"},{"location":"modules/day6/#understanding-publishdir-vs-work-directory","title":"Understanding publishDir vs work Directory","text":"<p>One of the most important concepts for beginners is understanding the difference between the <code>work/</code> directory and your results:</p> \ud83d\udd27 work/ Directory <ul> <li>Temporary - Can be deleted</li> <li>Messy - Mixed with logs and metadata</li> <li>Hash-named - Hard to navigate</li> <li>For debugging - When things go wrong</li> </ul> Use for: Debugging failed tasks      \ud83d\udcca results/ Directory <ul> <li>Permanent - Your final outputs</li> <li>Clean - Only important files</li> <li>Organized - Logical folder structure</li> <li>For sharing - With collaborators</li> </ul> Use for: Your actual research results"},{"location":"modules/day6/#common-directory-issues-and-solutions","title":"Common Directory Issues and Solutions\ud83d\udcbb Interactive Command Simulator","text":"<p>Problem: \"I can't find my results!\" </p><pre><code># Check if publishDir was used in your process\ngrep -n \"publishDir\" *.nf\n\n# Look in the work directory\nfind work/ -name \"*.html\" -o -name \"*.txt\" -o -name \"*.fasta\"\n</code></pre><p></p> <p>Problem: \"Pipeline failed, how do I debug?\" </p><pre><code># Find failed tasks\ngrep \"FAILED\" .nextflow.log\n\n# Get the work directory of failed task\ngrep -A 5 \"FAILED\" .nextflow.log | grep \"work/\"\n\n# Navigate to that directory and investigate\ncd work/xx/yyyy...\ncat .command.err\n</code></pre><p></p> <p>Problem: \"work/ directory is huge!\" </p><pre><code># Check work directory size\ndu -sh work/\n\n# Clean up after successful completion\nrm -rf work/\n\n# Or use Nextflow's clean command\nnextflow clean -f\n</code></pre><p></p> <p>Now that you understand these fundamentals, let's put them into practice!</p> <p>Practice Nextflow commands in this simulated terminal:</p> user@training:~/nextflow-training$  Welcome to the Nextflow command simulator! Try typing: nextflow -version Available commands: nextflow -version, nextflow run hello.nf, ls, pwd, mkdir, cat      <pre><code>\n</code></pre>"},{"location":"modules/day6/#your-first-genomics-pipeline","title":"Your First Genomics Pipeline","text":"<p>Here's what a basic microbial genomics analysis looks like:</p> <pre><code>flowchart LR\n    A[Raw Sequencing Data&lt;br/&gt;FASTQ files] --&gt; B[Quality Control&lt;br/&gt;FastQC]\n    B --&gt; C[Read Trimming&lt;br/&gt;Trimmomatic]\n    C --&gt; D[Genome Assembly&lt;br/&gt;SPAdes]\n    D --&gt; E[Assembly Quality&lt;br/&gt;QUAST]\n    E --&gt; F[Gene Annotation&lt;br/&gt;Prokka]\n    F --&gt; G[Final Results&lt;br/&gt;Annotated Genome]\n\n    B --&gt; H[Quality Report]\n    E --&gt; I[Assembly Stats]\n    F --&gt; J[Gene Predictions]\n\n    style A fill:#e1f5fe\n    style G fill:#c8e6c9\n    style H fill:#fff3e0\n    style I fill:#fff3e0\n    style J fill:#fff3e0</code></pre> <p>What Each Step Does: 1. Quality Control: Check if your sequencing data is good quality 2. Read Trimming: Remove low-quality parts of sequences 3. Genome Assembly: Put the pieces together to reconstruct the genome 4. Assembly Quality: Check how good your assembly is 5. Gene Annotation: Find and label genes in the genome</p>"},{"location":"modules/day6/#beginner-friendly-practical-exercises","title":"Beginner-Friendly Practical Exercises","text":""},{"location":"modules/day6/#exercise-1-your-first-nextflow-script-15-minutes","title":"Exercise 1: Your First Nextflow Script (15 minutes)","text":"<p>Let's start with the simplest possible Nextflow script to build confidence:</p> <p>Step 1: Create a \"Hello World\" pipeline</p> <pre><code>#!/usr/bin/env nextflow\n\n// This is your first Nextflow script!\n// It just prints a message for each sample\n\n// Define your samples (start with just 3)\nparams.samples = ['sample1', 'sample2', 'sample3']\n\n// Create a channel (think of it as a conveyor belt for data)\nChannel\n    .from(params.samples)\n    .set { samples_ch }\n\n// Define a process (a step in your pipeline)\nprocess sayHello {\n    // What this process does\n    input:\n    val sample_name from samples_ch\n\n    // What it produces\n    output:\n    stdout into results_ch\n\n    // The actual command\n    script:\n    \"\"\"\n    echo \"Hello from ${sample_name}!\"\n    \"\"\"\n}\n\n// Show the results\nresults_ch.view()\n</code></pre> <p>Step 2: Save and run the script</p> <p>First, save the script to a file: </p><pre><code># Create the file\nnano hello.nf\n# Copy-paste the script above, then save and exit (Ctrl+X, Y, Enter)\n</code></pre><p></p> <p>Now run your first Nextflow pipeline: </p><pre><code>nextflow run hello.nf\n</code></pre><p></p> Expected output <pre><code>N E X T F L O W  ~  version 23.10.0\nLaunching `hello.nf` [nostalgic_pasteur] - revision: 1a2b3c4d\nexecutor &gt;  local (3)\n[a1/b2c3d4] process &gt; sayHello (3) [100%] 3 of 3 \u2714\nHello from sample1!\nHello from sample2!\nHello from sample3!\n</code></pre> <p>What this means: - Nextflow automatically created 3 parallel tasks (one for each sample) - All 3 tasks completed successfully (3 of 3 \u2714) - The output shows messages from all samples</p> <p>Key Learning Points:</p> <ul> <li>Channels: Move data between processes (like a conveyor belt)</li> <li>Processes: Define what to do with each piece of data</li> <li>Parallelization: All samples run at the same time automatically!</li> </ul>"},{"location":"modules/day6/#exercise-2-adding-real-bioinformatics-30-minutes","title":"Exercise 2: Adding Real Bioinformatics (30 minutes)","text":"<p>Now let's do something useful - count reads in FASTQ files:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Parameters you can change\nparams.input = \"samplesheet.csv\"\nparams.outdir = \"results\"\n\n// Read sample sheet and create channel\nChannel\n    .fromPath(params.input)\n    .splitCsv(header: true)\n    .map { row -&gt;\n        def sample = row.sample\n        def fastq1 = file(row.fastq_1)\n        def fastq2 = file(row.fastq_2)\n        return [sample, fastq1, fastq2]\n    }\n    .set { samples_ch }\n\n// Process to count reads in paired FASTQ files\nprocess countReads {\n    // Where to save results\n    publishDir params.outdir, mode: 'copy'\n\n    // Use sample name for process identification\n    tag \"$sample\"\n\n    input:\n    tuple val(sample), path(fastq1), path(fastq2) from samples_ch\n\n    output:\n    path \"${sample}.count\" into counts_ch\n\n    script:\n    \"\"\"\n    echo \"Counting reads in sample: ${sample}\"\n    echo \"Forward reads (${fastq1}):\"\n\n    # Count reads in both files (compressed FASTQ)\n    reads1=\\$(zcat ${fastq1} | wc -l | awk '{print \\$1/4}')\n    reads2=\\$(zcat ${fastq2} | wc -l | awk '{print \\$1/4}')\n\n    echo \"Sample: ${sample}\" &gt; ${sample}.count\n    echo \"Forward reads: \\$reads1\" &gt;&gt; ${sample}.count\n    echo \"Reverse reads: \\$reads2\" &gt;&gt; ${sample}.count\n    echo \"Total read pairs: \\$reads1\" &gt;&gt; ${sample}.count\n\n    echo \"Finished counting ${sample}: \\$reads1 read pairs\"\n    \"\"\"\n}\n\n// Show results\ncounts_ch.view { \"Read count file: $it\" }\n</code></pre> <p>Step 1: Explore the available data </p><pre><code># Check the real genomic data available\nls -la /data/Dataset_Mt_Vc/\n\n# Look at TB (Mycobacterium tuberculosis) data\nls -la /data/Dataset_Mt_Vc/tb/raw_data/ | head -5\n\n# Look at VC (Vibrio cholerae) data\nls -la /data/Dataset_Mt_Vc/vc/raw_data/ | head -5\n\n# Create a workspace for our analysis\nmkdir -p ~/nextflow_workspace/data\ncd ~/nextflow_workspace\n</code></pre><p></p> <p>Real Data Available</p> <p>We have access to real genomic datasets:</p> <ul> <li>TB data: <code>/data/Dataset_Mt_Vc/tb/raw_data/</code> - 40 paired-end FASTQ files</li> <li>VC data: <code>/data/Dataset_Mt_Vc/vc/raw_data/</code> - 40 paired-end FASTQ files</li> </ul> <p>These are real sequencing data from Mycobacterium tuberculosis and Vibrio cholerae samples!</p> <p>Step 2: Create a sample sheet with real data </p><pre><code># Create a sample sheet with a few TB samples\ncat &gt; samplesheet.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nERR036221,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nERR036223,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_2.fastq.gz\nEOF\n\n# Check the sample sheet\ncat samplesheet.csv\n</code></pre><p></p> <p>Step 3: Update the script to use real data </p><pre><code># Save the script as count_reads.nf\nnano count_reads.nf\n# Copy-paste the script above, then save and exit\n</code></pre><p></p> <p>Step 4: Run the pipeline with real data </p><pre><code>nextflow run count_reads.nf --input samplesheet.csv\n</code></pre><p></p> Expected output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `count_reads.nf` [clever_volta] - revision: 5e6f7g8h\nexecutor &gt;  local (2)\n[c1/d2e3f4] process &gt; countReads (ERR036221) [100%] 2 of 2 \u2714\nRead count file: /path/to/results/ERR036221.count\nRead count file: /path/to/results/ERR036223.count\n</code></pre> <p>Step 5: Check your results </p><pre><code># Look at the results directory\nls results/\n\n# Check the read counts for real TB data\ncat results/ERR036221.count\ncat results/ERR036223.count\n\n# Compare file sizes\nls -lh /data/Dataset_Mt_Vc/tb/raw_data/ERR036221_*.fastq.gz\n</code></pre><p></p> Expected output <pre><code># ls results/\nsample1.count  sample2.count\n\n# cat results/sample1.count\n2\n\n# cat results/sample2.count\n3\n</code></pre> <p>What this pipeline does: 1. Reads sample information from a CSV file 2. Counts reads in paired FASTQ files (in parallel!) 3. Saves results to the <code>results/</code> directory 4. Each <code>.count</code> file contains detailed read statistics for that sample</p>"},{"location":"modules/day6/#exercise-2b-real-world-scenarios-30-minutes","title":"Exercise 2B: Real-World Scenarios (30 minutes)","text":"<p>Now let's explore common real-world scenarios you'll encounter when using Nextflow:</p>"},{"location":"modules/day6/#scenario-1-adding-more-samples","title":"Scenario 1: Adding More Samples","text":"<p>Let's add more TB samples to our analysis:</p> <pre><code># Update the sample sheet with additional samples\ncat &gt; samplesheet.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nERR036221,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nERR036223,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_2.fastq.gz\nERR036226,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_2.fastq.gz\nERR036227,/data/Dataset_Mt_Vc/tb/raw_data/ERR036227_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036227_2.fastq.gz\nEOF\n\n# Check what samples we have now\necho \"Updated sample sheet:\"\ncat samplesheet.csv\n</code></pre>"},{"location":"modules/day6/#scenario-2-running-without-resume-fresh-start","title":"Scenario 2: Running Without Resume (Fresh Start)","text":"<pre><code># Clean previous results\nrm -rf results/ work/\n\n# Run pipeline fresh (all processes will execute)\necho \"=== Running WITHOUT -resume ===\"\ntime nextflow run count_reads.nf --input samplesheet.csv\n</code></pre> Expected output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `count_reads.nf` [clever_volta] - revision: 5e6f7g8h\nexecutor &gt;  local (4)\n[c1/d2e3f4] process &gt; countReads (ERR036221) [100%] 4 of 4 \u2714\n[a5/b6c7d8] process &gt; countReads (ERR036223) [100%] 4 of 4 \u2714\n[e9/f0g1h2] process &gt; countReads (ERR036226) [100%] 4 of 4 \u2714\n[i3/j4k5l6] process &gt; countReads (ERR036227) [100%] 4 of 4 \u2714\n\n# All 4 samples processed from scratch\n# Time: ~2-3 minutes (depending on data size)\n</code></pre>"},{"location":"modules/day6/#scenario-3-using-resume-smart-restart","title":"Scenario 3: Using Resume (Smart Restart)","text":"<p>Now let's simulate a common scenario - adding one more sample:</p> <pre><code># Add one more sample to the sheet\ncat &gt;&gt; samplesheet.csv &lt;&lt; 'EOF'\nERR036232,/data/Dataset_Mt_Vc/tb/raw_data/ERR036232_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036232_2.fastq.gz\nEOF\n\n# Run with -resume (only new sample will be processed)\necho \"=== Running WITH -resume ===\"\ntime nextflow run count_reads.nf --input samplesheet.csv -resume\n</code></pre> Expected output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `count_reads.nf` [clever_volta] - revision: 5e6f7g8h\nexecutor &gt;  local (1)\n[c1/d2e3f4] process &gt; countReads (ERR036221) [100%] 4 of 4, cached: 4 \u2714\n[a5/b6c7d8] process &gt; countReads (ERR036223) [100%] 4 of 4, cached: 4 \u2714\n[e9/f0g1h2] process &gt; countReads (ERR036226) [100%] 4 of 4, cached: 4 \u2714\n[i3/j4k5l6] process &gt; countReads (ERR036227) [100%] 4 of 4, cached: 4 \u2714\n[m7/n8o9p0] process &gt; countReads (ERR036232) [100%] 1 of 1 \u2714\n\n# Only ERR036232 processed fresh, others cached!\n# Time: ~30 seconds (much faster!)\n</code></pre>"},{"location":"modules/day6/#scenario-4-local-vs-cluster-execution","title":"Scenario 4: Local vs Cluster Execution","text":"<p>Local Execution (Current): </p><pre><code># Running on local machine (default)\nnextflow run count_reads.nf --input samplesheet.csv -resume\n\n# Check resource usage\necho \"Local execution uses:\"\necho \"- All available CPU cores on this machine\"\necho \"- Local memory and storage\"\necho \"- Processes run sequentially if cores are limited\"\n</code></pre><p></p> <p>Cluster Execution (Advanced): </p><pre><code># Example cluster configuration (for reference)\ncat &gt; nextflow.config &lt;&lt; 'EOF'\nprocess {\n    executor = 'slurm'\n    queue = 'batch'\n    cpus = 2\n    memory = '4.GB'\n    time = '1.h'\n}\n\nprofiles {\n    cluster {\n        process.executor = 'slurm'\n        process.queue = 'batch'\n    }\n\n    local {\n        process.executor = 'local'\n    }\n}\nEOF\n\n# Would run on cluster (if available):\n# nextflow run count_reads.nf --input samplesheet.csv -profile cluster\n\necho \"Cluster execution would provide:\"\necho \"- Parallel execution across multiple nodes\"\necho \"- Better resource management\"\necho \"- Automatic job queuing and scheduling\"\necho \"- Fault tolerance across nodes\"\n</code></pre><p></p>"},{"location":"modules/day6/#scenario-5-monitoring-and-debugging","title":"Scenario 5: Monitoring and Debugging","text":"<pre><code># Check what's in the work directory\necho \"=== Work Directory Structure ===\"\nfind work -name \"*.count\" | head -5\n\n# Look at a specific process execution\nwork_dir=$(find work -name \"*ERR036221*\" -type d | head -1)\necho \"=== Process Details for ERR036221 ===\"\necho \"Work directory: $work_dir\"\nls -la \"$work_dir\"\n\n# Check the command that was executed\nif [ -f \"$work_dir/.command.sh\" ]; then\n    echo \"Command executed:\"\n    cat \"$work_dir/.command.sh\"\nfi\n\n# Check process logs\nif [ -f \"$work_dir/.command.log\" ]; then\n    echo \"Process output:\"\n    cat \"$work_dir/.command.log\"\nfi\n</code></pre> <p>Key Learning Points</p> <p>Resume Functionality: - <code>-resume</code> only re-runs processes that have changed - Saves time and computational resources - Essential for large-scale analyses - Works by comparing input file checksums</p> <p>Execution Environments: - Local: Good for development and small datasets - Cluster: Essential for production and large datasets - Cloud: Scalable option for variable workloads</p> <p>Best Practices: - Always use <code>-resume</code> when re-running pipelines - Test locally before moving to cluster - Monitor resource usage and adjust accordingly - Keep work directories for debugging</p>"},{"location":"modules/day6/#hands-on-timing-exercise","title":"Hands-On Timing Exercise\ud83d\udd04 Interactive Scenario Comparison","text":"<p>Let's measure the actual time difference:</p> <pre><code># Timing comparison exercise\necho \"=== TIMING COMPARISON EXERCISE ===\"\n\n# 1. Fresh run timing\necho \"1. Measuring fresh run time...\"\nrm -rf work/ results/\ntime nextflow run count_reads.nf --input samplesheet.csv &gt; fresh_run.log 2&gt;&amp;1\n\n# 2. Resume run timing (no changes)\necho \"2. Measuring resume time with no changes...\"\ntime nextflow run count_reads.nf --input samplesheet.csv -resume &gt; resume_run.log 2&gt;&amp;1\n\n# 3. Resume with new sample timing\necho \"3. Adding new sample and measuring resume time...\"\necho \"ERR036233,/data/Dataset_Mt_Vc/tb/raw_data/ERR036233_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036233_2.fastq.gz\" &gt;&gt; samplesheet.csv\ntime nextflow run count_reads.nf --input samplesheet.csv -resume &gt; resume_new.log 2&gt;&amp;1\n\n# 4. Compare results\necho \"=== TIMING RESULTS ===\"\necho \"Fresh run log:\"\ngrep \"Completed at:\" fresh_run.log\necho \"Resume run log (no changes):\"\ngrep \"Completed at:\" resume_run.log\necho \"Resume run log (with new sample):\"\ngrep \"Completed at:\" resume_new.log\n\necho \"=== CACHE EFFICIENCY ===\"\necho \"Resume run (no changes):\"\ngrep \"cached:\" resume_run.log\necho \"Resume run (with new sample):\"\ngrep \"cached:\" resume_new.log\n</code></pre> Expected timing results <pre><code>=== TIMING RESULTS ===\nFresh run: ~2-3 minutes (all samples processed)\nResume (no changes): ~10-15 seconds (all cached)\nResume (new sample): ~45-60 seconds (4 cached + 1 new)\n\n=== CACHE EFFICIENCY ===\nResume shows: \"cached: 4\" for existing samples\nOnly new sample executes fresh\n\nSpeed improvement: 80-90% faster with resume!\n</code></pre> Fresh Run With Resume Cluster Mode \ud83c\udd95 Fresh Run (No Resume) <ul> <li>Command: <code>nextflow run count_reads.nf --input samplesheet.csv</code></li> <li>Behavior: All processes execute from scratch</li> <li>Time: Full execution time (2-3 minutes)</li> <li>Use case: First run, major changes, clean start</li> <li>Work directory: Completely new hash directories</li> </ul> \u26a1 Resume Run <ul> <li>Command: <code>nextflow run count_reads.nf --input samplesheet.csv -resume</code></li> <li>Behavior: Only new/changed processes execute</li> <li>Time: Much faster (30 seconds for new samples)</li> <li>Use case: Adding samples, minor script changes</li> <li>Work directory: Reuses existing hash directories</li> </ul> \ud83d\udda5\ufe0f Cluster Execution <ul> <li>Command: <code>nextflow run count_reads.nf --input samplesheet.csv -profile cluster</code></li> <li>Behavior: Jobs submitted to SLURM/PBS queue</li> <li>Time: Depends on queue wait time + parallel execution</li> <li>Use case: Large datasets, production runs</li> <li>Resources: Multiple nodes, better memory/CPU allocation</li> </ul>"},{"location":"modules/day6/#exercise-3-complete-quality-control-pipeline-60-minutes","title":"Exercise 3: Complete Quality Control Pipeline (60 minutes)","text":"<p>Now let's build a realistic bioinformatics pipeline with multiple steps:</p>"},{"location":"modules/day6/#step-1-basic-fastqc-pipeline","title":"Step 1: Basic FastQC Pipeline","text":"<p>First, let's start with a simple FastQC pipeline:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Parameters\nparams.input = \"samplesheet.csv\"\nparams.outdir = \"results\"\n\n// Read sample sheet and create channel\nChannel\n    .fromPath(params.input)\n    .splitCsv(header: true)\n    .map { row -&gt;\n        def sample = row.sample\n        def fastq1 = file(row.fastq_1)\n        def fastq2 = file(row.fastq_2)\n        return [sample, [fastq1, fastq2]]\n    }\n    .set { read_pairs_ch }\n\n// FastQC process\nprocess fastqc {\n    // Load required modules\n    module 'fastqc/0.12.1'\n\n    // Save results\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    // Use sample name for process identification\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads) from read_pairs_ch\n\n    output:\n    path \"*_fastqc.{zip,html}\" into fastqc_ch\n\n    script:\n    \"\"\"\n    echo \"Running FastQC on ${sample_id}\"\n    echo \"Processing files: ${reads.join(', ')}\"\n    fastqc ${reads}\n    \"\"\"\n}\n\n// Show what files were created\nfastqc_ch.view { \"FastQC report: $it\" }\n</code></pre> <p>Save this as <code>qc_pipeline_v1.nf</code> and test it:</p> <pre><code># Load modules\nmodule load java/openjdk-17.0.2 nextflow/25.04.6 fastqc/0.12.1\n\n# Run basic FastQC pipeline\nnextflow run qc_pipeline_v1.nf --input samplesheet.csv\n</code></pre>"},{"location":"modules/day6/#step-2-complete-genomic-analysis-pipeline","title":"Step 2: Complete Genomic Analysis Pipeline","text":"<p>Now let's build a complete genomic analysis pipeline with quality control, trimming, and genome assembly:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Parameters\nparams.input = \"samplesheet.csv\"\nparams.outdir = \"results\"\nparams.adapters = \"/data/timmomatic_adapter_Combo.fa\"\n\n// Read sample sheet and create channel\nChannel\n    .fromPath(params.input)\n    .splitCsv(header: true)\n    .map { row -&gt;\n        def sample = row.sample\n        def fastq1 = file(row.fastq_1)\n        def fastq2 = file(row.fastq_2)\n        return [sample, [fastq1, fastq2]]\n    }\n    .set { read_pairs_ch }\n\n// Duplicate channel for parallel processing\nread_pairs_ch.into { fastqc_raw_ch; trimmomatic_ch }\n\n// FastQC on raw reads\nprocess fastqc_raw {\n    module 'fastqc/0.12.1'\n    publishDir \"${params.outdir}/fastqc_raw\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads) from fastqc_raw_ch\n\n    output:\n    path \"*_fastqc.{zip,html}\" into fastqc_raw_results\n\n    script:\n    \"\"\"\n    echo \"Running FastQC on raw reads: ${sample_id}\"\n    fastqc ${reads}\n    \"\"\"\n}\n\n// Trimmomatic for quality trimming\nprocess trimmomatic {\n    module 'trimmomatic/0.39'\n    publishDir \"${params.outdir}/trimmed\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads) from trimmomatic_ch\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_*_paired.fastq.gz\") into trimmed_ch\n    path \"${sample_id}_*_unpaired.fastq.gz\" into unpaired_ch\n\n    script:\n    \"\"\"\n    echo \"Running Trimmomatic on ${sample_id}\"\n\n    trimmomatic PE -threads 2 \\\\\n        ${reads[0]} ${reads[1]} \\\\\n        ${sample_id}_R1_paired.fastq.gz ${sample_id}_R1_unpaired.fastq.gz \\\\\n        ${sample_id}_R2_paired.fastq.gz ${sample_id}_R2_unpaired.fastq.gz \\\\\n        ILLUMINACLIP:${params.adapters}:2:30:10 \\\\\n        LEADING:3 TRAILING:3 \\\\\n        SLIDINGWINDOW:4:15 \\\\\n        MINLEN:36\n\n    echo \"Trimming completed for ${sample_id}\"\n    \"\"\"\n}\n\n// FastQC on trimmed reads\nprocess fastqc_trimmed {\n    module 'fastqc/0.12.1'\n    publishDir \"${params.outdir}/fastqc_trimmed\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads) from trimmed_ch\n\n    output:\n    path \"*_fastqc.{zip,html}\" into fastqc_trimmed_results\n\n    script:\n    \"\"\"\n    echo \"Running FastQC on trimmed reads: ${sample_id}\"\n    fastqc ${reads}\n    \"\"\"\n}\n\n// Collect all FastQC results\nfastqc_raw_results.mix(fastqc_trimmed_results).collect().set { all_fastqc_ch }\n\n// MultiQC to summarize all results\nprocess multiqc {\n    module 'multiqc/1.22.3'\n    publishDir \"${params.outdir}\", mode: 'copy'\n\n    input:\n    path fastqc_files from all_fastqc_ch\n\n    output:\n    path \"multiqc_report.html\" into multiqc_ch\n    path \"multiqc_data\" into multiqc_data_ch\n\n    script:\n    \"\"\"\n    echo \"Running MultiQC to summarize all results\"\n    multiqc . --filename multiqc_report.html\n    \"\"\"\n}\n\n// Show final results\nmultiqc_ch.view { \"MultiQC report created: $it\" }\n</code></pre> <p>Save this as <code>qc_pipeline_v2.nf</code> and test it:</p> <pre><code># Load all required modules\nmodule load java/openjdk-17.0.2 nextflow/25.04.6 fastqc/0.12.1 trimmomatic/0.39 multiqc/1.22.3\n\n# Run the enhanced pipeline\nnextflow run qc_pipeline_v2.nf --input samplesheet.csv\n</code></pre> Expected output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `qc_pipeline_v2.nf` [clever_volta] - revision: 5e6f7g8h\nexecutor &gt;  local (10)\n[a1/b2c3d4] process &gt; fastqc_raw (ERR036221)     [100%] 2 of 2 \u2714\n[e5/f6g7h8] process &gt; fastqc_raw (ERR036223)     [100%] 2 of 2 \u2714\n[i9/j0k1l2] process &gt; trimmomatic (ERR036221)    [100%] 2 of 2 \u2714\n[m3/n4o5p6] process &gt; trimmomatic (ERR036223)    [100%] 2 of 2 \u2714\n[q7/r8s9t0] process &gt; fastqc_trimmed (ERR036221) [100%] 2 of 2 \u2714\n[u1/v2w3x4] process &gt; fastqc_trimmed (ERR036223) [100%] 2 of 2 \u2714\n[y5/z6a7b8] process &gt; multiqc                    [100%] 1 of 1 \u2714\n\nMultiQC report created: /path/to/results/multiqc_report.html\n</code></pre>"},{"location":"modules/day6/#step-3-pipeline-scenarios-and-comparisons","title":"Step 3: Pipeline Scenarios and Comparisons","text":"<p>Scenario A: Compare Before and After Trimming</p> <pre><code># Check the results structure\ntree results/\n\n# Compare raw vs trimmed FastQC reports\necho \"=== Raw Data Quality ===\"\nls -la results/fastqc_raw/\n\necho \"=== Trimmed Data Quality ===\"\nls -la results/fastqc_trimmed/\n\necho \"=== Trimmed Files ===\"\nls -la results/trimmed/\n\n# Check file sizes (trimmed files should be smaller)\necho \"=== File Size Comparison ===\"\necho \"Original files:\"\nls -lh /data/Dataset_Mt_Vc/tb/raw_data/ERR036221_*.fastq.gz\necho \"Trimmed files:\"\nls -lh results/trimmed/ERR036221_*_paired.fastq.gz\n</code></pre> <p>Scenario B: Adding More Samples with Resume</p> <pre><code># Add more samples to test scalability\ncat &gt; samplesheet_extended.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nERR036221,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nERR036223,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_2.fastq.gz\nERR036226,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_2.fastq.gz\nERR036227,/data/Dataset_Mt_Vc/tb/raw_data/ERR036227_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036227_2.fastq.gz\nEOF\n\n# Run with resume (only new samples will be processed)\necho \"=== Running with more samples using -resume ===\"\ntime nextflow run qc_pipeline_v2.nf --input samplesheet_extended.csv -resume\n</code></pre> <p>Scenario C: Parameter Optimization</p> <pre><code># Create a configuration file for different trimming parameters\ncat &gt; nextflow.config &lt;&lt; 'EOF'\nparams {\n    input = \"samplesheet.csv\"\n    outdir = \"results\"\n    adapters = \"/data/timmomatic_adapter_Combo.fa\"\n}\n\nprofiles {\n    strict {\n        params.outdir = \"results_strict\"\n        // Stricter trimming parameters would go here\n    }\n\n    lenient {\n        params.outdir = \"results_lenient\"\n        // More lenient trimming parameters would go here\n    }\n}\nEOF\n\n# Run with different profiles\necho \"=== Testing different trimming strategies ===\"\nnextflow run qc_pipeline_v2.nf -profile strict\nnextflow run qc_pipeline_v2.nf -profile lenient\n\n# Compare results\necho \"=== Comparing trimming strategies ===\"\necho \"Strict trimming results:\"\nls -la results_strict/trimmed/\necho \"Lenient trimming results:\"\nls -la results_lenient/trimmed/\n</code></pre>"},{"location":"modules/day6/#step-4-cluster-execution-advanced","title":"Step 4: Cluster Execution (Advanced)","text":"<p>Now let's see how to run the same pipeline on an HPC cluster:</p> <p>Scenario D: Local vs Cluster Comparison</p> <pre><code># First, let's run locally (what we've been doing)\necho \"=== Local Execution ===\"\ntime nextflow run qc_pipeline.nf --input samplesheet.csv -profile standard\n\n# Now let's run on SLURM cluster\necho \"=== SLURM Cluster Execution ===\"\ntime nextflow run qc_pipeline.nf --input samplesheet.csv -profile slurm\n\n# For testing with reduced resources\necho \"=== Test Profile ===\"\nnextflow run qc_pipeline.nf --input samplesheet.csv -profile test\n</code></pre> <p>Scenario E: High-Memory Assembly</p> <pre><code># For large genomes or complex assemblies\necho \"=== High-Memory Cluster Execution ===\"\nnextflow run qc_pipeline.nf --input samplesheet_extended.csv -profile highmem\n\n# Monitor cluster jobs\nsqueue -u $USER  # SLURM\nqstat -u $USER   # PBS/Torque\n</code></pre> <p>Scenario F: Resource Monitoring and Reports</p> <pre><code># Run with comprehensive monitoring\nnextflow run qc_pipeline.nf --input samplesheet.csv -profile slurm -with-trace -with-timeline -with-report\n\n# Check the generated reports\necho \"=== Pipeline Reports Generated ===\"\nls -la results/pipeline_*\n\n# View resource usage\necho \"=== Resource Usage Summary ===\"\ncat results/pipeline_trace.txt | head -10\n</code></pre> <p>Local vs Cluster Execution Comparison</p> <p>Local Execution Benefits: - \u2705 Immediate start: No queue waiting time - \u2705 Interactive debugging: Easy to test and troubleshoot - \u2705 Simple setup: No cluster configuration needed - \u274c Limited resources: Constrained by local machine - \u274c No parallelization: Limited concurrent jobs</p> <p>Cluster Execution Benefits: - \u2705 Massive parallelization: 100+ samples simultaneously - \u2705 High-memory nodes: 64GB+ RAM for large assemblies - \u2705 Automatic scheduling: Optimal resource allocation - \u2705 Fault tolerance: Job restart on node failures - \u274c Queue waiting: May wait for resources - \u274c Complex setup: Requires cluster configuration</p> <p>When to Use Each: - Local: Testing, small datasets (1-5 samples), development - Cluster: Production runs, large datasets (10+ samples), resource-intensive tasks</p>"},{"location":"modules/day6/#cluster-configuration-examples","title":"Cluster Configuration Examples","text":"<p>SLURM Configuration: </p><pre><code># Create a SLURM-specific config\ncat &gt; slurm.config &lt;&lt; 'EOF'\nprocess {\n    executor = 'slurm'\n    queue = 'batch'\n    clusterOptions = '--account=genomics_project'\n\n    withName: spades_assembly {\n        cpus = 16\n        memory = '32 GB'\n        time = '6h'\n        queue = 'long'\n    }\n}\nEOF\n\n# Run with custom config\nnextflow run qc_pipeline.nf -c slurm.config --input samplesheet.csv\n</code></pre><p></p> <p>PBS/Torque Configuration: </p><pre><code># Create a PBS-specific config\ncat &gt; pbs.config &lt;&lt; 'EOF'\nprocess {\n    executor = 'pbs'\n    queue = 'batch'\n\n    withName: spades_assembly {\n        cpus = 8\n        memory = '16 GB'\n        time = '4h'\n        clusterOptions = '-l nodes=1:ppn=8'\n    }\n}\nEOF\n\n# Run with PBS config\nnextflow run qc_pipeline.nf -c pbs.config --input samplesheet.csv\n</code></pre><p></p> <p>Key Learning Points from Exercise 3</p> <p>Pipeline Design Concepts: - Channel Duplication: Use <code>.into{}</code> to send data to multiple processes - Process Dependencies: Trimmomatic \u2192 FastQC creates a dependency chain - Result Aggregation: MultiQC collects and summarizes all FastQC reports - Parallel Processing: Raw FastQC and Trimmomatic run simultaneously</p> <p>Real-World Bioinformatics: - Quality Control: Always check data quality before and after processing - Adapter Trimming: Remove sequencing adapters and low-quality bases - Comparative Analysis: Compare raw vs processed data quality - Comprehensive Reporting: MultiQC provides publication-ready summaries</p> <p>Nextflow Best Practices: - Modular Design: Each process does one thing well - Resource Management: Use <code>tag</code> for process identification - Result Organization: Use <code>publishDir</code> to organize outputs - Configuration: Use profiles for different analysis strategies</p> <p>Performance Optimization: - Resume Functionality: Only reprocess changed samples - Parallel Execution: Multiple samples processed simultaneously - Resource Allocation: Configure CPU/memory per process - Scalability: Easy to add more samples or processing steps</p>"},{"location":"modules/day6/#exercise-3-summary","title":"Exercise 3 Summary","text":"<p>You've now built a complete bioinformatics QC pipeline that:</p> <ol> <li>Performs quality control on raw sequencing data</li> <li>Trims adapters and low-quality bases using Trimmomatic</li> <li>Re-assesses quality after trimming</li> <li>Generates comprehensive reports with MultiQC</li> <li>Handles multiple samples in parallel</li> <li>Supports different analysis strategies via configuration profiles</li> </ol> <p>This pipeline demonstrates real-world bioinformatics workflow patterns that you'll use in production analyses!</p>"},{"location":"modules/day6/#exercise-3-enhanced-summary","title":"Exercise 3 Enhanced Summary","text":"<p>You've now built a complete genomic analysis pipeline that includes:</p> <ol> <li>Quality Assessment (FastQC on raw reads)</li> <li>Quality Trimming (Trimmomatic)</li> <li>Post-trimming QC (FastQC on trimmed reads)</li> <li>Genome Assembly (SPAdes)</li> <li>Cluster Execution (SLURM/PBS configuration)</li> <li>Resource Monitoring (Trace, timeline, and reports)</li> </ol> <p>Real Results Achieved:</p> <ul> <li>Processed: 4 TB clinical isolates (8+ million reads each)</li> <li>Generated: 16 FastQC reports + 4 genome assemblies</li> <li>Assembly Stats: ~250-264 contigs per genome, 4.3MB assemblies</li> <li>Resource Usage: Peak 3.6GB RAM, 300%+ CPU utilization</li> <li>Execution Time: 2-3 minutes per sample (local), scalable to 100+ samples (cluster)</li> </ul> <p>Production Skills Learned:</p> <ul> <li>\u2705 Multi-step pipeline design with process dependencies</li> <li>\u2705 Resource specification for different process types</li> <li>\u2705 Cluster configuration for SLURM and PBS systems</li> <li>\u2705 Performance monitoring with built-in reporting</li> <li>\u2705 Scalable execution from local to HPC environments</li> <li>\u2705 Resume functionality for efficient re-runs</li> </ul> <p>This represents a publication-ready genomic analysis workflow that students can adapt for their own research projects!</p> <p>Step 3: Run the pipeline with real data </p><pre><code>nextflow run fastqc_pipeline.nf --input samplesheet.csv\n</code></pre><p></p> Expected output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `fastqc_pipeline.nf` [romantic_curie] - revision: 9a0b1c2d\nexecutor &gt;  local (2)\n[e1/f2g3h4] process &gt; fastqc (ERR036221) [100%] 2 of 2 \u2714\n[a5/b6c7d8] process &gt; fastqc (ERR036223) [100%] 2 of 2 \u2714\nFastQC report: /path/to/results/fastqc/ERR036221_1_fastqc.html\nFastQC report: /path/to/results/fastqc/ERR036221_2_fastqc.html\nFastQC report: /path/to/results/fastqc/ERR036223_1_fastqc.html\nFastQC report: /path/to/results/fastqc/ERR036223_2_fastqc.html\n</code></pre> <p>Step 4: Check your results </p><pre><code># Look at the results structure\nls -la results/fastqc/\n\n# Check file sizes (real data produces substantial reports)\ndu -h results/fastqc/\n\n# Open an HTML report to see real quality metrics\n# firefox results/fastqc/ERR036221_1_fastqc.html &amp;\n</code></pre><p></p> Expected output <pre><code>results/\n\u2514\u2500\u2500 fastqc/\n    \u251c\u2500\u2500 ERR036221_1_fastqc.html\n    \u251c\u2500\u2500 ERR036221_1_fastqc.zip\n    \u251c\u2500\u2500 ERR036221_2_fastqc.html\n    \u251c\u2500\u2500 ERR036221_2_fastqc.zip\n    \u251c\u2500\u2500 ERR036223_1_fastqc.html\n    \u251c\u2500\u2500 ERR036223_1_fastqc.zip\n    \u251c\u2500\u2500 ERR036223_2_fastqc.html\n    \u2514\u2500\u2500 ERR036223_2_fastqc.zip\n\n# Real TB sequencing data will show:\n# - Millions of reads per file\n# - Quality scores across read positions\n# - GC content distribution\n# - Sequence duplication levels\n</code></pre> <p>Progressive Learning Concepts:</p> <ul> <li>Paired-end reads: Handle R1 and R2 files together using <code>fromFilePairs()</code></li> <li>Containers: Use Docker for consistent software environments</li> <li>publishDir: Automatically save results to specific folders</li> <li>Tuple inputs: Process sample ID and file paths together</li> </ul>"},{"location":"modules/day6/#understanding-your-exercise-results","title":"Understanding Your Exercise Results\ud83d\udcca Exercise Results Explorer","text":"<p>After completing the exercises, your directory structure should look like this:</p> <p>Click on exercises to see their expected output structure:</p>              \ud83c\udfaf Exercise 1: Hello World Results                       \ud83d\udcca Exercise 2: Read Counting Results                       \ud83d\udd2c Exercise 3: FastQC Pipeline Results"},{"location":"modules/day6/#interactive-learning-checklist","title":"Interactive Learning Checklist","text":""},{"location":"modules/day6/#before-you-start-setup-checklist","title":"Before You Start - Setup Checklist","text":"<p>Check if Nextflow is installed: </p><pre><code>nextflow -version\n</code></pre><p></p> Expected output <pre><code>nextflow version 23.10.0.5889\n</code></pre> <p>If you see a version number, you're ready to go!</p> If Nextflow is not installed <pre><code>bash: nextflow: command not found\n</code></pre> <p>Install Nextflow: </p><pre><code>curl -s https://get.nextflow.io | bash\nsudo mv nextflow /usr/local/bin/\n</code></pre><p></p> <p>Check if Docker is available: </p><pre><code>docker --version\n</code></pre><p></p> Expected output <pre><code>Docker version 24.0.7, build afdd53b\n</code></pre> Alternative: Check for Singularity <pre><code>singularity --version\n</code></pre> <p>Expected output: </p><pre><code>singularity-ce version 3.11.4\n</code></pre><p></p> <p>Create your workspace: </p><pre><code># Create a directory for today's exercises\nmkdir nextflow-training\ncd nextflow-training\n\n# Create subdirectories (no data dir needed - using /data)\nmkdir results scripts\n</code></pre><p></p> Expected output <pre><code># ls -la\ntotal 20\ndrwxr-xr-x 5 user user 4096 Jan 15 09:00 .\ndrwxr-xr-x 3 user user 4096 Jan 15 09:00 ..\ndrwxr-xr-x 2 user user 4096 Jan 15 09:00 data\ndrwxr-xr-x 2 user user 4096 Jan 15 09:00 results\ndrwxr-xr-x 2 user user 4096 Jan 15 09:00 scripts\n</code></pre> <p>Interactive Setup Checklist:</p> \ud83d\udccb Setup Progress Tracker Nextflow installed (run <code>nextflow -version</code>) Container system available (Docker or Singularity) Workspace created (<code>nextflow-training</code> directory) Terminal ready (in the correct directory) <p>Setup Progress: 0/4 completed</p>          \ud83c\udf89 Great! You're ready to start the exercises!"},{"location":"modules/day6/#your-first-pipeline-step-by-step","title":"Your First Pipeline - Step by Step","text":"\ud83c\udfaf Exercise Progress Tracker Exercise 1: Hello World                      Create and run your first Nextflow script with 3 samples                                   \u2705 Completed! You've successfully run your first Nextflow pipeline.                 Next: Try modifying the sample names and run again. Exercise 2: Read Counting                      Count reads in FASTQ files using Nextflow channels                                   \u2705 Completed! You've learned about channels and file processing.                 Next: Try the FastQC pipeline with containers. Exercise 3: FastQC Pipeline                      Quality control with containers and paired-end reads                                   \u2705 Completed! You've mastered containers and paired-end data.                 Next: Explore the complete beginner pipeline. <p>Exercise Progress: 0/3 completed</p>          \ud83c\udf89 Congratulations! You've completed all the basic exercises!          You're now ready for: <ul> <li>Building more complex pipelines</li> <li>Using nf-core community pipelines</li> <li>Deploying on HPC clusters</li> <li>Working with your own data</li> </ul>"},{"location":"modules/day6/#understanding-your-results","title":"Understanding Your Results","text":"<ul> <li> FastQC Reports: Open the HTML files in a web browser</li> <li> Log Files: Check the <code>.nextflow.log</code> file for any errors</li> <li> Work Directory: Look in the <code>work/</code> folder to see intermediate files</li> <li> Results Directory: Confirm your outputs are where you expect them</li> </ul>"},{"location":"modules/day6/#common-beginner-questions-solutions","title":"Common Beginner Questions &amp; Solutions","text":""},{"location":"modules/day6/#my-pipeline-failed-what-do-i-do","title":"\"My pipeline failed - what do I do?\"","text":"<p>Step 1: Check the error message</p> <p>Look at the main Nextflow log: </p><pre><code>cat .nextflow.log\n</code></pre><p></p> <p>Find specific errors: </p><pre><code>grep ERROR .nextflow.log\n</code></pre><p></p> Example error output <pre><code>ERROR ~ Error executing process &gt; 'fastqc (sample1)'\n\nCaused by:\n  Process `fastqc (sample1)` terminated with an error exit status (127)\n\nCommand executed:\n  fastqc sample1_R1.fastq sample1_R2.fastq\n\nCommand exit status:\n  127\n\nWork dir:\n  /path/to/work/a1/b2c3d4e5f6...\n</code></pre> <p>Step 2: Check the work directory</p> <p>Navigate to the failed task's work directory: </p><pre><code># Use the work directory path from the error message\ncd work/a1/b2c3d4e5f6...\n\n# Check what the process tried to do\ncat .command.sh\n</code></pre><p></p> Expected output <pre><code>#!/bin/bash -ue\nfastqc sample1_R1.fastq sample1_R2.fastq\n</code></pre> <p>Check for error messages: </p><pre><code>cat .command.err\n</code></pre><p></p> Example error content <pre><code>bash: fastqc: command not found\n</code></pre> <p>Check standard output: </p><pre><code>cat .command.out\n</code></pre><p></p> <p>Step 3: Understanding the error</p> <p>In this example: - Exit status 127: Command not found - Error message: \"fastqc: command not found\" - Solution: FastQC is not installed or not in PATH</p>"},{"location":"modules/day6/#how-do-i-know-if-my-pipeline-is-working","title":"\"How do I know if my pipeline is working?\"","text":"<p>Check pipeline status while running: </p><pre><code># In another terminal, monitor the pipeline\nnextflow log\n</code></pre><p></p> Good signs - pipeline working correctly <pre><code>TIMESTAMP    DURATION  RUN NAME         STATUS   REVISION ID  SESSION ID                            COMMAND\n2024-01-15   1m 30s    clever_volta     OK       a1b2c3d4     12345678-1234-1234-1234-123456789012  nextflow run hello.nf\n</code></pre> <p>What to look for: - STATUS: OK - Pipeline completed successfully - DURATION - Shows how long it took - No ERROR messages in the terminal output - Process completion: <code>[100%] X of X \u2714</code></p> <p>Check your results: </p><pre><code># List output directory contents\nls -la results/\n\n# Check if files were created\nfind results/ -type f -name \"*.html\" -o -name \"*.txt\" -o -name \"*.count\"\n</code></pre><p></p> Expected successful output <pre><code># ls -la results/\ntotal 12\ndrwxr-xr-x 3 user user 4096 Jan 15 10:30 .\ndrwxr-xr-x 5 user user 4096 Jan 15 10:29 ..\ndrwxr-xr-x 2 user user 4096 Jan 15 10:30 fastqc\n-rw-r--r-- 1 user user   42 Jan 15 10:30 sample1.count\n-rw-r--r-- 1 user user   38 Jan 15 10:30 sample2.count\n\n# find results/ -type f\nresults/sample1.count\nresults/sample2.count\nresults/fastqc/sample1_R1_fastqc.html\nresults/fastqc/sample1_R2_fastqc.html\n</code></pre> Warning signs - something went wrong <pre><code># Empty results directory\nls results/\n# (no output)\n\n# Error in nextflow log\nTIMESTAMP    DURATION  RUN NAME         STATUS   REVISION ID  SESSION ID                            COMMAND\n2024-01-15   30s       sad_einstein     ERR      a1b2c3d4     12345678-1234-1234-1234-123456789012  nextflow run hello.nf\n</code></pre> <p>Red flags: - STATUS: ERR - Pipeline failed - Empty results directory - No outputs created - Red ERROR text in terminal - Process failures: <code>[50%] 1 of 2, failed: 1</code></p>"},{"location":"modules/day6/#how-do-i-modify-the-pipeline-for-my-data","title":"\"How do I modify the pipeline for my data?\"","text":"<p>Start simple: 1. Change the <code>params.reads</code> path to point to your files 2. Make sure your file names match the pattern (e.g., <code>*_{R1,R2}.fastq</code>) 3. Test with just 1-2 samples first 4. Once it works, add more samples</p> <p>File naming examples: </p><pre><code>Good:\nsample1_R1.fastq, sample1_R2.fastq\nsample2_R1.fastq, sample2_R2.fastq\n\nAlso good:\ndata_001_R1.fastq.gz, data_001_R2.fastq.gz\ndata_002_R1.fastq.gz, data_002_R2.fastq.gz\n\nWon't work:\nsample1_forward.fastq, sample1_reverse.fastq\nsample1_1.fastq, sample1_2.fastq\n</code></pre><p></p>"},{"location":"modules/day6/#next-steps-for-beginners","title":"Next Steps for Beginners","text":""},{"location":"modules/day6/#once-youre-comfortable-with-basic-pipelines","title":"Once you're comfortable with basic pipelines:","text":"<ol> <li>Add more processes: Try adding genome annotation with Prokka</li> <li>Use parameters: Make your pipeline configurable</li> <li>Add error handling: Make your pipeline more robust</li> <li>Try nf-core: Use community-built pipelines</li> <li>Document your work: Create clear documentation and examples</li> </ol>"},{"location":"modules/day6/#recommended-learning-path","title":"Recommended Learning Path:","text":"<ol> <li>Week 1: Master the basic exercises above</li> <li>Week 2: Try the complete beginner pipeline</li> <li>Week 3: Modify pipelines for your own data</li> <li>Week 4: Explore nf-core pipelines</li> <li>Month 2: Start building your own custom pipelines</li> </ol> <p>Remember: Everyone starts as a beginner! The key is to practice with small examples and gradually build complexity. Don't try to create a complex pipeline on your first day.</p> \ud83d\udd27 Interactive Troubleshooting Guide <p>Having issues? Click on your problem to get specific help:</p>              \ud83d\udeab Nextflow is not installed or not found                       \ud83d\udd12 Permission denied errors                       \ud83d\udc33 Docker/container errors                       \ud83d\udcc1 No input files found                       \ud83d\udcbe Out of memory errors          <pre><code>### The Workflow Management Solution\n\nWith Nextflow, you define the workflow once and it handles:\n\n- **Automatic parallelization** of all 100 samples\n- **Intelligent resource management** (memory, CPUs)\n- **Automatic retry** of failed tasks with different resources\n- **Resume capability** from the last successful step\n- **Container integration** for reproducibility\n- **Detailed execution reports** and monitoring\n- **Platform portability** (laptop \u2192 HPC \u2192 cloud)\n\n## Part 2: Nextflow Architecture and Core Concepts\n\n### Nextflow's Key Components\n\n#### 1. **Nextflow Engine**\n\nThe core runtime that interprets and executes your pipeline:\n\n- Parses the workflow script\n- Manages task scheduling and execution\n- Handles data flow between processes\n- Provides caching and resume capabilities\n\n#### 2. **Work Directory**\n\nWhere Nextflow stores intermediate files and task execution:\n\n```text\nwork/\n\u251c\u2500\u2500 12/\n\u2502   \u2514\u2500\u2500 3456789abcdef.../\n\u2502       \u251c\u2500\u2500 .command.sh      # The actual script executed\n\u2502       \u251c\u2500\u2500 .command.run     # Wrapper script\n\u2502       \u251c\u2500\u2500 .command.out     # Standard output\n\u2502       \u251c\u2500\u2500 .command.err     # Standard error\n\u2502       \u251c\u2500\u2500 .command.log     # Execution log\n\u2502       \u251c\u2500\u2500 .exitcode       # Exit status\n\u2502       \u2514\u2500\u2500 input_file.fastq # Staged input files\n\u2514\u2500\u2500 ab/\n    \u2514\u2500\u2500 cdef123456789.../\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"modules/day6/#3-executors","title":"3. Executors","text":"<p>Interface with different computing platforms:</p> <ul> <li>Local: Run on your laptop/desktop</li> <li>SLURM: Submit jobs to HPC clusters</li> <li>AWS Batch: Execute on Amazon cloud</li> <li>Kubernetes: Run on container orchestration platforms</li> </ul>"},{"location":"modules/day6/#core-nextflow-components","title":"Core Nextflow Components","text":""},{"location":"modules/day6/#process","title":"Process","text":"<p>A process defines a task to be executed. It's the basic building block of a Nextflow pipeline:</p> <pre><code>process FASTQC {\n    // Process directives\n    tag \"$sample_id\"\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*_fastqc.{html,zip}\"), emit: reports\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n</code></pre> <p>Key Elements:</p> <ul> <li>Directives: Configure how the process runs (container, resources, etc.)</li> <li>Input: Define what data the process expects</li> <li>Output: Define what data the process produces</li> <li>Script: The actual command(s) to execute</li> </ul>"},{"location":"modules/day6/#channel","title":"Channel","text":"<p>Channels are asynchronous data streams that connect processes:</p> <pre><code>// Create channel from file pairs\nreads_ch = Channel.fromFilePairs(\"data/*_R{1,2}.fastq.gz\")\n\n// Create channel from a list\nsamples_ch = Channel.from(['sample1', 'sample2', 'sample3'])\n\n// Create channel from a file\nreference_ch = Channel.fromPath(\"reference.fasta\")\n</code></pre> <p>Channel Types:</p> <ul> <li>Queue channels: Can be consumed only once</li> <li>Value channels: Can be consumed multiple times</li> <li>File channels: Handle file paths and staging</li> </ul>"},{"location":"modules/day6/#workflow","title":"Workflow","text":"<p>The workflow block orchestrates process execution:</p> <pre><code>workflow {\n    // Define input channels\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    // Execute processes\n    FASTQC(reads_ch)\n\n    // Chain processes together\n    TRIMMOMATIC(reads_ch)\n    SPADES(TRIMMOMATIC.out.trimmed)\n\n    // Access outputs\n    FASTQC.out.reports.view()\n}\n</code></pre>"},{"location":"modules/day6/#part-3-hands-on-exercises","title":"Part 3: Hands-on Exercises","text":""},{"location":"modules/day6/#exercise-1-installation-and-setup-15-minutes","title":"Exercise 1: Installation and Setup (15 minutes)","text":"<p>Objective: Install Nextflow and verify the environment</p> <pre><code># Check Java version (must be 11 or later)\njava -version\n\n# Install Nextflow\ncurl -s https://get.nextflow.io | bash\n\n# Make executable and add to PATH\nchmod +x nextflow\nsudo mv nextflow /usr/local/bin/\n\n# Verify installation\nnextflow info\n\n# Test with hello world\nnextflow run hello\n</code></pre>"},{"location":"modules/day6/#exercise-2-your-first-nextflow-script-30-minutes","title":"Exercise 2: Your First Nextflow Script (30 minutes)","text":"<p>Objective: Create and run a simple Nextflow pipeline</p> <p>Create a file called <code>word_count.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Pipeline parameters - use real TB data\nparams.input = \"/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz\"\n\n// Input channel\ninput_ch = Channel.fromPath(params.input)\n\n// Main workflow\nworkflow {\n    NUM_LINES(input_ch)\n    NUM_LINES.out.view()\n}\n\n// Process definition\nprocess NUM_LINES {\n    input:\n    path read\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    printf '${read}\\\\t'\n    gunzip -c ${read} | wc -l\n    \"\"\"\n}\n</code></pre> <p>Run the pipeline:</p> <pre><code># Load modules\nmodule load java/openjdk-17.0.2 nextflow/25.04.6\n\n# Run the pipeline with real TB data\nnextflow run word_count.nf\n\n# Examine the work directory\nls -la work/\n\n# Check the actual file being processed\nls -lh /data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz\n</code></pre>"},{"location":"modules/day6/#exercise-3-understanding-channels-20-minutes","title":"Exercise 3: Understanding Channels (20 minutes)","text":"<p>Objective: Learn different ways to create and manipulate channels</p> <p>Create <code>channel_examples.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\nworkflow {\n    // Channel from file pairs\n    reads_ch = Channel.fromFilePairs(\"data/*_R{1,2}.fastq.gz\")\n    reads_ch.view { sample, files -&gt; \"Sample: $sample, Files: $files\" }\n\n    // Channel from list\n    samples_ch = Channel.from(['sample1', 'sample2', 'sample3'])\n    samples_ch.view { \"Processing: $it\" }\n\n    // Channel from path pattern\n    ref_ch = Channel.fromPath(\"*.fasta\")\n    ref_ch.view { \"Reference: $it\" }\n}\n</code></pre> <p>Save your pipeline script for future use and documentation.</p>"},{"location":"modules/day6/#key-concepts-summary","title":"Key Concepts Summary","text":""},{"location":"modules/day6/#nextflow-core-principles","title":"Nextflow Core Principles","text":"<ul> <li>Dataflow Programming: Data flows through processes via channels</li> <li>Parallelization: Automatic parallel execution of independent tasks</li> <li>Portability: Same code runs on laptop, HPC, or cloud</li> <li>Reproducibility: Consistent results across different environments</li> </ul>"},{"location":"modules/day6/#pipeline-development-best-practices","title":"Pipeline Development Best Practices","text":"<ul> <li>Start simple: Begin with basic processes and add complexity gradually</li> <li>Test frequently: Run your pipeline with small datasets during development</li> <li>Use containers: Ensure reproducible software environments</li> <li>Document clearly: Add comments and meaningful process names</li> <li>Handle errors: Plan for failures and edge cases</li> </ul>"},{"location":"modules/day6/#nextflow-workflow-patterns","title":"Nextflow Workflow Patterns","text":"<pre><code>Input Data \u2192 Process 1 \u2192 Process 2 \u2192 Process 3 \u2192 Final Results\n     \u2193           \u2193           \u2193           \u2193           \u2193\n  Channel    Channel     Channel     Channel    Published\n Creation   Transform   Transform   Transform    Output\n</code></pre>"},{"location":"modules/day6/#configuration-best-practices","title":"Configuration Best Practices","text":"<ul> <li>Use profiles for different execution environments</li> <li>Parameterize your pipelines for flexibility</li> <li>Set appropriate resource requirements</li> <li>Enable reporting and monitoring features</li> </ul>"},{"location":"modules/day6/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day6/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Successfully complete and run all three Nextflow exercises</li> <li>Understand the structure of Nextflow work directories</li> <li>Create and modify basic Nextflow processes</li> <li>Use channels to manage data flow between processes</li> <li>Configure pipeline parameters and execution profiles</li> </ul>"},{"location":"modules/day6/#group-discussion","title":"Group Discussion","text":"<ul> <li>Share pipeline design approaches and solutions</li> <li>Discuss common challenges and troubleshooting strategies</li> <li>Review different ways to structure Nextflow processes</li> <li>Compare execution results and performance observations</li> </ul>"},{"location":"modules/day6/#resources","title":"Resources","text":""},{"location":"modules/day6/#nextflow-resources","title":"Nextflow Resources","text":"<ul> <li>Nextflow Documentation - Official comprehensive documentation</li> <li>Nextflow Patterns - Common workflow patterns and best practices</li> <li>nf-core pipelines - Community-curated bioinformatics pipelines</li> <li>Nextflow Training - Official training materials and workshops</li> </ul>"},{"location":"modules/day6/#community-and-support","title":"Community and Support","text":"<ul> <li>Nextflow Slack - Community discussion and support</li> <li>nf-core Slack - Pipeline-specific discussions</li> <li>Nextflow GitHub - Source code and issue tracking</li> </ul>"},{"location":"modules/day6/#looking-ahead","title":"Looking Ahead","text":"<p>Day 7 Preview: Applied Genomics &amp; Advanced Topics</p>"},{"location":"modules/day6/#professional-development","title":"Professional Development","text":"<ul> <li>Git and GitHub for pipeline version control and collaboration</li> <li>Professional workflow development and team collaboration</li> </ul>"},{"location":"modules/day6/#applied-genomics","title":"Applied Genomics","text":"<ul> <li>MTB analysis pipeline development - Real-world tuberculosis genomics workflows</li> <li>Genome assembly workflows - Complete bacterial genome assembly pipelines</li> <li>Pathogen surveillance - Outbreak investigation and AMR detection pipelines</li> </ul>"},{"location":"modules/day6/#advanced-nextflow-deployment","title":"Advanced Nextflow &amp; Deployment","text":"<ul> <li>Container technologies - Docker and Singularity for reproducible environments</li> <li>Advanced Nextflow features - Complex workflow patterns and optimization</li> <li>Pipeline deployment - HPC, cloud, and container deployment strategies</li> <li>Performance optimization - Resource management and scaling techniques</li> <li>Best practices - Production-ready pipeline development</li> </ul>"},{"location":"modules/day6/#exercise-4-building-a-qc-process-30-minutes","title":"Exercise 4: Building a QC Process (30 minutes)","text":"<p>Objective: Create a real bioinformatics process</p> <p>Create <code>fastqc_pipeline.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Parameters\nparams.reads = \"data/*_R{1,2}.fastq.gz\"\nparams.outdir = \"results\"\n\n// Main workflow\nworkflow {\n    // Create channel from paired reads\n    reads_ch = Channel.fromFilePairs(params.reads, checkIfExists: true)\n\n    // Run FastQC\n    FASTQC(reads_ch)\n\n    // View results\n    FASTQC.out.view { sample, reports -&gt;\n        \"FastQC completed for $sample: $reports\"\n    }\n}\n\n// FastQC process\nprocess FASTQC {\n    tag \"$sample_id\"\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*_fastqc.{html,zip}\")\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n</code></pre> <p>Test the pipeline:</p> <pre><code># Load modules\nmodule load java/openjdk-17.0.2 nextflow/25.04.6 fastqc/0.12.1\n\n# Create sample sheet with real data\ncat &gt; samplesheet.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nERR036221,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nEOF\n\n# Run pipeline with real data\nnextflow run fastqc_pipeline.nf --input samplesheet.csv\n\n# Check results\nls -la results/fastqc/\n</code></pre>"},{"location":"modules/day6/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"modules/day6/#installation-issues","title":"Installation Issues","text":"<pre><code># Java version problems\njava -version  # Must be 11 or later\n\n# Nextflow not found\necho $PATH\nwhich nextflow\n\n# Permission issues\nchmod +x nextflow\n</code></pre>"},{"location":"modules/day6/#pipeline-debugging","title":"Pipeline Debugging","text":"<pre><code># Verbose output\nnextflow run pipeline.nf -with-trace -with-report -with-timeline\n\n# Check work directory\nls -la work/\n\n# Resume from failure\nnextflow run pipeline.nf -resume\n</code></pre> <p>Key Learning Outcome: Understanding workflow management fundamentals and Nextflow core concepts provides the foundation for building reproducible, scalable bioinformatics pipelines.</p>"},{"location":"modules/day7/","title":"Day 7 - Advanced Nextflow & Version Control with GitHub","text":""},{"location":"modules/day7/#day-7-advanced-nextflow-version-control-with-github","title":"Day 7: Advanced Nextflow &amp; Version Control with GitHub","text":"## \ud83c\udf73 Cooking up something amazing...  \ud83d\udc68\u200d\ud83c\udf73  ### Advanced Nextflow &amp; Professional Development  **Coming Soon:** - Git and GitHub for pipeline version control and collaboration - MTB analysis pipeline development and real-world applications   - Genome assembly workflows and pathogen surveillance - Advanced Nextflow features and optimization techniques - Pipeline deployment strategies for HPC and cloud environments  ---  *This comprehensive module is being carefully crafted to provide you with production-ready bioinformatics skills!*"},{"location":"modules/day8/","title":"Day 8 - Comparative Genomics","text":""},{"location":"modules/day8/#day-8-comparative-genomics","title":"Day 8: Comparative Genomics","text":"<p>Date: September 10, 2025 Duration: 09:00-13:00 CAT Focus: Pan-genome analysis, phylogenetic inference, tree construction and visualization</p>"},{"location":"modules/day8/#overview","title":"Overview","text":"<p>Day 8 focuses on comparative genomics approaches for understanding microbial diversity and evolutionary relationships. We'll explore pangenome analysis to understand core and accessory gene content, and phylogenomic methods to infer evolutionary relationships from genomic data, including SNP-based phylogeny and tree visualization techniques.</p>"},{"location":"modules/day8/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 8, you will be able to:</p> <ul> <li>Understand pangenome concepts and perform core/accessory genome analysis</li> <li>Identify conserved and variable genomic regions across strains</li> <li>Construct phylogenetic trees from core genome SNPs</li> <li>Visualize and interpret phylogenomic relationships</li> <li>Apply comparative genomics to understand pathogen evolution</li> <li>Use tools like Roary, Panaroo, and IQ-TREE for comparative analysis</li> </ul>"},{"location":"modules/day8/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Pangenomics Arash Iranzadeh 10:30 Phylogenomics: Inferring evolutionary relationships from core SNPs Arash Iranzadeh 11:30 Break 12:00 Phylogenomics: Tree construction and visualisation Arash Iranzadeh"},{"location":"modules/day8/#key-topics","title":"Key Topics","text":"<ul> <li>Core vs accessory genome concepts</li> <li>Gene presence/absence analysis</li> <li>Population structure assessment</li> <li>Functional annotation of variable regions</li> </ul>"},{"location":"modules/day8/#2-phylogenetic-analysis","title":"2. Phylogenetic Analysis","text":"<ul> <li>Maximum likelihood methods</li> <li>Bootstrap support assessment</li> <li>Root placement and outgroup selection</li> <li>Molecular clock analysis</li> </ul>"},{"location":"modules/day8/#3-snp-based-analysis","title":"3. SNP-based Analysis","text":"<ul> <li>Core genome SNP calling</li> <li>Recombination detection and removal</li> <li>Distance matrix construction</li> <li>Transmission cluster identification</li> </ul>"},{"location":"modules/day8/#4-outbreak-investigation","title":"4. Outbreak Investigation","text":"<ul> <li>Epidemiological data integration</li> <li>Transmission network inference</li> <li>Source attribution methods</li> <li>Temporal analysis of spread</li> <li></li> </ul>"},{"location":"modules/day8/#1-advanced-pipeline-architecture","title":"1. Advanced Pipeline Architecture","text":"<ul> <li>Multi-sample processing strategies</li> <li>Conditional execution and branching</li> <li>Pipeline modularity and reusability</li> <li>Configuration management across environments</li> </ul>"},{"location":"modules/day8/#2-testing-and-validation","title":"2. Testing and Validation","text":"<ul> <li>Unit testing for individual processes</li> <li>Integration testing for complete workflows</li> <li>Continuous integration setup</li> <li>Regression testing strategies</li> </ul>"},{"location":"modules/day8/#3-performance-optimization","title":"3. Performance Optimization","text":"<ul> <li>Resource allocation strategies</li> <li>Parallelization patterns</li> <li>Caching and resume functionality</li> <li>Profile-based optimization</li> </ul>"},{"location":"modules/day8/#4-production-deployment","title":"4. Production Deployment","text":"<ul> <li>Environment-specific configurations</li> <li>Error handling and retry strategies</li> <li>Logging and monitoring</li> <li>Version control and release management</li> </ul>"},{"location":"modules/day8/#advanced-tools","title":"Advanced Tools","text":""},{"location":"modules/day8/#testing-frameworks","title":"Testing Frameworks","text":"<ul> <li>nf-test - Modern testing framework for Nextflow</li> <li>pytest-workflow - Python-based workflow testing</li> <li>Nextflow Tower - Pipeline monitoring and management</li> </ul>"},{"location":"modules/day8/#development-tools","title":"Development Tools","text":"<ul> <li>nf-core lint - Code quality checking</li> <li>pre-commit hooks - Automated code validation</li> <li>GitHub Actions - Continuous integration</li> <li>Nextflow plugins - Extended functionality</li> </ul>"},{"location":"modules/day8/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day8/#exercise-1-multi-sample-pipeline-75-minutes","title":"Exercise 1: Multi-Sample Pipeline (75 minutes)","text":"<p>Develop a comprehensive pipeline that processes multiple samples in parallel.</p> <pre><code>#!/usr/bin/env nextflow\n\nnextflow.enable.dsl=2\n\n// Parameter definitions with validation\nparams.input_dir = null\nparams.outdir = \"results\"\nparams.reference = null\nparams.min_quality = 20\nparams.threads = 4\n\n// Input validation\nif (!params.input_dir) {\n    error \"Please provide input directory with --input_dir\"\n}\nif (!params.reference) {\n    error \"Please provide reference genome with --reference\"\n}\n\n// Include modules from separate files\ninclude { FASTQC } from './modules/fastqc.nf'\ninclude { TRIMMOMATIC } from './modules/trimmomatic.nf'\ninclude { BWA_MEM } from './modules/bwa.nf'\ninclude { VARIANT_CALLING } from './modules/variants.nf'\ninclude { MULTIQC } from './modules/multiqc.nf'\n\nworkflow VARIANT_ANALYSIS {\n    take:\n    reads_ch\n    reference\n\n    main:\n    // Quality control\n    FASTQC(reads_ch)\n\n    // Read trimming\n    TRIMMOMATIC(reads_ch)\n\n    // Alignment\n    BWA_MEM(TRIMMOMATIC.out.trimmed, reference)\n\n    // Variant calling\n    VARIANT_CALLING(BWA_MEM.out.bam, reference)\n\n    // Aggregate QC\n    qc_files = FASTQC.out.html.mix(TRIMMOMATIC.out.log).collect()\n    MULTIQC(qc_files)\n\n    emit:\n    variants = VARIANT_CALLING.out.vcf\n    reports = MULTIQC.out.html\n}\n\n// Main workflow\nworkflow {\n    // Create channel from input directory\n    reads_ch = Channel\n        .fromFilePairs(\"${params.input_dir}/*_R{1,2}_001.fastq.gz\")\n        .ifEmpty { error \"No input files found in ${params.input_dir}\" }\n\n    // Load reference\n    reference_ch = Channel.fromPath(params.reference, checkIfExists: true)\n\n    // Run analysis\n    VARIANT_ANALYSIS(reads_ch, reference_ch)\n\n    // Output summary\n    VARIANT_ANALYSIS.out.variants.view { \"Variants called for: ${it[0]}\" }\n}\n\nworkflow.onComplete {\n    log.info \"\"\"\n    Pipeline completed at: ${workflow.complete}\n    Duration: ${workflow.duration}\n    Success: ${workflow.success}\n    Results: ${params.outdir}\n    \"\"\".stripIndent()\n}\n</code></pre>"},{"location":"modules/day8/#exercise-2-pipeline-testing-60-minutes","title":"Exercise 2: Pipeline Testing (60 minutes)","text":"<p>Implement comprehensive testing for the variant calling pipeline.</p> <pre><code># .github/workflows/test.yml\nname: Pipeline Testing\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        nextflow: ['23.04.0', 'latest']\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Setup Nextflow\n      uses: nf-core/setup-nextflow@v1\n      with:\n        version: ${{ matrix.nextflow }}\n\n    - name: Setup Test Data\n      run: |\n        mkdir test_data\n        wget -O test_data/sample_R1.fastq.gz https://example.com/test_R1.fastq.gz\n        wget -O test_data/sample_R2.fastq.gz https://example.com/test_R2.fastq.gz\n        wget -O test_data/reference.fasta https://example.com/reference.fasta\n\n    - name: Run Pipeline Tests\n      run: |\n        nextflow run main.nf \\\n          --input_dir test_data \\\n          --reference test_data/reference.fasta \\\n          --outdir test_results \\\n          -profile test,docker\n\n    - name: Validate Outputs\n      run: |\n        # Check if expected output files exist\n        test -f test_results/variants/*.vcf\n        test -f test_results/multiqc/multiqc_report.html\n\n        # Validate variant file format\n        bcftools view test_results/variants/*.vcf | head -20\n</code></pre>"},{"location":"modules/day8/#exercise-3-performance-optimization-45-minutes","title":"Exercise 3: Performance Optimization (45 minutes)","text":"<p>Optimize pipeline performance through profiling and resource tuning.</p> <pre><code>// Performance optimized configuration\nprocess {\n    // Default resources\n    cpus = 2\n    memory = 4.GB\n    time = '1.hour'\n\n    // Process-specific optimization\n    withName: BWA_MEM {\n        cpus = { 8 * task.attempt }\n        memory = { 16.GB * task.attempt }\n        time = { 4.hour * task.attempt }\n        errorStrategy = 'retry'\n        maxRetries = 3\n    }\n\n    withName: VARIANT_CALLING {\n        cpus = 4\n        memory = { 8.GB + (2.GB * task.attempt) }\n        time = { 2.hour * task.attempt }\n\n        // Use faster local storage when available\n        scratch = '/tmp'\n    }\n\n    withName: FASTQC {\n        // Lightweight process can use minimal resources\n        cpus = 1\n        memory = 2.GB\n        time = 30.min\n    }\n}\n\n// Profile-specific optimizations\nprofiles {\n    standard {\n        process.executor = 'local'\n        process.cpus = 2\n        process.memory = '4 GB'\n    }\n\n    hpc {\n        process.executor = 'slurm'\n        process.queue = 'compute'\n        process.clusterOptions = '--account=genomics --qos=normal'\n\n        // Optimize for cluster environment\n        process {\n            withName: BWA_MEM {\n                cpus = 16\n                memory = 32.GB\n                time = 2.hour\n            }\n        }\n    }\n\n    cloud {\n        process.executor = 'awsbatch'\n        aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n        aws.region = 'us-west-2'\n\n        // Cloud-specific optimizations\n        process {\n            withName: '.*' {\n                container = 'your-ecr-repo/pipeline:latest'\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"modules/day8/#advanced-concepts","title":"Advanced Concepts","text":""},{"location":"modules/day8/#error-handling-strategies","title":"Error Handling Strategies","text":"<pre><code>process ROBUST_ASSEMBLY {\n    errorStrategy 'retry'\n    maxRetries 3\n\n    // Dynamic resource allocation\n    memory { 8.GB * task.attempt }\n    cpus { 4 * task.attempt }\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_assembly.fasta\"), emit: assembly\n    tuple val(sample_id), path(\"${sample_id}_assembly.log\"), emit: log\n\n    script:\n    \"\"\"\n    # Log system information for debugging\n    echo \"Hostname: \\$(hostname)\" &gt; ${sample_id}_assembly.log\n    echo \"Memory: ${task.memory}\" &gt;&gt; ${sample_id}_assembly.log\n    echo \"CPUs: ${task.cpus}\" &gt;&gt; ${sample_id}_assembly.log\n    echo \"Attempt: ${task.attempt}\" &gt;&gt; ${sample_id}_assembly.log\n\n    # Run assembly with error checking\n    spades.py --careful -1 ${reads[0]} -2 ${reads[1]} \\\n        -o spades_out --threads ${task.cpus} --memory ${task.memory.toGiga()} \\\n        2&gt;&amp;1 | tee -a ${sample_id}_assembly.log\n\n    if [ ! -f spades_out/scaffolds.fasta ]; then\n        echo \"ERROR: Assembly failed\" &gt;&gt; ${sample_id}_assembly.log\n        exit 1\n    fi\n\n    cp spades_out/scaffolds.fasta ${sample_id}_assembly.fasta\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day8/#conditional-workflows","title":"Conditional Workflows","text":"<pre><code>workflow ADAPTIVE_ANALYSIS {\n    take:\n    samples\n\n    main:\n    // Initial QC\n    FASTQC(samples)\n\n    // Conditional trimming based on quality\n    samples\n        .join(FASTQC.out.stats)\n        .branch { sample_id, reads, qc_stats -&gt;\n            high_quality: qc_stats.mean_quality &gt; 30\n            needs_trimming: qc_stats.mean_quality &lt;= 30\n        }\n        .set { qc_branched }\n\n    // Process high quality samples directly\n    high_qual_samples = qc_branched.high_quality.map { sample_id, reads, stats -&gt; [sample_id, reads] }\n\n    // Trim lower quality samples\n    TRIMMOMATIC(qc_branched.needs_trimming.map { sample_id, reads, stats -&gt; [sample_id, reads] })\n\n    // Combine processed samples\n    all_samples = high_qual_samples.mix(TRIMMOMATIC.out.trimmed)\n\n    // Continue with assembly\n    SPADES(all_samples)\n\n    emit:\n    assemblies = SPADES.out.assembly\n}\n</code></pre>"},{"location":"modules/day8/#best-practices","title":"Best Practices","text":""},{"location":"modules/day8/#1-code-organization","title":"1. Code Organization","text":"<ul> <li>Use modules for reusable processes</li> <li>Implement clear naming conventions</li> <li>Document complex logic</li> <li>Version control all configurations</li> </ul>"},{"location":"modules/day8/#2-resource-management","title":"2. Resource Management","text":"<pre><code>// Dynamic resource allocation\nprocess {\n    withLabel: 'high_memory' {\n        memory = { 32.GB * task.attempt }\n        errorStrategy = 'retry'\n        maxRetries = 2\n    }\n\n    withLabel: 'cpu_intensive' {\n        cpus = { Math.min(16, task.attempt * 4) }\n        time = { 4.hour * task.attempt }\n    }\n}\n</code></pre>"},{"location":"modules/day8/#3-monitoring-and-debugging","title":"3. Monitoring and Debugging","text":"<pre><code>// Enable comprehensive reporting\ntrace {\n    enabled = true\n    file = \"${params.outdir}/trace.txt\"\n    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes,vol_ctxt,inv_ctxt'\n}\n\nreport {\n    enabled = true\n    file = \"${params.outdir}/report.html\"\n}\n\ntimeline {\n    enabled = true\n    file = \"${params.outdir}/timeline.html\"\n}\n\ndag {\n    enabled = true\n    file = \"${params.outdir}/dag.html\"\n}\n</code></pre>"},{"location":"modules/day8/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day8/#individual-projects","title":"Individual Projects","text":"<ul> <li>Optimize existing pipeline for specific use case</li> <li>Implement comprehensive error handling</li> <li>Create test suite for pipeline validation</li> <li>Deploy pipeline to different computing environment</li> </ul>"},{"location":"modules/day8/#group-collaboration","title":"Group Collaboration","text":"<ul> <li>Code review session for pipeline improvements</li> <li>Troubleshooting complex pipeline failures</li> <li>Sharing optimization strategies</li> <li>Planning production deployment</li> </ul>"},{"location":"modules/day8/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day8/#memory-management","title":"Memory Management","text":"<pre><code>// Handle large datasets efficiently\nprocess LARGE_DATA_PROCESSING {\n    memory { task.attempt &lt; 3 ? 16.GB : 32.GB }\n    time { 2.hour * task.attempt }\n    errorStrategy 'retry'\n    maxRetries 3\n\n    // Use streaming where possible\n    script:\n    \"\"\"\n    # Process data in chunks to manage memory\n    split -l 1000000 ${large_input} chunk_\n\n    for chunk in chunk_*; do\n        process_chunk.py \\$chunk &gt;&gt; results.txt\n        rm \\$chunk  # Clean up as we go\n    done\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day8/#workflow-resume-issues","title":"Workflow Resume Issues","text":"<pre><code># Best practices for resumable workflows\nnextflow run pipeline.nf -resume -with-report report.html\n\n# Clean resume when needed\nnextflow clean -f\nrm -rf work/\n</code></pre>"},{"location":"modules/day8/#container-compatibility","title":"Container Compatibility","text":"<pre><code>process {\n    withName: PROBLEMATIC_TOOL {\n        container = 'custom/fixed-tool:v2.0'\n        containerOptions = '--user root --privileged'\n    }\n}\n</code></pre>"},{"location":"modules/day8/#production-deployment","title":"Production Deployment","text":""},{"location":"modules/day8/#environment-setup","title":"Environment Setup","text":"<pre><code># production.config\nprocess {\n    executor = 'slurm'\n    queue = 'production'\n\n    // Production-level resource allocation\n    cpus = 16\n    memory = '64 GB'\n    time = '12 hours'\n\n    // Enhanced error handling\n    errorStrategy = 'terminate'  // Fail fast in production\n    maxRetries = 1\n}\n\n// Enable comprehensive logging\ntrace.enabled = true\nreport.enabled = true\ntimeline.enabled = true\n</code></pre>"},{"location":"modules/day8/#monitoring-setup","title":"Monitoring Setup","text":"<pre><code># Set up pipeline monitoring\nnextflow run pipeline.nf -with-tower -profile production\n\n# Custom monitoring hooks\nnextflow run pipeline.nf \\\n  --hook-url https://monitoring.example.com/webhook \\\n  --notify-on-completion \\\n  --notify-on-failure\n</code></pre>"},{"location":"modules/day8/#resources","title":"Resources","text":""},{"location":"modules/day8/#documentation","title":"Documentation","text":"<ul> <li>Nextflow Patterns</li> <li>nf-core Developer Guide</li> <li>Nextflow Tower Documentation</li> </ul>"},{"location":"modules/day8/#testing-tools","title":"Testing Tools","text":"<ul> <li>nf-test</li> <li>pytest-workflow</li> <li>Nextflow CI/CD Examples</li> </ul>"},{"location":"modules/day8/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Nextflow Performance Tips</li> <li>Resource Requirements Guide</li> </ul>"},{"location":"modules/day8/#looking-ahead","title":"Looking Ahead","text":"<p>Day 9 Preview: Bring Your Own Data session including: - Applying learned skills to participant datasets - Troubleshooting real-world challenges - Customizing pipelines for specific needs - Preparing for independent analysis</p> <p>Key Learning Outcome: Advanced Nextflow development enables creation of production-ready, scalable bioinformatics pipelines that can handle complex datasets across diverse computing environments while maintaining reproducibility and reliability.</p>"},{"location":"modules/day9/","title":"Day 9 - Bring your own data","text":""},{"location":"modules/day9/#day-9-bring-your-own-data","title":"Day 9: Bring your own data","text":"<p>Date: September 11, 2025 Duration: 09:00-13:00 CAT Focus: Mobile genetic elements in AMR, independent analysis of participant datasets</p>"},{"location":"modules/day9/#overview","title":"Overview","text":"<p>Day 9 begins with understanding the role of mobile genetic elements in AMR spread, then transitions to hands-on application of all techniques learned throughout the course. Participants will analyze their own datasets with guidance from trainers, troubleshoot real-world challenges, and develop customized analysis approaches for their specific research questions.</p>"},{"location":"modules/day9/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 9, you will be able to:</p> <ul> <li>Understand the role of plasmids, integrons, and transposons in AMR dissemination</li> <li>Apply learned bioinformatics techniques to your own research data</li> <li>Troubleshoot common issues encountered in real-world analyses</li> <li>Adapt standard protocols to meet specific research requirements</li> <li>Develop analysis strategies for novel research questions</li> <li>Integrate multiple analysis approaches into comprehensive workflows</li> <li>Plan sustainable bioinformatics practices for ongoing research</li> </ul>"},{"location":"modules/day9/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Role of plasmids, integrons, and transposons in AMR spread Ephifania Geza 10:30 Participants to analyse their own data All trainers 11:30 Break 12:00 Participants to analyse their own data All trainers"},{"location":"modules/day9/#session-structure","title":"Session Structure","text":""},{"location":"modules/day9/#mobile-genetic-elements-session-0900-1030","title":"Mobile Genetic Elements Session (09:00-10:30)","text":"<ul> <li>Understanding plasmids and their role in horizontal gene transfer</li> <li>Integrons and gene cassette systems</li> <li>Transposons and insertion sequences</li> <li>Tools for mobile element detection and analysis</li> </ul>"},{"location":"modules/day9/#opening-data-analysis-session-1030-1045","title":"Opening Data Analysis Session (10:30-10:45)","text":"<ul> <li>Brief recap of key course concepts</li> <li>Overview of available support and resources</li> <li>Formation of analysis groups based on data types</li> <li>Technical setup verification</li> </ul>"},{"location":"modules/day9/#individual-analysis-time-0915-1130","title":"Individual Analysis Time (09:15-11:30)","text":"<ul> <li>Independent work on participant datasets</li> <li>One-on-one consultation with trainers</li> <li>Peer collaboration and knowledge sharing</li> <li>Documentation of analysis approaches</li> </ul>"},{"location":"modules/day9/#progress-sharing-1145-1200","title":"Progress Sharing (11:45-12:00)","text":"<ul> <li>Brief presentations of initial findings</li> <li>Discussion of challenges encountered</li> <li>Sharing of successful analysis strategies</li> </ul>"},{"location":"modules/day9/#advanced-analysis-1200-1300","title":"Advanced Analysis (12:00-13:00)","text":"<ul> <li>Continued independent work</li> <li>Focus on complex analyses or troubleshooting</li> <li>Preparation for Day 10 presentations</li> <li>Final consultations with trainers</li> </ul>"},{"location":"modules/day9/#data-types-and-analysis-approaches","title":"Data Types and Analysis Approaches","text":""},{"location":"modules/day9/#genomic-data","title":"Genomic Data","text":"<p>Common analyses for participants with genomic datasets:</p> <ul> <li>Quality control and preprocessing</li> <li>FastQC assessment and interpretation</li> <li>Adapter trimming and quality filtering</li> <li> <p>Species identification and contamination detection</p> </li> <li> <p>Genome assembly and annotation</p> </li> <li>De novo assembly optimization</li> <li>Assembly quality assessment</li> <li> <p>Functional annotation and gene prediction</p> </li> <li> <p>Comparative genomics</p> </li> <li>MLST and serotyping</li> <li>Antimicrobial resistance gene detection</li> <li>Phylogenetic analysis and clustering</li> </ul>"},{"location":"modules/day9/#metagenomic-data","title":"Metagenomic Data","text":"<p>For participants with microbiome or metagenomic samples:</p> <ul> <li>Community profiling</li> <li>Taxonomic classification</li> <li>Abundance estimation and normalization</li> <li> <p>Diversity analysis (alpha and beta)</p> </li> <li> <p>Functional analysis</p> </li> <li>Pathway reconstruction</li> <li>Antimicrobial resistance profiling</li> <li> <p>Metabolic potential assessment</p> </li> <li> <p>Clinical applications</p> </li> <li>Pathogen detection in complex samples</li> <li>Co-infection analysis</li> <li>Treatment response monitoring</li> </ul>"},{"location":"modules/day9/#outbreak-investigation-data","title":"Outbreak Investigation Data","text":"<p>For epidemiological and outbreak datasets:</p> <ul> <li>Transmission analysis</li> <li>SNP-based clustering</li> <li>Phylogenetic reconstruction</li> <li> <p>Temporal and geographic analysis</p> </li> <li> <p>Resistance surveillance</p> </li> <li>Multi-drug resistance patterns</li> <li>Resistance gene distribution</li> <li>Treatment outcome correlations</li> </ul>"},{"location":"modules/day9/#technical-support-available","title":"Technical Support Available","text":""},{"location":"modules/day9/#computational-resources","title":"Computational Resources","text":"<ul> <li>Access to high-performance computing cluster</li> <li>Pre-installed bioinformatics software environments</li> <li>Container images for reproducible analysis</li> <li>Shared storage for large datasets</li> </ul>"},{"location":"modules/day9/#analysis-pipelines","title":"Analysis Pipelines","text":"<ul> <li>Nextflow workflows developed during the course</li> <li>Customizable analysis templates</li> <li>Pre-configured environment profiles</li> <li>Automated reporting tools</li> </ul>"},{"location":"modules/day9/#expert-guidance","title":"Expert Guidance","text":"<p>Trainer specializations available:</p> Trainer Expertise Areas Ephifania Geza Genomic surveillance, AMR analysis, metagenomics, clinical applications Arash Iranzadeh Phylogenomics, comparative genomics, outbreak investigation Sindiswa Lukhele Sequencing technologies, quality control, species identification Mamana Mbiyavanga Workflow development, HPC systems, pipeline optimization"},{"location":"modules/day9/#common-analysis-workflows","title":"Common Analysis Workflows","text":""},{"location":"modules/day9/#genomic-surveillance-workflow","title":"Genomic Surveillance Workflow","text":"<pre><code># 1. Initial data assessment\nfastqc raw_data/*.fastq.gz\nmultiqc fastqc_results/\n\n# 2. Species identification\nkraken2 --db minikraken2_v2 --paired sample_R1.fastq sample_R2.fastq\n\n# 3. Quality trimming\ntrimmomatic PE sample_R1.fastq sample_R2.fastq \\\n    sample_R1_trimmed.fastq sample_R1_unpaired.fastq \\\n    sample_R2_trimmed.fastq sample_R2_unpaired.fastq \\\n    SLIDINGWINDOW:4:20 MINLEN:50\n\n# 4. Assembly\nspades.py -1 sample_R1_trimmed.fastq -2 sample_R2_trimmed.fastq -o assembly/\n\n# 5. Assembly quality assessment\nquast.py assembly/scaffolds.fasta -o quast_results/\n\n# 6. Annotation\nprokka assembly/scaffolds.fasta --outdir annotation/ --prefix sample\n</code></pre>"},{"location":"modules/day9/#metagenomic-analysis-workflow","title":"Metagenomic Analysis Workflow","text":"<pre><code># 1. Host DNA removal (if applicable)\nkneaddata --input sample_R1.fastq --input sample_R2.fastq \\\n    --reference-db human_genome --output cleaned_data/\n\n# 2. Taxonomic profiling\nmetaphlan cleaned_data/sample_paired_1.fastq,cleaned_data/sample_paired_2.fastq \\\n    --bowtie2out sample.bowtie2.bz2 --nproc 4 --input_type fastq \\\n    --output_file sample_profile.txt\n\n# 3. Functional profiling\nhumann --input cleaned_data/sample_paired.fastq \\\n    --output functional_analysis/ --nucleotide-database chocophlan \\\n    --protein-database uniref90\n\n# 4. Diversity analysis in R\nRscript diversity_analysis.R sample_profile.txt metadata.csv\n</code></pre>"},{"location":"modules/day9/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"modules/day9/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"modules/day9/#low-quality-data","title":"Low-Quality Data","text":"<pre><code># Check read quality distribution\nfastqc *.fastq.gz\n\n# Aggressive quality trimming if needed\ntrimmomatic PE input_R1.fastq input_R2.fastq \\\n    output_R1.fastq unpaired_R1.fastq \\\n    output_R2.fastq unpaired_R2.fastq \\\n    SLIDINGWINDOW:4:25 LEADING:20 TRAILING:20 MINLEN:75\n\n# Consider different assembly strategies\n# For poor quality data, try more conservative parameters\nspades.py --careful --cov-cutoff auto -1 R1.fastq -2 R2.fastq -o assembly/\n</code></pre>"},{"location":"modules/day9/#contamination-issues","title":"Contamination Issues","text":"<pre><code># Check for contamination\nkraken2 --db standard --paired sample_R1.fastq sample_R2.fastq \\\n    --report contamination_report.txt\n\n# Remove contaminant sequences\nextract_kraken_reads.py -k sample.kraken -s1 sample_R1.fastq \\\n    -s2 sample_R2.fastq -o clean_R1.fastq -o2 clean_R2.fastq \\\n    --exclude --taxid 9606  # Exclude human reads\n</code></pre>"},{"location":"modules/day9/#assembly-problems","title":"Assembly Problems","text":"<pre><code># If SPAdes fails, try different assemblers\n# Unicycler for hybrid assembly\nunicycler -1 short_R1.fastq -2 short_R2.fastq -l long_reads.fastq -o assembly/\n\n# Or SKESA for quick assembly\nskesa --reads short_R1.fastq,short_R2.fastq --cores 8 &gt; assembly.fasta\n</code></pre>"},{"location":"modules/day9/#memoryresource-issues","title":"Memory/Resource Issues","text":"<pre><code># Monitor resource usage\nhtop\n\n# Reduce memory usage for large datasets\nspades.py --memory 16 -1 R1.fastq -2 R2.fastq -o assembly/\n\n# Use subsampling for initial testing\nseqtk sample -s100 input_R1.fastq 100000 &gt; subset_R1.fastq\nseqtk sample -s100 input_R2.fastq 100000 &gt; subset_R2.fastq\n</code></pre>"},{"location":"modules/day9/#analysis-documentation","title":"Analysis Documentation","text":""},{"location":"modules/day9/#laboratory-notebook-template","title":"Laboratory Notebook Template","text":"<pre><code># Analysis Log: [Your Dataset Name]\n**Date**: [Current Date]\n**Analyst**: [Your Name]\n**Data Source**: [Description of samples]\n\n## Objectives\n- Primary research question\n- Specific analyses planned\n- Expected outcomes\n\n## Data Description\n- Sample type and collection method\n- Sequencing platform and parameters\n- Data quality metrics\n\n## Analysis Steps\n### Step 1: Quality Control\n- Command used: `fastqc *.fastq.gz`\n- Results: [Summary of quality metrics]\n- Decision: [Any quality filtering applied]\n\n### Step 2: [Next Analysis]\n- Command: [Exact command used]\n- Parameters chosen: [Rationale for parameter selection]\n- Results: [Key findings]\n\n## Challenges Encountered\n- Issue: [Description of problem]\n- Solution attempted: [What was tried]\n- Outcome: [Whether resolved]\n\n## Key Findings\n- [Major results from analysis]\n- [Statistical summaries]\n- [Biological interpretations]\n\n## Next Steps\n- Additional analyses needed\n- Questions raised\n- Follow-up experiments\n</code></pre>"},{"location":"modules/day9/#resource-management","title":"Resource Management","text":""},{"location":"modules/day9/#data-organization","title":"Data Organization","text":"<pre><code># Recommended directory structure\nproject_name/\n\u251c\u2500\u2500 raw_data/          # Original sequencing files\n\u251c\u2500\u2500 quality_control/   # QC reports and cleaned data\n\u251c\u2500\u2500 analysis/          # Main analysis outputs\n\u251c\u2500\u2500 scripts/           # Custom scripts and commands\n\u251c\u2500\u2500 results/           # Final results and figures\n\u2514\u2500\u2500 documentation/     # Analysis logs and notes\n</code></pre>"},{"location":"modules/day9/#backup-strategies","title":"Backup Strategies","text":"<pre><code># Regular backup of important results\nrsync -av results/ backup_drive/project_results/\ntar -czf analysis_$(date +%Y%m%d).tar.gz analysis/\n\n# Version control for scripts\ngit init\ngit add scripts/\ngit commit -m \"Initial analysis scripts\"\n</code></pre>"},{"location":"modules/day9/#collaboration-guidelines","title":"Collaboration Guidelines","text":""},{"location":"modules/day9/#peer-support","title":"Peer Support","text":"<ul> <li>Form analysis groups based on similar data types</li> <li>Share successful parameter combinations</li> <li>Collaborate on troubleshooting challenging datasets</li> <li>Review each other's analysis approaches</li> </ul>"},{"location":"modules/day9/#trainer-consultation","title":"Trainer Consultation","text":"<ul> <li>Prepare specific questions about your data</li> <li>Document issues with exact error messages</li> <li>Have your analysis objectives clearly defined</li> <li>Be ready to explain your research context</li> </ul>"},{"location":"modules/day9/#assessment-and-preparation-for-day-10","title":"Assessment and Preparation for Day 10","text":""},{"location":"modules/day9/#presentation-preparation","title":"Presentation Preparation","text":"<p>Participants should prepare a 5-minute presentation covering:</p> <ol> <li>Research Question: What you aimed to investigate</li> <li>Data Overview: Type and source of your dataset</li> <li>Methods Applied: Which course techniques you used</li> <li>Key Results: Main findings from your analysis</li> <li>Challenges: Obstacles encountered and solutions found</li> <li>Future Directions: Next steps for your research</li> </ol>"},{"location":"modules/day9/#technical-documentation","title":"Technical Documentation","text":"<ul> <li>Save all commands used in a script file</li> <li>Document parameter choices and rationale</li> <li>Prepare summary statistics and key figures</li> <li>Note any analysis limitations or assumptions</li> </ul>"},{"location":"modules/day9/#success-metrics","title":"Success Metrics","text":"<p>By the end of Day 9, participants should have:</p> <ul> <li> Successfully processed their own dataset</li> <li> Applied at least 3 different analysis techniques from the course</li> <li> Documented their analysis workflow</li> <li> Identified key findings relevant to their research</li> <li> Prepared materials for Day 10 presentation</li> <li> Established ongoing analysis plan</li> </ul>"},{"location":"modules/day9/#resources-for-continued-learning","title":"Resources for Continued Learning","text":""},{"location":"modules/day9/#online-communities","title":"Online Communities","text":"<ul> <li>Bioinformatics Stack Exchange</li> <li>BioStars Forum</li> <li>Galaxy Community</li> </ul>"},{"location":"modules/day9/#software-documentation","title":"Software Documentation","text":"<ul> <li>Tool-specific manuals and tutorials</li> <li>GitHub repositories for pipeline development</li> <li>Container registries for reproducible environments</li> </ul>"},{"location":"modules/day9/#professional-development","title":"Professional Development","text":"<ul> <li>Local bioinformatics user groups</li> <li>International conferences and workshops</li> <li>Online course platforms for advanced topics</li> </ul>"},{"location":"modules/day9/#looking-ahead","title":"Looking Ahead","text":"<p>Day 10 Preview: Wrap-up session including: - Participant presentations of analysis results - Discussion of lessons learned and best practices - Information about ongoing support resources - Course completion and next steps planning</p> <p>Key Learning Outcome: Independent application of bioinformatics skills to real research data builds confidence and reveals the practical challenges and rewards of computational biology in actual research contexts.</p>"}]}