{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Microbial Genomics &amp; Metagenomics Training","text":"<p>Welcome to the comprehensive training course on Microbial Genomics &amp; Metagenomics for Clinical and Public Health Applications.</p> <ul> <li> <p>:material-school: Course Overview</p> <p>Learn core principles and practical applications of microbial genomics and metagenomics in clinical and public health contexts.</p> <p>:octicons-arrow-right-24: Learn more</p> </li> <li> <p>:material-clock-outline: 10-Day Program</p> <p>Intensive hands-on training covering everything from command line basics to advanced outbreak investigation.</p> <p>:octicons-arrow-right-24: View schedule</p> </li> <li> <p>:material-dna: Real-World Data</p> <p>Work with actual clinical datasets including M. tuberculosis and V. cholerae.</p> <p>:octicons-arrow-right-24: Explore datasets</p> </li> <li> <p>:material-tools: Practical Skills</p> <p>Master bioinformatics tools, workflows, and reproducible analysis techniques used in clinical microbiology.</p> <p>:octicons-arrow-right-24: See objectives</p> </li> </ul>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<p>This course provides hands-on experience with:</p>"},{"location":"#material-microscope-pathogen-analysis","title":":material-microscope: Pathogen Analysis","text":"<ul> <li>Analyze genomic diversity and evolutionary relationships</li> <li>Investigate antimicrobial resistance (AMR) profiles</li> <li>Study mobile genetic elements (MGE) in resistance spread</li> </ul>"},{"location":"#material-network-metagenomics","title":":material-network: Metagenomics","text":"<ul> <li>Explore microbial communities in clinical and environmental samples</li> <li>Apply advanced sequencing analysis techniques</li> <li>Interpret complex metagenomic datasets</li> </ul>"},{"location":"#material-source-branch-reproducible-workflows","title":":material-source-branch: Reproducible Workflows","text":"<ul> <li>Use version control with Git</li> <li>Work with containerized environments</li> <li>Implement Nextflow pipelines for scalable analysis</li> </ul>"},{"location":"#material-chart-line-data-interpretation","title":":material-chart-line: Data Interpretation","text":"<ul> <li>Generate publication-ready visualizations</li> <li>Conduct epidemiological investigations</li> <li>Present findings effectively</li> </ul>"},{"location":"#course-highlights","title":"Course Highlights","text":"<p>Interactive Learning</p> <ul> <li>Hands-on exercises with real datasets</li> <li>Group discussions and case studies  </li> <li>Individual project presentations</li> <li>Expert guest speakers</li> </ul> <p>Technical Requirements</p> <ul> <li>Laptop (Linux/macOS preferred, Git Bash for Windows)</li> <li>Basic Linux command-line knowledge</li> <li>HPC access (recommended)</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to begin your journey in microbial genomics? Start with our setup guide:</p> <p>Get Started :material-arrow-right:</p>"},{"location":"#course-information","title":"Course Information","text":"<p>| Duration | 10 days (September 1-12, 2025) | | Format | Hands-on workshops with lectures | | Level | Intermediate | | Prerequisites | Basic bioinformatics knowledge |</p>"},{"location":"#support","title":"Support","text":"<p>Need help? Check our troubleshooting guide or reach out to the course instructors.</p> <ul> <li> <p>:material-github: Repository</p> <p>Access all course materials, datasets, and workflows on GitHub.</p> <p>:octicons-arrow-right-24: Visit repository</p> </li> <li> <p>:material-book-open: Documentation</p> <p>Comprehensive guides, references, and additional resources.</p> <p>:octicons-arrow-right-24: Browse docs</p> </li> </ul>"},{"location":"datasets/","title":"Training Datasets","text":""},{"location":"datasets/#overview","title":"Overview","text":"<p>This course uses carefully curated datasets representing real-world scenarios in microbial genomics. All data has been quality-controlled and prepared for educational use, focusing on Mycobacterium tuberculosis and Vibrio cholerae collections.</p>"},{"location":"datasets/#dataset-categories","title":"Dataset Categories","text":""},{"location":"datasets/#1-primary-training-datasets","title":"1. Primary Training Datasets","text":""},{"location":"datasets/#mycobacterium-tuberculosis-collection","title":"Mycobacterium tuberculosis Collection","text":"<ul> <li>Sample Size: 20 clinical isolates</li> <li>Geographic Origin: Global collection (South Africa, India, UK, Peru)</li> <li>Drug Resistance: Mixed MDR, XDR, and drug-susceptible strains</li> <li>Lineages: Representatives from Lineages 1-4</li> <li>Sequencing: Illumina paired-end (2\u00d7150bp, 80-120x coverage)</li> <li>Size: ~2.5 GB</li> <li>Use Cases: Drug resistance analysis, lineage typing, phylogenetic analysis</li> </ul>"},{"location":"datasets/#vibrio-cholerae-outbreak-investigation","title":"Vibrio cholerae Outbreak Investigation","text":"<ul> <li>Sample Size: 15 outbreak isolates + 3 environmental samples</li> <li>Source: Simulated cholera outbreak (based on real data)</li> <li>Timeframe: 6-month epidemic period</li> <li>Geographic: Coastal urban setting</li> <li>Sequencing: Illumina paired-end (2\u00d7150bp, 60-100x coverage)</li> <li>Size: ~1.8 GB</li> <li>Use Cases: Outbreak tracking, source attribution, transmission analysis</li> </ul>"},{"location":"datasets/#2-reference-materials","title":"2. Reference Materials","text":""},{"location":"datasets/#reference-genomes","title":"Reference Genomes","text":"<ul> <li>High-quality reference assemblies for M. tuberculosis and V. cholerae</li> <li>Annotation files (GFF, GenBank formats)</li> <li>Resistance gene databases</li> <li>Size: ~500 MB</li> </ul>"},{"location":"datasets/#validation-datasets","title":"Validation Datasets","text":"<ul> <li>Known outbreak collections with confirmed epidemiological links</li> <li>Quality control standards</li> <li>Benchmark datasets for method comparison</li> <li>Size: ~1.5 GB</li> </ul>"},{"location":"datasets/#dataset-access","title":"Dataset Access","text":""},{"location":"datasets/#file-organization","title":"File Organization","text":"<pre><code>datasets/\n\u251c\u2500\u2500 genomics/\n\u2502   \u251c\u2500\u2500 mtb/                    # M. tuberculosis isolates\n\u2502   \u2514\u2500\u2500 vibrio/                 # V. cholerae outbreak\n\u251c\u2500\u2500 references/\n\u2502   \u251c\u2500\u2500 genomes/                # Reference assemblies\n\u2502   \u251c\u2500\u2500 databases/              # Resistance/virulence databases\n\u2502   \u2514\u2500\u2500 annotations/            # Gene annotations\n\u2514\u2500\u2500 validation/\n    \u251c\u2500\u2500 benchmarks/             # Method comparison datasets\n    \u2514\u2500\u2500 qc_standards/           # Quality control references\n</code></pre>"},{"location":"datasets/#download-instructions","title":"Download Instructions","text":""},{"location":"datasets/#during-course","title":"During Course","text":"<p>Data is pre-loaded on course HPC systems: <pre><code># Access course data directory\ncd /data/course/datasets/\n\n# Copy to your workspace\ncp -r /data/course/datasets/ ~/workspace/\n</code></pre></p>"},{"location":"datasets/#post-course-access","title":"Post-Course Access","text":"<p>Datasets remain available through: <pre><code># Clone dataset repository\ngit clone https://github.com/CIDRI-Africa/microbial-genomics-datasets.git\n\n# Download specific collections\nwget https://datasets.microbial-genomics.org/mtb_collection.tar.gz\n</code></pre></p>"},{"location":"datasets/#data-formats","title":"Data Formats","text":""},{"location":"datasets/#raw-sequencing-data","title":"Raw Sequencing Data","text":"<ul> <li>Format: FASTQ (compressed with gzip)</li> <li>Quality: Phred+33 encoding</li> <li>Naming: <code>SampleID_R1.fastq.gz</code>, <code>SampleID_R2.fastq.gz</code></li> </ul>"},{"location":"datasets/#processed-data","title":"Processed Data","text":"<ul> <li>Assemblies: FASTA format</li> <li>Annotations: GFF3, GenBank</li> <li>Alignments: SAM/BAM format</li> <li>Variants: VCF format</li> </ul>"},{"location":"datasets/#metadata","title":"Metadata","text":"<ul> <li>Sample Information: CSV/TSV format</li> <li>Study Design: Detailed README files</li> <li>Quality Metrics: MultiQC reports included</li> </ul>"},{"location":"datasets/#metadata-schema","title":"Metadata Schema","text":""},{"location":"datasets/#genomic-samples","title":"Genomic Samples","text":"Field Description Example sample_id Unique identifier MTB_001 species Organism name Mycobacterium tuberculosis collection_date Sample date 2023-01-15 location Geographic origin Cape Town, South Africa resistance_profile Known resistance INH-R, RIF-R sequencing_platform Technology Illumina MiSeq coverage_depth Average coverage 85x"},{"location":"datasets/#quality-control","title":"Quality Control","text":""},{"location":"datasets/#pre-processing-standards","title":"Pre-processing Standards","text":"<ul> <li>Quality Score: Minimum Q30 for 80% of bases</li> <li>Contamination: &lt;2% non-target DNA</li> <li>Coverage: Minimum 30x for genomic samples</li> <li>Assembly Quality: N50 &gt;50kb, &lt;200 contigs</li> </ul>"},{"location":"datasets/#validation-procedures","title":"Validation Procedures","text":"<ul> <li>Species confirmation by 16S rRNA or genome similarity</li> <li>Contamination screening with multiple tools</li> <li>Assembly quality assessment with standard metrics</li> <li>Metadata validation and consistency checking</li> </ul>"},{"location":"datasets/#ethical-considerations","title":"Ethical Considerations","text":""},{"location":"datasets/#data-privacy","title":"Data Privacy","text":"<ul> <li>All clinical data de-identified according to HIPAA standards</li> <li>Geographic information limited to city/region level</li> <li>No patient identifiers or medical record linkage possible</li> </ul>"},{"location":"datasets/#usage-rights","title":"Usage Rights","text":"<ul> <li>Educational use permitted under Creative Commons License</li> <li>Commercial use requires separate permission</li> <li>Attribution required for publications using these datasets</li> <li>Redistribution allowed with proper citation</li> </ul>"},{"location":"datasets/#responsible-use","title":"Responsible Use","text":"<ul> <li>Data should not be used to identify individuals</li> <li>Results should not be used for clinical decision-making</li> <li>Sharing outside course requires instructor approval</li> </ul>"},{"location":"datasets/#dataset-specific-notes","title":"Dataset-Specific Notes","text":""},{"location":"datasets/#m-tuberculosis-collection","title":"M. tuberculosis Collection","text":"<ul> <li>Lineage assignments based on SNP typing</li> <li>Drug resistance confirmed by phenotypic testing</li> <li>Geographic sampling represents global diversity</li> <li>Suitable for phylogeographic analysis</li> </ul>"},{"location":"datasets/#v-cholerae-outbreak","title":"V. cholerae Outbreak","text":"<ul> <li>Temporal sampling allows transmission inference</li> <li>Environmental samples included for source attribution</li> <li>Metadata includes case demographics and exposure history</li> <li>Excellent dataset for outbreak investigation training</li> </ul>"},{"location":"datasets/#troubleshooting","title":"Troubleshooting","text":""},{"location":"datasets/#common-issues","title":"Common Issues","text":""},{"location":"datasets/#file-access-problems","title":"File Access Problems","text":"<pre><code># Check file permissions\nls -la datasets/\n# Fix permissions if needed\nchmod -R 755 datasets/\n</code></pre>"},{"location":"datasets/#corrupted-downloads","title":"Corrupted Downloads","text":"<pre><code># Verify file integrity\nmd5sum -c checksums.md5\n# Re-download corrupted files\nwget -c https://datasets.url/file.tar.gz\n</code></pre>"},{"location":"datasets/#storage-space-issues","title":"Storage Space Issues","text":"<pre><code># Check available space\ndf -h\n# Compress unused files\ngzip *.fastq\n# Remove temporary files\nrm -rf temp/\n</code></pre>"},{"location":"datasets/#citation-information","title":"Citation Information","text":"<p>When using these datasets in publications, please cite:</p> <p>CIDRI-Africa Microbial Genomics Training Consortium. (2024).  Comprehensive training datasets for microbial genomics and metagenomics education.  Microbial Genomics Education, Dataset Repository.</p> <p>Individual dataset citations available in respective README files.</p>"},{"location":"datasets/#support","title":"Support","text":"<p>For dataset-related questions: - Technical Issues: Submit issue on GitHub repository - Scientific Questions: Contact course instructors - Access Problems: Email dataset-admin@cidri-africa.org</p>"},{"location":"datasets/#updates-and-versioning","title":"Updates and Versioning","text":"<ul> <li>Current Version: v2.1 (September 2025)</li> <li>Update Frequency: Annually or as needed</li> <li>Change Log: Available in repository documentation</li> <li>Notification: Users notified of major updates via email</li> </ul> <p>Remember: These datasets represent real scientific data and should be treated with appropriate care and respect for the original sample donors and research contexts.</p>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":""},{"location":"troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"troubleshooting/#setup-and-installation-issues","title":"Setup and Installation Issues","text":""},{"location":"troubleshooting/#git-configuration-problems","title":"Git Configuration Problems","text":"<p>Problem: Git not configured properly <pre><code># Check current configuration\ngit config --list\n\n# Error: Please tell me who you are\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre></p> <p>Problem: SSH key authentication fails <pre><code># Generate new SSH key\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Add to SSH agent\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\n# Test connection\nssh -T git@github.com\n</code></pre></p>"},{"location":"troubleshooting/#permission-denied-errors","title":"Permission Denied Errors","text":"<p>Problem: Cannot execute scripts or access files <pre><code># Fix script permissions\nchmod +x script.sh\n\n# Fix directory permissions\nchmod 755 directory/\nchmod -R 755 directory/  # Recursive\n\n# Fix SSH key permissions\nchmod 600 ~/.ssh/id_ed25519\nchmod 644 ~/.ssh/id_ed25519.pub\n</code></pre></p>"},{"location":"troubleshooting/#data-analysis-issues","title":"Data Analysis Issues","text":""},{"location":"troubleshooting/#fastqc-problems","title":"FastQC Problems","text":"<p>Problem: FastQC fails to run <pre><code># Check Java installation\njava -version\n\n# Install Java if missing (Ubuntu/Debian)\nsudo apt install default-jdk\n\n# Run FastQC with memory limit\nfastqc --memory 4096 *.fastq.gz\n</code></pre></p> <p>Problem: Out of memory errors <pre><code># Increase memory allocation\nfastqc --memory 8192 file.fastq.gz\n\n# Process files individually\nfor file in *.fastq.gz; do\n    fastqc --memory 4096 \"$file\"\ndone\n</code></pre></p>"},{"location":"troubleshooting/#assembly-issues","title":"Assembly Issues","text":"<p>Problem: SPAdes assembly fails <pre><code># Check available memory\nfree -h\n\n# Run with memory limit\nspades.py --memory 32 -1 R1.fastq.gz -2 R2.fastq.gz -o output/\n\n# Try different k-mer sizes\nspades.py -k 21,33,55 -1 R1.fastq.gz -2 R2.fastq.gz -o output/\n</code></pre></p> <p>Problem: Poor assembly quality (high fragmentation) <pre><code># Check input data quality first\nfastqc input_files.fastq.gz\n\n# Try more aggressive trimming\ntrimmomatic PE input_R1.fastq.gz input_R2.fastq.gz \\\n    output_R1.fastq.gz output_R1_unpaired.fastq.gz \\\n    output_R2.fastq.gz output_R2_unpaired.fastq.gz \\\n    LEADING:10 TRAILING:10 SLIDINGWINDOW:4:20 MINLEN:50\n\n# Use careful mode in SPAdes\nspades.py --careful -1 trimmed_R1.fastq.gz -2 trimmed_R2.fastq.gz -o careful_assembly/\n</code></pre></p>"},{"location":"troubleshooting/#tool-installation-and-dependencies","title":"Tool Installation and Dependencies","text":""},{"location":"troubleshooting/#condamamba-issues","title":"Conda/Mamba Issues","text":"<p>Problem: Environment creation fails <pre><code># Update conda\nconda update conda\n\n# Clear package cache\nconda clean --all\n\n# Create environment with specific Python version\nconda create -n genomics python=3.9\n\n# Use mamba for faster solving\nmamba create -n genomics python=3.9\n</code></pre></p> <p>Problem: Package conflicts <pre><code># Create minimal environment first\nconda create -n clean_env python=3.9\n\n# Activate and install packages one by one\nconda activate clean_env\nconda install -c bioconda fastqc\nconda install -c bioconda spades\n</code></pre></p>"},{"location":"troubleshooting/#dockersingularity-issues","title":"Docker/Singularity Issues","text":"<p>Problem: Permission denied with Docker <pre><code># Add user to docker group\nsudo usermod -aG docker $USER\n\n# Log out and back in, then test\ndocker run hello-world\n</code></pre></p> <p>Problem: Singularity image won't run <pre><code># Pull image explicitly\nsingularity pull docker://biocontainers/fastqc:v0.11.9_cv8\n\n# Run with specific bind paths\nsingularity exec -B /data:/data image.sif fastqc --version\n\n# Check image integrity\nsingularity verify image.sif\n</code></pre></p>"},{"location":"troubleshooting/#hpc-and-remote-access-issues","title":"HPC and Remote Access Issues","text":""},{"location":"troubleshooting/#ssh-connection-problems","title":"SSH Connection Problems","text":"<p>Problem: Connection timed out <pre><code># Test basic connectivity\nping hostname\n\n# Try different port\nssh -p 2222 username@hostname\n\n# Use verbose mode for debugging\nssh -v username@hostname\n</code></pre></p> <p>Problem: Key exchange failed <pre><code># Generate compatible key\nssh-keygen -t rsa -b 4096\n\n# Specify key explicitly\nssh -i ~/.ssh/specific_key username@hostname\n\n# Check SSH config\ncat ~/.ssh/config\n</code></pre></p>"},{"location":"troubleshooting/#slurm-job-issues","title":"SLURM Job Issues","text":"<p>Problem: Job stuck in queue <pre><code># Check queue status\nsqueue -u $USER\n\n# Check job details\nscontrol show job JOBID\n\n# Check partition availability\nsinfo\n</code></pre></p> <p>Problem: Job fails with memory errors <pre><code># Check job output\ncat slurm-JOBID.out\n\n# Increase memory request\n#SBATCH --mem=32G\n\n# Use multiple cores if available\n#SBATCH --cpus-per-task=8\n</code></pre></p>"},{"location":"troubleshooting/#data-processing-errors","title":"Data Processing Errors","text":""},{"location":"troubleshooting/#file-format-issues","title":"File Format Issues","text":"<p>Problem: Unexpected file format <pre><code># Check file type\nfile filename\nhead filename\n\n# Convert line endings if needed\ndos2unix filename\n\n# Check compression\ngunzip -t file.gz\n</code></pre></p> <p>Problem: Corrupt or truncated files <pre><code># Check file integrity\nmd5sum file.fastq.gz\n# Compare with provided checksum\n\n# Test gzip integrity\ngunzip -t file.fastq.gz\n\n# Repair if possible (may lose data)\ngzip -d file.fastq.gz\ngzip file.fastq\n</code></pre></p>"},{"location":"troubleshooting/#large-file-handling","title":"Large File Handling","text":"<p>Problem: Running out of disk space <pre><code># Check disk usage\ndf -h\ndu -sh directory/\n\n# Clean up temporary files\nrm -rf temp/\nrm *.tmp\n\n# Compress large files\ngzip *.fastq\ntar -czf archive.tar.gz directory/\n</code></pre></p> <p>Problem: Processing very large files <pre><code># Process in chunks\nsplit -l 4000000 large_file.fastq chunk_\n# Process each chunk separately\n\n# Use streaming where possible\nzcat file.fastq.gz | head -n 1000000 | tool\n\n# Use efficient tools\nseqtk sample file.fastq.gz 10000 &gt; sample.fastq\n</code></pre></p>"},{"location":"troubleshooting/#analysis-and-interpretation-issues","title":"Analysis and Interpretation Issues","text":""},{"location":"troubleshooting/#resistance-gene-detection","title":"Resistance Gene Detection","text":"<p>Problem: No resistance genes found (expected some) <pre><code># Check assembly quality\nquast.py assembly.fasta\n\n# Try multiple databases\nabricate --db resfinder assembly.fasta\nabricate --db card assembly.fasta\nabricate --db argannot assembly.fasta\n\n# Reduce stringency\nabricate --minid 80 --mincov 60 assembly.fasta\n</code></pre></p> <p>Problem: Too many false positives <pre><code># Increase stringency\nabricate --minid 95 --mincov 90 assembly.fasta\n\n# Verify hits manually\nblast -query resistance_gene.fasta -subject assembly.fasta\n\n# Check for truncated genes\nabricate --mincov 95 assembly.fasta\n</code></pre></p>"},{"location":"troubleshooting/#phylogenetic-analysis","title":"Phylogenetic Analysis","text":"<p>Problem: Tree looks wrong or unrealistic <pre><code># Check sequence alignment quality\naliview alignment.fasta\n\n# Remove problematic sequences\nseqtk subseq sequences.fasta good_ids.txt &gt; clean.fasta\n\n# Try different tree method\nFastTree -nt alignment.fasta &gt; tree.newick\niqtree -s alignment.fasta -m TEST\n</code></pre></p> <p>Problem: Low bootstrap support <pre><code># Increase bootstrap replicates\niqtree -s alignment.fasta -bb 1000\n\n# Check for recombination\ngubbins alignment.fasta\n\n# Use only core SNPs\nsnp-sites -c alignment.fasta &gt; core_snps.fasta\n</code></pre></p>"},{"location":"troubleshooting/#performance-and-resource-issues","title":"Performance and Resource Issues","text":""},{"location":"troubleshooting/#memory-management","title":"Memory Management","text":"<p>Problem: Out of memory errors <pre><code># Check memory usage\nfree -h\ntop\n\n# Limit memory usage\nulimit -v 8000000  # Limit to ~8GB\n\n# Use memory-efficient tools\nminimap2 instead of BWA-MEM for large references\n</code></pre></p> <p>Problem: Process running too slowly <pre><code># Use multiple cores\ntool -t 8 input output\n\n# Optimize I/O\n# Use local storage instead of network drives\ncp data /tmp/\ncd /tmp/\n# Run analysis\ncp results back/to/network/storage\n</code></pre></p>"},{"location":"troubleshooting/#storage-management","title":"Storage Management","text":"<p>Problem: Quota exceeded <pre><code># Find large files\nfind . -size +100M -ls\n\n# Clean up intermediate files\nrm *.sam  # Keep only BAM files\nrm temp_*\n\n# Compress old data\ntar -czf old_analysis.tar.gz old_directory/\nrm -rf old_directory/\n</code></pre></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#before-asking-for-help","title":"Before Asking for Help","text":"<ol> <li>Check error messages carefully - Often contain specific solutions</li> <li>Search documentation - Tool manuals usually have troubleshooting sections</li> <li>Try simple test cases - Use small datasets to isolate problems</li> <li>Check system resources - Memory, disk space, permissions</li> </ol>"},{"location":"troubleshooting/#how-to-ask-for-help","title":"How to Ask for Help","text":""},{"location":"troubleshooting/#include-essential-information","title":"Include Essential Information","text":"<ul> <li>Exact error message (copy-paste, don't retype)</li> <li>Command that failed (exact command with parameters)</li> <li>System information (OS, tool versions)</li> <li>Input file details (size, format, sample content)</li> </ul>"},{"location":"troubleshooting/#good-help-request-example","title":"Good Help Request Example","text":"<pre><code>Subject: SPAdes assembly fails with error code 1\n\nI'm running SPAdes on paired-end M. tuberculosis data:\nCommand: spades.py -1 sample_R1.fastq.gz -2 sample_R2.fastq.gz -o spades_out/\n\nError message:\n\"== Error ==  system call for: ['/usr/bin/python3', '/opt/spades/bin/spades_init.py'] finished abnormally, err code: 1\"\n\nSystem: Ubuntu 20.04, SPAdes v3.15.3\nInput files: 2x150bp Illumina, ~50x coverage, 2.3GB total\nAvailable memory: 32GB\nDisk space: 500GB free\n\nI've tried with --careful flag and different k-mer sizes but get the same error.\n</code></pre>"},{"location":"troubleshooting/#support-resources","title":"Support Resources","text":""},{"location":"troubleshooting/#course-support","title":"Course Support","text":"<ul> <li>Instructors: Available during course hours</li> <li>Slack Channel: <code>#troubleshooting</code></li> <li>Office Hours: Daily 17:00-18:00 (course week)</li> <li>Peer Support: Encouraged among participants</li> </ul>"},{"location":"troubleshooting/#online-resources","title":"Online Resources","text":"<ul> <li>Biostars: General bioinformatics Q&amp;A</li> <li>Stack Overflow: Programming and command line issues</li> <li>Tool Documentation: Always check official documentation</li> <li>Galaxy Training: Alternative tutorials and explanations</li> </ul>"},{"location":"troubleshooting/#emergency-contacts","title":"Emergency Contacts","text":"<ul> <li>Technical Issues: tech-support@course.org</li> <li>Data Access Problems: data-admin@course.org</li> <li>General Questions: instructors@course.org</li> </ul>"},{"location":"troubleshooting/#prevention-tips","title":"Prevention Tips","text":""},{"location":"troubleshooting/#best-practices","title":"Best Practices","text":"<ol> <li>Test with small datasets first</li> <li>Keep detailed logs of commands</li> <li>Use version control for scripts</li> <li>Regular backups of important results</li> <li>Document your workflow steps</li> </ol>"},{"location":"troubleshooting/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>Running analysis without checking input quality</li> <li>Using inappropriate parameters for your data type</li> <li>Ignoring error messages and logs</li> <li>Not checking intermediate results</li> <li>Working in directories with spaces in names</li> <li>Not backing up important data</li> </ul> <p>Remember: Most bioinformatics problems have been encountered before. Don't hesitate to search online and ask for help - the community is generally very supportive!</p>"},{"location":"course/objectives/","title":"Learning Objectives","text":"<p>After completing this workshop, participants will be able to:</p>"},{"location":"course/objectives/#core-competencies","title":"Core Competencies","text":""},{"location":"course/objectives/#1-pathogen-genomics-principles","title":"1. Pathogen Genomics Principles","text":"<ul> <li>Understand the principles of pathogen genomics and its role in infectious disease surveillance and control</li> <li>Explain how genomic data complements traditional epidemiological approaches</li> <li>Describe the impact of genomic surveillance on public health decision-making</li> </ul>"},{"location":"course/objectives/#2-data-generation-workflow-days-1-2","title":"2. Data Generation Workflow (Days 1-2)","text":"<ul> <li>Describe the complete workflow of genomic data generation, from sample collection to sequence analysis</li> <li>Evaluate different sequencing technologies and their applications (Day 1)</li> <li>Design appropriate sampling strategies for genomic surveillance</li> <li>Navigate high-performance computing systems (Day 2)</li> </ul>"},{"location":"course/objectives/#3-bioinformatics-analysis-days-2-3","title":"3. Bioinformatics Analysis (Days 2-3)","text":"<ul> <li>Apply basic bioinformatics tools to analyze pathogen genomic data</li> <li>Perform quality control, assembly, and annotation of microbial genomes (Days 2-3)</li> <li>Execute MLST typing and serotyping for strain characterization (Day 3)</li> <li>Implement reproducible analysis workflows using command-line tools</li> </ul>"},{"location":"course/objectives/#4-epidemiological-applications-day-4","title":"4. Epidemiological Applications (Day 4)","text":"<ul> <li>Interpret genomic data for epidemiological insights, including outbreak detection and tracking</li> <li>Construct and interpret phylogenetic trees for transmission analysis (Day 4)</li> <li>Perform pangenome analysis and comparative genomics (Day 4)</li> <li>Integrate genomic and epidemiological data for outbreak investigation</li> </ul>"},{"location":"course/objectives/#5-antimicrobial-resistance-day-3","title":"5. Antimicrobial Resistance (Day 3)","text":"<ul> <li>Understand the role of genomics in identifying antimicrobial resistance mechanisms</li> <li>Detect AMR genes and mutations in genomic data (Day 3)</li> <li>Analyze mobile genetic elements and resistance spread (Day 3)</li> <li>Predict resistance phenotypes from genomic data</li> </ul>"},{"location":"course/objectives/#6-metagenomics-days-5-6","title":"6. Metagenomics (Days 5-6)","text":"<ul> <li>Apply metagenomic approaches to study microbial communities (Day 5)</li> <li>Analyze microbiome composition and diversity metrics (Day 5)</li> <li>Identify pathogens in complex microbial communities (Day 6)</li> <li>Detect co-infections and community shifts (Day 6)</li> </ul>"},{"location":"course/objectives/#7-advanced-analysis-days-6-8","title":"7. Advanced Analysis (Days 6-8)","text":"<ul> <li>Perform variant calling and mutation analysis (Day 6)</li> <li>Establish genotype-phenotype correlations (Day 6)</li> <li>Develop reproducible Nextflow pipelines (Days 7-8)</li> <li>Optimize workflow performance and testing (Day 8)</li> </ul>"},{"location":"course/objectives/#8-data-management-and-reproducibility-days-7-8","title":"8. Data Management and Reproducibility (Days 7-8)","text":"<ul> <li>Implement version control for research projects</li> <li>Create reproducible analysis pipelines using workflow managers (Days 7-8)</li> <li>Apply best practices for data management and sharing</li> </ul>"},{"location":"course/objectives/#9-critical-analysis-and-communication-days-9-10","title":"9. Critical Analysis and Communication (Days 9-10)","text":"<ul> <li>Apply learned skills to real research data (Day 9)</li> <li>Troubleshoot analysis challenges independently (Day 9)</li> <li>Evaluate the quality and reliability of genomic analyses</li> <li>Communicate genomic findings to diverse audiences</li> <li>Present research findings effectively (Day 10)</li> </ul>"},{"location":"course/objectives/#practical-skills","title":"Practical Skills","text":"<p>Participants will gain hands-on experience with:</p> <ul> <li>Unix/Linux command-line interface</li> <li>Git version control</li> <li>Docker containerization</li> <li>Nextflow workflow management</li> <li>Popular bioinformatics tools and databases</li> <li>Data visualization and interpretation</li> <li>Scientific presentation and communication</li> </ul>"},{"location":"course/objectives/#assessment-criteria","title":"Assessment Criteria","text":"<p>Progress will be evaluated through:</p> <ul> <li>Completion of hands-on exercises</li> <li>Quality of individual analysis projects</li> <li>Participation in group discussions</li> <li>Final project presentation</li> <li>Understanding demonstrated in Q&amp;A sessions</li> </ul>"},{"location":"course/overview/","title":"Course Overview","text":""},{"location":"course/overview/#about-the-course","title":"About the Course","text":"<p>This comprehensive 10-day training course introduces participants to the core principles and practical applications of microbial genomics and metagenomics in clinical and public health contexts.</p>"},{"location":"course/overview/#course-schedule-september-1-12-2025","title":"Course Schedule (September 1-12, 2025)","text":"<p>The training covers 10 days of intensive hands-on learning:</p>"},{"location":"course/overview/#week-1-foundations","title":"Week 1: Foundations","text":"<ul> <li>Day 1: Course welcome, genomic surveillance, sequencing technologies, PubMLST</li> <li>Day 2: HPC introduction, command line, quality control, species identification  </li> <li>Day 3: Genome assembly, annotation, MLST, serotyping, AMR detection</li> <li>Day 4: Comparative genomics, pangenomics, phylogenomics</li> <li>Day 5: Metagenomic profiling and microbiome analysis</li> </ul>"},{"location":"course/overview/#week-2-advanced-applications","title":"Week 2: Advanced Applications","text":"<ul> <li>Day 6: Co-infection detection, variant calling, genotype-phenotype correlation</li> <li>Day 7: Nextflow workflow development and nf-core</li> <li>Day 8: Advanced pipeline development and optimization</li> <li>Day 9: Bring-your-own-data analysis session</li> <li>Day 10: Final presentations and course wrap-up</li> </ul>"},{"location":"course/overview/#target-pathogens","title":"Target Pathogens","text":"<p>Using real-world datasets, participants will analyze:</p> <ul> <li>Mycobacterium tuberculosis - TB genomics and drug resistance analysis</li> <li>Vibrio cholerae - Outbreak investigation and epidemiological analysis</li> </ul>"},{"location":"course/overview/#key-focus-areas","title":"Key Focus Areas","text":""},{"location":"course/overview/#genomic-analysis","title":"Genomic Analysis","text":"<ul> <li>Analyze genomic diversity and evolutionary relationships</li> <li>Investigate antimicrobial resistance (AMR) profiles</li> <li>Study mobile genetic elements (MGE) in resistance spread</li> </ul>"},{"location":"course/overview/#metagenomics","title":"Metagenomics","text":"<ul> <li>Explore microbial communities in clinical and environmental samples</li> <li>Apply advanced sequencing analysis techniques</li> <li>Interpret complex metagenomic datasets</li> </ul>"},{"location":"course/overview/#reproducible-workflows","title":"Reproducible Workflows","text":"<ul> <li>Master version control with Git</li> <li>Work with containerized environments (Docker/Singularity)</li> <li>Implement Nextflow pipelines for scalable analysis</li> </ul>"},{"location":"course/overview/#course-format","title":"Course Format","text":"<ul> <li>Interactive workshops with hands-on exercises</li> <li>Real-world datasets from clinical and surveillance studies</li> <li>Expert instruction from experienced genomics researchers</li> <li>Group projects and individual presentations</li> <li>Bring-your-own-data analysis sessions</li> </ul>"},{"location":"course/overview/#expected-outcomes","title":"Expected Outcomes","text":"<p>By the end of this course, participants will be equipped with practical skills to conduct genomic surveillance, investigate outbreaks, and contribute to public health decision-making through genomic analysis.</p>"},{"location":"course/prerequisites/","title":"Prerequisites and Requirements","text":""},{"location":"course/prerequisites/#technical-prerequisites","title":"Technical Prerequisites","text":""},{"location":"course/prerequisites/#essential-requirements","title":"Essential Requirements","text":""},{"location":"course/prerequisites/#1-computing-equipment","title":"1. Computing Equipment","text":"<ul> <li>Laptop with at least 8GB RAM (16GB recommended)</li> <li>Operating System: Linux or macOS preferred</li> <li>Windows users must install Git Bash before the course</li> <li>Reliable internet connection for downloading datasets and software</li> </ul>"},{"location":"course/prerequisites/#2-basic-technical-knowledge","title":"2. Basic Technical Knowledge","text":"<ul> <li>Basic understanding of Linux command-line tools</li> <li>File navigation (cd, ls, pwd)</li> <li>File manipulation (cp, mv, rm, mkdir)</li> <li>Text viewing (cat, less, head, tail)</li> <li>Familiarity with text editors (nano, vim, or similar)</li> <li>Basic understanding of file systems and permissions</li> </ul>"},{"location":"course/prerequisites/#3-scientific-background","title":"3. Scientific Background","text":"<ul> <li>Undergraduate-level biology or microbiology</li> <li>Basic understanding of genetics and molecular biology</li> <li>Familiarity with concepts of:</li> <li>DNA sequencing</li> <li>Bacterial genetics</li> <li>Infectious diseases</li> <li>Public health surveillance</li> </ul>"},{"location":"course/prerequisites/#recommended-not-required","title":"Recommended (Not Required)","text":""},{"location":"course/prerequisites/#1-programming-experience","title":"1. Programming Experience","text":"<ul> <li>Basic scripting in Python, R, or shell scripting</li> <li>Experience with data analysis workflows</li> <li>Version control with Git (we'll cover this in Day 1)</li> </ul>"},{"location":"course/prerequisites/#2-bioinformatics-background","title":"2. Bioinformatics Background","text":"<ul> <li>Previous experience with sequence analysis tools</li> <li>Understanding of genomic file formats (FASTA, FASTQ, VCF)</li> <li>Familiarity with biological databases</li> </ul>"},{"location":"course/prerequisites/#3-high-performance-computing","title":"3. High-Performance Computing","text":"<ul> <li>Access to HPC resources (recommended but not essential)</li> <li>Experience with job schedulers (SLURM, PBS)</li> </ul>"},{"location":"course/prerequisites/#software-requirements","title":"Software Requirements","text":""},{"location":"course/prerequisites/#pre-course-installation","title":"Pre-course Installation","text":"<p>Please install the following software before the course begins:</p>"},{"location":"course/prerequisites/#1-git-all-operating-systems","title":"1. Git (All Operating Systems)","text":"<ul> <li>Linux: <code>sudo apt install git</code> or equivalent</li> <li>macOS: Install Xcode Command Line Tools or use Homebrew</li> <li>Windows: Download and install Git Bash from git-scm.com</li> </ul>"},{"location":"course/prerequisites/#2-text-editor","title":"2. Text Editor","text":"<p>Choose one of: - VS Code (recommended for beginners) - Sublime Text - Atom - Vim/Emacs (for advanced users)</p>"},{"location":"course/prerequisites/#3-ssh-client","title":"3. SSH Client","text":"<ul> <li>Linux/macOS: Built-in terminal</li> <li>Windows: Use Git Bash or Windows Subsystem for Linux (WSL)</li> </ul>"},{"location":"course/prerequisites/#course-provided-software","title":"Course-Provided Software","text":"<p>The following will be provided during the course:</p> <ul> <li>Docker/Singularity containers with pre-installed bioinformatics tools</li> <li>Nextflow for workflow management</li> <li>Access to high-performance computing resources</li> <li>Pre-configured analysis environments</li> </ul>"},{"location":"course/prerequisites/#data-requirements","title":"Data Requirements","text":""},{"location":"course/prerequisites/#practice-datasets","title":"Practice Datasets","text":"<p>We will provide: - Curated training datasets for all major pathogens - Quality-controlled sequence data in standard formats - Reference genomes and databases - Example analysis outputs for comparison</p>"},{"location":"course/prerequisites/#bring-your-own-data-optional","title":"Bring Your Own Data (Optional)","text":"<p>If you have your own datasets: - Raw sequencing data (FASTQ format) - Associated metadata (sample information, collection details) - Ethical approval for data sharing (if applicable) - Data should be &lt;100GB total for practical analysis</p>"},{"location":"course/prerequisites/#pre-course-preparation","title":"Pre-course Preparation","text":""},{"location":"course/prerequisites/#1-complete-the-setup-guide","title":"1. Complete the Setup Guide","text":"<p>Follow our detailed Setup Guide to prepare your computing environment.</p>"},{"location":"course/prerequisites/#2-review-basic-concepts","title":"2. Review Basic Concepts","text":"<p>Refresh your knowledge of: - Basic microbiology and infectious diseases - DNA sequencing principles - File formats in bioinformatics</p>"},{"location":"course/prerequisites/#3-practice-command-line","title":"3. Practice Command Line","text":"<p>If you're new to command line: - Complete an online tutorial (e.g., \"Command Line Crash Course\") - Practice basic file operations - Get comfortable with navigating directories</p>"},{"location":"course/prerequisites/#4-test-your-setup","title":"4. Test Your Setup","text":"<ul> <li>Verify Git installation: <code>git --version</code></li> <li>Test SSH connectivity (instructions will be provided)</li> <li>Ensure you can create and edit text files</li> </ul>"},{"location":"course/prerequisites/#accessibility-and-support","title":"Accessibility and Support","text":""},{"location":"course/prerequisites/#technical-support","title":"Technical Support","text":"<ul> <li>Pre-course: Contact instructors for setup assistance</li> <li>During course: Dedicated technical support available</li> <li>Post-course: Community forum for ongoing questions</li> </ul>"},{"location":"course/prerequisites/#accommodations","title":"Accommodations","text":"<p>Please inform instructors of any: - Accessibility requirements - Dietary restrictions (for course meals) - Special technical needs</p>"},{"location":"course/prerequisites/#language-support","title":"Language Support","text":"<ul> <li>Course materials are in English</li> <li>Instructors can provide support in multiple languages</li> <li>Translated resources available for key concepts</li> </ul>"},{"location":"course/prerequisites/#questions","title":"Questions?","text":"<p>If you have questions about prerequisites or setup:</p> <ol> <li>Check the Troubleshooting Guide</li> <li>Contact the course coordinators</li> <li>Join our pre-course Q&amp;A sessions (dates TBD)</li> </ol>"},{"location":"course/schedule/","title":"Course Schedule","text":""},{"location":"course/schedule/#overview","title":"Overview","text":"<p>The 10-day intensive training course covers fundamental to advanced topics in microbial genomics and metagenomics. Each day combines theoretical presentations with hands-on practical exercises.</p> <p>!!! info \"Course Information\"</p> <ul> <li>Duration: 10 days (September 1-12, 2025)</li> <li>Time: 09:00-13:00 CAT daily</li> <li>Venue: MAC room, level 2, Health Science UCT, Barnard Fuller Building</li> <li>Address: Anzio Rd, Observatory, Cape Town, 7935</li> <li>Format: Hands-on workshops with lectures</li> <li>Break: 11:00/11:30 AM coffee break</li> </ul> <p>!!! note \"Participant Requirements\"  - Participants are required to bring their own data to perform the analysis. If a participant does not have data, it will be made available to them.</p>"},{"location":"course/schedule/#day-1-welcome-to-the-course","title":"Day 1: Welcome to the Course","text":"<p>Date: September 1, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Introductions All 09:10 Overview of clinical pathogens and genomic surveillance Slides Ephifania Geza 09:40 Overview of sequencing technologies and data types Sindiswa Lukhele 10:00 Setting up and exploring PubMLST Sindiswa Lukhele 11:00 Break 11:30 Introduction to command line interface Practical Arash Iranzadeh <p>Key Learning Outcomes: Introduction to genomic surveillance, sequencing technologies, command line basics</p>"},{"location":"course/schedule/#day-2-introduction-to-commandline-and-guest-talk","title":"Day 2: Introduction to Commandline and Guest Talk","text":"<p>Date: September 2, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Introduction to command line interface Practical Arash Iranzadeh 11:30 Break 12:00 Guest talk: MtB and co-infection Bio. Presentation Bethlehem Adnew <p>Key Learning Outcomes: Command line proficiency, HPC fundamentals</p>"},{"location":"course/schedule/#day-3-accelerating-bioinformatics-hpc-qc-and-species-identification-essentials","title":"Day 3: Accelerating Bioinformatics: HPC, QC, and Species Identification Essentials","text":"<p>Date: September 3, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Introduction High Performance Computing (HPC) \u2013 Ilifu Notes \u2022 Practical 1 \u2022 Practical 2 Mamana Mbiyavanga 11:00 Break 12:00 Quality checking and control, as well as species identification Practical Arash Iranzadeh <p>Key Learning Outcomes: HPC fundamentals, Quality control fundamentals</p>"},{"location":"course/schedule/#day-4-genome-assembly-essentials-qc-identification-and-annotation","title":"Day 4: Genome Assembly Essentials: QC, Identification, and Annotation","text":"<p>Date: September 4, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Recap: Quality checking and control, and species identification Practical Arash Iranzadeh 10:30 Genome assembly, quality assessment Notes. Practical Ephifania Geza 11:00 Break 11:30 Genome assembly, quality assessment: Continuation Notes. Practical Ephifania Geza <p>Key Learning Outcomes: Quality control, Genome assembly, assessment</p>"},{"location":"course/schedule/#day-5-tracking-threats-genomic-detection-of-amr-virulence-and-plasmid-mobility","title":"Day 5: Tracking Threats: Genomic Detection of AMR, Virulence, and Plasmid Mobility","text":"<p>Date: September 5, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Recap: Genome assembly, quality assessment Notes. Practical 10:00 Genome annotation and Multi-Locus Sequence Typing Notes Arash Iranzadeh 11:30 Break 12:00 Antimicrobial Resistance gene detection and resistance prediction Ephifania Geza <p>Key Learning Outcomes: Genome quality and functional gene annotation fundamentals, AMR and virulence factors and plasmid detection</p>"},{"location":"course/schedule/#day-6-nextflow-foundations-core-concepts","title":"Day 6: Nextflow Foundations &amp; Core Concepts","text":"<p>Date: September 8, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Reproducible workflows with Nextflow and nf-core Mamana Mbiyavanga 10:30 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga 11:30 Break 12:00 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga <p>Key Learning Outcomes: Workflow reproducibility, Nextflow basics, pipeline development</p>"},{"location":"course/schedule/#day-7-advanced-nextflow-version-control-with-github-real-genomics-applications","title":"Day 7: Advanced Nextflow, Version Control with GitHub &amp; Real Genomics Applications","text":"<p>Date: September 9, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Git and GitHub for Pipeline Version Control Mamana Mbiyavanga 09:45 MTB Analysis Pipeline Development Mamana Mbiyavanga 10:30 Break 10:45 Genome Assembly Workflows Mamana Mbiyavanga 11:30 Advanced Nextflow Features &amp; Optimization Mamana Mbiyavanga 12:15 Pipeline Deployment Strategies Mamana Mbiyavanga 13:00 End <p>Key Learning Outcomes: - Professional version control with Git/GitHub for collaborative pipeline development - Real-world MTB genomics workflows for clinical applications - Complete genome assembly pipelines from raw reads to annotated genomes - Advanced Nextflow features including modules, subworkflows, and optimization - Multi-platform deployment strategies (local, HPC, cloud environments)</p>"},{"location":"course/schedule/#day-8-metagenomic-profiling","title":"Day 8: Metagenomic profiling","text":"<p>Date: September 10, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Nextflow Mamana 11:30 Break 12:00 Metagenomics profiling [Notes](../day8/metagenomics.md Ephifania Geza <p>Key Learning Outcomes: Metagenomic sequencing principles, microbiome analysis, diversity metrics</p>"},{"location":"course/schedule/#day-9-comparative-genomics","title":"Day 9: Comparative Genomics","text":"<p>Date: September 11, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Pangenomics Arash Iranzadeh 10:30 Phylogenomics: Inferring evolutionary relationships from core SNPs Arash Iranzadeh 11:30 Break 12:00 Phylogenomics: Tree construction and visualisation Arash Iranzadeh <p>Key Learning Outcomes: Pan-genome analysis, phylogenetic inference, tree construction and visualization</p>"},{"location":"course/schedule/#day-10-wrap-up-session","title":"Day 10: Wrap-up session","text":"<p>Date: September 12, 2025 \u2192 View detailed module</p> Time (CAT) Topic Links Trainer 09:00 Participant presentations 11:15 Short talks NGS-Academy/AfriGen-D/eLwazi ODSP 11:40 End of the course <p>Key Learning Outcomes: Applying learned skills to real data, Scientific presentation skills, course completion</p>"},{"location":"course/schedule/#trainers","title":"Trainers","text":""},{"location":"course/schedule/#core-training-team","title":"Core Training Team","text":"Trainer Role Expertise Ephifania Geza Lead Instructor Genomic surveillance, AMR analysis, metagenomics Arash Iranzadeh Technical Instructor Command line, QC, assembly, phylogenomics Sindiswa Lukhele Technical Instructor Sequencing technologies, PubMLST Mamana Mbiyavanga HPC/Workflow Specialist High-performance computing, Nextflow pipelines"},{"location":"course/schedule/#guest-speaker","title":"Guest Speaker","text":"Speaker Topic Date Bethlehem Adnew MtB and co-infection September 2, 2025 <p>This comprehensive training provides participants with both theoretical knowledge and practical skills needed for microbial genomics analysis in clinical and public health settings.</p>"},{"location":"course/setup/","title":"Setup Guide","text":""},{"location":"course/setup/#before-you-arrive","title":"Before You Arrive","text":"<p>Complete these setup steps at least one week before the course begins.</p>"},{"location":"course/setup/#1-system-requirements","title":"1. System Requirements","text":""},{"location":"course/setup/#minimum-specifications","title":"Minimum Specifications","text":"<ul> <li>RAM: 8GB (16GB recommended)</li> <li>Storage: 50GB free space</li> <li>OS: Linux, macOS, or Windows 10/11</li> <li>Internet: Stable broadband connection</li> </ul>"},{"location":"course/setup/#operating-system-setup","title":"Operating System Setup","text":"Linux (Ubuntu/Debian)macOSWindows <pre><code># Update package list\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install essential tools\nsudo apt install -y git curl wget build-essential\n</code></pre> <pre><code># Install Homebrew (if not already installed)\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install essential tools\nbrew install git curl wget\n</code></pre> <ol> <li>Install Git Bash: Download from git-scm.com</li> <li>Enable WSL2 (optional but recommended):    <pre><code>wsl --install\n</code></pre></li> <li>Install Windows Terminal from Microsoft Store</li> </ol>"},{"location":"course/setup/#2-git-configuration","title":"2. Git Configuration","text":"<p>Configure Git with your information:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\ngit config --global init.defaultBranch main\n</code></pre> <p>Verify configuration: <pre><code>git config --list\n</code></pre></p>"},{"location":"course/setup/#3-ssh-setup","title":"3. SSH Setup","text":"<p>Generate SSH key for secure connections:</p> <pre><code># Generate SSH key pair\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Start SSH agent\neval \"$(ssh-agent -s)\"\n\n# Add key to agent\nssh-add ~/.ssh/id_ed25519\n\n# Display public key (copy this for later use)\ncat ~/.ssh/id_ed25519.pub\n</code></pre>"},{"location":"course/setup/#4-text-editor","title":"4. Text Editor","text":"<p>Choose and install a text editor:</p> VS Code (Recommended)Sublime TextCommand Line Editors <ul> <li>Download from code.visualstudio.com</li> <li>Install useful extensions:</li> <li>Python</li> <li>Markdown All in One</li> <li>GitLens</li> </ul> <ul> <li>Download from sublimetext.com</li> <li>Consider Sublime Merge for Git integration</li> </ul> <pre><code># For vim users\nsudo apt install vim  # Linux\nbrew install vim      # macOS\n\n# For nano users (usually pre-installed)\nwhich nano\n</code></pre>"},{"location":"course/setup/#5-test-your-setup","title":"5. Test Your Setup","text":""},{"location":"course/setup/#basic-command-line-test","title":"Basic Command Line Test","text":"<pre><code># Check versions\ngit --version\ncurl --version\nwget --version\n\n# Test directory operations\nmkdir test_setup\ncd test_setup\necho \"Hello World\" &gt; test.txt\ncat test.txt\ncd ..\nrm -rf test_setup\n</code></pre>"},{"location":"course/setup/#git-test","title":"Git Test","text":"<pre><code># Clone a test repository\ngit clone https://github.com/CIDRI-Africa/microbial-genomics-training.git\ncd microbial-genomics-training\nls -la\ncd ..\nrm -rf microbial-genomics-training\n</code></pre>"},{"location":"course/setup/#6-course-specific-setup","title":"6. Course-Specific Setup","text":""},{"location":"course/setup/#hpc-access-if-provided","title":"HPC Access (If Provided)","text":"<p>You will receive: - SSH connection details - Username and login instructions - VPN setup (if required)</p>"},{"location":"course/setup/#dockersingularity-optional","title":"Docker/Singularity (Optional)","text":"<p>For local analysis (advanced users):</p> DockerSingularity <pre><code># Linux\nsudo apt install docker.io\nsudo usermod -aG docker $USER\n\n# macOS\n# Download Docker Desktop from docker.com\n\n# Test installation\ndocker --version\ndocker run hello-world\n</code></pre> <pre><code># Linux (Ubuntu/Debian)\nsudo apt install singularity-container\n\n# Test installation\nsingularity --version\n</code></pre>"},{"location":"course/setup/#7-download-course-materials","title":"7. Download Course Materials","text":"<p>One week before the course:</p> <pre><code># Create course directory\nmkdir -p ~/microbial-genomics-course\ncd ~/microbial-genomics-course\n\n# Clone course repository\ngit clone https://github.com/CIDRI-Africa/microbial-genomics-training.git\n\n# Check repository contents\ncd microbial-genomics-training\nls -la\n</code></pre>"},{"location":"course/setup/#8-pre-course-data-download","title":"8. Pre-course Data Download","text":"<p>Large datasets will be provided via: - Shared storage on HPC systems - Cloud storage links (Google Drive/Dropbox) - Direct download scripts (provided before course)</p>"},{"location":"course/setup/#9-connectivity-test","title":"9. Connectivity Test","text":""},{"location":"course/setup/#ssh-connection-test","title":"SSH Connection Test","text":"<pre><code># Test SSH connectivity (replace with provided details)\nssh -T username@hostname\n\n# If successful, you should see a welcome message\n</code></pre>"},{"location":"course/setup/#internet-speed-test","title":"Internet Speed Test","text":"<p>Ensure you have adequate bandwidth: - Minimum: 10 Mbps download - Recommended: 50+ Mbps download - Upload: 5+ Mbps for video calls</p>"},{"location":"course/setup/#10-backup-and-recovery","title":"10. Backup and Recovery","text":""},{"location":"course/setup/#create-backups","title":"Create Backups","text":"<pre><code># Backup SSH keys\ncp ~/.ssh/id_ed25519* ~/backup_location/\n\n# Backup Git configuration\ngit config --list &gt; ~/git_config_backup.txt\n</code></pre>"},{"location":"course/setup/#recovery-commands","title":"Recovery Commands","text":"<p>Keep these handy in case of issues: <pre><code># Reset Git configuration\ngit config --global --unset-all user.name\ngit config --global --unset-all user.email\n\n# Regenerate SSH keys\nrm ~/.ssh/id_ed25519*\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n</code></pre></p>"},{"location":"course/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"course/setup/#common-issues","title":"Common Issues","text":""},{"location":"course/setup/#git-issues","title":"Git Issues","text":"<pre><code># Fix permission issues (Linux/macOS)\nsudo chown -R $USER ~/.ssh/\n\n# Reset SSH agent\neval \"$(ssh-agent -k)\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n</code></pre>"},{"location":"course/setup/#network-issues","title":"Network Issues","text":"<pre><code># Test connectivity\nping google.com\ncurl -I https://github.com\n\n# Check proxy settings (if behind firewall)\necho $http_proxy\necho $https_proxy\n</code></pre>"},{"location":"course/setup/#permission-issues-linux","title":"Permission Issues (Linux)","text":"<pre><code># Fix common permission problems\nsudo chown -R $USER:$USER ~/\nchmod 755 ~/.ssh\nchmod 600 ~/.ssh/id_ed25519\nchmod 644 ~/.ssh/id_ed25519.pub\n</code></pre>"},{"location":"course/setup/#getting-help","title":"Getting Help","text":""},{"location":"course/setup/#before-the-course","title":"Before the Course","text":"<ul> <li>Email: instructors@microbial-genomics-training.org</li> <li>Slack: Join our pre-course channel</li> <li>Office Hours: Weekly setup sessions (schedule TBD)</li> </ul>"},{"location":"course/setup/#documentation","title":"Documentation","text":"<ul> <li>Command Line Tutorial</li> <li>Git Tutorial</li> <li>SSH Tutorial</li> </ul>"},{"location":"course/setup/#final-checklist","title":"Final Checklist","text":"<p>Before the course starts, verify:</p> <ul> <li> Git installed and configured</li> <li> SSH key generated and working</li> <li> Text editor installed and functional</li> <li> Course repository cloned</li> <li> HPC access tested (if provided)</li> <li> Internet connection stable</li> <li> Backup of important configurations created</li> </ul>"},{"location":"course/setup/#day-1-preview","title":"Day 1 Preview","text":"<p>On the first day (September 1, 2025), we'll: 1. Course introductions and welcome session 2. Overview of clinical pathogens and genomic surveillance 3. Sequencing technologies and data types overview 4. Hands-on PubMLST database exploration 5. Command line interface basics and R environment setup</p> <p>Come prepared with your laptop and the software installed above. Don't worry if you encounter setup problems \u2013 we'll troubleshoot together during the session!</p>"},{"location":"day2/hpc-ilifu-training/","title":"HPC and ILIFU Training Materials","text":""},{"location":"day2/hpc-ilifu-training/#getting-started","title":"Getting Started","text":"<p>This document provides an overview of HPC concepts and ILIFU infrastructure. </p> <p>For hands-on practice:</p> <ul> <li>SLURM Jobs: See High Performance Computing with SLURM: Practical Tutorial for step-by-step SLURM exercises</li> <li>Unix Commands: See Unix Commands for Pathogen Genomics - Practical Tutorial for genomics command-line basics</li> </ul> <p>Quick Setup (if needed for examples below): <pre><code>mkdir -p ~/hpc_practice &amp;&amp; cd ~/hpc_practice\ncp -r /cbio/training/courses/2025/micmet-genomics/sample-data/* .\n</code></pre></p>"},{"location":"day2/hpc-ilifu-training/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to High Performance Computing (HPC)</li> <li>ILIFU Infrastructure Overview</li> <li>Getting Started with ILIFU</li> <li>SLURM Job Scheduling</li> <li>Resource Allocation and Management</li> <li>Best Practices</li> <li>Practical Examples</li> <li>Troubleshooting</li> </ol>"},{"location":"day2/hpc-ilifu-training/#introduction-to-hpc","title":"Introduction to HPC","text":""},{"location":"day2/hpc-ilifu-training/#what-is-high-performance-computing","title":"What is High Performance Computing?","text":"<p>High Performance Computing (HPC) is the use of powerful computers with multiple processors working in parallel to solve complex computational problems that require significant processing power, memory, or time.</p> <p>Key Characteristics:</p> <ul> <li>Parallel processing: Multiple CPUs/cores work simultaneously on the same problem</li> <li>Cluster architecture: Hundreds or thousands of interconnected compute nodes</li> <li>High memory capacity: Large RAM for data-intensive computations</li> <li>Fast storage systems: High-speed file systems for handling large datasets</li> <li>Job scheduling: Queue management systems to optimize resource allocation</li> <li>Specialized hardware: GPUs, high-speed interconnects (InfiniBand), and custom processors</li> </ul>"},{"location":"day2/hpc-ilifu-training/#why-use-hpc","title":"Why Use HPC?","text":"<ul> <li>Speed: Complete computations faster than desktop computers</li> <li>Scale: Handle larger datasets and more complex problems</li> <li>Efficiency: Optimize resource utilization</li> <li>Cost-effective: Share expensive hardware among researchers</li> </ul>"},{"location":"day2/hpc-ilifu-training/#traditional-computing-vs-hpc","title":"Traditional Computing vs HPC","text":"<pre><code>Desktop Computer          HPC Cluster\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   1 CPU      \u2502   vs    \u2502Node1\u2502Node2\u2502Node3\u2502\n\u2502   8GB RAM    \u2502         \u2502 32  \u2502 64  \u2502128  \u2502\n\u2502   1TB Disk   \u2502         \u2502cores\u2502cores\u2502cores\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>HPC = Many computers working together</p> Traditional Computing HPC Cluster Single processor Hundreds of processors Limited memory (8-32GB) Massive shared memory (TB) Local storage (TB) Distributed storage (PB) Individual use Shared resources Desktop/Laptop Specialized data centers"},{"location":"day2/hpc-ilifu-training/#why-do-we-need-hpc","title":"Why Do We Need HPC?","text":""},{"location":"day2/hpc-ilifu-training/#real-world-problems-that-need-hpc","title":"Real-world Problems That Need HPC","text":""},{"location":"day2/hpc-ilifu-training/#astronomy-processing-telescope-data","title":"\ud83d\udd2c Astronomy: Processing telescope data","text":"<ul> <li>Data volume: 1-2 TB per night from modern telescopes</li> <li>Local machine: 3-4 weeks to process one night's data</li> <li>HPC cluster: 2-3 hours with parallel processing</li> <li>Example: MeerKAT telescope generates 2.5 TB/hour during observations</li> </ul>"},{"location":"day2/hpc-ilifu-training/#bioinformatics-genome-assembly","title":"\ud83e\uddec Bioinformatics: Genome assembly","text":"<ul> <li>Data size: 100-300 GB of raw sequencing reads</li> <li>Local machine (8GB RAM): Often fails due to memory limits</li> <li>Local machine (32GB RAM): 2-3 weeks for bacterial genome</li> <li>HPC cluster: 4-6 hours with 256GB RAM</li> <li>Example: Human genome assembly needs ~1TB RAM, impossible on most desktops</li> </ul>"},{"location":"day2/hpc-ilifu-training/#climate-modeling-weather-simulations","title":"\ud83c\udf21\ufe0f Climate Modeling: Weather simulations","text":"<ul> <li>Computation: Millions of grid points \u00d7 thousands of time steps</li> <li>Local machine: 6-8 months for regional model (if it runs at all)</li> <li>HPC cluster: 12-24 hours on 100+ cores</li> <li>Example: 10km resolution global model needs 10,000+ CPU hours</li> </ul>"},{"location":"day2/hpc-ilifu-training/#machine-learning-training-deep-neural-networks","title":"\ud83e\uddee Machine Learning: Training deep neural networks","text":"<ul> <li>Model size: GPT-3 has 175 billion parameters</li> <li>Local machine (single GPU): 355 years to train</li> <li>HPC cluster (1000 GPUs): 34 days</li> <li>Example: Training ResNet-50 on ImageNet: 2 weeks (laptop) \u2192 1 hour (8 GPUs)</li> </ul>"},{"location":"day2/hpc-ilifu-training/#pathogen-genomics-outbreak-analysis","title":"\ud83e\udda0 Pathogen Genomics: Outbreak analysis","text":"<ul> <li>Dataset: 1000 M. tuberculosis genomes for outbreak investigation</li> <li>Local machine tasks and times:</li> <li>Quality control: 50 hours (3 min/sample)</li> <li>Read alignment: 167 hours (10 min/sample)  </li> <li>Variant calling: 83 hours (5 min/sample)</li> <li>Phylogenetic tree: 48-72 hours</li> <li>Total: ~15 days of continuous processing</li> <li>HPC cluster: </li> <li>All samples in parallel: 4-6 hours total</li> <li>Tree construction on high-memory node: 2-3 hours</li> <li>Real example: COVID-19 surveillance processing 10,000 genomes weekly - impossible without HPC</li> </ul>"},{"location":"day2/hpc-ilifu-training/#additional-pathogen-genomics-use-cases","title":"Additional Pathogen Genomics Use Cases","text":""},{"location":"day2/hpc-ilifu-training/#bacterial-genome-assembly-illumina-nanopore","title":"\ud83e\uddec Bacterial Genome Assembly (Illumina + Nanopore)","text":"<ul> <li>Dataset: Hybrid assembly of 50 bacterial isolates</li> <li>Computational requirements:</li> <li>RAM: 16-32GB per genome</li> <li>CPU: 8-16 cores optimal per assembly</li> <li>Local machine (16GB RAM, 4 cores):</li> <li>One genome at a time only</li> <li>Per genome: 3-4 hours</li> <li>Total time: 150-200 hours (6-8 days)</li> <li>Risk of crashes with large genomes</li> <li>HPC cluster (256GB RAM, 32 cores/node):</li> <li>Process 8 genomes simultaneously per node</li> <li>Use 7 nodes for all 50 genomes</li> <li>Total time: 3-4 hours</li> <li>Speedup: 50x faster</li> </ul>"},{"location":"day2/hpc-ilifu-training/#amr-gene-detection-across-multiple-species","title":"\ud83d\udc8a AMR Gene Detection Across Multiple Species","text":"<ul> <li>Dataset: 5000 bacterial genomes from hospital surveillance</li> <li>Tools: AMRFinder, CARD-RGI, ResFinder</li> <li>Computational requirements:</li> <li>Database size: 2-5GB per tool</li> <li>RAM: 4-8GB per genome</li> <li>CPU time: 5-10 minutes per genome per tool</li> <li>Local machine (8 cores):</li> <li>Sequential processing: 5000 \u00d7 3 tools \u00d7 7.5 min = 1875 hours (78 days)</li> <li>Database loading overhead adds 20% more time</li> <li>HPC cluster (100 nodes, 32 cores each):</li> <li>Parallel processing across nodes</li> <li>Shared database in memory</li> <li>Total time: 6-8 hours</li> <li>Speedup: 230x faster</li> </ul>"},{"location":"day2/hpc-ilifu-training/#phylogeographic-analysis-of-cholera-outbreak","title":"\ud83c\udf0d Phylogeographic Analysis of Cholera Outbreak","text":"<ul> <li>Dataset: 2000 V. cholerae genomes from Haiti outbreak</li> <li>Computational requirements:</li> <li>Alignment: 100GB RAM for reference-based</li> <li>SNP calling: 4GB per genome</li> <li>Tree building (RAxML-NG): 64-128GB RAM</li> <li>BEAST analysis: 32GB RAM, 1000+ hours CPU time</li> <li>Local machine attempts:</li> <li>Alignment: Often fails (out of memory)</li> <li>If successful: 48 hours</li> <li>SNP calling: 133 hours (4 min/genome)</li> <li>RAxML tree: Fails on most laptops (needs &gt;64GB RAM)</li> <li>BEAST: 6-8 weeks for proper MCMC convergence</li> <li>HPC cluster:</li> <li>Alignment: 2 hours on high-memory node</li> <li>SNP calling: 2 hours (parallel)</li> <li>RAxML: 4-6 hours on 64 cores</li> <li>BEAST: 48 hours on 32 cores</li> <li>Total: 2-3 days vs 2-3 months</li> </ul>"},{"location":"day2/hpc-ilifu-training/#real-time-nanopore-sequencing-analysis","title":"\ud83d\udd2c Real-time Nanopore Sequencing Analysis","text":"<ul> <li>Scenario: Meningitis outbreak, need results in &lt;24 hours</li> <li>Data flow: 20 samples, 5GB data/sample, arriving over 12 hours</li> <li>Pipeline: Basecalling \u2192 QC \u2192 Assembly \u2192 Typing \u2192 AMR</li> <li>Local machine challenges:</li> <li>Can't keep up with data generation</li> <li>Basecalling alone: 2 hours/sample (40 hours total)</li> <li>Sequential processing: Miss the 24-hour deadline</li> <li>HPC solution:</li> <li>Real-time processing as data arrives</li> <li>GPU nodes for basecalling: 10 min/sample</li> <li>Parallel assembly and analysis</li> <li>Results available within 2-3 hours of sequencing</li> <li>Clinical impact: Treatment decisions in same day</li> </ul>"},{"location":"day2/hpc-ilifu-training/#computational-requirements-comparison-table","title":"Computational Requirements Comparison Table","text":"Task Local Machine HPC Cluster Speedup 100 TB genomes QC 8GB RAM, 5 hours 256GB RAM, 10 min 30x 1000 genome alignment 16GB RAM, 7 days 32GB/node \u00d7 50, 3 hours 56x Phylogenetic tree (5000 taxa) Often fails (&gt;64GB needed) 512GB RAM, 6 hours \u221e Pan-genome analysis (500 genomes) 32GB RAM, 2 weeks 256GB RAM, 8 hours 42x GWAS (10,000 samples) Impossible (&lt;1TB RAM) 1TB RAM node, 24 hours \u221e Metagenomic assembly 64GB RAM, 3 days 512GB RAM, 4 hours 18x"},{"location":"day2/hpc-ilifu-training/#why-these-tasks-fail-on-local-machines","title":"Why These Tasks Fail on Local Machines","text":"<ol> <li>Memory Walls:</li> <li>De novo assembly: Needs 100-1000x coverage data in RAM</li> <li>Tree building: O(n\u00b2) memory for distance matrices</li> <li> <p>Pan-genome: Stores all genomes simultaneously</p> </li> <li> <p>Time Constraints:</p> </li> <li>Outbreak response: Need results in hours, not weeks</li> <li>Grant deadlines: Can't wait months for analysis</li> <li> <p>Iterative analysis: Need to test multiple parameters</p> </li> <li> <p>Data Volume:</p> </li> <li>Modern sequencer: 100-500GB per run</li> <li>Surveillance programs: 100s of genomes weekly</li> <li>Can't even store data on laptop (typical: 256GB-1TB SSD)</li> </ol>"},{"location":"day2/hpc-ilifu-training/#ilifu-infrastructure","title":"ILIFU Infrastructure","text":""},{"location":"day2/hpc-ilifu-training/#what-is-ilifu","title":"What is ILIFU?","text":"<ul> <li>Inter-University Institute for Data Intensive Astronomy</li> <li>South African national research data facility</li> <li>Supports astronomy, bioinformatics, and other data-intensive sciences</li> <li>Located at University of Cape Town and University of the Western Cape</li> </ul>"},{"location":"day2/hpc-ilifu-training/#ilifu-services","title":"ILIFU Services","text":"<ol> <li>Compute Cluster: High-performance computing resources</li> <li>Storage: Large-scale data storage solutions</li> <li>Cloud Services: Virtualized computing environments</li> <li>Data Transfer: High-speed data movement capabilities</li> <li>Support: Technical assistance and training</li> </ol>"},{"location":"day2/hpc-ilifu-training/#ilifu-cluster-architecture","title":"ILIFU Cluster Architecture","text":"<p>ILIFU (Inter-University Institute for Data Intensive Astronomy) is a cloud computing infrastructure designed for data-intensive research in astronomy, bioinformatics, and other computational sciences. The facility operates on an OpenStack platform with containerized workloads using Singularity and job scheduling through SLURM.</p> <pre><code>graph TB\n    subgraph \"ILIFU Infrastructure\"\n        subgraph \"Cloud Platform\"\n            OS[OpenStack Cloud&lt;br/&gt;Infrastructure-as-a-Service]\n        end\n\n        subgraph \"Compute Resources\"\n            CN[Compute Nodes&lt;br/&gt;Max: 96 CPUs per job&lt;br/&gt;Max: 1500 GB RAM per job]\n        end\n\n        subgraph \"Container Platform\"\n            SP[Singularity Containers&lt;br/&gt;HPC-optimized&lt;br/&gt;Rootless execution]\n        end\n\n        subgraph \"Job Scheduler\"\n            SL[SLURM Workload Manager&lt;br/&gt;Max runtime: 336 hours]\n        end\n    end\n\n    subgraph \"Research Domains\"\n        AST[Astronomy&lt;br/&gt;MeerKAT, SKA]\n        BIO[Bioinformatics&lt;br/&gt;Genomics, Metagenomics]\n        DS[Data Science&lt;br/&gt;ML/AI Research]\n    end\n\n    OS --&gt; CN\n    CN --&gt; SP\n    SP --&gt; SL\n\n    AST --&gt; SL\n    BIO --&gt; SL\n    DS --&gt; SL\n\n    style OS fill:#e1f5fe\n    style CN fill:#e8f5e9\n    style SP fill:#fff3e0\n    style SL fill:#f3e5f5</code></pre> <p>Figure: ILIFU cloud infrastructure architecture supporting multiple research domains</p>"},{"location":"day2/hpc-ilifu-training/#resource-specifications","title":"Resource Specifications","text":"<p>Based on ILIFU's cloud infrastructure configuration:</p>"},{"location":"day2/hpc-ilifu-training/#maximum-job-resources","title":"Maximum Job Resources","text":"<ul> <li>CPUs: Up to 96 cores per job</li> <li>Memory: Up to 1500 GB (1.5 TB) RAM per job</li> <li>Runtime: Maximum 336 hours (14 days) per job</li> <li>Storage: Distributed file systems for large-scale data</li> </ul>"},{"location":"day2/hpc-ilifu-training/#key-features","title":"Key Features","text":"<ul> <li>OpenStack Platform: Provides flexible cloud computing resources</li> <li>Singularity Containers: Enables reproducible, portable workflows</li> <li>SLURM Scheduler: Manages resource allocation and job queuing</li> <li>Multi-domain Support: Serves astronomy, bioinformatics, and data science communities</li> </ul>"},{"location":"day2/hpc-ilifu-training/#access-methods","title":"Access Methods","text":"<ul> <li>SSH access to login nodes</li> <li>Jupyter notebooks for interactive computing</li> <li>Web-based interfaces for specific services</li> <li>API access for programmatic interaction</li> </ul>"},{"location":"day2/hpc-ilifu-training/#infrastructure-components","title":"Infrastructure Components","text":"Component Description Purpose OpenStack Cloud computing platform Infrastructure management and virtualization SLURM Workload manager Job scheduling and resource allocation Singularity Container platform Application deployment and portability CephFS Distributed storage High-performance shared file system Login Nodes Access points User entry and job submission Compute Nodes Processing units Actual computation execution <p>Note: ILIFU operates as a cloud infrastructure rather than a traditional fixed HPC cluster, allowing dynamic resource allocation based on user requirements. Specific hardware configurations may vary as resources are allocated on-demand through the OpenStack platform</p>"},{"location":"day2/hpc-ilifu-training/#getting-started-with-ilifu","title":"Getting Started with ILIFU","text":""},{"location":"day2/hpc-ilifu-training/#account-setup","title":"Account Setup","text":"<ol> <li>Request Access: Apply through your institution</li> <li>SSH Keys: Generate and register SSH key pairs</li> <li>VPN: Configure institutional VPN if required</li> <li>Initial Login: Connect to login nodes</li> </ol>"},{"location":"day2/hpc-ilifu-training/#basic-commands","title":"Basic Commands","text":"<pre><code># Login to ILIFU\nssh username@training.ilifu.ac.za\n\n# Check your home directory\nls -la ~\n\n# Check available modules\nmodule avail\n\n# Load a module\nmodule load python/3.12.3  # Or use system python3\n</code></pre> <p>\ud83d\udca1 Next Steps: After logging in, follow the hands-on exercises in High Performance Computing with SLURM: Practical Tutorial</p>"},{"location":"day2/hpc-ilifu-training/#file-system-layout","title":"File System Layout","text":"<pre><code>/home/username/          # Your home directory (limited space)\n/scratch/username/       # Temporary fast storage\n/data/project/          # Shared project data\n/software/              # Installed software\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#data-management","title":"Data Management","text":"<ul> <li>Home Directory: Small, backed up, permanent</li> <li>Scratch Space: Large, fast, temporary (auto-cleaned)</li> <li>Project Directories: Shared, persistent, for collaboration</li> </ul>"},{"location":"day2/hpc-ilifu-training/#slurm-basics","title":"SLURM Basics","text":"<p>\ud83d\udcda For detailed SLURM tutorials and exercises, see: High Performance Computing with SLURM: Practical Tutorial</p>"},{"location":"day2/hpc-ilifu-training/#what-is-slurm","title":"What is SLURM?","text":"<pre><code>graph TB\n    subgraph \"Users\"\n        U1[User 1]\n        U2[User 2]\n        U3[User 3]\n    end\n\n    subgraph \"Login Nodes\"\n        LN[Login Node&lt;br/&gt;- SSH Access&lt;br/&gt;- Job Submission&lt;br/&gt;- File Editing]\n    end\n\n    subgraph \"SLURM Controller\"\n        SC[SLURM Scheduler&lt;br/&gt;- Resource Allocation&lt;br/&gt;- Job Queuing&lt;br/&gt;- Priority Management]\n        DB[(Accounting&lt;br/&gt;Database)]\n    end\n\n    subgraph \"Compute Nodes\"\n        CN1[Compute Node 1&lt;br/&gt;CPUs: 32&lt;br/&gt;RAM: 128GB]\n        CN2[Compute Node 2&lt;br/&gt;CPUs: 32&lt;br/&gt;RAM: 128GB]\n        CN3[Compute Node 3&lt;br/&gt;CPUs: 32&lt;br/&gt;RAM: 128GB]\n        CNN[... More Nodes]\n    end\n\n    subgraph \"Storage\"\n        FS[Shared Filesystem&lt;br/&gt;/home&lt;br/&gt;/scratch&lt;br/&gt;/data]\n    end\n\n    U1 --&gt; LN\n    U2 --&gt; LN\n    U3 --&gt; LN\n\n    LN --&gt;|sbatch/srun| SC\n    SC --&gt; DB\n    SC --&gt;|Allocates| CN1\n    SC --&gt;|Allocates| CN2\n    SC --&gt;|Allocates| CN3\n    SC --&gt;|Allocates| CNN\n\n    CN1 --&gt; FS\n    CN2 --&gt; FS\n    CN3 --&gt; FS\n    CNN --&gt; FS\n    LN --&gt; FS\n\n    style U1 fill:#e1f5fe\n    style U2 fill:#e1f5fe\n    style U3 fill:#e1f5fe\n    style LN fill:#fff3e0\n    style SC fill:#f3e5f5\n    style DB fill:#f3e5f5\n    style CN1 fill:#e8f5e9\n    style CN2 fill:#e8f5e9\n    style CN3 fill:#e8f5e9\n    style CNN fill:#e8f5e9\n    style FS fill:#fce4ec</code></pre> <p>Figure: HPC cluster architecture showing the relationship between users, login nodes, SLURM scheduler, compute nodes, and shared storage</p>"},{"location":"day2/hpc-ilifu-training/#about-slurm","title":"About SLURM","text":"<p>Simple Linux Utility for Resource Management (SLURM) is a job scheduling and cluster management tool that:</p> <ul> <li>Job scheduler: Allocates compute resources efficiently among users</li> <li>Resource manager: Controls access to CPUs, memory, and other resources  </li> <li>Workload manager: Manages job queues and priorities based on fairness policies</li> <li>Framework components: Login nodes for access, compute nodes for execution, scheduler for coordination, and accounting database for tracking</li> </ul>"},{"location":"day2/hpc-ilifu-training/#key-slurm-concepts","title":"Key SLURM Concepts","text":"<ul> <li>Job: A computational task submitted to the cluster</li> <li>Partition: Group of nodes with similar characteristics</li> <li>Queue: Collection of jobs waiting for resources</li> <li>Node: Individual compute server</li> <li>Core/CPU: Processing unit within a node</li> </ul>"},{"location":"day2/hpc-ilifu-training/#basic-slurm-commands","title":"Basic SLURM Commands","text":"<pre><code># Submit a job\nsbatch job_script.sh\n\n# Check job status\nsqueue -u username\n\n# Cancel a job\nscancel job_id\n\n# Check node information\nsinfo\n\n# Check your job history\nsacct -u username\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-script-template","title":"Job Script Template","text":"<p>To create a job script, use the nano text editor:</p> <pre><code># Open nano editor to create your script\nnano my_job.sh\n</code></pre> <p>Then copy and paste the following template:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=my_job\n#SBATCH --partition=Main\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8GB\n#SBATCH --time=01:00:00\n#SBATCH --output=output_%j.log\n#SBATCH --error=error_%j.log\n\n# Load modules\nmodule load python/3.12.3  # Or use system python3\n\n# Run your program\npython my_script.py\n</code></pre> <p>To save and exit nano: - Press <code>Ctrl+X</code> to exit - Press <code>Y</code> to confirm save - Press <code>Enter</code> to accept the filename</p>"},{"location":"day2/hpc-ilifu-training/#slurm-directives-explained","title":"SLURM Directives Explained","text":"<ul> <li><code>--job-name</code>: Human-readable job name</li> <li><code>--partition</code>: Which partition to use</li> <li><code>--nodes</code>: Number of nodes required</li> <li><code>--ntasks-per-node</code>: Tasks per node</li> <li><code>--cpus-per-task</code>: CPUs per task</li> <li><code>--mem</code>: Memory requirement</li> <li><code>--time</code>: Maximum runtime</li> <li><code>--output/--error</code>: Log file locations</li> </ul>"},{"location":"day2/hpc-ilifu-training/#resource-management","title":"Resource Management","text":""},{"location":"day2/hpc-ilifu-training/#understanding-resources","title":"Understanding Resources","text":"<ul> <li>CPU Cores: Processing units</li> <li>Memory (RAM): Working memory</li> <li>GPU: Graphics processing units</li> <li>Storage: Disk space</li> <li>Network: Data transfer bandwidth</li> </ul>"},{"location":"day2/hpc-ilifu-training/#resource-allocation-strategies","title":"Resource Allocation Strategies","text":"<pre><code># CPU-intensive job\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=32GB\n\n# Memory-intensive job\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=64GB\n\n# GPU job\n#SBATCH --gres=gpu:1\n#SBATCH --partition=GPU\n\n# Parallel job\n#SBATCH --nodes=2\n#SBATCH --ntasks=32\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#monitoring-resource-usage","title":"Monitoring Resource Usage","text":"<pre><code># Check job efficiency\nseff job_id\n\n# Real-time job monitoring\nsstat job_id\n\n# Detailed job information\nscontrol show job job_id\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#best-practices","title":"Best Practices","text":""},{"location":"day2/hpc-ilifu-training/#job-submission","title":"Job Submission","text":"<ul> <li>Test small first: Start with short test runs</li> <li>Use checkpoints: Save progress regularly</li> <li>Estimate resources: Don't over-request</li> <li>Use appropriate partitions: Match job to partition</li> <li>Clean up: Remove temporary files</li> </ul>"},{"location":"day2/hpc-ilifu-training/#code-optimization","title":"Code Optimization","text":"<pre><code># Use parallel processing\n#SBATCH --cpus-per-task=8\n\n# In Python\nfrom multiprocessing import Pool\nwith Pool(8) as pool:\n    results = pool.map(my_function, data)\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#data-management_1","title":"Data Management","text":"<ul> <li>Use scratch space for temporary files</li> <li>Compress data when possible</li> <li>Clean up regularly</li> <li>Use appropriate file formats</li> </ul>"},{"location":"day2/hpc-ilifu-training/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":"<ul> <li>Requesting too many resources</li> <li>Running jobs on login nodes</li> <li>Not using version control</li> <li>Ignoring error messages</li> <li>Not testing scripts locally first</li> </ul>"},{"location":"day2/hpc-ilifu-training/#practical-examples","title":"Practical Examples","text":"<p>\ud83d\udcdd Complete Step-by-Step Tutorials: For detailed, hands-on SLURM exercises with explanations, see High Performance Computing with SLURM: Practical Tutorial</p>"},{"location":"day2/hpc-ilifu-training/#example-1-python-data-analysis","title":"Example 1: Python Data Analysis","text":"<p>To create this script:</p> <pre><code># Open nano editor\nnano data_analysis.sh\n\n# Copy and paste the script below, then:\n# Press Ctrl+X to exit\n# Press Y to save\n# Press Enter to confirm filename\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=data_analysis\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16GB\n#SBATCH --time=02:00:00\n#SBATCH --output=analysis_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n# Install with: pip install pandas numpy matplotlib\n\npython data_analysis.py input.csv\n</code></pre> <p>Submit with: <code>sbatch data_analysis.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#example-2-r-statistical-analysis","title":"Example 2: R Statistical Analysis","text":"<p>Create the script with nano:</p> <pre><code>nano r_stats.sh\n# Paste the script below, save with Ctrl+X, Y, Enter\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=r_stats\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8GB\n#SBATCH --time=01:30:00\n\nmodule load R/4.4.1  # Check available version\n\nRscript statistical_analysis.R\n</code></pre> <p>Submit with: <code>sbatch r_stats.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#example-3-gpu-machine-learning","title":"Example 3: GPU Machine Learning","text":"<p>Create the script:</p> <pre><code>nano ml_training.sh\n# Paste content, save with Ctrl+X, Y, Enter\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=ml_training\n#SBATCH --partition=GPU\n#SBATCH --gres=gpu:1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32GB\n#SBATCH --time=04:00:00\n\n# module load cuda  # Check if GPU/CUDA is available\nmodule load python/3.12.3  # Or use system python3\n\npython train_model.py\n</code></pre> <p>Submit with: <code>sbatch ml_training.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#example-4-array-jobs","title":"Example 4: Array Jobs","text":"<p>Create the array job script:</p> <pre><code>nano array_job.sh\n# Paste content, save with Ctrl+X, Y, Enter\n</code></pre> <p>Script content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=array_job\n#SBATCH --partition=Main\n#SBATCH --array=1-100\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4GB\n#SBATCH --time=00:30:00\n\n# Process different files based on array index\ninput_file=\"data_${SLURM_ARRAY_TASK_ID}.txt\"\noutput_file=\"result_${SLURM_ARRAY_TASK_ID}.txt\"\n\npython process_data.py $input_file $output_file\n</code></pre> <p>Submit with: <code>sbatch array_job.sh</code></p>"},{"location":"day2/hpc-ilifu-training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"day2/hpc-ilifu-training/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"day2/hpc-ilifu-training/#job-wont-start","title":"Job Won't Start","text":"<pre><code># Check partition limits\nscontrol show partition\n\n# Check job details\nscontrol show job job_id\n\n# Check node availability\nsinfo -N\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#out-of-memory-errors","title":"Out of Memory Errors","text":"<pre><code># Check memory usage\nsstat -j job_id --format=AveCPU,AvePages,AveRSS,AveVMSize\n\n# Increase memory request\n#SBATCH --mem=32GB\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-timeouts","title":"Job Timeouts","text":"<pre><code># Check time limits\nscontrol show partition\n\n# Increase time limit\n#SBATCH --time=04:00:00\n\n# Use checkpointing for long jobs\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#module-issues","title":"Module Issues","text":"<pre><code># List available modules\nmodule avail\n\n# Check module conflicts\nmodule list\n\n# Purge and reload\nmodule purge\nmodule load python/3.12.3  # Or use system python3\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Check ILIFU docs</li> <li>Help Desk: Submit support tickets</li> <li>Community: Ask on forums or Slack</li> <li>Training: Attend workshops</li> <li>Practical Tutorials: Work through High Performance Computing with SLURM: Practical Tutorial</li> </ul>"},{"location":"day2/hpc-ilifu-training/#quick-reference","title":"Quick Reference","text":""},{"location":"day2/hpc-ilifu-training/#essential-slurm-commands","title":"Essential SLURM Commands","text":"Command Purpose Example Output <code>sbatch script.sh</code> Submit job <code>Submitted batch job 10</code> <code>squeue -u $USER</code> Check your jobs Shows running/pending jobs <code>scancel job_id</code> Cancel job Terminates specified job <code>sinfo</code> Node information Shows partition and node status <code>sacct -j job_id</code> Job accounting Shows job completion details <code>seff job_id</code> Job efficiency Shows resource utilization"},{"location":"day2/hpc-ilifu-training/#example-command-outputs","title":"Example Command Outputs","text":""},{"location":"day2/hpc-ilifu-training/#checking-partition-information","title":"Checking Partition Information","text":"<pre><code>$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\ntraining*    up 14-00:00:0      7   idle compute-1-sep2025,compute-2-sep2025,compute-3-sep2025,compute-4-sep2025,compute-5-sep2025,compute-6-sep2025,compute-7-sep2025\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-submission-and-status","title":"Job Submission and Status","text":"<pre><code>$ sbatch hello.sh\nSubmitted batch job 10\n\n$ squeue -u mamana\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                10  training    hello   mamana  R       0:01      1 compute-1-sep2025\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#job-efficiency-report","title":"Job Efficiency Report","text":"<pre><code>$ seff 10\nJob ID: 10\nCluster: training\nUser/Group: mamana/training\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 1\nCPU Utilized: 00:00:00\nCPU Efficiency: 0.00% of 00:00:01 core-walltime\nJob Wall-clock time: 00:00:01\nMemory Utilized: 4.80 MB\nMemory Efficiency: 0.48% of 1.00 GB\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#common-sbatch-directives","title":"Common SBATCH Directives","text":"Directive Purpose Example <code>--job-name</code> Job name <code>my_analysis</code> <code>--partition</code> Partition <code>Main</code>, <code>GPU</code> <code>--cpus-per-task</code> CPU cores <code>4</code> <code>--mem</code> Memory <code>16GB</code> <code>--time</code> Runtime limit <code>02:00:00</code> <code>--gres</code> GPU resources <code>gpu:1</code>"},{"location":"day2/hpc-ilifu-training/#file-transfer","title":"File Transfer","text":"<pre><code># Upload data\nscp local_file.txt username@training.ilifu.ac.za:~/\n\n# Download results\nscp username@training.ilifu.ac.za:~/results.txt ./\n\n# Sync directories\nrsync -av local_dir/ username@training.ilifu.ac.za:~/remote_dir/\n</code></pre>"},{"location":"day2/hpc-ilifu-training/#additional-resources","title":"Additional Resources","text":"<ul> <li>ILIFU Documentation: https://docs.ilifu.ac.za</li> <li>SLURM Documentation: https://slurm.schedmd.com/documentation.html</li> <li>HPC Best Practices: Various online resources</li> <li>Training Materials: Regular workshops and tutorials</li> </ul>"},{"location":"day2/slurm-practical-tutorial/","title":"High Performance Computing with SLURM: Practical Tutorial","text":""},{"location":"day2/slurm-practical-tutorial/#getting-started-setup-instructions","title":"Getting Started - Setup Instructions","text":"<p>Before starting the exercises, you need to set up your working environment and copy the sample data files to your home directory. Follow these steps:</p>"},{"location":"day2/slurm-practical-tutorial/#step-1-create-your-working-directory","title":"Step 1: Create Your Working Directory","text":"<pre><code># The mkdir command creates a new directory\n# The -p flag creates parent directories if they don't exist\nmkdir -p ~/hpc_practice\n\n# Change to your new working directory\n# The ~ symbol represents your home directory\ncd ~/hpc_practice\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-copy-sample-data-files","title":"Step 2: Copy Sample Data Files","text":"<pre><code># Copy all sample data from the shared course directory to your current directory\n# The -r flag means \"recursive\" - it copies directories and their contents\n# The * wildcard matches all files in the source directory\n# The . (dot) means \"current directory\" (where you are now)\ncp -r /cbio/training/courses/2025/micmet-genomics/sample-data/* .\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-verify-your-setup","title":"Step 3: Verify Your Setup","text":"<pre><code># List all files in your directory to confirm they copied correctly\n# The -l flag shows detailed information (permissions, size, date)\n# The -a flag shows all files including hidden ones (starting with .)\nls -la\n\n# You should see these files:\n# - sample.fastq.gz    : Compressed DNA sequencing data (gzipped FASTQ format)\n# - sample1.fastq      : Uncompressed sequencing reads for practice\n# - sample2.fastq      : Another set of sequencing reads\n# - reference.fasta    : Reference genome sequence for alignment exercises\n# - data.txt          : Tab-delimited data for text processing examples\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#what-these-files-contain","title":"What These Files Contain","text":"<ul> <li>FASTQ files: Contain DNA sequences and quality scores from sequencing machines</li> <li>FASTA files: Contain reference sequences without quality scores</li> <li>Text files: Contain structured data for analysis practice</li> </ul> <p>Now you're ready to start the exercises!</p>"},{"location":"day2/slurm-practical-tutorial/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites - Unix Commands</li> <li>Getting Started - Your First Job</li> <li>Basic Job Templates</li> <li>Python and Bash Examples</li> <li>Advanced Job Types</li> <li>Resource Optimization</li> <li>Troubleshooting Examples</li> </ol>"},{"location":"day2/slurm-practical-tutorial/#prerequisites","title":"Prerequisites","text":""},{"location":"day2/slurm-practical-tutorial/#essential-unix-commands-for-hpc","title":"Essential Unix Commands for HPC","text":"<p>Before submitting SLURM jobs, master these Unix commands for pathogen genomics:</p> <pre><code># Navigate and organize\nmkdir -p project/{data,results,scripts}\ncd project\npwd\n\n# Inspect FASTQ files\nzcat sample.fastq.gz | head -20\nzcat sample.fastq.gz | wc -l | awk '{print $1/4}'  # Count reads\n\n# Search and filter\ngrep \"^&gt;\" reference.fasta  # Find FASTA headers\ngrep -c \"PASS\" variants.vcf  # Count PASS variants\n\n# Process text\nawk '{print $1, $2}' data.txt\nsed 's/old/new/g' file.txt\n</code></pre> <p>\ud83d\udcda Full Unix guide: See Unix Commands for Pathogen Genomics - Practical Tutorial for comprehensive examples and exercises.</p>"},{"location":"day2/slurm-practical-tutorial/#tutorial-your-first-slurm-jobs-step-by-step","title":"Tutorial: Your First SLURM Jobs - Step by Step","text":""},{"location":"day2/slurm-practical-tutorial/#tutorial-overview","title":"Tutorial Overview","text":"<p>In this hands-on tutorial, you'll learn to:</p> <ol> <li>Write and submit your first SLURM job</li> <li>Monitor job status and view outputs</li> <li>Run Python scripts on HPC</li> <li>Process genomics data with SLURM</li> <li>Handle errors and optimize resources</li> </ol> <p>Time needed: 30-45 minutes Prerequisites: Basic Unix commands (covered above)</p>"},{"location":"day2/slurm-practical-tutorial/#tutorial-1-hello-world-on-hpc","title":"Tutorial 1: Hello World on HPC","text":""},{"location":"day2/slurm-practical-tutorial/#step-1-write-your-first-job-script","title":"Step 1: Write Your First Job Script","text":"<p>Create a simple SLURM job that prints a greeting:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=hello\n#SBATCH --time=00:05:00\n\necho \"Hello from HPC!\"\necho \"This job ran on node: $(hostname)\"\necho \"Current time: $(date)\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-save-the-script","title":"Step 2: Save the Script","text":"<pre><code># Use nano editor to create the file\nnano hello.sh\n\n# Paste the script above, then:\n# Press Ctrl+X to exit\n# Press Y to save\n# Press Enter to confirm filename\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-submit-your-job","title":"Step 3: Submit Your Job","text":"<pre><code># Submit the job to SLURM\nsbatch hello.sh\n</code></pre> <p>You'll see: <code>Submitted batch job 12345</code> (your job ID will differ)</p>"},{"location":"day2/slurm-practical-tutorial/#step-4-monitor-your-job","title":"Step 4: Monitor Your Job","text":"<pre><code># Check if your job is running\nsqueue -u $USER\n\n# You'll see something like:\n# JOBID PARTITION     NAME     USER ST       TIME  NODES\n# 12345      Main    hello  yourname  R       0:01      1\n# ST column: PD=Pending, R=Running, CG=Completing\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-5-view-the-output","title":"Step 5: View the Output","text":"<pre><code># Once job completes (status disappears from squeue)\n# View the output file (replace 12345 with your job ID)\ncat slurm-12345.out\n</code></pre> <p>Example run:</p> <pre><code>$ sbatch hello.sh\nSubmitted batch job 10\n\n$ squeue -u $USER\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                10  training    hello   mamana  R       0:01      1 compute-1-sep2025\n\n$ cat slurm-10.out\nHello from HPC!\nThis job ran on node: compute-1-sep2025\nCurrent time: Mon Sep 1 23:57:07 SAST 2025\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#tutorial-2-running-python-on-hpc","title":"Tutorial 2: Running Python on HPC","text":""},{"location":"day2/slurm-practical-tutorial/#step-1-create-a-python-job-script","title":"Step 1: Create a Python Job Script","text":"<p>Let's run Python code on the cluster:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=python_hello\n#SBATCH --time=00:10:00\n#SBATCH --mem=1GB\n\n# Load Python (or use system python3 if modules not available)\nmodule load python/3.12.3  # Or use system python3 || echo \"Using system Python\"\n\n# Run your Python script\npython3 &lt;&lt; 'EOF'\nprint(\"Hello from Python on HPC!\")\nimport os\nprint(f\"Running on: {os.uname().nodename}\")\n\n# Simple calculation\nresult = sum(range(1000))\nprint(f\"Sum of 0-999 = {result}\")\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-submit-and-monitor","title":"Step 2: Submit and Monitor","text":"<pre><code># Save the script\nnano python_job.sh\n# (paste script, save with Ctrl+X, Y, Enter)\n\n# Submit the job\nsbatch python_job.sh\n\n# Watch it run (updates every 2 seconds)\nwatch -n 2 squeue -u $USER\n# Press Ctrl+C to stop watching\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-check-the-output","title":"Step 3: Check the Output","text":"<pre><code># Find your output file\nls -lt slurm-*.out | head -5\n\n# View the results\ncat slurm-[JOBID].out\n</code></pre> <p>Expected output:</p> <pre><code>Hello from Python on HPC!\nRunning on: compute-1-sep2025\nSum of 0-999 = 499500\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"Problem Solution \"Module not found\" Use <code>python3</code> instead of loading module \"Python: command not found\" Check with <code>which python3</code> Job stays pending too long Check resources with <code>sinfo</code>"},{"location":"day2/slurm-practical-tutorial/#tutorial-3-real-genomics-analysis","title":"Tutorial 3: Real Genomics Analysis","text":""},{"location":"day2/slurm-practical-tutorial/#objective","title":"Objective","text":"<p>Process FASTQ files using SLURM, simulating a real bioinformatics pipeline.</p>"},{"location":"day2/slurm-practical-tutorial/#step-1-create-the-analysis-script","title":"Step 1: Create the Analysis Script","text":"<p>This script demonstrates a typical genomics workflow:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=fastq_analysis\n#SBATCH --time=00:05:00\n#SBATCH --mem=2GB\n#SBATCH --cpus-per-task=2\n\necho \"=== FASTQ Analysis Pipeline Starting ===\"\necho \"Job ID: $SLURM_JOB_ID\"\necho \"Node: $(hostname)\"\necho \"Start time: $(date)\"\n\n# Create sample FASTQ files for analysis\necho \"Creating sample FASTQ files...\"\ncat &gt; sample1.fastq &lt;&lt; 'EOF'\n@SEQ_1\nGATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65\n@SEQ_2\nACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGT\n+\nBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\nEOF\n\ncat &gt; sample2.fastq &lt;&lt; 'EOF'\n@SEQ_3\nTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n+\nIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n@SEQ_4\nCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n+\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nEOF\n\necho \"Step 1: Initial file validation...\"\nsleep 45  # Simulate file checking and validation\n\necho \"Step 2: Sequence counting and basic stats...\"\nfor file in sample*.fastq; do\n    echo \"Processing $file...\"\n    sequences=$(wc -l &lt; \"$file\")\n    sequences=$((sequences / 4))\n    echo \"  Found $sequences sequences\"\n\n    # Simulate per-file analysis time\n    echo \"  Analyzing sequence lengths...\"\n    sleep 25  # Processing time per file\n\n    avg_length=60\n    echo \"  Average sequence length: ${avg_length}bp\"\ndone\n\necho \"Step 3: Quality score analysis...\"\necho \"Analyzing quality scores across all sequences...\"\nsleep 60  # Simulate quality analysis\n\necho \"Step 4: Generating contamination check...\"\necho \"Checking for adapter sequences and contaminants...\"\nsleep 45  # Simulate contamination screening\n\necho \"Step 5: Creating final summary report...\"\ntotal_sequences=0\nfor file in sample*.fastq; do\n    seqs=$(wc -l &lt; \"$file\")\n    seqs=$((seqs / 4))\n    total_sequences=$((total_sequences + seqs))\ndone\n\necho \"Step 6: Finalizing results and cleanup...\"\nsleep 20  # Final processing and cleanup\n\necho \"=== Analysis Complete ===\"\necho \"Total sequences analyzed: $total_sequences\"\necho \"Analysis completed at: $(date)\"\necho \"Total runtime: ~4 minutes\"\n\n# Create a summary file\ncat &gt; analysis_summary.txt &lt;&lt; EOF\nFASTQ Analysis Summary\n=====================\nTotal files processed: 2\nTotal sequences: $total_sequences\nAverage sequence length: 60bp\nQuality check: PASSED\nContamination check: CLEAN\nAnalysis date: $(date)\nEOF\n\necho \"Summary report saved to: analysis_summary.txt\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-2-submit-and-monitor-the-job","title":"Step 2: Submit and Monitor the Job","text":"<pre><code># Save the script\nnano fastq_analysis.sh\n\n# Submit the job\nsbatch fastq_analysis.sh\n# Note your job ID (e.g., \"Submitted batch job 12347\")\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-3-monitor-job-progress-in-real-time","title":"Step 3: Monitor Job Progress in Real-Time","text":"<p>Open multiple terminal windows to watch different aspects:</p> <p>Terminal 1: Submit and monitor queue</p> <pre><code># Submit the job\nsbatch fastq_analysis.sh\nSubmitted batch job 15\n\n# Watch it in the queue (run multiple times)\n# Watch the queue (repeat every 10 seconds)\nsqueue -u $USER\n# Status codes: PD=Pending, R=Running, CG=Completing\n</code></pre> <p>Terminal 2: Watch live output</p> <pre><code># Once job starts running (status = R), watch the output\ntail -f slurm-15.out\n# Press Ctrl+C to stop watching\n</code></pre> <p>Terminal 3: Check job details</p> <pre><code># Get detailed job information\nscontrol show job 15\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#step-4-understanding-job-states","title":"Step 4: Understanding Job States","text":"<p>During the 4-minute runtime, you'll observe these states:</p> Time Status What's Happening 0:00-0:05 PD (Pending) Job waiting for resources 0:05-4:00 R (Running) Job executing on compute node 4:00+ - (Completed) Job finished, no longer in queue <p>Timeline of analysis steps:</p> <ul> <li>0:00-0:45 - File validation</li> <li>0:45-1:35 - Sequence counting (sample1.fastq)</li> <li>1:35-2:25 - Sequence counting (sample2.fastq)  </li> <li>2:25-3:25 - Quality score analysis</li> <li>3:25-4:10 - Contamination screening</li> <li>4:10-4:30 - Final report generation</li> </ul> <p>Learning opportunity: This 4-minute window allows everyone to:</p> <ul> <li>Practice using <code>squeue</code> to monitor jobs multiple times</li> <li>See job state transitions and timing in real-time  </li> <li>Understand queue system behavior with sufficient time for discussion</li> <li>Watch live output with <code>tail -f</code> to see analysis progress</li> <li>Check intermediate results and final efficiency reports</li> </ul> <p>\ud83d\udca1 Training Tip: Have participants submit this job, then use the 4-minute window to demonstrate:</p> <ul> <li>Refreshing <code>squeue -u $USER</code> every 30 seconds to track progress</li> <li>Using <code>scontrol show job JOBID</code> for detailed job information</li> <li>Explaining what PENDING vs RUNNING states mean</li> <li>Demonstrating <code>tail -f slurm-JOBID.out</code> to watch live step-by-step output</li> <li>Discussing resource allocation while job runs</li> <li>Explaining the difference between walltime and CPU time</li> </ul> <p>Expected final output files:</p> <ul> <li><code>slurm-JOBID.out</code> - Complete log of all analysis steps</li> <li><code>analysis_summary.txt</code> - Final summary report</li> <li><code>sample1.fastq</code> &amp; <code>sample2.fastq</code> - Generated test data files</li> </ul> <p>Sample log output:</p> <pre><code>=== FASTQ Analysis Pipeline Starting ===\nJob ID: 15\nNode: compute-2-sep2025\nStart time: Mon Sep 2 10:15:23 SAST 2025\nCreating sample FASTQ files...\nStep 1: Initial file validation...\nStep 2: Sequence counting and basic stats...\nProcessing sample1.fastq...\n  Found 2 sequences\n  Analyzing sequence lengths...\n  Average sequence length: 60bp\nProcessing sample2.fastq...\n  Found 2 sequences\n  Analyzing sequence lengths...\n  Average sequence length: 60bp\nStep 3: Quality score analysis...\nAnalyzing quality scores across all sequences...\nStep 4: Generating contamination check...\nChecking for adapter sequences and contaminants...\nStep 5: Creating final summary report...\nStep 6: Finalizing results and cleanup...\n=== Analysis Complete ===\nTotal sequences analyzed: 4\nAnalysis completed at: Mon Sep 2 10:19:45 SAST 2025\nTotal runtime: ~4 minutes\nSummary report saved to: analysis_summary.txt\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#practice-exercises","title":"Practice Exercises","text":""},{"location":"day2/slurm-practical-tutorial/#exercise-1-modify-and-submit-a-job","title":"Exercise 1: Modify and Submit a Job","text":"<p>Task: Modify the hello.sh script to include your name and the current date.</p> <pre><code># Step 1: Edit the script\nnano hello.sh\n\n# Step 2: Add these lines:\necho \"Submitted by: [YOUR NAME]\"\necho \"Analysis date: $(date +%Y-%m-%d)\"\n\n# Step 3: Submit and check\nsbatch hello.sh\nsqueue -u $USER\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#exercise-2-resource-monitoring","title":"Exercise 2: Resource Monitoring","text":"<p>Task: Create a job that uses specific resources and monitor them.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=resource_test\n#SBATCH --time=00:02:00\n#SBATCH --mem=500MB\n#SBATCH --cpus-per-task=2\n\necho \"Allocated CPUs: $SLURM_CPUS_PER_TASK\"\necho \"Allocated Memory: $SLURM_MEM_PER_NODE MB\"\necho \"Running on node: $(hostname)\"\n\n# Use the allocated CPUs\nstress --cpu $SLURM_CPUS_PER_TASK --timeout 30s\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#exercise-3-array-jobs","title":"Exercise 3: Array Jobs","text":"<p>Task: Process multiple files in parallel using array jobs.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=array_demo\n#SBATCH --array=1-3\n#SBATCH --time=00:05:00\n\necho \"Processing file number: $SLURM_ARRAY_TASK_ID\"\n# Your processing command here\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#basic-templates","title":"Basic Templates","text":""},{"location":"day2/slurm-practical-tutorial/#1-standard-job-template","title":"1. Standard Job Template","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=my_job          # Give your job a name\n#SBATCH --time=01:00:00            # Max runtime (1 hour)\n#SBATCH --mem=4GB                  # Memory needed\n#SBATCH --output=output_%j.log     # Output file (%j = job ID)\n\n# Load software you need\nmodule load python/3.12.3  # Or use system python3\n\n# Run your command\necho \"Job started on $(hostname) at $(date)\"\npython my_script.py\necho \"Job completed at $(date)\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-multi-core-parallel-job","title":"2. Multi-core Parallel Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=parallel_job\n#SBATCH --partition=Main\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16GB\n#SBATCH --time=02:00:00\n#SBATCH --output=parallel_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n\n# Use all available cores\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\necho \"Using $SLURM_CPUS_PER_TASK CPU cores\"\npython parallel_script.py\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#python-and-bash-examples","title":"Python and Bash Examples","text":""},{"location":"day2/slurm-practical-tutorial/#python-jobs","title":"Python Jobs","text":""},{"location":"day2/slurm-practical-tutorial/#basic-python-analysis","title":"Basic Python Analysis","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=python_analysis\n#SBATCH --time=01:30:00\n#SBATCH --mem=8GB\n#SBATCH --cpus-per-task=4\n\n# Load Python module\nmodule load python/3.12.3  # Or use system python3\n\n# Run your analysis\npython genome_analysis.py sample_data.fasta\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#python-with-virtual-environment","title":"Python with Virtual Environment","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=python_venv\n#SBATCH --time=02:00:00\n#SBATCH --mem=16GB\n\nmodule load python/3.12.3  # Or use system python3\n\n# Create and activate virtual environment\npython -m venv pathogen_env\nsource pathogen_env/bin/activate\n\n# Install bioinformatics packages\npip install biopython pandas numpy matplotlib\n\n# Run pathogen analysis\npython pathogen_analysis.py\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#pathogen-genomics-snp-analysis","title":"Pathogen Genomics - SNP Analysis","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=snp_analysis\n#SBATCH --time=04:00:00\n#SBATCH --mem=32GB\n#SBATCH --cpus-per-task=8\n\nmodule load python/3.12.3  # Or use system python3\n\n# Python script for SNP analysis\npython &lt;&lt; 'EOF'\nimport pandas as pd\nfrom multiprocessing import Pool\nimport os\n\ndef analyze_sample(vcf_file):\n    \"\"\"Analyze SNPs in a VCF file\"\"\"\n    print(f\"Processing {vcf_file}\")\n\n    # Count SNPs (simplified example)\n    with open(vcf_file, 'r') as f:\n        snp_count = sum(1 for line in f if not line.startswith('#'))\n\n    return vcf_file, snp_count\n\n# Get all VCF files\nvcf_files = [f for f in os.listdir('.') if f.endswith('.vcf')]\n\n# Use all available CPU cores\nwith Pool(int(os.environ['SLURM_CPUS_PER_TASK'])) as pool:\n    results = pool.map(analyze_sample, vcf_files)\n\n# Save results\nresults_df = pd.DataFrame(results, columns=['Sample', 'SNP_Count'])\nresults_df.to_csv('snp_analysis_results.csv', index=False)\nprint(f\"Analyzed {len(vcf_files)} samples\")\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#bashshell-script-jobs","title":"Bash/Shell Script Jobs","text":""},{"location":"day2/slurm-practical-tutorial/#basic-fastq-processing","title":"Basic FASTQ Processing","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=fastq_processing\n#SBATCH --time=01:00:00\n#SBATCH --mem=4GB\n\n# Process multiple FASTQ files\nfor file in *.fastq; do\n    echo \"Processing $file...\"\n\n    # Count sequences (FASTQ has 4 lines per sequence)\n    sequences=$(wc -l &lt; \"$file\")\n    sequences=$((sequences / 4))\n\n    # Get basic stats\n    echo \"File: $file - Sequences: $sequences\"\n\n    # Count reads with quality scores above threshold\n    good_reads=$(awk 'NR%4==0 &amp;&amp; length($0)&gt;20' \"$file\" | wc -l)\n    echo \"High quality reads: $good_reads\"\ndone\n\necho \"Processing complete!\"\n</code></pre> <p>Expected output:</p> <pre><code>Processing sample1.fastq...\nFile: sample1.fastq - Sequences: 3\nHigh quality reads: 3\nProcessing sample2.fastq...\nFile: sample2.fastq - Sequences: 2\nHigh quality reads: 2\nProcessing complete!\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#pathogen-genomics-pipeline","title":"Pathogen Genomics Pipeline","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=pathogen_pipeline\n#SBATCH --time=06:00:00\n#SBATCH --mem=64GB\n#SBATCH --cpus-per-task=16\n\n# Load bioinformatics tools\nmodule load fastqc/0.12.1  # Check available version with 'module avail'\n# module load trimmomatic  # Install if needed\nmodule load bwa/github  # Check available version\nmodule load samtools/1.22.1\nmodule load bcftools/1.22\n\n# Sample information\nSAMPLE=\"pathogen_sample\"\nREFERENCE=\"reference_genome.fasta\"\n\necho \"=== Pathogen Genomics Pipeline Starting ===\"\necho \"Sample: $SAMPLE\"\necho \"Reference: $REFERENCE\"\necho \"CPUs: $SLURM_CPUS_PER_TASK\"\n\n# Step 1: Quality control\necho \"Step 1: Running FastQC...\"\nmkdir -p qc_reports\nfastqc \"${SAMPLE}_R1.fastq\" \"${SAMPLE}_R2.fastq\" -o qc_reports/\n\n# Step 2: Trim low-quality reads and adapters\necho \"Step 2: Trimming reads...\"\ntrimmomatic PE -threads $SLURM_CPUS_PER_TASK \\\n    \"${SAMPLE}_R1.fastq\" \"${SAMPLE}_R2.fastq\" \\\n    \"${SAMPLE}_R1_trimmed.fastq\" \"${SAMPLE}_R1_unpaired.fastq\" \\\n    \"${SAMPLE}_R2_trimmed.fastq\" \"${SAMPLE}_R2_unpaired.fastq\" \\\n    ILLUMINACLIP:adapters.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n\n# Step 3: Align reads to reference genome\necho \"Step 3: Aligning to reference genome...\"\nbwa mem -t $SLURM_CPUS_PER_TASK \"$REFERENCE\" \\\n    \"${SAMPLE}_R1_trimmed.fastq\" \"${SAMPLE}_R2_trimmed.fastq\" | \\\n    samtools sort -@ $SLURM_CPUS_PER_TASK -o \"${SAMPLE}_sorted.bam\"\n\n# Step 4: Index BAM file\necho \"Step 4: Indexing BAM file...\"\nsamtools index \"${SAMPLE}_sorted.bam\"\n\n# Step 5: Variant calling\necho \"Step 5: Calling variants...\"\nbcftools mpileup -f \"$REFERENCE\" \"${SAMPLE}_sorted.bam\" | \\\n    bcftools call -mv -Oz -o \"${SAMPLE}_variants.vcf.gz\"\n\n# Step 6: Index VCF and get basic stats\necho \"Step 6: Processing variants...\"\nbcftools index \"${SAMPLE}_variants.vcf.gz\"\nbcftools stats \"${SAMPLE}_variants.vcf.gz\" &gt; \"${SAMPLE}_variant_stats.txt\"\n\n# Summary\necho \"=== Pipeline Summary ===\"\necho \"Alignment stats:\"\nsamtools flagstat \"${SAMPLE}_sorted.bam\"\n\necho \"Variant counts:\"\nbcftools view -H \"${SAMPLE}_variants.vcf.gz\" | wc -l\n\necho \"=== Pathogen Genomics Pipeline Complete ===\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#multi-sample-outbreak-analysis","title":"Multi-Sample Outbreak Analysis","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=outbreak_analysis\n#SBATCH --time=08:00:00\n#SBATCH --mem=128GB\n#SBATCH --cpus-per-task=32\n\n# Load required modules\nmodule load python/3.12.3  # Or use system python3\nmodule load iqtree/2.2.0\nmodule load mafft/7.490\n\necho \"=== Multi-Sample Outbreak Analysis ===\"\n\n# Step 1: Concatenate all consensus sequences\necho \"Step 1: Preparing sequences for phylogenetic analysis...\"\ncat *.consensus.fasta &gt; all_samples.fasta\n\n# Step 2: Multiple sequence alignment\necho \"Step 2: Performing multiple sequence alignment...\"\nmafft --auto --thread $SLURM_CPUS_PER_TASK all_samples.fasta &gt; aligned_sequences.fasta\n\n# Step 3: Build phylogenetic tree\necho \"Step 3: Building phylogenetic tree...\"\niqtree2 -s aligned_sequences.fasta -nt $SLURM_CPUS_PER_TASK -bb 1000\n\n# Step 4: Calculate pairwise distances\necho \"Step 4: Calculating genetic distances...\"\npython &lt;&lt; 'EOF'\nfrom Bio import AlignIO\nfrom Bio.Phylo.TreeConstruction import DistanceCalculator\nimport pandas as pd\n\n# Read alignment\nalignment = AlignIO.read(\"aligned_sequences.fasta\", \"fasta\")\n\n# Calculate distances\ncalculator = DistanceCalculator('identity')\ndistance_matrix = calculator.get_distance(alignment)\n\n# Convert to DataFrame for easier handling\nsamples = [record.id for record in alignment]\ndist_df = pd.DataFrame(distance_matrix.matrix, \n                      index=samples, \n                      columns=samples)\n\n# Save distance matrix\ndist_df.to_csv('genetic_distances.csv')\n\n# Find closely related samples (distance &lt; 0.001)\nclose_pairs = []\nfor i, sample1 in enumerate(samples):\n    for j, sample2 in enumerate(samples[i+1:], i+1):\n        distance = distance_matrix.matrix[i][j]\n        if distance &lt; 0.001:  # Very similar sequences\n            close_pairs.append([sample1, sample2, distance])\n\nif close_pairs:\n    close_df = pd.DataFrame(close_pairs, \n                           columns=['Sample1', 'Sample2', 'Distance'])\n    close_df.to_csv('potential_transmission_links.csv', index=False)\n    print(f\"Found {len(close_pairs)} potential transmission links\")\nelse:\n    print(\"No closely related samples found\")\nEOF\n\necho \"=== Outbreak Analysis Complete ===\"\necho \"Results:\"\necho \"- Phylogenetic tree: aligned_sequences.fasta.treefile\"\necho \"- Genetic distances: genetic_distances.csv\"\necho \"- Potential links: potential_transmission_links.csv\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#advanced-job-types","title":"Advanced Job Types","text":""},{"location":"day2/slurm-practical-tutorial/#1-array-jobs","title":"1. Array Jobs","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=array_processing\n#SBATCH --partition=Main\n#SBATCH --array=1-100%10        # 100 jobs, max 10 concurrent\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4GB\n#SBATCH --time=00:30:00\n#SBATCH --output=array_%A_%a.log\n\nmodule load python/3.12.3  # Or use system python3\n\n# Use array task ID to process different files\nINPUT_FILE=\"input_${SLURM_ARRAY_TASK_ID}.txt\"\nOUTPUT_FILE=\"output_${SLURM_ARRAY_TASK_ID}.txt\"\n\necho \"Processing $INPUT_FILE on $(hostname)\"\npython process_file.py $INPUT_FILE $OUTPUT_FILE\n\necho \"Task $SLURM_ARRAY_TASK_ID completed\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-job-dependencies","title":"2. Job Dependencies","text":"<pre><code>#!/bin/bash\n# Submit first job\nJOB1=$(sbatch --parsable preprocess.sh)\n\n# Submit second job that depends on first\nJOB2=$(sbatch --parsable --dependency=afterok:$JOB1 analysis.sh)\n\n# Submit final job that depends on second\nsbatch --dependency=afterok:$JOB2 postprocess.sh\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#3-multi-node-mpi-job","title":"3. Multi-node MPI Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=mpi_job\n#SBATCH --partition=Main\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=16\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=2GB\n#SBATCH --time=04:00:00\n\n# module load openmpi  # Check if MPI is available\n\n# Total tasks = nodes * ntasks-per-node = 4 * 16 = 64\necho \"Running on $SLURM_NNODES nodes with $SLURM_NTASKS total tasks\"\n\nmpirun ./my_mpi_program input.dat\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#4-interactive-job","title":"4. Interactive Job","text":"<pre><code>graph TB\n    subgraph \"Interactive Jobs\"\n        I1[User logs in] --&gt; I2[Request interactive session&lt;br/&gt;sinteractive/srun --pty]\n        I2 --&gt; I3[Wait for resources]\n        I3 --&gt; I4[Get shell on compute node]\n        I4 --&gt; I5[Run commands interactively]\n        I5 --&gt; I6[See output in real-time]\n        I6 --&gt; I7[Exit when done]\n\n        style I4 fill:#e8f5e9\n        style I5 fill:#e8f5e9\n        style I6 fill:#e8f5e9\n    end\n\n    subgraph \"Batch Jobs\"\n        B1[User logs in] --&gt; B2[Write job script]\n        B2 --&gt; B3[Submit with sbatch]\n        B3 --&gt; B4[Job queued]\n        B4 --&gt; B5[Job runs automatically]\n        B5 --&gt; B6[Output to files]\n        B6 --&gt; B7[Check results later]\n\n        style B5 fill:#e1f5fe\n        style B6 fill:#e1f5fe\n    end\n\n    subgraph \"When to Use\"\n        UI[Interactive: Development,&lt;br/&gt;Testing, Debugging]\n        UB[Batch: Production runs,&lt;br/&gt;Long jobs, Multiple jobs]\n    end\n\n    I7 --&gt; UI\n    B7 --&gt; UB</code></pre> <p>Figure: Comparison between interactive and batch job workflows in SLURM</p> <pre><code># Request interactive session using sinteractive (ILIFU-specific)\nsinteractive -c 1 --time 03:00                    # 1 CPU for 3 hours (default)\nsinteractive -c 5 --time 5-00:00                 # 5 CPUs for 5 days (maximum)\n\n# Alternative: Use srun for interactive session\nsrun --partition=Main --cpus-per-task=4 --mem=8GB --time=02:00:00 --pty bash\n\n# Once in interactive session:\nmodule load python/3.12.3  # Or use system python3\npython  # Start interactive Python\n</code></pre> <p>Note: Resources on the Devel partition are shared (CPU and memory). For dedicated resources, use <code>srun</code> on the Main partition.</p>"},{"location":"day2/slurm-practical-tutorial/#5-jupyter-notebook-on-compute-node","title":"5. Jupyter Notebook on Compute Node","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=jupyter\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16GB\n#SBATCH --time=04:00:00\n#SBATCH --output=jupyter_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n\n# Install jupyter if needed\npip install --user jupyter\n\n# Get node info\nNODE=$(hostname -s)\nPORT=8888\n\necho \"Starting Jupyter notebook on node $NODE, port $PORT\"\necho \"SSH tunnel command:\"\necho \"ssh -N -L ${PORT}:${NODE}:${PORT} ${USER}@training.ilifu.ac.za\"\n\n# Start Jupyter\njupyter notebook --no-browser --port=$PORT --ip=0.0.0.0\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#resource-optimization","title":"Resource Optimization","text":""},{"location":"day2/slurm-practical-tutorial/#1-memory-optimization-examples","title":"1. Memory Optimization Examples","text":""},{"location":"day2/slurm-practical-tutorial/#low-memory-job","title":"Low Memory Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=low_mem\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=2GB           # Conservative memory request\n#SBATCH --time=01:00:00\n\nmodule load python/3.12.3  # Or use system python3\n\n# Process data in chunks to save memory\npython &lt;&lt; 'EOF'\nimport pandas as pd\n\n# Read in chunks instead of loading entire file\nchunk_size = 10000\nresults = []\n\nfor chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n    # Process chunk\n    processed = chunk.groupby('category').sum()\n    results.append(processed)\n\n# Combine results\nfinal_result = pd.concat(results)\nfinal_result.to_csv('output.csv')\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#memory-intensive-job","title":"Memory-intensive Job","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=high_mem\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=64GB          # High memory for large datasets\n#SBATCH --time=04:00:00\n\nmodule load python/3.12.3  # Or use system python3\n\n# Load large dataset into memory\npython &lt;&lt; 'EOF'\nimport pandas as pd\nimport numpy as np\n\n# Load entire large dataset\ndf = pd.read_csv('very_large_file.csv')\nprint(f\"Loaded dataset with shape: {df.shape}\")\n\n# Memory-intensive operations\ncorrelation_matrix = df.corr()\ncorrelation_matrix.to_csv('correlations.csv')\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-time-optimization","title":"2. Time Optimization","text":""},{"location":"day2/slurm-practical-tutorial/#checkpointing-example","title":"Checkpointing Example","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=checkpointed_job\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16GB\n#SBATCH --time=02:00:00\n#SBATCH --output=checkpoint_%j.log\n\nmodule load python/3.12.3  # Or use system python3\n\npython &lt;&lt; 'EOF'\nimport pickle\nimport os\nimport time\n\ncheckpoint_file = 'checkpoint.pkl'\n\n# Try to load previous state\nif os.path.exists(checkpoint_file):\n    with open(checkpoint_file, 'rb') as f:\n        state = pickle.load(f)\n    start_iteration = state['iteration']\n    results = state['results']\n    print(f\"Resuming from iteration {start_iteration}\")\nelse:\n    start_iteration = 0\n    results = []\n    print(\"Starting from scratch\")\n\n# Main computation loop\nfor i in range(start_iteration, 1000):\n    # Simulate some work\n    time.sleep(1)\n    result = i ** 2\n    results.append(result)\n\n    # Save checkpoint every 100 iterations\n    if i % 100 == 0:\n        state = {'iteration': i + 1, 'results': results}\n        with open(checkpoint_file, 'wb') as f:\n            pickle.dump(state, f)\n        print(f\"Checkpoint saved at iteration {i}\")\n\nprint(\"Computation completed\")\n\n# Clean up checkpoint file\nif os.path.exists(checkpoint_file):\n    os.remove(checkpoint_file)\nEOF\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#troubleshooting-examples","title":"Troubleshooting Examples","text":""},{"location":"day2/slurm-practical-tutorial/#1-debug-job-failures","title":"1. Debug Job Failures","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=debug_job\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4GB\n#SBATCH --time=00:15:00\n#SBATCH --output=debug_%j.log\n#SBATCH --error=debug_%j.err\n\n# Enable debugging\nset -e  # Exit on any error\nset -x  # Print commands as they execute\n\necho \"=== Environment Information ===\"\necho \"Node: $(hostname)\"\necho \"Date: $(date)\"\necho \"Working directory: $(pwd)\"\necho \"User: $(whoami)\"\necho \"SLURM Job ID: $SLURM_JOB_ID\"\necho \"SLURM CPUs: $SLURM_CPUS_PER_TASK\"\n\necho \"=== Module Information ===\"\nmodule list\n\necho \"=== Python Information ===\"\nmodule load python/3.12.3  # Or use system python3\nwhich python\npython --version\n\necho \"=== Running Script ===\"\npython my_script.py 2&gt;&amp;1 | tee python_output.log\n\necho \"=== Job Completed ===\"\necho \"Exit code: $?\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#2-memory-usage-monitoring","title":"2. Memory Usage Monitoring","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=memory_monitor\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8GB\n#SBATCH --time=01:00:00\n\n# Function to monitor memory usage\nmonitor_memory() {\n    while true; do\n        echo \"$(date): Memory usage: $(free -h | grep '^Mem' | awk '{print $3}')\"\n        sleep 30\n    done\n}\n\n# Start memory monitoring in background\nmonitor_memory &amp;\nMONITOR_PID=$!\n\n# Load modules and run main task\nmodule load python/3.12.3  # Or use system python3\npython memory_intensive_script.py\n\n# Stop monitoring\nkill $MONITOR_PID\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#3-file-permission-issues","title":"3. File Permission Issues","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=file_check\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=2GB\n#SBATCH --time=00:10:00\n\necho \"=== File System Checks ===\"\n\n# Check input files exist and are readable\nINPUT_FILES=(\"input1.txt\" \"input2.txt\" \"config.json\")\n\nfor file in \"${INPUT_FILES[@]}\"; do\n    if [[ -f \"$file\" ]]; then\n        if [[ -r \"$file\" ]]; then\n            echo \"\u2713 $file exists and is readable\"\n        else\n            echo \"\u2717 $file exists but is not readable\"\n            ls -l \"$file\"\n            exit 1\n        fi\n    else\n        echo \"\u2717 $file does not exist\"\n        exit 1\n    fi\ndone\n\n# Check output directory is writable\nOUTPUT_DIR=\"results\"\nif [[ ! -d \"$OUTPUT_DIR\" ]]; then\n    mkdir -p \"$OUTPUT_DIR\" || {\n        echo \"\u2717 Cannot create output directory $OUTPUT_DIR\"\n        exit 1\n    }\nfi\n\nif [[ -w \"$OUTPUT_DIR\" ]]; then\n    echo \"\u2713 Output directory $OUTPUT_DIR is writable\"\nelse\n    echo \"\u2717 Output directory $OUTPUT_DIR is not writable\"\n    ls -ld \"$OUTPUT_DIR\"\n    exit 1\nfi\n\necho \"All file checks passed!\"\n\n# Proceed with actual work\npython main_script.py\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#job-submission-scripts","title":"Job Submission Scripts","text":""},{"location":"day2/slurm-practical-tutorial/#batch-submit-multiple-jobs","title":"Batch Submit Multiple Jobs","text":"<pre><code>#!/bin/bash\n# submit_multiple.sh - Submit multiple related jobs\n\n# Array of input files\nINPUT_FILES=(data1.txt data2.txt data3.txt data4.txt)\n\n# Submit a job for each input file\nfor i in \"${!INPUT_FILES[@]}\"; do\n    input_file=\"${INPUT_FILES[$i]}\"\n    job_name=\"process_$(basename $input_file .txt)\"\n\n    echo \"Submitting job for $input_file\"\n\n    sbatch --job-name=\"$job_name\" \\\n           --output=\"${job_name}_%j.log\" \\\n           --export=INPUT_FILE=\"$input_file\" \\\n           process_template.sh\n\n    sleep 1  # Brief pause between submissions\ndone\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#template-with-environment-variables","title":"Template with Environment Variables","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=templated_job\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8GB\n#SBATCH --time=02:00:00\n#SBATCH --output=%x_%j.log  # %x = job name, %j = job id\n\n# Use environment variables passed from submission script\necho \"Processing file: $INPUT_FILE\"\necho \"Output directory: $OUTPUT_DIR\"\necho \"Parameters: $PARAMS\"\n\nmodule load python/3.12.3  # Or use system python3\n\n# Use the variables in your script\npython analysis.py \\\n    --input \"$INPUT_FILE\" \\\n    --output \"$OUTPUT_DIR\" \\\n    --params \"$PARAMS\"\n</code></pre>"},{"location":"day2/slurm-practical-tutorial/#performance-testing-template","title":"Performance Testing Template","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=performance_test\n#SBATCH --partition=Main\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=16GB\n#SBATCH --time=01:00:00\n#SBATCH --output=perf_%j.log\n\necho \"=== Performance Test Started ===\"\necho \"Job ID: $SLURM_JOB_ID\"\necho \"Node: $(hostname)\"\necho \"CPUs allocated: $SLURM_CPUS_PER_TASK\"\necho \"Memory allocated: ${SLURM_MEM_PER_NODE}MB\"\necho \"Start time: $(date)\"\n\n# Record resource usage\necho \"=== Initial Resource Usage ===\"\nfree -h\ndf -h $HOME\ndf -h /scratch/$USER\n\nmodule load python/3.12.3  # Or use system python3\n\n# Time the main computation\necho \"=== Starting Main Computation ===\"\nstart_time=$(date +%s)\n\npython performance_test_script.py\n\nend_time=$(date +%s)\nruntime=$((end_time - start_time))\n\necho \"=== Performance Summary ===\"\necho \"Runtime: ${runtime} seconds\"\necho \"End time: $(date)\"\n\n# Check final resource usage\necho \"=== Final Resource Usage ===\"\nfree -h\n\necho \"=== Performance Test Completed ===\"\n</code></pre> <p>This comprehensive set of SLURM examples covers most common use cases and provides templates that can be adapted for specific needs. Each example includes comments explaining the key parameters and concepts.</p>"},{"location":"day2/unix-commands-pathogen-examples/","title":"Unix Commands for Pathogen Genomics - Practical Tutorial","text":"<p>Adapted from Microbial-Genomics practice scripts</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-overview","title":"Tutorial Overview","text":""},{"location":"day2/unix-commands-pathogen-examples/#what-youll-learn","title":"What You'll Learn","text":"<p>This hands-on tutorial will teach you essential Unix commands for pathogen genomics analysis. By the end, you'll be able to: - Navigate and organize genomics project directories - Process FASTQ and FASTA files efficiently - Extract meaningful information from sequencing data - Build simple analysis pipelines - Prepare data for HPC analysis</p>"},{"location":"day2/unix-commands-pathogen-examples/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic terminal/command line access</li> <li>No prior Unix experience required</li> <li>Access to the training server</li> </ul>"},{"location":"day2/unix-commands-pathogen-examples/#time-required","title":"Time Required","text":"<ul> <li>Complete tutorial: 2-3 hours</li> <li>Quick essentials: 45 minutes</li> </ul>"},{"location":"day2/unix-commands-pathogen-examples/#learning-path","title":"Learning Path","text":"<ol> <li>Setup \u2192 2. Basic Navigation \u2192 3. File Operations \u2192 4. Data Processing \u2192 5. Pipeline Building</li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#setup-instructions","title":"Setup Instructions","text":"<p>Before starting the Unix command exercises, you need to prepare your workspace with sample genomics data. Here's how:</p>"},{"location":"day2/unix-commands-pathogen-examples/#step-1-create-your-practice-directory","title":"Step 1: Create Your Practice Directory","text":"<pre><code># Create a new directory for your practice exercises\n# mkdir = \"make directory\"\n# -p = create parent directories if needed (won't error if directory exists)\n# ~ = shortcut for your home directory (/home/username)\nmkdir -p ~/hpc_practice\n\n# Navigate into your new directory\n# cd = \"change directory\"\ncd ~/hpc_practice\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-copy-sample-genomics-data","title":"Step 2: Copy Sample Genomics Data","text":"<pre><code># Copy all sample files from the shared course folder to your current location\n# cp = \"copy\" command\n# -r = \"recursive\" - copy directories and all their contents\n# * = wildcard that matches all files\n# . = current directory (destination)\ncp -r /cbio/training/courses/2025/micmet-genomics/sample-data/* .\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-verify-your-files","title":"Step 3: Verify Your Files","text":"<pre><code># List all files with details to confirm the copy was successful\n# ls = \"list\" command\n# -l = long format (shows permissions, size, dates)\n# -a = show all files (including hidden files starting with .)\nls -la\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#understanding-your-sample-files","title":"Understanding Your Sample Files","text":"<p>The following files are now in your directory for practice:</p> File Description Used For <code>sample.fastq.gz</code> Compressed DNA sequencing reads Learning <code>zcat</code>, <code>gunzip</code>, file compression <code>sample1.fastq</code> Uncompressed sequencing data (3 reads) Practicing <code>grep</code>, <code>wc</code>, sequence counting <code>sample2.fastq</code> Another FASTQ file (2 reads) Array processing, file comparisons <code>reference.fasta</code> Reference genome sequences Learning <code>grep</code> with FASTA headers, sequence extraction <code>data.txt</code> Tab-delimited sample metadata Practicing <code>awk</code>, <code>sed</code>, <code>cut</code>, <code>sort</code> commands <p>File Formats Explained: - FASTQ: Contains sequences + quality scores (4 lines per read) - FASTA: Contains sequences only (2 lines per sequence: header + sequence) - GZ: Gzip compressed file (saves space, common in genomics)</p>"},{"location":"day2/unix-commands-pathogen-examples/#quick-command-reference-with-detailed-explanations","title":"Quick Command Reference with Detailed Explanations","text":""},{"location":"day2/unix-commands-pathogen-examples/#essential-commands-for-genomics-analysis","title":"Essential Commands for Genomics Analysis","text":"<p>Before diving into detailed modules, here's a quick reference of the most commonly used commands in pathogen genomics, with detailed explanations of what each component does:</p>"},{"location":"day2/unix-commands-pathogen-examples/#navigate-and-organize","title":"Navigate and Organize","text":"<pre><code># Create nested directories for a genomics project\nmkdir -p project/{data,results,scripts}\n# Explanation:\n# mkdir = make directory command\n# -p = create parent directories as needed (won't error if they exist)\n# project/ = main project folder\n# {data,results,scripts} = brace expansion creates 3 subdirectories at once\n#   - data/ for raw sequencing files\n#   - results/ for analysis outputs\n#   - scripts/ for your analysis code\n\n# Navigate to your project directory\ncd project\n# cd = change directory\n# project = destination directory (relative path from current location)\n\n# Show current working directory\npwd\n# pwd = print working directory\n# Returns absolute path like: /home/username/hpc_practice/project\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#inspect-fastq-files","title":"Inspect FASTQ Files","text":"<pre><code># View compressed FASTQ file content\nzcat sample.fastq.gz | head -20\n# Explanation:\n# zcat = view compressed file without extracting (z = gzip, cat = concatenate)\n# sample.fastq.gz = compressed FASTQ file (common in genomics to save space)\n# | = pipe operator, sends output to next command\n# head -20 = show first 20 lines (5 complete reads since FASTQ uses 4 lines/read)\n\n# Count number of reads in FASTQ file\nzcat sample.fastq.gz | wc -l | awk '{print $1/4}'\n# Explanation:\n# zcat sample.fastq.gz = decompress and output file content\n# wc -l = word count with -l flag counts lines\n# | = pipe the line count to awk\n# awk '{print $1/4}' = divide line count by 4 (FASTQ has 4 lines per read)\n#   - $1 = first field (the line count)\n#   - /4 = division to get read count\n# Example: 400 lines / 4 = 100 reads\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#search-and-filter","title":"Search and Filter","text":"<pre><code># Find all FASTA headers in reference genome\ngrep \"^&gt;\" reference.fasta\n# Explanation:\n# grep = global regular expression print (searches for patterns)\n# \"^&gt;\" = pattern to search for\n#   - ^ = start of line anchor (line must begin with &gt;)\n#   - &gt; = literal \"&gt;\" character (FASTA headers start with &gt;)\n# reference.fasta = file to search in\n# Output: Shows all sequence headers like \"&gt;chr1\", \"&gt;gene_ABC123\"\n\n# Count high-quality variants\ngrep -c \"PASS\" variants.vcf\n# Explanation:\n# grep = search command\n# -c = count matching lines instead of showing them\n# \"PASS\" = quality filter status in VCF files\n# variants.vcf = variant call format file\n# Returns: Number like \"1234\" (count of variants passing quality filters)\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#process-text-data","title":"Process Text Data","text":"<pre><code># Extract specific columns from data\nawk '{print $1, $2}' data.txt\n# Explanation:\n# awk = powerful text processing tool\n# '{print $1, $2}' = awk program\n#   - {} = action block\n#   - print = output command\n#   - $1 = first column/field\n#   - $2 = second column/field\n#   - , = adds space between fields in output\n# data.txt = input file\n# Example input:  \"Sample1 100 resistant\"\n# Example output: \"Sample1 100\"\n\n# Replace text in files\nsed 's/old/new/g' file.txt\n# Explanation:\n# sed = stream editor for text transformation\n# 's/old/new/g' = substitution command\n#   - s = substitute command\n#   - /old/ = pattern to find\n#   - /new/ = replacement text\n#   - g = global flag (replace all occurrences, not just first)\n# file.txt = input file\n# Example: Changes \"old_sample_name\" to \"new_sample_name\" throughout file\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#combined-pipeline-examples","title":"Combined Pipeline Examples","text":""},{"location":"day2/unix-commands-pathogen-examples/#example-1-quick-fastq-quality-check","title":"Example 1: Quick FASTQ Quality Check","text":"<pre><code># Count reads and check quality score distribution\nzcat sample.fastq.gz | \\\n  awk 'NR%4==0' | \\\n  cut -c1-10 | \\\n  sort | \\\n  uniq -c | \\\n  sort -rn\n\n# Line-by-line explanation:\n# zcat sample.fastq.gz = decompress FASTQ\n# awk 'NR%4==0' = get every 4th line (quality scores)\n#   - NR = line number\n#   - %4==0 = divisible by 4 (4th, 8th, 12th lines...)\n# cut -c1-10 = first 10 characters of quality string\n# sort = alphabetically sort quality patterns\n# uniq -c = count unique patterns\n# sort -rn = sort by count, highest first\n#   - -r = reverse order\n#   - -n = numerical sort\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#example-2-extract-high-quality-reads","title":"Example 2: Extract High-Quality Reads","text":"<pre><code># Get read IDs with average quality &gt; 30\nzcat sample.fastq.gz | \\\n  paste - - - - | \\\n  awk '{if(length($4) &gt; 0) print $1, length($4)}' | \\\n  grep \"^@\"\n\n# Explanation:\n# paste - - - - = combine every 4 lines into 1 tab-delimited line\n# awk = process the combined lines\n# $1 = read ID, $4 = quality string\n# grep \"^@\" = filter for valid read IDs\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#pro-tips-for-these-commands","title":"Pro Tips for These Commands","text":"<ol> <li> <p>Always preview before processing:    <pre><code>zcat file.gz | head -20  # Check format first\n</code></pre></p> </li> <li> <p>Count before and after filtering:    <pre><code># Before\ngrep -c \"^@\" input.fastq\n# After filtering\ngrep -c \"^@\" filtered.fastq\n</code></pre></p> </li> <li> <p>Use quotes for patterns with special characters:    <pre><code>grep \"^&gt;\" file.fasta     # Correct\ngrep ^&gt; file.fasta        # May fail - shell interprets &gt;\n</code></pre></p> </li> <li> <p>Combine commands efficiently:    <pre><code># Instead of creating intermediate files:\nzcat file.gz &gt; temp.txt\ngrep \"pattern\" temp.txt &gt; result.txt\n\n# Use pipes:\nzcat file.gz | grep \"pattern\" &gt; result.txt\n</code></pre></p> </li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#module-1-directory-organization-for-genomics-projects","title":"Module 1: Directory Organization for Genomics Projects","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives","title":"Learning Objectives","text":"<p>\u2713 Create organized project directories \u2713 Navigate between directories efficiently \u2713 Understand genomics project structure</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-11-creating-your-first-project-structure","title":"Tutorial 1.1: Creating Your First Project Structure","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-start-with-a-simple-structure","title":"Step 1: Start with a Simple Structure","text":"<pre><code># Create your main project directory\nmkdir my_first_project\n\n# Enter the directory\ncd my_first_project\n\n# Check where you are\npwd\n# Output: /home/username/hpc_practice/my_first_project\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-add-subdirectories","title":"Step 2: Add Subdirectories","text":"<pre><code># Create data directories\nmkdir data\nmkdir results\nmkdir scripts\n\n# List what you created\nls\n# Output: data  results  scripts\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-create-a-complex-structure","title":"Step 3: Create a Complex Structure","text":"<pre><code># Use -p to create nested directories\nmkdir -p data/{raw_reads,reference_genomes,metadata}\nmkdir -p results/{qc,alignment,variants,phylogeny}\nmkdir -p scripts logs tmp\n\n# View the structure\nls -la\n# The -la flags show: l=long format, a=all files\n</code></pre> <p>Try It Yourself: <pre><code># Exercise: Create this structure\n# project/\n#   \u251c\u2500\u2500 input/\n#   \u2502   \u251c\u2500\u2500 sequences/\n#   \u2502   \u2514\u2500\u2500 references/\n#   \u2514\u2500\u2500 output/\n#       \u251c\u2500\u2500 aligned/\n#       \u2514\u2500\u2500 reports/\n\n# Solution:\nmkdir -p project/{input/{sequences,references},output/{aligned,reports}}\n</code></pre></p> <p>Real-world application: <pre><code># Set up M. tuberculosis outbreak analysis\nmkdir -p mtb_outbreak_2025/{data,results,scripts,logs}\ncd mtb_outbreak_2025\nmkdir -p data/{fastq,references,clinical_metadata}\nmkdir -p results/{fastqc,trimming,bwa_alignment,vcf_files,phylogenetic_tree}\n</code></pre></p>"},{"location":"day2/unix-commands-pathogen-examples/#module-2-file-management-for-sequencing-data","title":"Module 2: File Management for Sequencing Data","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_1","title":"Learning Objectives","text":"<p>\u2713 Create and edit text files \u2713 Copy and rename sequencing files \u2713 Organize data systematically</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-21-working-with-sample-lists","title":"Tutorial 2.1: Working with Sample Lists","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-create-a-sample-list","title":"Step 1: Create a Sample List","text":"<pre><code># First, ensure you're in the right place\npwd\ncd ~/hpc_practice\n\n# Create an empty file\ntouch sample_list.txt\n\n# Check it was created\nls -la sample_list.txt\n# Output: -rw-r--r-- 1 user group 0 Sep 2 10:00 sample_list.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-add-content-to-the-file","title":"Step 2: Add Content to the File","text":"<pre><code># Method 1: Using echo (for single lines)\necho \"Sample_001\" &gt; sample_list.txt\necho \"Sample_002\" &gt;&gt; sample_list.txt  # &gt;&gt; appends, &gt; overwrites!\n\n# Method 2: Using nano editor (recommended for multiple lines)\nnano sample_list.txt\n# Type or paste the following content:\n# MTB_sample_001\n# MTB_sample_002\n# MTB_sample_003\n# MTB_sample_004\n# Then save with: Ctrl+X, Y, Enter\n\n# View what you created\ncat sample_list.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-count-and-verify","title":"Step 3: Count and Verify","text":"<pre><code># Count lines in file\nwc -l sample_list.txt\n# Output: 4 sample_list.txt\n\n# Count words\nwc -w sample_list.txt\n# Output: 4 sample_list.txt\n\n# Get full statistics\nwc sample_list.txt\n# Output: 4  4  58 sample_list.txt\n#        (lines, words, characters)\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-22-organizing-sequencing-files","title":"Tutorial 2.2: Organizing Sequencing Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-copy-files-safely","title":"Step 1: Copy Files Safely","text":"<pre><code># Copy sample files to practice with\ncp sample*.fastq .\n\n# List files before renaming\nls sample*.fastq\n# Output: sample1.fastq  sample2.fastq\n\n# Create a backup first (always!)\nmkdir backups\ncp sample*.fastq backups/\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-rename-files-systematically","title":"Step 2: Rename Files Systematically","text":"<pre><code># Rename a single file\nmv sample1.fastq patient001_reads.fastq\n\n# Batch rename using a loop\nfor file in sample*.fastq; do\n    # Extract the number from filename\n    num=$(echo $file | grep -o '[0-9]\\+')\n    # Create new name\n    newname=\"patient_${num}_sequences.fastq\"\n    echo \"Renaming $file to $newname\"\n    mv \"$file\" \"$newname\"\ndone\n\n# Verify the renaming\nls *.fastq\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#practice-exercise","title":"Practice Exercise:","text":"<pre><code># Exercise: Create copies with dates\n# Copy sample.fastq.gz to sample_20250902.fastq.gz\n\n# Solution:\ndate_stamp=$(date +%Y%m%d)\ncp sample.fastq.gz sample_${date_stamp}.fastq.gz\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#module-3-viewing-and-inspecting-genomics-files","title":"Module 3: Viewing and Inspecting Genomics Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_2","title":"Learning Objectives","text":"<p>\u2713 View compressed and uncompressed files \u2713 Count sequences in FASTQ/FASTA files \u2713 Extract specific parts of files</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-31-working-with-fastq-files","title":"Tutorial 3.1: Working with FASTQ Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-view-compressed-files","title":"Step 1: View Compressed Files","text":"<pre><code># View first 4 lines (1 complete read) of compressed file\nzcat sample.fastq.gz | head -4\n# Output:\n# @SEQ_001\n# ACGTACGTACGTACGTACGTACGTACGTACGTACGT\n# +\n# IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n\n# View first 2 reads (8 lines)\nzcat sample.fastq.gz | head -8\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-count-sequences","title":"Step 2: Count Sequences","text":"<pre><code># Count total lines\nzcat sample.fastq.gz | wc -l\n# Output: 12  (for 3 reads)\n\n# Count number of reads (FASTQ has 4 lines per read)\nzcat sample.fastq.gz | wc -l | awk '{print $1/4}'\n# Output: 3\n\n# Alternative: Count sequence headers\nzcat sample.fastq.gz | grep -c \"^@\"\n# Output: 3\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-3-view-with-line-numbers","title":"Step 3: View with Line Numbers","text":"<pre><code># Add line numbers to output\nzcat sample.fastq.gz | head -8 | cat -n\n# Output:\n#      1    @SEQ_001\n#      2    ACGTACGTACGTACGTACGTACGTACGTACGTACGT\n#      3    +\n#      4    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n#      5    @SEQ_002\n#      6    TGCATGCATGCATGCATGCATGCATGCATGCATGCA\n#      7    +\n#      8    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-32-working-with-fasta-files","title":"Tutorial 3.2: Working with FASTA Files","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-view-fasta-headers","title":"Step 1: View FASTA Headers","text":"<pre><code># View first line (header) of FASTA\nhead -1 reference.fasta\n# Output: &gt;Sequence_1 gene=ABC123\n\n# View all headers in file\ngrep \"^&gt;\" reference.fasta\n# Output:\n# &gt;Sequence_1 gene=ABC123\n# &gt;Sequence_2 gene=DEF456\n\n# Count sequences\ngrep -c \"^&gt;\" reference.fasta\n# Output: 2\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-extract-sequences","title":"Step 2: Extract Sequences","text":"<pre><code># View first 2 lines (header + sequence start)\nhead -2 reference.fasta\n\n# Skip header, view sequence only\ntail -n +2 reference.fasta | head -1\n# Output: ACGTACGTACGTACGTACGTACGTACGTACGTACGT\n\n# Get sequence length\ntail -n +2 reference.fasta | head -1 | wc -c\n# Output: 37 (includes newline)\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#practice-exercise_1","title":"Practice Exercise:","text":"<pre><code># Exercise: Count total bases in all sequences\n# Hint: Remove headers first\n\n# Solution:\ngrep -v \"^&gt;\" reference.fasta | tr -d '\\n' | wc -c\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#module-4-searching-and-filtering-genomics-data","title":"Module 4: Searching and Filtering Genomics Data","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_3","title":"Learning Objectives","text":"<p>\u2713 Search for patterns in files \u2713 Filter genomics data \u2713 Use regular expressions</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-41-basic-pattern-searching","title":"Tutorial 4.1: Basic Pattern Searching","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-simple-searches","title":"Step 1: Simple Searches","text":"<pre><code># Search for a word in a file\ngrep \"resistant\" data.txt\n# Output: Sample1 100 resistant\n#         Sample3 150 resistant\n\n# Case-insensitive search (-i flag)\ngrep -i \"SAMPLE\" data.txt\n# Finds: Sample1, Sample2, etc.\n\n# Count matches (-c flag)\ngrep -c \"resistant\" data.txt\n# Output: 2\n\n# Show line numbers (-n flag)\ngrep -n \"resistant\" data.txt\n# Output: 1:Sample1 100 resistant\n#         3:Sample3 150 resistant\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-search-in-genomics-files","title":"Step 2: Search in Genomics Files","text":"<pre><code># Find sequence headers in FASTA\ngrep \"^&gt;\" reference.fasta\n# ^ means \"start of line\"\n\n# Find adapter sequences in FASTQ\ngrep \"AGATCGGAAGAG\" sample1.fastq\n\n# Search compressed files\nzcat sample.fastq.gz | grep \"ACGT\"\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-42-advanced-pattern-matching","title":"Tutorial 4.2: Advanced Pattern Matching","text":""},{"location":"day2/unix-commands-pathogen-examples/#using-regular-expressions","title":"Using Regular Expressions","text":"<pre><code># Find lines with numbers\ngrep '[0-9]' data.txt\n# [0-9] matches any digit\n\n# Find lines ending with specific pattern\ngrep 'resistant$' data.txt\n# $ means \"end of line\"\n\n# Extract only the matching part (-o flag)\necho \"Sample123\" | grep -o '[0-9]\\+'\n# Output: 123\n# \\+ means \"one or more\"\n\n# Multiple patterns (OR)\ngrep -E 'resistant|sensitive' data.txt\n# -E enables extended regex\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#practice-exercise_2","title":"Practice Exercise:","text":"<pre><code># Exercise: Find all samples with values &gt; 150\n# Hint: Use awk instead of grep for numeric comparisons\n\n# Solution:\nawk '$2 &gt; 150' data.txt\n# Output: Sample2 200 sensitive\n#         Sample4 300 sensitive\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#module-5-text-processing-for-genomics","title":"Module 5: Text Processing for Genomics","text":""},{"location":"day2/unix-commands-pathogen-examples/#learning-objectives_4","title":"Learning Objectives","text":"<p>\u2713 Extract specific columns from files \u2713 Perform calculations on data \u2713 Transform text efficiently</p>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-51-using-awk-for-data-processing","title":"Tutorial 5.1: Using awk for Data Processing","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-extract-columns","title":"Step 1: Extract Columns","text":"<pre><code># Print specific columns (1st and 2nd)\nawk '{print $1, $2}' data.txt\n# Output: Sample1 100\n#         Sample2 200\n#         Sample3 150\n#         Sample4 300\n\n# Print with custom separator\nawk '{print $1 \",\" $2}' data.txt\n# Output: Sample1,100\n\n# Add text to output\nawk '{print \"Sample:\" $1 \" Value:\" $2}' data.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-perform-calculations","title":"Step 2: Perform Calculations","text":"<pre><code># Sum values in column 2\nawk '{sum+=$2} END {print \"Total:\", sum}' data.txt\n# Output: Total: 750\n\n# Calculate average\nawk '{sum+=$2; count++} END {print \"Average:\", sum/count}' data.txt\n# Output: Average: 187.5\n\n# Filter based on value\nawk '$2 &gt; 150 {print $0}' data.txt\n# Output: Sample2 200 sensitive\n#         Sample4 300 sensitive\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tutorial-52-using-sed-for-text-manipulation","title":"Tutorial 5.2: Using sed for Text Manipulation","text":""},{"location":"day2/unix-commands-pathogen-examples/#step-1-basic-substitutions","title":"Step 1: Basic Substitutions","text":"<pre><code># Replace text (s = substitute)\necho \"Sample1\" | sed 's/1/A/'\n# Output: SampleA\n\n# Global replacement (g = global)\necho \"Sample111\" | sed 's/1/A/g'\n# Output: SampleAAA\n\n# Replace in file and save\nsed 's/Sample/Patient/g' data.txt &gt; patients.txt\n\n# Edit file in place (careful!)\nsed -i.bak 's/Sample/Patient/g' data.txt\n# Creates data.txt.bak as backup\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#step-2-advanced-manipulations","title":"Step 2: Advanced Manipulations","text":"<pre><code># Delete lines containing pattern\nsed '/sensitive/d' data.txt\n# Removes lines with \"sensitive\"\n\n# Add text to beginning of lines\nsed 's/^/PREFIX_/' data.txt\n# Adds PREFIX_ to each line start\n\n# Convert spaces to tabs\nsed 's/ /\\t/g' data.txt &gt; data.tsv\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#6-file-permissions-and-management","title":"6. File Permissions and Management","text":""},{"location":"day2/unix-commands-pathogen-examples/#managing-permissions","title":"Managing Permissions","text":"<pre><code># Make scripts executable\nchmod +x scripts/analysis_pipeline.sh\n\n# Protect raw data from accidental modification\nchmod 444 data/raw_reads/*.fastq.gz\n\n# Set directory permissions\nchmod 755 results/\n\n# Check permissions\nls -la data/raw_reads/\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#7-sorting-and-unique-operations","title":"7. Sorting and Unique Operations","text":""},{"location":"day2/unix-commands-pathogen-examples/#processing-sample-lists","title":"Processing Sample Lists","text":"<pre><code># Sort sample names\nsort data/sample_list.txt\n\n# Sort numerically by coverage\nsort -k2 -n coverage_stats.txt\n\n# Get unique mutations\ncut -f1,2 variants.txt | sort | uniq\n\n# Count occurrences of each mutation\ncut -f3 mutations.txt | sort | uniq -c | sort -rn\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#8-pipelines-and-redirection","title":"8. Pipelines and Redirection","text":""},{"location":"day2/unix-commands-pathogen-examples/#creating-simple-analysis-pipelines","title":"Creating Simple Analysis Pipelines","text":"<pre><code># Count reads per sample and save to file\nfor file in data/raw_reads/*.fastq.gz; do\n    sample=$(basename $file .fastq.gz)\n    count=$(zcat $file | wc -l | awk '{print $1/4}')\n    echo -e \"$sample\\t$count\"\ndone &gt; results/qc/read_counts.txt\n\n# Extract and count specific genes\ngrep \"^&gt;\" reference.fasta | cut -d' ' -f1 | sed 's/&gt;//' | sort | uniq -c &gt; gene_counts.txt\n\n# Process multiple VCF files\nfor vcf in results/variants/*.vcf; do\n    sample=$(basename $vcf .vcf)\n    pass_count=$(grep -c \"PASS\" $vcf)\n    total_count=$(grep -v \"^#\" $vcf | wc -l)\n    echo -e \"$sample\\t$total_count\\t$pass_count\"\ndone &gt; results/variant_summary.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#9-practical-slurm-integration","title":"9. Practical SLURM Integration","text":""},{"location":"day2/unix-commands-pathogen-examples/#preparing-files-for-hpc-analysis","title":"Preparing Files for HPC Analysis","text":"<p>To create a SLURM job script:</p> <pre><code># Open nano to create the script\nnano prep_pathogen_data.sh\n</code></pre> <p>Copy and paste the following content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=prep_pathogen_data\n#SBATCH --time=00:30:00\n#SBATCH --mem=4GB\n\n# Create directory structure\nmkdir -p ${SLURM_JOB_ID}_analysis/{data,results,logs}\n\n# Copy and organize files\ncp /shared/data/*.fastq.gz ${SLURM_JOB_ID}_analysis/data/\n\n# Generate file list for processing\nls ${SLURM_JOB_ID}_analysis/data/*.fastq.gz &gt; file_list.txt\n\n# Count and verify files\necho \"Total files to process: $(wc -l &lt; file_list.txt)\"\n\n# Create metadata file\nfor file in ${SLURM_JOB_ID}_analysis/data/*.fastq.gz; do\n    size=$(du -h $file | cut -f1)\n    reads=$(zcat $file | wc -l | awk '{print $1/4}')\n    echo \"$(basename $file)\\t$size\\t$reads\"\ndone &gt; ${SLURM_JOB_ID}_analysis/data/file_metadata.tsv\n\necho \"Preparation complete. Ready for analysis.\"\n</code></pre> <p>Save the file with: <code>Ctrl+X</code>, then <code>Y</code>, then <code>Enter</code></p> <p>Submit the job with: <code>sbatch prep_pathogen_data.sh</code></p>"},{"location":"day2/unix-commands-pathogen-examples/#hands-on-exercise-complete-pathogen-analysis-workflow","title":"Hands-On Exercise: Complete Pathogen Analysis Workflow","text":""},{"location":"day2/unix-commands-pathogen-examples/#exercise-overview","title":"Exercise Overview","text":"<p>Build a complete analysis pipeline step-by-step, applying all the skills you've learned.</p>"},{"location":"day2/unix-commands-pathogen-examples/#part-1-setup-and-data-preparation","title":"Part 1: Setup and Data Preparation","text":"<pre><code># Step 1: Create project structure\nmkdir -p pathogen_practice/{data,results,scripts}\ncd pathogen_practice\npwd  # Verify you're in the right place\n\n# Step 2: Create sample metadata\nnano data/samples.txt\n# Paste the following content:\n# Mtb_patient_001_resistant\n# Mtb_patient_002_susceptible  \n# Mtb_patient_003_resistant\n# Salmonella_outbreak_001\n# Salmonella_outbreak_002\n# Save with: Ctrl+X, Y, Enter\n\n# Verify the file was created\ncat data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#part-2-data-analysis-tasks","title":"Part 2: Data Analysis Tasks","text":""},{"location":"day2/unix-commands-pathogen-examples/#task-1-find-resistant-samples","title":"Task 1: Find Resistant Samples","text":"<pre><code># Use grep to find resistant samples\ngrep \"resistant\" data/samples.txt\n\n# Save results to file\ngrep \"resistant\" data/samples.txt &gt; results/resistant_samples.txt\n\n# Count how many\ngrep -c \"resistant\" data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#task-2-count-by-pathogen-type","title":"Task 2: Count by Pathogen Type","text":"<pre><code># Count MTB samples\ngrep -c \"Mtb\" data/samples.txt\n\n# Count Salmonella samples\ngrep -c \"Salmonella\" data/samples.txt\n\n# Save counts\necho \"MTB samples: $(grep -c 'Mtb' data/samples.txt)\" &gt; results/pathogen_counts.txt\necho \"Salmonella samples: $(grep -c 'Salmonella' data/samples.txt)\" &gt;&gt; results/pathogen_counts.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#task-3-generate-summary-report","title":"Task 3: Generate Summary Report","text":"<pre><code># Create a comprehensive summary using nano\nnano results/summary_report.txt\n\n# Type the following content (replace the values with actual counts):\n# === Pathogen Analysis Summary ===\n# Date: [current date]\n# Total samples: 5\n# Resistant samples: 2\n# Susceptible samples: 1\n# MTB samples: 3\n# Salmonella samples: 2\n# =================================\n# Save with: Ctrl+X, Y, Enter\n\n# Or use echo commands to generate it automatically:\necho \"=== Pathogen Analysis Summary ===\" &gt; results/summary_report.txt\necho \"Date: $(date)\" &gt;&gt; results/summary_report.txt\necho \"Total samples: $(wc -l &lt; data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"Resistant samples: $(grep -c \"resistant\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"Susceptible samples: $(grep -c \"susceptible\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"MTB samples: $(grep -c \"Mtb\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"Salmonella samples: $(grep -c \"Salmonella\" data/samples.txt)\" &gt;&gt; results/summary_report.txt\necho \"=================================\" &gt;&gt; results/summary_report.txt\n\n# Display the report\ncat results/summary_report.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#part-3-challenge-exercises","title":"Part 3: Challenge Exercises","text":""},{"location":"day2/unix-commands-pathogen-examples/#challenge-1-extract-sample-ids","title":"Challenge 1: Extract Sample IDs","text":"<pre><code># Extract just the patient IDs (hint: use cut or awk)\n# Try it yourself first!\n\n# Solution:\ncut -d'_' -f2,3 data/samples.txt\n# Or using awk:\nawk -F'_' '{print $2\"_\"$3}' data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#challenge-2-sort-and-count-unique-pathogens","title":"Challenge 2: Sort and Count Unique Pathogens","text":"<pre><code># Extract pathogen names and count occurrences\n# Try it yourself first!\n\n# Solution:\ncut -d'_' -f1 data/samples.txt | sort | uniq -c\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#challenge-3-create-a-pipeline","title":"Challenge 3: Create a Pipeline","text":"<pre><code># Find all resistant MTB samples in one command\n# Try it yourself first!\n\n# Solution:\ngrep \"Mtb\" data/samples.txt | grep \"resistant\"\n# Or more elegantly:\ngrep \"Mtb.*resistant\" data/samples.txt\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#tips-for-pathogen-genomics-unix-usage","title":"Tips for Pathogen Genomics Unix Usage","text":"<ol> <li>Always work with copies of raw sequencing data</li> <li>Use meaningful file names with sample IDs and dates</li> <li>Document your commands in scripts for reproducibility</li> <li>Check file integrity after transfers (md5sum)</li> <li>Compress large files to save space (gzip/bgzip)</li> <li>Use screen or tmux for long-running processes</li> <li>Regular backups of analysis results</li> <li>Version control for scripts (git)</li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#common-file-formats-in-pathogen-genomics","title":"Common File Formats in Pathogen Genomics","text":"Extension Format View Command Description <code>.fastq.gz</code> Compressed FASTQ <code>zcat file.fastq.gz \\| head</code> Raw sequencing reads <code>.fasta</code> FASTA <code>cat file.fasta</code> Reference genomes <code>.sam/.bam</code> SAM/BAM <code>samtools view file.bam \\| head</code> Alignments <code>.vcf</code> VCF <code>cat file.vcf</code> Variant calls <code>.gff/.gtf</code> GFF/GTF <code>cat file.gff</code> Gene annotations <code>.newick/.tree</code> Newick <code>cat file.tree</code> Phylogenetic trees"},{"location":"day2/unix-commands-pathogen-examples/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"day2/unix-commands-pathogen-examples/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"day2/unix-commands-pathogen-examples/#issue-1-permission-denied","title":"Issue 1: \"Permission denied\"","text":"<pre><code># Problem: Can't access or modify a file\n# Solution: Check permissions\nls -la filename\n\n# Fix: Change permissions if you own the file\nchmod u+rw filename\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-2-no-such-file-or-directory","title":"Issue 2: \"No such file or directory\"","text":"<pre><code># Problem: File path is wrong\n# Solution: Check your current location\npwd\n\n# List files to verify\nls -la\n\n# Use absolute paths to be sure\n/full/path/to/file\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-3-command-not-found","title":"Issue 3: \"Command not found\"","text":"<pre><code># Problem: Tool not installed or not in PATH\n# Solution: Check if command exists\nwhich command_name\n\n# Load module if available\nmodule avail\nmodule load tool_name\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-4-file-is-empty-or-corrupted","title":"Issue 4: File is empty or corrupted","text":"<pre><code># Check file size\nls -lh filename\n\n# Check file type\nfile filename\n\n# For compressed files, test integrity\ngzip -t file.gz\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#issue-5-out-of-disk-space","title":"Issue 5: Out of disk space","text":"<pre><code># Check available space\ndf -h\n\n# Find large files\ndu -sh * | sort -h\n\n# Clean up temporary files\nrm -rf tmp/*\n</code></pre>"},{"location":"day2/unix-commands-pathogen-examples/#best-practices-to-avoid-issues","title":"Best Practices to Avoid Issues","text":"<ol> <li> <p>Always backup before modifying <pre><code>cp important_file important_file.backup\n</code></pre></p> </li> <li> <p>Use tab completion to avoid typos    <pre><code>cat sam[TAB]  # Completes to sample.fastq\n</code></pre></p> </li> <li> <p>Preview commands with echo first <pre><code>echo mv *.fastq backup/  # See what would happen\nmv *.fastq backup/       # Then run for real\n</code></pre></p> </li> <li> <p>Check file contents before processing <pre><code>head -5 file.txt  # Preview first 5 lines\nwc -l file.txt    # Count total lines\n</code></pre></p> </li> </ol>"},{"location":"day2/unix-commands-pathogen-examples/#quick-reference-card","title":"Quick Reference Card","text":""},{"location":"day2/unix-commands-pathogen-examples/#essential-commands-summary","title":"Essential Commands Summary","text":"Task Command Example List files <code>ls -la</code> <code>ls -la *.fastq</code> Change directory <code>cd</code> <code>cd ~/hpc_practice</code> Create directory <code>mkdir -p</code> <code>mkdir -p data/reads</code> Copy files <code>cp -r</code> <code>cp sample.fastq backup/</code> Move/rename <code>mv</code> <code>mv old.txt new.txt</code> View compressed <code>zcat</code> <code>zcat file.gz \\| head</code> Count lines <code>wc -l</code> <code>wc -l sample.txt</code> Search text <code>grep</code> <code>grep \"pattern\" file</code> Extract columns <code>awk</code> <code>awk '{print $1}' file</code> Replace text <code>sed</code> <code>sed 's/old/new/g' file</code> Sort data <code>sort</code> <code>sort -n numbers.txt</code> Get unique <code>uniq</code> <code>sort file \\| uniq</code>"},{"location":"day2/unix-commands-pathogen-examples/#next-steps","title":"Next Steps","text":"<p>After mastering these Unix commands, you're ready to: 1. Submit SLURM jobs - See High Performance Computing with SLURM: Practical Tutorial 2. Learn HPC concepts - See HPC and ILIFU Training Materials 3. Build analysis pipelines with Nextflow 4. Perform quality control with FastQC 5. Align reads with BWA 6. Call variants with SAMtools/BCFtools</p> <p>Remember: Unix commands are the foundation of all bioinformatics pipelines!</p>"},{"location":"day3/genome_assembly_notes/","title":"Objectives","text":"<ol> <li>To perform De novo genome assembly and reference-based genome assembly</li> <li>Assess assembly quality using quality metrics</li> </ol>"},{"location":"day3/genome_assembly_notes/#genome-assembly","title":"Genome assembly","text":"<p>Genome assembly involves reconstructing a genome from a set of fragmented DNA sequences (reads) obtained from sequencing technologies. </p> <p>It aims to piece together the reads to create a continuous sequence that represents the genome of an organism.</p>"},{"location":"day3/genome_assembly_notes/#key-concepts","title":"Key Concepts","text":"<ul> <li>Reads: Fragmented sequences of DNA obtained from sequencing technologies.</li> <li>Contigs: Continuous sequences formed by overlapping reads.</li> <li>Scaffolds: Ordered and oriented sets of contigs, sometimes with gaps, which represent larger regions of the genome.</li> <li>Coverage: The average number of times each base in the genome is sequenced, which affects the accuracy of the assembly.</li> <li>Assembly can be:<ul> <li>De novo assembly - the construction of the genome from scratch without a reference.</li> <li>Reference-guided assembly - uses a known reference genome to guide the assembly of the new genome.</li> </ul> </li> </ul>"},{"location":"day3/genome_assembly_notes/#steps-in-genome-assembly","title":"Steps in Genome Assembly","text":"<ol> <li>Preprocessing (quality control, adapter sequences and low-quality bases filtering with FASTQC, TRIMMOMATIC/FASTP/BBMap).</li> <li>Assembly</li> <li>Scaffolding (involves use of long-range information from paired-end or mate-pair reads to order and orient contigs into scaffolds (e.g., SSPACE).</li> <li>Gap Filling (use additional reads or assembly techniques to close gaps within scaffolds (e.g., GapCloser).</li> <li>Polishing (correcting sequencing errors and improving the accuracy of the assembly, e.g., Pilon).</li> <li>Evaluation (Assembly Metrics including N50 (length of the contig such that 50% of the total assembly length is in contigs of this length or longer); genome completeness; and accuracy, e.g., QUAST).</li> </ol>"},{"location":"day3/genome_assembly_notes/#genome-assembly-algorithms","title":"Genome Assembly Algorithms","text":""},{"location":"day3/genome_assembly_notes/#overlap-layout-consensus-olc","title":"Overlap Layout Consensus (OLC)","text":"<ul> <li>Suitable for long-read sequencing (e.g., PacBio, Oxford Nanopore)</li> <li>Determines overlap between reads</li> <li>Arranges reads based on ovelaps</li> <li>Resolves conflict to build the final sequences</li> <li>Handles long reads and complex regions well</li> <li>Computationally intensive</li> </ul>"},{"location":"day3/genome_assembly_notes/#de-bruijn-graph-dbg","title":"De Bruijn Graph (DBG)","text":"<ul> <li>Best for short-read sequencing (e.g., illumina).</li> <li>uses small overlapping sequences (k-mers) to build a graph, nodes: k-mers, and edges: overlaps.</li> <li>Fast and memory efficient for large datasets</li> <li>Struggles with repetitive regions</li> </ul>"},{"location":"day3/genome_assembly_notes/#hybrid-methods","title":"Hybrid Methods","text":"<ul> <li>Combine DBG and OLC strengths to improve assembly quality, especially for error-prone long reads.\u200b</li> </ul>"},{"location":"day3/genome_assembly_notes/#assembler-classification-by-read-type","title":"Assembler Classification by Read Type","text":""},{"location":"day3/genome_assembly_notes/#short-read-assemblers","title":"Short-Read Assemblers\u200b","text":"<ul> <li>Short-read assemblers process high volumes of short sequences using DBG algorithms</li> <li>Ideal for technologies like Illumina.\u200b</li> <li>SPAdes (widely used for small genomes)</li> <li>Velvet (older though useful)</li> <li>SOAPdenovo (short-read assembler for larger genomes),</li> <li>ABySS (large genomes).</li> </ul>"},{"location":"day3/genome_assembly_notes/#long-read-assemblers","title":"Long-Read Assemblers\u200b","text":"<ul> <li>Long-read assemblers handle fewer, longer sequences using OLC algorithms</li> <li>Can span entire genes and repetitive regions</li> <li>Optimized for platforms like PacBio and ONT.\u200b</li> <li>Higher error rates (can be corrected by short reads or polishing)</li> <li>Canu, Celera, Flye</li> </ul>"},{"location":"day3/genome_assembly_notes/#hybrid-assemblers","title":"Hybrid Assemblers\u200b","text":"<ul> <li>Combine short and long reads, integrating DBG and OLC methods for improved accuracy and contiguity.\u200b</li> <li>Balances accuracy (short reads) and completeness (long reads, resolving repeats and structural variants)</li> <li>Requires careful data integration</li> <li>Unicycler, MaSuRCA</li> </ul>"},{"location":"day3/genome_assembly_notes/#reference-guided-assembly","title":"Reference-guided Assembly","text":"<ul> <li>Works well given a closely related reference genome</li> <li>Aligns reads to a reference genome and fills gaps</li> <li>Faster and less computationally demanding</li> <li>May miss novel sequences or structural variations</li> </ul>"},{"location":"day3/genome_assembly_notes/#note","title":"NOTE","text":"Assembly Strategy Subtypes Read Type Compatibility Examples Notes De novo assembly DBG, OLC, Hybrid Short, long, hybrid Velvet, SPAdes, Canu, Flye, MaSuRCA No reference genome used Reference-guided assembly Mapping-based Short, long BWA, Bowtie2, Novoalign, Minimap2 Aligns reads to a known reference genome"},{"location":"day3/genome_assembly_notes/#assembler-selection-factors","title":"Assembler Selection Factors\u200b","text":"<p>Choosing an assembler depends on\" - Sequencing technology, - Project goals, and - Available computational resources.\u200b</p>"},{"location":"day3/genome_assembly_notes/#best-practices-for-genome-assembly","title":"Best Practices for Genome Assembly","text":"<ul> <li>Start with high-quality, high-coverage sequencing data to improve the accuracy of the assembly.</li> <li>Try multiple assemblers and compare results, as different tools may perform better for different data types.</li> <li>Use iterative rounds of assembly, scaffolding, and polishing to gradually improve the assembly.</li> <li>Validate the final assembly using independent data, such as long-read sequencing or optical mapping.</li> <li>Keep detailed records of all parameters and steps used in the assembly process for reproducibility.</li> </ul>"},{"location":"day3/practical_genome_assmbly/","title":"De novo Genome Assembly Practical","text":"<p>Before setting up you need to know the current workig directory  <pre><code># Check current working directory\npwd\n\n# List the contents of the working diresctory\nls\n\n# Create relevant output directories. -p so that it creates parent dir if it doesn't exist\nmkdir -p /users/${USER}/results/02_assembly/tb\nmkdir -p /users/${USER}/results/02_assembly/vc\nmkdir -p /users/${USER}/results/03_quality/tb\nmkdir -p /users/${USER}/results/03_quality/vc\n</code></pre></p> <p>Delete /users/${USER}/results/02_assembly and /users/user24/results/03_quality and create thenm using one line code and not four as above</p> <pre><code>mkdir /users/${USER}/results/02_assembly/tb /users/user24/results/02_assembly/vc \\\n  /users/${USER}/results/03_quality/tb /users/user24/results/03_quality/vc\n\n# Request for an interactive node\nsrun --cpus-per-tasks=32 --mem=128g --time 5:00:00 --job-name \"ephie\" --pty /bin/bash\n\n# # Clear all modules\nmodule purge\n\n# Load the required modules\nmodule load spades/4.2.0\n\n## How can I get the help documentation?\nspades.py --help\n\n# Lets run spades with a test dataset\n/software/bio/spades/4.2.0/bin/spades.py  --test  --careful\n\n# Run spades with test data but with more details on the reads\nspades.py  -1 /software/bio/spades/4.2.0/share/spades/test_dataset/ecoli_1K_1.fq.gz \\\n    -2 /software/bio/spades/4.2.0/share/spades/test_dataset/ecoli_1K_2.fq.gz \\\n  --careful -o results/02_assembly-test\n\n## Subset the data\nhead -5 tb_IDs &gt; 02_tb_IDs\nhead -5 vc_IDs &gt; 02_vc_IDs\n\n# Perform denovo assembly for all TB samples\nmkdir -p /users/${USER}/scripts\nnano /users/${USER}/scripts/02_assembly.sh\n\n#!/bin/bash\n#SBATCH --job-name='02_assembly'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=120GB\n#SBATCH --output=/users/${USER}/logs/02_assembly-stdout.txt\n#SBATCH --error=/users/${USER}/logs/02_assembly-stdout.txt\n#SBATCH --time=12:00:00\n\nfor SAMPLE in $(cat tb_IDs); do\n  echo \"[TB|SPADES] $SAMPLE\"\n  spades.py -1 /users/${USER}/results/trimmed_trimmomatic/tb/${SAMPLE}_1.fastq.gz \\\n    -2 /users/${USER}/results/trimmed_trimmomatic/tb/${SAMPLE}_2.fastq.gz \\\n    --careful \\\n    -o /users/${USER}/results/02_assembly/tb/${SAMPLE}\ndone\n\n## Save\n# Ctrl + O\n\n# CLOSE\n# ctrl + x\n\nsbatch /users/${USER}/scripts/02_assembly.sh\n</code></pre>"},{"location":"day3/practical_genome_assmbly/#notes","title":"Notes","text":"<ul> <li>-1                file with forward paired-end reads <li>-2                file with reverse paired-end reads <li>--careful                   will reduce number of mismatches and short indels</li> <li>-o                          path to output file (directory shouldn't be existing)</li>"},{"location":"day3/practical_genome_assmbly/#quality-assessment","title":"Quality Assessment","text":"<p>We need assess the quality of the contigs so that we are confident that they are of good quality.</p>"},{"location":"day4/genome_assembly/","title":"Objectives","text":"<ol> <li>To perform De novo genome assembly and reference-based genome assembly</li> <li>Assess assembly quality using quality metrics</li> </ol>"},{"location":"day4/genome_assembly/#target-organisms","title":"Target Organisms","text":""},{"location":"day4/genome_assembly/#mycobacterium-tuberculosis","title":"Mycobacterium tuberculosis","text":"<ul> <li>Genome size: ~4.4 Mb</li> <li>Characteristics: High GC content ~65.6%, complex secondary structures</li> <li>Genes ~4,000 protein-coding genes</li> <li>Clinical relevance: Major global pathogen, drug resistance concerns</li> <li>Assembly challenges: Repetitive sequences, PE/PPE gene families</li> </ul>"},{"location":"day4/genome_assembly/#vibrio-cholerae","title":"Vibrio cholerae","text":"<ul> <li>Characteristics: Dual chromosome structure</li> <li>Genome size: ~4.0 Mb (two chromosomes: ~3.0 Mb + ~1.1 Mb)</li> <li>GC content: ~47.7%</li> <li>Genes: ~3,800 protein-coding genes</li> <li>Clinical relevance: Cholera pandemics, epidemic strain tracking</li> <li>Assembly challenges: Chromosome separation, mobile genetic elements</li> </ul>"},{"location":"day4/genome_assembly/#genome-assembly","title":"Genome assembly","text":"<p>Genome assembly involves reconstructing a genome from a set of fragmented DNA sequences (reads) obtained from sequencing technologies. </p> <p>It aims to piece together the reads to create a continuous sequence that represents the genome of an organism.</p>"},{"location":"day4/genome_assembly/#key-concepts","title":"Key Concepts","text":"<ul> <li>Reads: Fragmented sequences of DNA obtained from sequencing technologies.</li> <li>Contigs: Continuous sequences formed by overlapping reads.</li> <li>Scaffolds: Ordered and oriented sets of contigs, sometimes with gaps, which represent larger regions of the genome.</li> <li>Coverage: The average number of times each base in the genome is sequenced, which affects the accuracy of the assembly.</li> <li>Assembly can be:<ul> <li>De novo assembly - the construction of the genome from scratch without a reference.</li> <li>Reference-guided assembly - uses a known reference genome to guide the assembly of the new genome.</li> </ul> </li> </ul>"},{"location":"day4/genome_assembly/#steps-in-genome-assembly","title":"Steps in Genome Assembly","text":"<ol> <li>Preprocessing (quality control, adapter sequences and low-quality bases filtering with FASTQC, TRIMMOMATIC/FASTP/BBMap).</li> <li>Assembly</li> <li>Scaffolding (involves use of long-range information from paired-end or mate-pair reads to order and orient contigs into scaffolds (e.g., SSPACE).</li> <li>Gap Filling (use additional reads or assembly techniques to close gaps within scaffolds (e.g., GapCloser).</li> <li>Polishing (correcting sequencing errors and improving the accuracy of the assembly, e.g., Pilon).</li> <li>Evaluation (Assembly Metrics including N50 (length of the contig such that 50% of the total assembly length is in contigs of this length or longer); genome completeness; and accuracy, e.g., QUAST).</li> </ol>"},{"location":"day4/genome_assembly/#genome-assembly-algorithms","title":"Genome Assembly Algorithms","text":""},{"location":"day4/genome_assembly/#overlap-layout-consensus-olc","title":"Overlap Layout Consensus (OLC)","text":"<ul> <li>Suitable for long-read sequencing (e.g., PacBio, Oxford Nanopore)</li> <li>Determines overlap between reads</li> <li>Arranges reads based on ovelaps</li> <li>Resolves conflict to build the final sequences</li> <li>Handles long reads and complex regions well</li> <li>Computationally intensive</li> </ul>"},{"location":"day4/genome_assembly/#de-bruijn-graph-dbg","title":"De Bruijn Graph (DBG)","text":"<ul> <li>Best for short-read sequencing (e.g., illumina).</li> <li>Uses small overlapping sequences (k-mers) to build a graph, nodes: k-mers, and edges: overlaps.</li> <li>Fast and memory efficient for large datasets</li> <li>Struggles with repetitive regions</li> </ul>"},{"location":"day4/genome_assembly/#hybrid-methods","title":"Hybrid Methods","text":"<ul> <li>Combine DBG and OLC strengths to improve assembly quality, especially for error-prone long reads.\u200b</li> </ul>"},{"location":"day4/genome_assembly/#assembler-classification-by-read-type","title":"Assembler Classification by Read Type","text":""},{"location":"day4/genome_assembly/#short-read-assemblers","title":"Short-Read Assemblers\u200b","text":"<ul> <li>Short-read assemblers process high volumes of short sequences using DBG algorithms</li> <li>Ideal for technologies like Illumina.\u200b</li> <li>SPAdes (widely used for small genomes)</li> <li>Velvet (older though useful)</li> <li>SOAPdenovo (short-read assembler for larger genomes),</li> <li>ABySS (large genomes).</li> </ul>"},{"location":"day4/genome_assembly/#long-read-assemblers","title":"Long-Read Assemblers\u200b","text":"<ul> <li>Long-read assemblers handle fewer, longer sequences using OLC algorithms</li> <li>Can span entire genes and repetitive regions</li> <li>Optimized for platforms like PacBio and ONT.\u200b</li> <li>Higher error rates (can be corrected by short reads or polishing)</li> </ul>"},{"location":"day4/genome_assembly/#examples-of-long-read-assemblers-for-bacterial-genomes","title":"Examples of Long-Read Assemblers for Bacterial Genomes","text":"<ol> <li> <p>FLYE (Recommended for most bacterial genomes)</p> </li> <li> <p>Excellent for PacBio and Oxford Nanopore</p> </li> <li>Good repeat resolution</li> <li> <p>Usage: flye --nano-raw reads.fastq --out-dir output --genome-size 4.5m</p> </li> <li> <p>Canu (High accuracy, slower)</p> </li> <li> <p>Gold standard for accuracy</p> </li> <li>Requires significant computational resources</li> <li> <p>Usage: canu -p prefix -d output genomeSize=4.5m -nanopore reads.fastq</p> </li> <li> <p>Unicycler (Hybrid approach)</p> </li> <li> <p>Combines short and long reads</p> </li> <li>Excellent for complete genomes</li> <li> <p>Usage: unicycler -1 short_R1.fq -2 short_R2.fq -l long_reads.fq -o output</p> </li> <li> <p>Raven (Fast, lightweight)</p> </li> <li> <p>Quick assemblies for preliminary analysis</p> </li> <li>Good for large datasets</li> <li> <p>Usage: raven reads.fastq &gt; assembly.fasta</p> </li> <li> <p>NextDenovo (High accuracy for Nanopore)</p> </li> <li> <p>Specialized for Oxford Nanopore data</p> </li> <li>Good error correction</li> <li>Usage: nextDenovo config.txt</li> </ol> <p>Polishing Tools:</p> <ul> <li>Medaka (Nanopore): medaka_consensus -i reads.fastq -d assembly.fasta -o polished</li> <li>Pilon (with short reads): pilon --genome assembly.fasta --frags mapped_reads.bam</li> <li>Racon: racon reads.fastq mappings.paf assembly.fasta</li> </ul>"},{"location":"day4/genome_assembly/#hybrid-assemblers","title":"Hybrid Assemblers\u200b","text":"<ul> <li>Combine short and long reads, integrating DBG and OLC methods for improved accuracy and contiguity.\u200b</li> <li>Balances accuracy (short reads) and completeness (long reads, resolving repeats and structural variants)</li> <li>Requires careful data integration</li> <li>Unicycler, MaSuRCA</li> </ul>"},{"location":"day4/genome_assembly/#reference-guided-assembly","title":"Reference-guided Assembly","text":"<ul> <li>Works well given a closely related reference genome</li> <li>Aligns reads to a reference genome and fills gaps</li> <li>Faster and less computationally demanding</li> <li>May miss novel sequences or structural variations</li> </ul>"},{"location":"day4/genome_assembly/#note","title":"NOTE","text":"Assembly Strategy Subtypes Read Type Compatibility Examples Notes De novo assembly DBG, OLC, Hybrid Short, long, hybrid Velvet, SPAdes, Canu, Flye, MaSuRCA, UniCycler No reference genome used Reference-guided assembly Mapping-based Short, long BWA, Bowtie2, Novoalign, Minimap2 Aligns reads to a known reference genome"},{"location":"day4/genome_assembly/#assembler-selection-factors","title":"Assembler Selection Factors\u200b","text":"<p>Choosing an assembler depends on\" - Sequencing technology, - Project goals, and - Available computational resources.\u200b</p>"},{"location":"day4/genome_assembly/#best-practices-for-genome-assembly","title":"Best Practices for Genome Assembly","text":"<ul> <li>Start with high-quality, high-coverage sequencing data to improve the accuracy of the assembly.</li> <li>Try multiple assemblers and compare results, as different tools may perform better for different data types.</li> <li>Use iterative rounds of assembly, scaffolding, and polishing to gradually improve the assembly.</li> <li>Validate the final assembly using independent data, such as long-read sequencing or optical mapping.</li> <li>Keep detailed records of all parameters and steps used in the assembly process for reproducibility.</li> </ul>"},{"location":"day4/practical_assembly/","title":"De novo Genome Assembly Practical","text":"<p>Before setting up you need to know the current workig directory  <pre><code># Check current working directory\npwd\n# List the contents of the working diresctory\nls\n# Define output dir\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n# Create relevant output directories. -p so that it creates parent dir if it doesn't exist\nmkdir -p ${outspades}\"tb\"\nmkdir -p ${outspades}\"vc\"\nmkdir -p ${outquast}\"tb\"\nmkdir -p ${outquast}\"vc\"\n</code></pre></p> <p>Delete ${outspades} and ${outquast} and create them using one line of code and not four as above</p> <p><pre><code># Request for an interactive node (Resources)\n## Use this command to retrieve the previously used \"srun\" command\nhistory | grep \"srun\"\nsrun --cpus-per-tasks=16 --mem=128g --time 3:00:00 --job-name \"${USER}-assembly\" --pty /bin/bash\n\n# # Clear all modules\nmodule purge\n# Load the required modules\nmodule load spades/4.2.0\n## How can I get the help documentation?\nspades.py --help | less\n# Lets run spades with a test dataset\n/software/bio/spades/4.2.0/bin/spades.py  --test  --careful\n</code></pre> We  will use trimmed reads and file with IDs that we created during the initial data cleaning process. Check where the ID files are  and CHANGE DIR to where these are. Use \"cd /path/to/tb_IDs\"</p> <pre><code>## Subset the data\nhead -2 /data/users/${USER}/data_analysis/tb_IDs &gt; /data/users/${USER}/02_tb_IDs\nhead -2 /data/users/${USER}/data_analysis/vc_IDs &gt; /data/users/${USER}/02_vc_IDs\n# Define dirs\nindir=\"/data/users/${USER}/data_analysis/trimmed_trimmomatic/\"\noutdir=\"/data/users/${USER}/data_analysis/assembly-test/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outspades}tb ${outspades}vc ${outquast}tb ${outquast}vc\n\ncd /data/users/${USER}/\n# Run SPAdes for M. tuberculosis\necho \"Starting M. tuberculosis assembly at $(date)\"\n\nfor SAMPLE in $(cat 02_tb_IDs)\ndo\n  echo \"[TB|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}tb/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}tb/${SAMPLE}_2.fastq.gz \\\n    --careful \\\n    --cov-cutoff auto \\\n    -t 16 \\\n    -o ${outspades}tb/${SAMPLE}\ndone\n</code></pre>"},{"location":"day4/practical_assembly/#notes","title":"Notes","text":"<ul> <li>-1                file with forward paired-end reads <li>-2                file with reverse paired-end reads <li>--careful                   error and mismatch correction</li> <li>-o                          path to output file (directory shouldn't be existing)</li> <li>--cov-cutoff auto           Automatic coverage cutoff (usiful mostly for high GC genomes)</li> <p><pre><code># Perform denovo assembly for all TB ands VC samples\nmkdir -p /users/${USER}/scripts /users/${USER}/logs\n\n## Now let's write our submission script\nnano /users/${USER}/scripts/assembly_01.sh\n\n#!/bin/bash\n#SBATCH --job-name='assembly-${USER}'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=128GB\n#SBATCH --output=/users/${USER}/logs/assembly_01-stdout.txt\n#SBATCH --error=/users/${USER}/logs/assembly_01-stdout.txt\n#SBATCH --time=12:00:00\n\n# Define dir\nindir=\"/data/users/${USER}/data_analysis/trimmed_trimmomatic/\"\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\n# Change into the DIR with \"tb_IDs\"\ncd /data/users/${USER}/data_analysis/\n# Run SPAdes for M. tuberculosis\necho \"Starting M. tuberculosis assembly at $(date)\"\nfor SAMPLE in $(cat tb_IDs); do\n  echo \"[TB|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}tb/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}tb/${SAMPLE}_2.fastq.gz \\\n    --careful --cov-cutoff auto -t 16 \\\n    -o ${outspades}tb/${SAMPLE}\ndone\necho \"M. tuberculosis assembly completed at $(date)\"\n\nfor SAMPLE in $(cat vc_IDs); do\n  echo \"[VC|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}vc/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}vc/${SAMPLE}_2.fastq.gz \\\n    --careful -t 16 \\\n    -o ${outspades}vc/${SAMPLE}\ndone\n## SAVE THIS IN NANO BY\n# Ctrl + O\n\n# CLOSE THE FILE\n# ctrl + x\n</code></pre> Before submission of your script, verify if all the input directory and files exist.</p> <pre><code>## SUBMIT THE ASSEMBLY SCRIPT\nsbatch /users/${USER}/scripts/assembly_01.sh\n</code></pre>"},{"location":"day4/practical_assembly/#genome-assembly-quality","title":"Genome Assembly Quality","text":"<p>Assess the quality of your assemblies to be confident of your downstream analysis. </p>"},{"location":"day4/practical_assembly/#common-assembly-quality-metrics","title":"Common Assembly Quality Metrics","text":"<ul> <li>Expected genome size</li> <li>Expected GC content %</li> <li>Number of contigs</li> <li>N50 and L50</li> </ul>"},{"location":"day4/practical_assembly/#common-assembly-issues-and-solutions","title":"Common Assembly Issues and Solutions","text":"<p>===================================</p> \ud83d\udea8 Issue \ud83d\udd0d Possible Cause \ud83d\udee0\ufe0f Solution High number of contigs (&gt;100) \u2022 Low coverage (&lt;30x)  \u2022 Poor quality reads  \u2022 Highly repetitive genome  \u2022 Contamination \u2022 Increase sequencing depth  \u2022 Better read trimming/filtering  \u2022 Use hybrid assembly with long reads  \u2022 Check contamination with Kraken2 Low N50 (&lt;50kb for bacteria) \u2022 Repetitive sequences  \u2022 Low coverage  \u2022 Assembly parameter issues \u2022 Adjust k-mer sizes in SPAdes  \u2022 Use <code>--careful</code> flag  \u2022 Try different assemblers (Unicycler, SKESA) Assembly size much larger than expected \u2022 Contamination  \u2022 Duplication in assembly  \u2022 Presence of plasmids \u2022 Perform contamination screening  \u2022 Check duplication ratio in QUAST  \u2022 Separate plasmid sequences High number of hypothetical proteins (&gt;50%) \u2022 Novel organism/strain  \u2022 Poor annotation database match  \u2022 Assembly quality issues \u2022 Use specialized databases (RefSeq, UniProt)  \u2022 Manual curation of key genes  \u2022 Functional annotation with KEGG/COG"},{"location":"day4/practical_assembly/#running-quast-tool","title":"Running QUAST Tool","text":"<pre><code># Run QUAST for a subset of M. tuberculosis dataset\necho \"Running QUAST analysis for M. tuberculosis...\"\n\nindir=\"/data/users/${USER}/data_analysis/trimmed_trimmomatic/\"\noutdir=\"/data/users/${USER}/data_analysis/assembly-test/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outdir}\"02_quast/\"\n# Remove modules in your env\nmodules purge\n# Load module\nmodule load quast\n# Use the subset of our data\ncd /data/users/${USER}/\nfor SAMPLE in $(cat 02_tb_IDs)\ndo\n  echo \"[TB|SPADES] $SAMPLE\"\n  quast.py ${outspades}tb/${SAMPLE}/contigs.fasta \\\n    #-r /data/TB_H37Rv.fasta -g /data/TB_H37rv.gff \\\n    -o ${outquast}tb/${SAMPLE} \\\n    --threads 4 --min-contig 200 \\\n    --labels \"MTB_Assembly\"\ndone\n</code></pre>"},{"location":"day4/practical_assembly/#quast-submission-script-for-all-samples","title":"Quast Submission script for all samples","text":"<pre><code># Assess all assembly results for all TB ands VC samples\n# In case you didn't create these above\nmkdir -p /users/${USER}/scripts /users/${USER}/logs\n\n## Now let's write our submission script\nnano /users/${USER}/scripts/assembly_02.sh\n\n#!/bin/bash\n#SBATCH --job-name='quast-${USER}'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=128GB\n#SBATCH --output=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --error=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --time=12:00:00\n\n# Define dir\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outdir}\"01_spades\" ${outdir}\"02_quast\"\n# Run QUAST for a subset of M. tuberculosis dataset\necho \"Running QUAST analysis for M. tuberculosis...\"\n# Remove modules in your env\nmodules purge\n# Load module\nmodule load quast\n# Use the sample IDs for our data\ncd /data/users/${USER}/data_analysis\nfor SAMPLE in $(cat tb_IDs)\ndo\n  echo \"[TB|QUAST] $SAMPLE\"\n  quast.py ${outspades}tb/${SAMPLE}/contigs.fasta \\\n    #-r /data/TB_H37Rv.fasta -g /data/TB_H37rv.gff \\\n    -o ${outquast}tb/${SAMPLE} \\\n    --threads 4 --min-contig 200 \\\n    --labels \"MTB_Assembly\"\ndone\n\nfor SAMPLE in $(cat vc_IDs); do\n  echo \"[VC|QUAST] $SAMPLE\"\n  quast.py ${outspades}vc/${SAMPLE}/contigs.fasta \\\n    #-r /data/TB_H37Rv.fasta -g /data/TB_H37rv.gff \\\n    -o ${outquast}tb/${SAMPLE} \\\n    --threads 4 --min-contig 200 \\\n    --labels \"MTB_Assembly\"\ndone\n## SAVE THIS IN NANO BY\n# Ctrl + O\n\n# CLOSE THE FILE\n# ctrl + x\n</code></pre> <pre><code># Assess all assembly results for all TB ands VC samples\n# In case you didn't create these above\nmkdir -p /users/${USER}/scripts /users/${USER}/logs\n\n## Now let's write our submission script\nnano /users/${USER}/scripts/assembly_02.sh\n\n#!/bin/bash\n#SBATCH --job-name='quast-${USER}'\n#SBATCH --nodes=1 --ntasks=16\n#SBATCH --partition=Main\n#SBATCH --mem=128GB\n#SBATCH --output=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --error=/users/${USER}/logs/assembly_02-stdout.txt\n#SBATCH --time=12:00:00\n\n# Define dir\noutdir=\"/data/users/${USER}/data_analysis/assembly/\"\noutspades=${outdir}\"01_spades/\"\noutquast=${outdir}\"02_quast/\"\n\nmkdir -p ${outdir}\"02_quast/tb\" ${outdir}\"02_quast/vc\"\n# Run Prokka for M. tuberculosis\necho \"Starting M. tuberculosis annotation at $(date)\"\nfor SAMPLE in $(cat tb_IDs); do\n  echo \"[TB|SPADES] $SAMPLE\"\n  prokka ${outspades}tb/${SAMPLE}/contigs.fasta \\\n       --outdir ${outquast}/${SAMPLE} \\\n       --cpus 8 --genus Mycobacterium\ndone\necho \"M. tuberculosis annotation completed at $(date)\"\n\nfor SAMPLE in $(cat vc_IDs); do\n  echo \"[VC|SPADES] $SAMPLE\"\n  spades.py -1 ${indir}vc/${SAMPLE}_1.fastq.gz \\\n    -2 ${indir}vc/${SAMPLE}_2.fastq.gz \\\n    --careful -t 8 \\\n    -o ${outspades}vc/${SAMPLE}\ndone\n## SAVE THIS IN NANO BY\n# Ctrl + O\n\n# CLOSE THE FILE\n# ctrl + x\n</code></pre> <pre><code>## Clean-up\nrm -rf /data/users/${USER}/data_analysis/assembly-test/\n</code></pre>"},{"location":"day5/amr_prediction/","title":"Amr prediction","text":""},{"location":"day5/amr_prediction/#antimicrobial-resistance","title":"Antimicrobial Resistance","text":""},{"location":"day8/metagenomics/","title":"Metagenomics in Clinical &amp; Public Health","text":""},{"location":"day8/metagenomics/#1-overview","title":"1. Overview","text":"<p>This section introduces core principles of metagenomics. Metagenomics is the study of genetic material recovered directly from environmental or clinical samples, allowing  analysis of entire microbial communities without the need for cultivation. This section includes conceptual notes, rationale for each step, practical commands,  and a mini toy dataset exercise. Unlike genomics (WGS) which focuses on analyzing individual genomes (such as bacterial isolate), metagenomics studies the collective genomes or markers from microbial communities.</p>"},{"location":"day8/metagenomics/#genomics-vs-metagenomics","title":"Genomics vs Metagenomics","text":"Description Whole Genome Sequencing (WGS) Metagenomics Umbrella term Microbial Genomics Microbial Profiling Scope Sequencing of the entire genome of a single isolate (pure culture) Sequences all genetic material in mixed community (without culturing) Applications Species identification, genome assembly, annotation, AMR/virulence detection, plasmid/MGE analysis, outbreak tracking Community profiling, functional potential, pathogen detection in mixed samples, ecology studies Features Organism is known \\&amp; isolated before sequencing Captures both known and unknown microbes from environment or host"},{"location":"day8/metagenomics/#metagenomic-strategies-shotgun-vs-16s-rrna","title":"Metagenomic Strategies: Shotgun vs 16S rRNA","text":"Description Shotgun Metagenomics 16S rRNA Amplicon Gene Sequencing Definition Random sequencing of All DNA in a sample Targeted amplicon sequencing of the 16 S rRNA gene Resolution Species- and strain-level, functional genes (AMR, metabolism, plasmids, MGEs) Genus-level (sometimes species-level), no direct functional information Merits Comprehensive (taxonomy + function), detects viruses and fungi Limited to taxonomic resolution, can't detect fungi/viruses, lacks functional insights, good for bacterial surveys Demerits More expensive, higher computational load Cost-effective, standardized"},{"location":"day8/metagenomics/#2-workflow-from-sequencing-to-interpretation","title":"2. Workflow: From Sequencing to Interpretation","text":""},{"location":"day8/metagenomics/#step-1-study-design-sequencing","title":"Step 1. Study Design &amp; Sequencing","text":"<ul> <li>Why: Sequencing depth, read length, and platform choice directly influence resolution of taxa, detection of low-abundance organisms, and assembly quality.</li> <li>Example:</li> <li>Illumina (short reads): accurate, cost-effective, widely used for clinical metagenomics.</li> <li>ONT/PacBio (long reads): useful for resolving repeats, plasmids, MGEs.</li> </ul> <p>For this training we will use the data in <code>/data/users/user29/metagenomes/shotgun/</code> for shotgun metagenomics. It is important to note that, one can download shotgun metagenome sequences from NCBI-SRA using <code>ncbi-tools</code>. Install <code>ncbi-tools</code> and run</p> <pre><code>## Fetch the data from NCBI-SRA\n# fasterq-dump SRR13827118 --progress --threads 8\n\n## Compress the files\ngzip SRR13827118*.fastq\n</code></pre>"},{"location":"day8/metagenomics/#shotgun-metagenomics","title":"Shotgun metagenomics","text":""},{"location":"day8/metagenomics/#step-2-quality-control","title":"Step 2. Quality Control","text":"<ul> <li>Why: To reduce the effects of sequencing errors, remove adapters, and low-quality  reads which may reduce false positives.</li> <li>Tools: <code>fastqc</code>, <code>multiqc</code></li> </ul> <pre><code># Load modules\nmodule load fastqc\nmodule load multiqc\nindata=\"/data/users/user29/metagenomes/shotgun/\"\n\n## Create Dir\nmkdir -p ${outfastqc} /data/users/user24/metagenomes/shotgun/scripts /data/users/user24/metagenomes/shotgun/logs\n\n# create a samplesheet.csv with three columns samplename,fastq_1,fastq_2\npython /data/users/user24/metagenomes/shotgun/scripts/samplesheet_generator.py ${indata} \\\n  /data/users/${USER}/metagenomes/shotgun/samplesheet.csv\n\n# Run raw QC\nnextflow run /data/users/user24/metagenomes/shotgun/scripts/qc_pipeline_v1.nf \\\n  --input /data/users/${USER}/metagenomes/shotgun/samplesheet.csv \\\n  --outdir /data/users/${USER}/metagenomes/shotgun/results/rawfastqc\n</code></pre> <p>What to think about? - Which parts of the data are flagged as potentially problematic? GC% content of the dataset. NB: We are dealing with mixed community and organisms, so its difficult to  have a perfect normal distribution around the average. As such we consider this as normal given the biology of the system. - Does the sequencing length matches the libraries used? If sequences are shorter than expected, are adapters  a concern? - Are adapters and/or barcodes removed?   - Look at the Per base sequence content to diagnose this. - Is there unexpected sequence duplication?   - This can occur when low-input library preparations are used. - Are over-represented k-mers present?   - This can be a sign of adapter and barcode contamination.</p>"},{"location":"day8/metagenomics/#step-3-trimming-filtering","title":"Step 3. Trimming &amp; Filtering","text":"<ul> <li>Why: Removes adapters (which can cause false alignments \\&amp; false taxonomic assignments), low-quality bases (increase error rates), and very short  reads (to standardize read lengths) that bias downstream analysis.</li> <li>Tool: <code>fastp</code>, <code>trimmomatic</code>, <code>BBMap</code>, <code>sickle</code>, <code>cutadapt</code>, etc.</li> </ul> <pre><code>#!/bin/bash\n\n# Load required modules/tools\nmodule load trimmomatic\n\n# Define input and output dir\nindata=\"/data/users/user24/metagenomes/\"\nwkdir=\"/data/users/${USER}/metagenomics/shotgun/\"\nouttrimmomatic=${wkdir}\"/data_analysis/02_trimmomatic\"\n\nmkdir -p ${outtrimmomatic}\n\n# Trimming with Trimmomatic\nfor file in `ls ${data}*1.fastq.gz`\ndo\n  sample=$(basename ${file} _1.fastq.gz)\n  trimmomatic PE -threads 8 \\\n      ${indata}${sample}_R1.fastq.gz {indata}${sample}_R2.fastq.gz \\\n      ${outtrimmomatic}${sample}_R1.fastq.gz ${outtrimmomatic}${sample}_R1_unpaired.fastq.gz \\\n      ${outtrimmomatic}${sample}_R2.fastq.gz ${outtrimmomatic}${sample}_R2_unpaired.fastq.gz \\\n      ILLUMINACLIP:adapters.fa:2:30:10 \\\n      LEADING:3 TRAILING:3 \\\n      SLIDINGWINDOW:4:20 \\\n      MINLEN:50\ndone\necho \"Trimming with Trimmomatic completed\"\n</code></pre> Parameter Type Description PE positional Specifies whether we are analysing single- or paired-end reads -threads 2 keyword Specifies the number of threads to use when processing -phred33 keyword Specifies the fastq encoding used \\({indata}\\)_R2.fastq.gz}_R1.fastq.gz {indata}${sample positional The paired forward and reverse reads to trim ILLUMINACLIP:adapters.fa:2:30:10 positional Adapter trimming allowing for 2 seed mismatch, palindrome clip score threshold of 30, and simple clip score threshold of 10 SLIDINGWINDOW:4:20 positional Quality filtering command. Analyse each sequence in a 4 base pair sliding window and then truncate if the average quality drops below Q20 MINLEN:50 positional Length filtering command. Discard sequences that are shorter than 80 base pairs after trimming <p>Note: The trimming parameters in <code>trimmomatic</code> are processed in the order they are specified. For instance, </p> <p><pre><code>for file in `ls ${data}*1.fastq.gz`\ndo\n  sample=$(basename ${file} _1.fastq.gz)\n  trimmomatic PE -threads 8 \\\n        ${indata}${sample}_R1.fastq.gz {indata}${sample}_R2.fastq.gz \\\n        ${outtrimmomatic}${sample}_R1.fastq.gz ${outtrimmomatic}${sample}_R1_unpaired.fastq.gz \\\n        ${outtrimmomatic}${sample}_R2.fastq.gz ${outtrimmomatic}${sample}_R2_unpaired.fastq.gz \\\n        ILLUMINACLIP:adapters.fa:2:30:10 \\\n        LEADING:3 TRAILING:3 \\\n        MINLEN:50 \\\n        SLIDINGWINDOW:4:20 \ndone\n</code></pre> means we remove sequences shorter than 50 bps and then qualiyty trim, thus if a sequence is trimmed to a length shorter than 50bps after trimming, the <code>MINLEN</code> filtering does not execute a second time.</p> <pre><code># Delete unnecessary files\nrm -rf ${outtrimmomatic}*${sample}*_unpaired.fastq.gz\n\n# Quality checking\nmodule load fastqc multiqc\nfastqc ${outtrimmomatic}* -o ${outtrimmomatic}\nmultiqc  ${outtrimmomatic} -o  ${outtrimmomatic}\n</code></pre>"},{"location":"day8/metagenomics/#step-4-deduplication-host-dna-removal","title":"Step 4. Deduplication &amp; Host DNA Removal","text":"<ul> <li>Why:</li> <li>Often metagenomes are obtained from host-associated microbial communities. As a result, they contain significant amount of host DNA which may interfere with microbial analysis   and create privacy concerns.</li> <li>Specifically any studies invloving human subjects or samples derived from Taonga species.</li> <li> <p>Although several approaches are used for this, the most popular is to map reads to a reference genome (includin human genome). That is remove all reads that map to the reference of the dataset.</p> </li> <li> <p>Tools: <code>clumpify</code> (dedup), <code>bowtie2</code> or <code>bwa mem</code> (host removal).</p> </li> </ul> <pre><code>#!/bin/bash\nset -e  # Exit on error\n\n# Define dirs\ngenome_dir=\"/data/users/user24/refs/human_reference/\"\n\n## Remove human contamination using BWA and SAMtools\n## Create dir\nmkdir -p ${genome_dir}\n# Download the Human refence genome\ncd ${genome_dir}\n\n# Configuration\nGENOME_VERSION=\"GRCh38\"\nBASE_URL=\"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38\"\n\necho \"=== Downloading Human Reference Genome (${GENOME_VERSION}) ===\"\n\n# Option 1: Download complete genome assembly (recommended for most applications)\necho \"Downloading complete genome assembly...\"\nwget -c \"${BASE_URL}/GCA_000001405.15_GRCh38_genomic.fna.gz\" \\\n     -O \"GRCh38_genomic.fna.gz\"\n# Decompress\necho \"Decompressing genome file...\"\ngunzip -f GRCh38_genomic.fna.gz\n\n# Option 2: Download chromosome-only version (excludes contigs/scaffolds)\necho \"Downloading chromosome-only version...\"\nwget -c \"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\" \\\n     -O \"hg38_chromosomes_only.fa.gz\"\n\ngunzip -f hg38_chromosomes_only.fa.gz\n</code></pre> <pre><code>#!/bin/bash\n\n# Load modules\nmodule load bwa\nmodule load bowtie2\n\necho \"=== Building BWA Index ===\"\n# Build BWA index for alignment (required for host removal, one time set-up)\nbwa index GRCh38_genomic.fna\n\necho \"=== Verifying Download ===\"\n# Check file integrity\necho \"Genome file size:\"\nls -lh *.fna\n\necho \"Number of sequences:\"\ngrep -c \"&gt;\" GRCh38_genomic.fna\n\necho \"First few sequence headers:\"\ngrep \"&gt;\" GRCh38_genomic.fna | head -10\n\necho \"=== Download Complete ===\"\necho \"Reference genome files are ready in: $(pwd)\"\necho \"Main genome file: GRCh38_genomic.fna\"\necho \"Chromosome-only file: hg38_chromosomes_only.fa\"\necho \"\"\necho \"Files generated:\"\necho \"- GRCh38_genomic.fna (main reference)\"\necho \"- GRCh38_genomic.fna.amb, .ann, .bwt, .pac, .sa (BWA index)\"\necho \"- GRCh38_genomic.fna.fai (samtools index)\"\n\n# Align reads to human genome\nfor file in `ls ${indata}*1.fastq.gz`\ndo\n  sample=$(basename ${file} _1.fastq.gz)\n  bowtie2 -x ${refs}human_index -1 {sample}${data}_trimmed_R1.fastq.gz -2 sample_trimmed_R2.fastq.gz \\\n    --un-conc sample_host_removed.fastq.gz -S /dev/null\n\n\n  bwa mem -t 8 human_reference.fasta \\\n      trimmed_R1_paired.fastq.gz trimmed_R2_paired.fastq.gz \\\n      | samtools view -b -f 4 - &gt; unmapped_reads.bam\n\n  # Convert unmapped reads back to FASTQ\n  samtools fastq -1 dehosted_R1.fastq.gz -2 dehosted_R2.fastq.gz unmapped_reads.bam\n\necho \"Host removal completed\"\n</code></pre>"},{"location":"day8/metagenomics/#step-5-taxonomic-profiling-read-based","title":"Step 5. Taxonomic Profiling (Read-Based)","text":"<ul> <li>Why: Directly assigns taxonomy without assembly, faster and less computationally intensive.</li> <li>Tools: Kraken2 + Bracken, MetaPhlAn</li> </ul> <pre><code>kraken2 --db kraken2_db --paired sample_host_removed.1.fq.gz sample_host_removed.2.fq.gz \\\n  --output kraken2_output.txt --report kraken2_report.txt\nbracken -d kraken2_db -i kraken2_report.txt -o bracken_species.txt -r 150 -l S\n</code></pre>"},{"location":"day8/metagenomics/#step-6-functional-profiling-read-based","title":"Step 6. Functional Profiling (Read-Based)","text":"<ul> <li>Why: Identifies pathways/genes present without needing assembly.</li> <li>Tool: HUMAnN</li> </ul> <pre><code>humann --input sample_host_removed.fastq.gz --output humann_out/\n</code></pre>"},{"location":"day8/metagenomics/#step-7-assembly-based-profiling","title":"Step 7. Assembly-Based Profiling","text":"<ul> <li>Why: Reconstructs contigs/MAGs b\u0006\u0012 enables strain-level analysis, discovery of new genes, plasmids, MGEs.</li> <li>Tools: MEGAHIT or metaSPAdes</li> </ul>"},{"location":"day8/metagenomics/#megahit","title":"MEGAHIT","text":"<pre><code>megahit -1 sample_host_removed.1.fq.gz -2 sample_host_removed.2.fq.gz -o megahit_out/\n</code></pre>"},{"location":"day8/metagenomics/#metaspades","title":"metaSPAdes","text":"<pre><code>spades.py --meta -1 sample_host_removed.1.fq.gz -2 sample_host_removed.2.fq.gz -o metaspades_out/\n</code></pre> <p>MEGAHIT vs metaSPAdes:</p> <ul> <li>MEGAHIT Advantages:</li> <li>Extremely fast, lower memory usage.</li> <li>Scales better for very large datasets (e.g., population metagenomes).</li> <li>MEGAHIT Disadvantages:</li> <li>May produce slightly shorter contigs than metaSPAdes.</li> <li>metaSPAdes Advantages:</li> <li>Produces higher-quality, longer assemblies (useful for MAG recovery).</li> <li>metaSPAdes Disadvantages:</li> <li>Requires more RAM and CPU time.</li> </ul>"},{"location":"day8/metagenomics/#step-8-mapping-binning","title":"Step 8. Mapping &amp; Binning","text":"<ul> <li>Why: Group contigs into MAGs, quantify abundances.</li> <li>Tools: <code>bowtie2</code>, <code>samtools</code>, <code>MetaBAT2</code></li> </ul> <pre><code>bowtie2 -x megahit_out/final.contigs.fa -1 sample_host_removed.1.fq.gz -2 sample_host_removed.2.fq.gz | samtools sort -o aln.bam\nmetabat2 -i megahit_out/final.contigs.fa -a depth.txt -o bins_dir/bin\n</code></pre>"},{"location":"day8/metagenomics/#step-9-mag-quality-control","title":"Step 9. MAG Quality Control","text":"<ul> <li>Why: Ensures completeness &amp; contamination are acceptable.</li> <li>Tool: CheckM</li> </ul> <pre><code>checkm lineage_wf bins_dir/ checkm_out/\n</code></pre>"},{"location":"day8/metagenomics/#step-10-annotation-specialized-analyses","title":"Step 10. Annotation &amp; Specialized Analyses","text":"<ul> <li>Why: Identify AMR, virulence, plasmids, metabolic capacity.</li> <li>Tools: Prokka, Bakta, AMRFinderPlus, ABRicate, DRAM</li> </ul>"},{"location":"day8/metagenomics/#where-dram-fits-in","title":"Where DRAM fits in:","text":"<ul> <li>Swap in DRAM in place of Prokka/Bakta for functional annotation of MAGs.</li> <li>DRAM produces detailed metabolic profiles, pathway reconstruction, and microbial ecology insights.</li> </ul> <pre><code>DRAM.py annotate -i bins_dir/ -o dram_out/ --threads 16\n</code></pre>"},{"location":"day8/metagenomics/#step-11-abundance-estimation-visualization","title":"Step 11. Abundance Estimation &amp; Visualization","text":"<ul> <li>Why: Quantifies taxa/genes b\u0006\u0012 links to clinical or epidemiological metadata.</li> <li>Tools: CoverM, Krona, R for plots.</li> </ul> <pre><code>coverm contig --bam-files aln.bam --reference megahit_out/final.contigs.fa --methods tpm &gt; coverm_tpm.tsv\n</code></pre>"},{"location":"day8/metagenomics/#step-12-reporting-reproducibility","title":"Step 12. Reporting &amp; Reproducibility","text":"<ul> <li>Why: Essential for public health applications.</li> <li>Use Nextflow + Singularity/Conda for reproducible pipelines.</li> <li>Summarize results in Excel or RMarkdown reports.</li> </ul>"},{"location":"day8/metagenomics/#nextflow-pipelines","title":"Nextflow pipelines","text":""},{"location":"day8/metagenomics/#nfcoremag-pipeline-httpsnf-coremag400","title":"nfcore/mag Pipeline https://nf-co.re/mag/4.0.0","text":"<ul> <li>Use an nf-core/mag pipeline for assembly, binning and annotation of metagenomes, github repository.</li> <li>This pipeline works for short- and/or long-reads.</li> <li>\u2705Key Features:</li> <li>Preprocessing:<ul> <li>Short reads: fastp, Bowtie2, FastQC</li> <li>Long reads: Porechop, NanoLyse, Filtlong, NanoPlot</li> </ul> </li> <li>Assembly:<ul> <li>Short reads: MEGAHIT, SPAdes</li> <li>Hybrid: hybridSPAdes</li> </ul> </li> <li>Binning:<ul> <li>Tools: MetaBAT2, MaxBin2, CONCOCT, DAS Tool</li> <li>Quality checks: BUSCO, CheckM, GUNC</li> </ul> </li> <li>Taxonomic Classification:<ul> <li>Tools: GTDB-Tk, CAT/BAT</li> <li>Co-assembly and co-abundance:</li> <li>Supports sample grouping for co-assembly and binning</li> </ul> </li> <li>\ud83d\udce6 Reproducibility:</li> <li>Uses Nextflow DSL2, Docker/Singularity containers</li> <li>Fully portable across HPC, cloud, and local systems</li> <li>Includes test datasets and CI testing</li> <li>It requires a sample sheet in <code>csv</code>  format with five columns: sample,group,short_reads_1,short_reads_2,long_reads</li> <li>Assuming all your raw reads (short- and long-reads) are in the same folder, run the python script:</li> </ul> <pre><code>proj=\"/data/users/${USER}/metagenomes/shotgun/\"\n# Create required directories\nmkdir -p ${proj}scripts ${proj}logs\n# Get the script to create samplesheet\ncp /data/users/user24/metagenomes/shotgun/scripts/generate_mag_samplesheet.py /data/users/${USER}/metagenomes/shotgun/scripts/\n\n# Create samplesheet for running mag\npython3 /data/users/${USER}/metagenomes/shotgun/scripts/generate_mag_samplesheet.py /data/users/user29/metagenomes/shotgun/ mag-samplesheet.csv\n# Create submission script\nnano data/users/${USER}/metagenomes/shotgun/scripts/mag-nf_submit.sh\n\n#!/bin/bash\n#SBATCH --job-name='mag'\n#SBATCH --time=24:00:00\n#SBATCH --mem=128g\n#SBATCH --ntasks=16\n#SBATCH --output=/data/users/user24/metagenomes/shotgun/logs/nfcore-mag-stdout.log\n#SBATCH --error=/data/users/user24/metagenomes/shotgun/logs/nfcore-mag-stderr.log\n#SBATCH --mail-user=ephie.geza@uct.ac.za\n\nproj=\"/data/users/user24/metagenomes/shotgun/\"\n\nmodule load nextflow/25.04.6\n#### Unload JAVA 18 as it doesn't work and load JAVA 17\nmodule unload java/openjdk-18.0.2\nmodule load java/openjdk-17.0.2\n####Unset conflicting environment variables (optional but recommended)\nunset JAVA_CMD\nunset JAVA_HOME\n\n#### Run pipeline\nnextflow run ${proj}mag \\\n      --input ${proj}mag-samplesheet.csv \\\n      --outdir ${proj}nfcore-mag \\\n      -w ${work}work/nfcore-mag \\\n      -profile singularity \\\n      -resume --skip_gtdbtk\n\n## Save and submit\n</code></pre>"},{"location":"day8/metagenomics/#nf-corefuncscan","title":"nf-core/funcscan","text":"<p>Using contigs to screen for functional and natural gene sequences</p>"},{"location":"day8/metagenomics/#viralgenie-nf-coreviralmetagenome","title":"Viralgenie nf-core/viralmetagenome","text":""},{"location":"day8/metagenomics/#nf-coretaxprofiler","title":"nf-core/taxprofiler","text":""},{"location":"day8/metagenomics/#targeted-metagenomics","title":"Targeted Metagenomics","text":""},{"location":"day8/metagenomics/#nf-coreampliseq","title":"nf-core/ampliseq","text":""},{"location":"modules/day1/","title":"Day 1: Welcome to the Course!","text":"<p>Date: September 1, 2025 Duration: 09:00-13:00 CAT Focus: Course introduction, genomic surveillance overview, sequencing technologies</p>"},{"location":"modules/day1/#overview","title":"Overview","text":"<p>Welcome to the Microbial Genomics &amp; Metagenomics Training Course! Day 1 introduces the course, provides essential background on genomic surveillance, covers sequencing technologies, and introduces key databases and tools used throughout the training.</p>"},{"location":"modules/day1/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 1, you will be able to:</p> <ul> <li>Understand the role of genomic surveillance in public health</li> <li>Recognize different sequencing technologies and their applications</li> <li>Navigate and use PubMLST database resources</li> <li>Perform basic command line operations</li> <li>Set up and configure analysis environments</li> </ul>"},{"location":"modules/day1/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Introductions All 09:10 Overview of clinical pathogens and genomic surveillance Slides Ephifania Geza 09:40 Overview of sequencing technologies and data types Sindiswa Lukhele 10:00 Setting up and exploring PubMLST Sindiswa Lukhele 11:00 Break 11:30 Introduction to command line interface Practical Arash Iranzadeh"},{"location":"modules/day1/#key-topics","title":"Key Topics","text":""},{"location":"modules/day1/#1-course-introduction-and-participant-introductions","title":"1. Course Introduction and Participant Introductions","text":"<ul> <li>Welcome and course overview</li> <li>Trainer and participant introductions</li> <li>Course objectives and structure</li> <li>Training schedule and logistics</li> </ul>"},{"location":"modules/day1/#2-clinical-pathogens-and-genomic-surveillance","title":"2. Clinical Pathogens and Genomic Surveillance","text":"<ul> <li>Role of genomics in infectious disease surveillance</li> <li>Applications in outbreak investigation</li> <li>Antimicrobial resistance monitoring</li> <li>Integration with epidemiological data</li> </ul>"},{"location":"modules/day1/#3-sequencing-technologies-and-data-types","title":"3. Sequencing Technologies and Data Types","text":"<ul> <li>Next-generation sequencing platforms</li> <li>Illumina, Oxford Nanopore, PacBio comparison</li> <li>Short-read vs long-read technologies</li> <li>Data quality considerations and file formats</li> </ul>"},{"location":"modules/day1/#4-pubmlst-database-system","title":"4. PubMLST Database System","text":"<ul> <li>Multi-locus sequence typing (MLST) concepts</li> <li>Database navigation and search functions</li> <li>Species-specific typing schemes</li> <li>Data submission and retrieval</li> </ul>"},{"location":"modules/day1/#5-command-line-interface-basics","title":"5. Command Line Interface Basics","text":"<ul> <li>Introduction to Unix/Linux command line</li> <li>Git Bash setup for Windows users</li> <li>Basic file operations and navigation</li> <li>Introduction to R statistical environment</li> </ul>"},{"location":"modules/day1/#tools-and-resources","title":"Tools and Resources","text":""},{"location":"modules/day1/#databases-explored","title":"Databases Explored","text":"<ul> <li>PubMLST - Public databases for molecular typing</li> <li>Pathogen databases - Species-specific resources</li> <li>MLST schemes - Standardized typing protocols</li> </ul>"},{"location":"modules/day1/#software-introduced","title":"Software Introduced","text":"<ul> <li>Git Bash - Command line interface for Windows</li> <li>R/RStudio - Statistical computing environment</li> <li>Web browsers - For database navigation</li> <li>Terminal applications - Command line access</li> </ul>"},{"location":"modules/day1/#hands-on-activities","title":"Hands-on Activities","text":""},{"location":"modules/day1/#exercise-1-pubmlst-exploration-30-minutes","title":"Exercise 1: PubMLST Exploration (30 minutes)","text":"<p>Navigate the PubMLST website and explore available databases for different pathogens.</p>"},{"location":"modules/day1/#exercise-2-basic-command-line-operations-45-minutes","title":"Exercise 2: Basic Command Line Operations (45 minutes)","text":"<p>Practice essential Unix commands and file system navigation.</p>"},{"location":"modules/day1/#exercise-3-r-environment-setup-30-minutes","title":"Exercise 3: R Environment Setup (30 minutes)","text":"<p>Install and configure R/RStudio for data analysis.</p>"},{"location":"modules/day1/#exercise-4-database-search-practice-15-minutes","title":"Exercise 4: Database Search Practice (15 minutes)","text":"<p>Search for MLST data for specific bacterial isolates.</p>"},{"location":"modules/day1/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day1/#genomic-surveillance-applications","title":"Genomic Surveillance Applications","text":"<ul> <li>Outbreak investigation: Tracking transmission patterns</li> <li>Antimicrobial resistance: Monitoring resistance emergence</li> <li>Epidemiological studies: Population structure analysis</li> <li>Public health response: Informing control measures</li> </ul>"},{"location":"modules/day1/#sequencing-technology-comparison","title":"Sequencing Technology Comparison","text":"Platform Read Length Accuracy Throughput Cost Best For Illumina 150-300 bp &gt;99% High Low Routine surveillance Oxford Nanopore 1-100 kb ~95% Medium Medium Structural variants PacBio 10-25 kb &gt;99% Medium High Complete genomes"},{"location":"modules/day1/#mlst-fundamentals","title":"MLST Fundamentals","text":"<ul> <li>Housekeeping genes: Conserved sequences for typing</li> <li>Allelic profiles: Unique combinations define sequence types</li> <li>Population structure: Understanding strain relationships</li> <li>Standardization: Reproducible typing across laboratories</li> </ul>"},{"location":"modules/day1/#resources","title":"Resources","text":""},{"location":"modules/day1/#essential-websites","title":"Essential Websites","text":"<ul> <li>PubMLST - Public databases for molecular typing</li> <li>Pathogen Watch - Genomic surveillance platform</li> <li>NCBI SRA - Sequence Read Archive</li> </ul>"},{"location":"modules/day1/#documentation","title":"Documentation","text":"<ul> <li>Git Bash User Guide</li> <li>R Project Documentation</li> <li>PubMLST User Guide</li> </ul>"},{"location":"modules/day1/#training-materials","title":"Training Materials","text":"<ul> <li>Command line cheat sheets</li> <li>MLST database tutorials</li> <li>Sequencing technology overviews</li> </ul>"},{"location":"modules/day1/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day1/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Navigate PubMLST interface successfully</li> <li>Execute basic command line operations</li> <li>Identify appropriate sequencing platforms for different applications</li> <li>Understand MLST typing principles</li> </ul>"},{"location":"modules/day1/#group-discussion","title":"Group Discussion","text":"<ul> <li>Share experiences with different pathogens</li> <li>Discuss genomic surveillance challenges in different settings</li> <li>Compare sequencing technology applications</li> <li>Explore database search strategies</li> </ul>"},{"location":"modules/day1/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day1/#command-line-anxiety","title":"Command Line Anxiety","text":"<p>Many participants are new to command line interfaces. We provide: - Patient, step-by-step instruction - Plenty of practice time - Peer support and collaboration - Reference materials for later use</p>"},{"location":"modules/day1/#technical-setup-issues","title":"Technical Setup Issues","text":"<pre><code># Common Git Bash issues on Windows\n# Check if Git Bash is properly installed\ngit --version\n\n# Verify R installation\nR --version\n</code></pre>"},{"location":"modules/day1/#database-navigation","title":"Database Navigation","text":"<ul> <li>Start with simple searches</li> <li>Use guided examples</li> <li>Practice with known organisms</li> <li>Build confidence gradually</li> </ul>"},{"location":"modules/day1/#looking-ahead","title":"Looking Ahead","text":"<p>Day 2 Preview: Introduction to Command Line, HPC, &amp; Quality Control including: - High Performance Computing (HPC) introduction - Advanced command line operations - Quality checking and control methods - Species identification techniques - Guest talk on M. tuberculosis and co-infection</p> <p>Key Learning Outcome: Day 1 establishes the foundational knowledge of genomic surveillance principles, sequencing technologies, and essential database resources that underpin all subsequent training activities.</p>"},{"location":"modules/day10/","title":"Day 10: Wrap-up session","text":"<p>Date: September 12, 2025 Duration: 09:00-11:40 CAT Focus: Course conclusion, presentations, and future directions</p>"},{"location":"modules/day10/#overview","title":"Overview","text":"<p>The final day of the microbial genomics training course brings together all learning experiences through participant presentations, showcases ongoing research initiatives, and provides guidance for continued professional development in computational biology.</p>"},{"location":"modules/day10/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 10, you will be able to:</p> <ul> <li>Present bioinformatics analysis results effectively to scientific audiences</li> <li>Demonstrate mastery of key concepts covered throughout the course</li> <li>Identify resources for continued learning and professional development</li> <li>Connect with ongoing research initiatives and training opportunities</li> <li>Plan next steps for implementing learned skills in your research</li> <li>Build professional networks within the computational biology community</li> </ul>"},{"location":"modules/day10/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Participant presentations 11:15 Short talks NGS-Academy/AfriGen-D/eLwazi ODSP 11:40 End of the course [Resources]"},{"location":"modules/day10/#presentation-session-0900-1115","title":"Presentation Session (09:00-11:15)","text":""},{"location":"modules/day10/#presentation-guidelines","title":"Presentation Guidelines","text":"<p>Each participant will deliver a 5-minute presentation covering their Day 9 analysis work:</p>"},{"location":"modules/day10/#presentation-structure","title":"Presentation Structure","text":"<ol> <li>Introduction (1 minute)</li> <li>Research question or objective</li> <li>Brief background context</li> <li> <p>Dataset description</p> </li> <li> <p>Methods (1.5 minutes)</p> </li> <li>Analysis workflow overview</li> <li>Key tools and techniques used</li> <li> <p>Parameter choices and rationale</p> </li> <li> <p>Results (2 minutes)</p> </li> <li>Major findings from analysis</li> <li>Key figures or summary statistics</li> <li> <p>Interpretation of results</p> </li> <li> <p>Challenges &amp; Solutions (30 seconds)</p> </li> <li>Main obstacles encountered</li> <li>How they were addressed</li> <li>Lessons learned</li> </ol>"},{"location":"modules/day10/#technical-requirements","title":"Technical Requirements","text":"<ul> <li>Format: PDF slides or live demonstration</li> <li>Time limit: Strictly enforced 5 minutes</li> <li>Q&amp;A: 2-3 minutes for questions after each presentation</li> <li>Backup: Have presentation files ready on USB drive</li> </ul>"},{"location":"modules/day10/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>Presentations will be assessed on:</p> <ul> <li>Scientific rigor: Appropriate methods and interpretation</li> <li>Technical competence: Correct use of bioinformatics tools</li> <li>Communication clarity: Clear explanation of complex concepts</li> <li>Problem-solving: Evidence of troubleshooting and adaptation</li> <li>Course integration: Application of multiple course concepts</li> </ul>"},{"location":"modules/day10/#sample-presentation-topics","title":"Sample Presentation Topics","text":"<p>Based on participant data types, presentations may cover:</p>"},{"location":"modules/day10/#genomic-analysis-examples","title":"Genomic Analysis Examples","text":"<ul> <li>\"Antimicrobial resistance profiling in Mycobacterium tuberculosis isolates\"</li> <li>\"Phylogenetic analysis of Mycobacterium tuberculosis outbreak strains\"</li> <li>\"Comparative genomics of Vibrio cholerae from environmental samples\"</li> <li>\"Transmission analysis of Vibrio cholerae epidemic strains\"</li> </ul>"},{"location":"modules/day10/#methodological-examples","title":"Methodological Examples","text":"<ul> <li>\"Optimizing assembly parameters for low-coverage genomes\"</li> <li>\"Developing Nextflow pipeline for routine surveillance\"</li> <li>\"Quality control strategies for degraded DNA samples\"</li> </ul>"},{"location":"modules/day10/#presentation-order","title":"Presentation Order","text":"<p>Presentations will be grouped by analysis type to facilitate discussion:</p> <ol> <li>Genomic Surveillance (9:00-9:45)</li> <li>Methodological &amp; Pipeline Development (9:45-11:15)</li> </ol>"},{"location":"modules/day10/#special-presentations-1115-1140","title":"Special Presentations (11:15-11:40)","text":""},{"location":"modules/day10/#ngs-academy-initiative","title":"NGS-Academy Initiative","text":"<p>Overview of next-generation sequencing training programs - Advanced training opportunities - Certification pathways - Research collaboration opportunities - International exchange programs</p>"},{"location":"modules/day10/#afrigen-d-project","title":"AfriGen-D Project","text":"<p>African Genome Diversity Project updates - Current research initiatives - Collaboration opportunities for participants - Data sharing and analysis platforms - Future funding opportunities</p>"},{"location":"modules/day10/#elwazi-odsp-open-data-science-platform","title":"eLwazi ODSP (Open Data Science Platform)","text":"<p>Data science infrastructure and resources - Platform capabilities and access - Training modules and resources - Research project support - Community building initiatives</p>"},{"location":"modules/day10/#course-completion","title":"Course Completion","text":""},{"location":"modules/day10/#certificate-requirements","title":"Certificate Requirements","text":"<p>To receive course completion certificate, participants must:</p> <ul> <li> Attend at least 8 out of 10 training days</li> <li> Complete all hands-on exercises</li> <li> Submit Day 9 analysis documentation</li> <li> Deliver Day 10 presentation</li> <li> Participate in course evaluation</li> </ul>"},{"location":"modules/day10/#skills-assessment-summary","title":"Skills Assessment Summary","text":"<p>By course completion, participants will have demonstrated:</p>"},{"location":"modules/day10/#technical-skills","title":"Technical Skills","text":"<ul> <li>Command line proficiency: Navigation, file management, and tool execution</li> <li>Quality control: Assessment and improvement of sequencing data</li> <li>Genome assembly: De novo assembly and quality assessment</li> <li>Annotation: Functional and structural genome annotation</li> <li>Phylogenetics: Tree construction and interpretation</li> <li>Workflow development: Nextflow pipeline creation and optimization</li> </ul>"},{"location":"modules/day10/#analytical-skills","title":"Analytical Skills","text":"<ul> <li>Data interpretation: Drawing biological conclusions from computational results</li> <li>Method selection: Choosing appropriate tools for specific analyses</li> <li>Parameter optimization: Adjusting analysis parameters for data characteristics</li> <li>Quality assessment: Evaluating reliability of computational results</li> <li>Troubleshooting: Diagnosing and solving technical problems</li> </ul>"},{"location":"modules/day10/#professional-skills","title":"Professional Skills","text":"<ul> <li>Documentation: Maintaining analysis logs and reproducible workflows</li> <li>Presentation: Communicating results to scientific audiences</li> <li>Collaboration: Working effectively in computational research teams</li> <li>Continuous learning: Accessing resources for ongoing skill development</li> </ul>"},{"location":"modules/day10/#post-course-resources","title":"Post-Course Resources","text":""},{"location":"modules/day10/#immediate-support-next-3-months","title":"Immediate Support (Next 3 months)","text":"<ul> <li>Email support: Continued access to trainer expertise</li> <li>Online forum: Participant discussion platform</li> <li>Monthly virtual meetups: Progress sharing and troubleshooting</li> <li>Resource sharing: Access to course materials and datasets</li> </ul>"},{"location":"modules/day10/#long-term-development","title":"Long-term Development","text":"<ul> <li>Advanced training: Information about specialized workshops</li> <li>Research collaboration: Connections to ongoing projects</li> <li>Professional networks: Links to regional and international communities</li> <li>Career opportunities: Job postings and fellowship announcements</li> </ul>"},{"location":"modules/day10/#recommended-next-steps","title":"Recommended Next Steps","text":""},{"location":"modules/day10/#for-beginners","title":"For Beginners","text":"<ol> <li>Practice with additional datasets</li> <li>Complete online tutorials for specific tools</li> <li>Join local bioinformatics user groups</li> <li>Consider formal coursework in computational biology</li> </ol>"},{"location":"modules/day10/#for-intermediate-users","title":"For Intermediate Users","text":"<ol> <li>Develop specialized analysis pipelines</li> <li>Contribute to open-source bioinformatics projects</li> <li>Attend specialized conferences and workshops</li> <li>Mentor others in computational skills</li> </ol>"},{"location":"modules/day10/#for-advanced-users","title":"for Advanced Users","text":"<ol> <li>Lead research projects using learned techniques</li> <li>Develop novel analytical methods</li> <li>Teach and train others in the community</li> <li>Collaborate on large-scale genomics initiatives</li> </ol>"},{"location":"modules/day10/#course-evaluation","title":"Course Evaluation","text":""},{"location":"modules/day10/#feedback-categories","title":"Feedback Categories","text":""},{"location":"modules/day10/#content-assessment","title":"Content Assessment","text":"<ul> <li>Relevance to research needs</li> <li>Appropriate level of technical detail</li> <li>Balance of theory and practical application</li> <li>Currency of methods and tools</li> </ul>"},{"location":"modules/day10/#delivery-evaluation","title":"Delivery Evaluation","text":"<ul> <li>Trainer expertise and communication</li> <li>Hands-on exercise quality</li> <li>Technical support adequacy</li> <li>Course pacing and organization</li> </ul>"},{"location":"modules/day10/#impact-measurement","title":"Impact Measurement","text":"<ul> <li>Confidence in using bioinformatics tools</li> <li>Likelihood of applying learned skills</li> <li>Interest in advanced training</li> <li>Recommendations to colleagues</li> </ul>"},{"location":"modules/day10/#improvement-suggestions","title":"Improvement Suggestions","text":"<p>Participants are encouraged to provide specific suggestions for: - Additional topics to cover - Alternative teaching methods - Better integration of concepts - Enhanced practical exercises - Improved course materials</p>"},{"location":"modules/day10/#networking-and-community-building","title":"Networking and Community Building","text":""},{"location":"modules/day10/#contact-information-exchange","title":"Contact Information Exchange","text":"<ul> <li>WhatsApp group for ongoing communication</li> <li>LinkedIn professional network connections</li> <li>GitHub collaboration on analysis projects</li> <li>Research ResearchGate connections</li> </ul>"},{"location":"modules/day10/#regional-initiatives","title":"Regional Initiatives","text":"<ul> <li>South African Bioinformatics Society: Local meetings and conferences</li> <li>H3ABioNet: Pan-African bioinformatics network</li> <li>ISCB Regional Student Groups: International student connections</li> <li>Local university partnerships: Ongoing collaboration opportunities</li> </ul>"},{"location":"modules/day10/#final-remarks","title":"Final Remarks","text":""},{"location":"modules/day10/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Bioinformatics is a journey: Continuous learning and adaptation required</li> <li>Community matters: Collaboration accelerates learning and research</li> <li>Documentation is crucial: Reproducible research starts with good record-keeping</li> <li>Practice makes perfect: Regular use of skills prevents deterioration</li> <li>Stay current: Field evolves rapidly, ongoing education essential</li> </ol>"},{"location":"modules/day10/#words-of-encouragement","title":"Words of Encouragement","text":"<p>The computational biology field welcomes contributors from diverse backgrounds. Your unique perspective, combined with the skills learned in this course, positions you to make meaningful contributions to understanding microbial genomics and its applications to human health.</p>"},{"location":"modules/day10/#course-legacy","title":"Course Legacy","text":"<p>This training represents an investment in the future of computational biology in Africa. As you apply these skills in your research and career, you become part of a growing network of scientists advancing genomic medicine and public health through computational approaches.</p>"},{"location":"modules/day10/#acknowledgments","title":"Acknowledgments","text":""},{"location":"modules/day10/#training-team-recognition","title":"Training Team Recognition","text":"<p>Special thanks to the course instructors: - Ephifania Geza: Lead instructor and course coordinator - Arash Iranzadeh: Technical instruction and phylogenomics expertise - Sindiswa Lukhele: Sequencing technologies and quality control - Mamana Mbiyavanga: HPC systems and workflow development - Bethlehem Adnew: Guest expertise on tuberculosis genomics</p>"},{"location":"modules/day10/#institutional-support","title":"Institutional Support","text":"<ul> <li>University of Cape Town Computational Biology Division</li> <li>CIDRI-Africa research infrastructure</li> <li>High-performance computing facility access</li> <li>Guest lecture coordination and logistics</li> </ul>"},{"location":"modules/day10/#community-contributions","title":"Community Contributions","text":"<ul> <li>Dataset providers and research collaborators</li> <li>Open-source software developers</li> <li>International training program partnerships</li> <li>Participant engagement and peer learning</li> </ul>"},{"location":"modules/day10/#contact-information","title":"Contact Information","text":""},{"location":"modules/day10/#immediate-questions","title":"Immediate Questions","text":"<p>Course Coordinator: Ephifania Geza Email: ephifania.geza@uct.ac.za</p>"},{"location":"modules/day10/#technical-support","title":"Technical Support","text":"<p>HPC and Workflows: Mamana Mbiyavanga Email: mamana.mbiyavanga@uct.ac.za</p>"},{"location":"modules/day10/#general-inquiries","title":"General Inquiries","text":"<p>Training Program: microbial-genomics-training@uct.ac.za</p>"},{"location":"modules/day10/#follow-up-resources","title":"Follow-up Resources","text":"<ul> <li>Course Materials: GitHub repository access maintained</li> <li>Discussion Forum: Access links provided via email</li> <li>Newsletter: Quarterly updates on opportunities and resources</li> </ul> <p>Final Learning Outcome: Completion of this intensive training program provides participants with both the technical skills and professional network needed to pursue independent research in microbial genomics, contributing to advances in infectious disease understanding, antimicrobial resistance surveillance, and public health genomics.</p>"},{"location":"modules/day2/","title":"Day 2: Introduction to Commandline","text":"<p>Date: September 2, 2025 Duration: 09:00-13:00 CAT Focus: Command line proficiency, M. tuberculosis genomics</p>"},{"location":"modules/day2/#overview","title":"Overview","text":"<p>Day 2 focuses on building strong command line skills essential for bioinformatics work. This day provides the computational foundation needed for all subsequent genomic analyses in the course.</p>"},{"location":"modules/day2/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 2, you will be able to:</p> <ul> <li>Master essential Unix/Linux command line operations for bioinformatics workflows</li> <li>Understand M. tuberculosis genomics and co-infection patterns</li> </ul>"},{"location":"modules/day2/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Introduction to command line interface Practical Arash Iranzadeh 11:30 Break 12:00 Guest talk: MtB and co-infection Speaker Bio Bethlehem Adnew"},{"location":"modules/day2/#key-topics","title":"Key Topics","text":""},{"location":"modules/day2/#1-command-line-interface-fundamentals","title":"1. Command Line Interface Fundamentals","text":"<ul> <li>Unix/Linux file system navigation</li> <li>Essential commands for bioinformatics (grep, awk, sed)</li> <li>File manipulation and text processing</li> <li>Pipes and command chaining</li> <li>Working with compressed files (gzip, tar)</li> <li>Shell scripting basics for automation</li> </ul>"},{"location":"modules/day2/#2-m-tuberculosis-and-co-infection","title":"2. M. tuberculosis and Co-infection","text":"<ul> <li>TB genomics and strain typing</li> <li>Co-infection patterns and detection</li> <li>Clinical implications</li> <li>Molecular epidemiology approaches</li> <li>Drug resistance mechanisms</li> <li>Public health applications</li> </ul>"},{"location":"modules/day2/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day2/#command-line-tools","title":"Command Line Tools","text":"<ul> <li>Bash shell - Command line interface and scripting</li> <li>GNU coreutils - Essential Unix utilities (ls, cd, grep, etc.)</li> <li>Text processing - awk, sed, cut, sort, uniq</li> <li>File compression - gzip, tar, zip</li> <li>tmux/screen - Terminal session management</li> <li>rsync/scp - File transfer and synchronization</li> </ul>"},{"location":"modules/day2/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day2/#exercise-1-command-line-fundamentals-90-minutes","title":"Exercise 1: Command Line Fundamentals (90 minutes)","text":"<p>Master essential Unix commands for bioinformatics through practical exercises.</p> <pre><code># Navigate file systems and manipulate files\ncd ~/data\nls -la\nmkdir analysis_output\n\n# Process text files with Unix tools\ngrep \"^&gt;\" sequences.fasta | wc -l  # Count sequences\ncat sample.fastq | head -20         # View file contents\n\n# Work with compressed files\ngzip large_file.txt\ngunzip -c compressed.gz | head\n\n# Use pipes and redirection\ncat data.txt | sort | uniq &gt; unique_values.txt\n</code></pre>"},{"location":"modules/day2/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day2/#command-line-essentials","title":"Command Line Essentials","text":"<ul> <li>File system navigation: Understanding directory structure and paths</li> <li>Text processing: Using grep, sed, awk for data manipulation</li> <li>Pipes and redirection: Chaining commands for complex operations</li> <li>Shell scripting: Automating repetitive tasks</li> <li>Regular expressions: Pattern matching in bioinformatics data</li> </ul>"},{"location":"modules/day2/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day2/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Successfully connect to Ilifu HPC system</li> <li>Navigate Unix file system and manipulate files</li> <li>Complete command line exercises for pathogen genomics data</li> </ul>"},{"location":"modules/day2/#group-discussion","title":"Group Discussion","text":"<ul> <li>Share command line tips and tricks</li> <li>Discuss HPC resource management strategies</li> <li>Troubleshoot connection and job submission issues</li> <li>Compare different approaches to batch processing</li> </ul>"},{"location":"modules/day2/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day2/#command-line-challenges","title":"Command Line Challenges","text":"<pre><code># Permission denied errors\nchmod +x script.sh    # Make script executable\nls -la                # Check file permissions\n\n# Path issues\necho $PATH            # Check current PATH\nexport PATH=$PATH:/new/path  # Add to PATH\n</code></pre>"},{"location":"modules/day2/#command-line-resources","title":"Command Line Resources","text":"<ul> <li>Unix for Bioinformatics</li> <li>Bash Scripting Guide</li> <li>Command Line for Genomics</li> </ul>"},{"location":"modules/day2/#m-tuberculosis-resources","title":"M. tuberculosis Resources","text":"<ul> <li>TB-Profiler</li> <li>ReSeqTB</li> <li>TBDB</li> </ul>"},{"location":"modules/day2/#guest-lecture-mtb-and-co-infection","title":"Guest Lecture: MtB and Co-infection","text":""},{"location":"modules/day2/#speaker-bethlehem-adnew","title":"Speaker: Bethlehem Adnew","text":""},{"location":"modules/day2/#key-topics-covered","title":"Key Topics Covered","text":"<ul> <li>M. tuberculosis genomics: Strain diversity and typing methods</li> <li>Co-infection dynamics: TB-HIV and other respiratory pathogens</li> <li>Diagnostic challenges: Molecular detection in complex samples</li> <li>Treatment implications: Drug resistance in co-infected patients</li> <li>Epidemiological insights: Transmission patterns and control strategies</li> </ul>"},{"location":"modules/day2/#interactive-discussion-points","title":"Interactive Discussion Points","text":"<ul> <li>Current challenges in TB diagnosis</li> <li>Role of genomics in outbreak investigation</li> <li>Future directions in TB research</li> <li>Integration of genomic and clinical data</li> </ul>"},{"location":"modules/day2/#looking-ahead","title":"Looking Ahead","text":"<p>Day 3 Preview:  - Command line proficiency, - HPC fundamentals - Quality checking and control with FastQC - Species identification using Kraken2</p> <p>Key Learning Outcome: Mastery of command line operations and HPC infrastructure usage provides the essential computational foundation for all subsequent genomic analyses in the course. </p>"},{"location":"modules/day3/","title":"Day 3: Accelerating Bioinformatics: HPC, QC, and Species Identification Essentials","text":"<p>Date: September 3, 2025 Duration: 09:00-13:00 CAT Focus: HPC infrastructure, quality control, species identification</p>"},{"location":"modules/day3/#overview","title":"Overview","text":"<p>Day 3 continues building computational skills with an introduction to the Ilifu HPC infrastructure, then introduces essential genomic characterization techniques including quality control, species identification. These foundational skills are critical for all downstream genomic analyses.</p>"},{"location":"modules/day3/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 3, you will be able to:</p> <ul> <li>Connect to and navigate the Ilifu high-performance computing cluster</li> <li>Submit and manage jobs using the SLURM scheduler</li> <li>Understand resource allocation and job queue management on HPC systems</li> <li>Perform quality checking and control on sequencing data using FastQC</li> <li>Identify species from genomic data using Kraken2 and other tools</li> </ul>"},{"location":"modules/day3/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Introduction High Performance Computing (HPC) \u2013 Ilifu Notes \u2022 Practical 1 \u2022 Practical 2 Mamana Mbiyavanga 11:00 Break 11:30 Quality checking and control, as well as species identification Practical Arash Iranzadeh"},{"location":"modules/day3/#key-topics","title":"Key Topics","text":""},{"location":"modules/day3/#1-qc-and-species-identification-essentials","title":"1. QC and Species Identification Essentials","text":"<ul> <li>Quality checking, and adapter and low read quality filtering</li> <li>Contamination detection and removal</li> <li>Species identification or confirmation</li> </ul>"},{"location":"modules/day3/#2-high-performance-computing-hpc-ilifu-infrastructure","title":"2. High Performance Computing (HPC) - Ilifu Infrastructure","text":"<ul> <li>Introduction to cluster computing concepts</li> <li>Ilifu cluster architecture and capabilities</li> <li>SSH connections and authentication</li> <li>SLURM job scheduling system</li> <li>Resource allocation (CPU, memory, time)</li> <li>Module system for software management</li> </ul>"},{"location":"modules/day3/#3-slurm-job-management","title":"3. SLURM Job Management","text":"<ul> <li>Writing and submitting batch scripts</li> <li>Interactive vs batch jobs</li> <li>Job monitoring and queue management</li> <li>Resource specification and optimization</li> <li>Output and error file handling</li> <li>Best practices for efficient HPC usage</li> </ul>"},{"location":"modules/day3/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day3/#hpc-environment","title":"HPC Environment","text":"<ul> <li>Ilifu cluster - High-performance computing infrastructure</li> <li>SLURM - Job scheduling system</li> <li>Module system - Software environment management</li> <li>SSH clients - Remote connection tools</li> </ul>"},{"location":"modules/day3/#quality-control-tools","title":"Quality Control Tools","text":"<ul> <li>FASTQC - Quality checking</li> <li>MULTIQC - Quality checking and amalgation of reports</li> <li>Trimmomatic - Filter adapters and low quality reads</li> <li>Fastp -  Filter adapters and low quality reads</li> <li>KRAKEN2 - Species Identification</li> </ul>"},{"location":"modules/day3/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day3/#exercise-2-ilifu-hpc-connection-and-setup-30-minutes","title":"Exercise 2: Ilifu HPC Connection and Setup (30 minutes)","text":"<p>Connect to the Ilifu cluster and set up your working environment.</p> <pre><code># Connect to Ilifu via SSH\nssh username@slurm.ilifu.ac.za\n\n# Explore the HPC environment\npwd                    # Check current directory\nmodule avail          # List available software\nmodule load python    # Load software module\n\n# Check cluster resources\nsinfo                 # View cluster partitions\nsqueue               # Check job queue\n</code></pre>"},{"location":"modules/day3/#exercise-3-slurm-job-submission-60-minutes","title":"Exercise 3: SLURM Job Submission (60 minutes)","text":"<p>Learn to submit and manage jobs on the HPC cluster.</p> <pre><code># Create a simple batch script\ncat &gt; my_first_job.sh &lt;&lt; 'EOF'\n#!/bin/bash\n#SBATCH --job-name=test_job\n#SBATCH --time=00:10:00\n#SBATCH --mem=1G\n#SBATCH --cpus-per-task=1\n\necho \"Hello from HPC!\"\necho \"Running on node: $HOSTNAME\"\ndate\nEOF\n\n# Submit the job\nsbatch my_first_job.sh\n\n# Monitor job progress\nsqueue -u $USER\n</code></pre>"},{"location":"modules/day3/#exercise-1-species-identification-60-minutes","title":"Exercise 1: Species Identification (60 minutes)","text":"<p>Species identification and contamination screening</p>"},{"location":"modules/day3/#contamination-screening","title":"Contamination screening","text":"<p>kraken2 --db minikraken2_v2 assembly_output/scaffolds.fasta --report contamination_check.txt <pre><code>## Key Concepts\n\n### HPC Computing Principles\n- **Cluster architecture**: Login nodes vs compute nodes\n- **Job scheduling**: SLURM queue management and priority\n- **Resource allocation**: CPU, memory, and time specifications\n- **Module system**: Managing software environments\n- **Parallel processing**: Utilizing multiple cores efficiently\n\n### SLURM Job Management\n| Component | Description | Example |\n|-----------|-------------|---------|\n| Partition | Compute resource group | `main`, `gpu`, `bigmem` |\n| Job State | Current job status | `PD` (pending), `R` (running) |\n| Resources | CPU/Memory/Time | `--cpus-per-task=4 --mem=8G` |\n| Output | Job results and logs | `slurm-jobid.out` |\n\n## Assessment Activities\n\n### Individual Analysis\n- Write and submit a SLURM batch script\n- Monitor job status and retrieve results\n- Complete genome assembly workflow\n- Perform quality assessment and interpretation\n\n### HPC Connection Issues\n```bash\n# SSH key problems\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nssh-copy-id username@slurm.ilifu.ac.za\n\n# Module loading issues\nmodule avail    # List available modules\nmodule list     # Show loaded modules\nmodule purge    # Clear all modules\n</code></pre></p>"},{"location":"modules/day3/#slurm-job-troubleshooting","title":"SLURM Job Troubleshooting","text":"<pre><code># Job stuck in pending\nscontrol show job &lt;jobid&gt;  # Check job details\nsqueue -j &lt;jobid&gt;          # Check specific job\n\n# Resource issues\nsacct -j &lt;jobid&gt; --format=JobID,State,ExitCode,MaxRSS,Elapsed\n</code></pre>"},{"location":"modules/day3/#resources","title":"Resources","text":""},{"location":"modules/day3/#hpc-documentation","title":"HPC Documentation","text":"<ul> <li>Ilifu User Guide</li> <li>SLURM Quick Start</li> <li>SSH Key Management</li> </ul>"},{"location":"modules/day3/#assembly-issues","title":"Assembly Issues","text":"<pre><code># Low coverage assemblies\nspades.py --careful --cov-cutoff 5 -1 R1.fastq -2 R2.fastq -o low_cov_assembly/\n\n# Contamination removal\n# Remove contaminant contigs based on taxonomy\nseqtk subseq scaffolds.fasta clean_contigs.txt &gt; clean_assembly.fasta\n</code></pre>"},{"location":"modules/day3/#clinical-applications","title":"Clinical Applications","text":""},{"location":"modules/day3/#routine-surveillance","title":"Routine Surveillance","text":"<ul> <li>Mantaining data quality control standards</li> <li>Rapid species identification and typing</li> </ul>"},{"location":"modules/day3/#looking-ahead","title":"Looking Ahead","text":"<p>Day 4 Preview:  - Genome assembly and - Genome quality assessment - Genome annotation with Prokka</p> <p>Key Learning Outcome: Quality genomes to increase our confidence in our characterization capabilities essential for clinical genomics and public health surveillance.</p>"},{"location":"modules/day4/","title":"Day 4: Genome Assembly Essentials: QC, Identification, and assembly","text":"<p>Date: September 4, 2025 Duration: 09:00-13:00 CAT Focus: Genome assembly and assessment</p>"},{"location":"modules/day4/#overview","title":"Overview","text":"<p>Day 4 builds on Day 3 by starting with a recap on quality control, followed by genome assembly and assessment of assemblies to ensure.</p>"},{"location":"modules/day4/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 4, you will be able to:</p> <ul> <li>Execute de novo genome assembly using SPAdes or other assemblers</li> <li>Assess assembly quality using QUAST and other metrics</li> </ul>"},{"location":"modules/day4/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Recap: Quality checking and control, and species identification Practical Arash Iranzadeh 10:30 Genome assembly, quality assessment Notes. Ephifania Geza 11:00 Break 11:30 Genome assembly, quality assessment: Continuation Practical Ephifania Geza"},{"location":"modules/day4/#key-topics","title":"Key Topics","text":""},{"location":"modules/day4/#1-genome-assembly-and-assessing-contigs","title":"1. Genome assembly and assessing contigs","text":"<ul> <li>De novo assembly algorithms and approaches</li> <li>Short-read vs long-read assembly strategies</li> <li>Assembly quality metrics and interpretation</li> <li>Assembly polishing and gap filling</li> </ul>"},{"location":"modules/day4/#datasets-used","title":"Datasets Used","text":""},{"location":"modules/day4/#v-cholerae-outbreak-collection","title":"V. cholerae Outbreak Collection","text":"<ul> <li>Source: Multi-country cholera outbreak (2019)</li> <li>Samples: 25 clinical isolates + environmental samples</li> <li>Timespan: 8-month outbreak period</li> <li>Geographic: Three countries, coastal regions</li> <li>Epidemiological data: Case demographics, travel history</li> </ul>"},{"location":"modules/day4/#tools-introduced","title":"Tools Introduced","text":""},{"location":"modules/day4/#pangenome-analysis","title":"Pangenome Analysis","text":"<ul> <li>SPAdes - De novo genome assembler</li> <li>Unicycler - Hybrid assembly pipeline</li> <li>Flye - Long-read assembly</li> <li>QUAST - Assembly quality assessment</li> </ul>"},{"location":"modules/day4/#annotation-tools","title":"Annotation Tools","text":"<ul> <li>Prokka - Automated prokaryotic annotation</li> <li>RAST - Rapid Annotation using Subsystem Technology</li> <li>NCBI PGAP - Prokaryotic Genome Annotation Pipeline</li> <li> <p>Bakta - Rapid bacterial genome annotation</p> </li> <li> <p>Panaroo - Pangenome pipeline</p> </li> <li>Roary - Rapid large-scale prokaryote pangenome analysis</li> <li>PPanGGOLiN - Depicting microbial diversity via pangenomes</li> </ul>"},{"location":"modules/day4/#phylogenetic-analysis","title":"Phylogenetic Analysis","text":"<ul> <li>IQ-TREE - Maximum likelihood phylogenetic inference</li> <li>RAxML - Randomized Axelerated Maximum Likelihood</li> <li>FastTree - Approximately maximum-likelihood trees</li> </ul>"},{"location":"modules/day4/#snp-analysis","title":"SNP Analysis","text":"<ul> <li>Snippy - Rapid haploid variant calling</li> <li>ParSNP - Rapid core genome SNP typing</li> <li>Gubbins - Recombination detection in bacterial genomes</li> </ul>"},{"location":"modules/day4/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day4/#exercise-1-genome-assembly-and-quality-assessment-60-minutes","title":"Exercise 1: Genome Assembly and Quality Assessment (60 minutes)","text":"<p>Assemble bacterial genomes and evaluate assembly quality.</p> <pre><code># De novo assembly with SPAdes\nspades.py --careful -1 sample_R1.fastq.gz -2 sample_R2.fastq.gz -o assembly_output/\n\n# Assembly quality assessment\nquast.py assembly_output/scaffolds.fasta -o quast_results/\n\n# Check assembly statistics\nassembly-stats assembly_output/scaffolds.fasta\n\n\n### Exercise 1: Pangenome Analysis (60 minutes)\nAnalyze the core and accessory genome of *V. cholerae* outbreak strains.\n\n```bash\n# Run Panaroo pangenome analysis\npanaroo -i *.gff -o panaroo_output --clean-mode strict\n\n# Visualize results\npython3 scripts/visualize_pangenome.py panaroo_output/\n</code></pre>"},{"location":"modules/day4/#exercise-2-phylogenetic-tree-construction-60-minutes","title":"Exercise 2: Phylogenetic Tree Construction (60 minutes)","text":"<p>Build maximum likelihood trees from core genome alignments.</p> <pre><code># Generate core genome alignment\nsnippy-core --ref reference.gbk snippy_output/*\n\n# Build phylogenetic tree\niqtree -s core_alignment.aln -m GTR+G -bb 1000 -nt 4\n\n# Visualize tree\nfigtree core_alignment.aln.treefile\n</code></pre>"},{"location":"modules/day4/#exercise-3-outbreak-investigation-45-minutes","title":"Exercise 3: Outbreak Investigation (45 minutes)","text":"<p>Integrate genomic and epidemiological data to investigate transmission.</p> <pre><code># Calculate pairwise SNP distances\nsnp-dists core_alignment.aln &gt; snp_distances.tsv\n\n# Identify transmission clusters\ncluster_analysis.py --snp-threshold 10 --epi-data metadata.csv\n</code></pre>"},{"location":"modules/day4/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day4/#assembly-quality-metrics","title":"Assembly Quality Metrics","text":"Metric Good Assembly Poor Assembly Action N50 &gt;50 kb &lt;10 kb Optimize parameters Contigs &lt;100 &gt;500 Check contamination Genome size Expected \u00b110% &gt;20% difference Review input data Coverage &gt;50x &lt;20x Sequence more"},{"location":"modules/day4/#pangenome-structure","title":"Pangenome Structure","text":"<ul> <li>Core genome: Genes present in all strains (housekeeping functions)</li> <li>Accessory genome: Variable genes (adaptation, virulence, resistance)</li> <li>Singleton genes: Present in single strain only</li> <li>Shell genes: Present in several but not all strains</li> </ul>"},{"location":"modules/day4/#phylogenetic-inference","title":"Phylogenetic Inference","text":"<ul> <li>Substitution models: GTR, HKY, JC69 for nucleotide evolution</li> <li>Rate heterogeneity: Gamma distribution for variable sites</li> <li>Bootstrap support: Statistical confidence in tree topology</li> <li>Branch lengths: Evolutionary distance (substitutions per site)</li> </ul>"},{"location":"modules/day4/#snp-thresholds-for-outbreak-investigation","title":"SNP Thresholds for Outbreak Investigation","text":"Pathogen SNP Threshold Timeframe Context M. tuberculosis 0-5 SNPs Recent transmission Same household S. Typhi 0-20 SNPs Outbreak cluster Weeks to months V. cholerae 0-10 SNPs Epidemic spread Days to weeks E. coli O157 0-15 SNPs Foodborne outbreak Days"},{"location":"modules/day4/#advanced-topics","title":"Advanced Topics","text":""},{"location":"modules/day4/#molecular-dating","title":"Molecular Dating","text":"<ul> <li>Tip dating: Using collection dates for molecular clock</li> <li>Bayesian methods: BEAST, MrBayes for time-resolved phylogenies</li> <li>Substitution rates: Pathogen-specific evolutionary rates</li> </ul>"},{"location":"modules/day4/#network-analysis","title":"Network Analysis","text":"<ul> <li>Minimum spanning trees: Alternative to bifurcating phylogenies</li> <li>Median-joining networks: Visualization of reticulate evolution</li> <li>Transmission networks: Direct transmission inference</li> </ul>"},{"location":"modules/day4/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day4/#individual-analysis","title":"Individual Analysis","text":"<ul> <li>Generate pangenome summary statistics</li> <li>Construct phylogenetic tree with bootstrap support</li> <li>Calculate SNP distances between outbreak isolates</li> <li>Identify transmission clusters based on genomic data</li> </ul>"},{"location":"modules/day4/#group-discussion","title":"Group Discussion","text":"<ul> <li>Compare assembly strategies and results</li> </ul>"},{"location":"modules/day4/#common-challenges","title":"Common Challenges","text":"<ul> <li>Interpret pangenome diversity in outbreak context</li> <li>Evaluate phylogenetic support for transmission hypotheses</li> <li>Discuss integration of genomic and epidemiological evidence</li> <li>Consider limitations of genomic outbreak investigation</li> </ul>"},{"location":"modules/day4/#common-challenges_1","title":"Common Challenges","text":""},{"location":"modules/day4/#low-phylogenetic-resolution","title":"Low Phylogenetic Resolution","text":"<pre><code># Try different substitution models\niqtree -s alignment.aln -m MFP -bb 1000  # Model selection\n\n# Use more informative sites only\niqtree -s alignment.aln -m GTR+G -bb 1000 --rate\n</code></pre>"},{"location":"modules/day4/#recombination-issues","title":"Recombination Issues","text":"<pre><code># Detect and remove recombinant regions\ngubbins alignment.aln\n\n# Use recombination-free alignment\niqtree -s alignment.filtered_polymorphic_sites.fasta\n</code></pre>"},{"location":"modules/day4/#missing-epidemiological-links","title":"Missing Epidemiological Links","text":"<pre><code># Lower SNP threshold for exploration\ncluster_analysis.py --snp-threshold 20 --epi-data metadata.csv\n\n# Consider longer transmission chains\ntransmission_chains.py --max-generations 3\n</code></pre>"},{"location":"modules/day4/#resources","title":"Resources","text":""},{"location":"modules/day4/#resources_1","title":"Resources","text":""},{"location":"modules/day4/#assembly-resources","title":"Assembly Resources","text":"<ul> <li>SPAdes Manual</li> <li>QUAST Documentation</li> <li>Assembly Best Practices</li> </ul>"},{"location":"modules/day4/#key-publications","title":"Key Publications","text":"<ul> <li>Page et al. (2015). Roary: rapid large-scale prokaryote pangenome analysis</li> <li>Tonkin-Hill et al. (2020). Producing polished prokaryotic pangenomes</li> <li>Croucher et al. (2015). Rapid phylogenetic analysis of bacterial genomes</li> </ul>"},{"location":"modules/day4/#software-documentation","title":"Software Documentation","text":"<ul> <li>IQ-TREE Manual</li> <li>Panaroo Documentation</li> <li>Snippy Manual</li> </ul>"},{"location":"modules/day4/#online-resources","title":"Online Resources","text":"<ul> <li>Microreact - Visualization platform</li> <li>iTOL - Interactive tree visualization</li> <li>FigTree - Tree viewing software</li> </ul>"},{"location":"modules/day4/#looking-ahead","title":"Looking Ahead","text":"<p>Day 5 Preview: Metagenomics analysis including: - Microbiome profiling from clinical samples - Pathogen detection in complex communities - Functional analysis of metagenomes - Association with host health outcomes</p>"},{"location":"modules/day4/#homework-optional","title":"Homework (Optional)","text":"<ol> <li>Analyze pangenome diversity in additional pathogen datasets</li> <li>Experiment with different phylogenetic methods and compare results</li> <li>Read case studies of genomic outbreak investigations</li> <li>Practice tree interpretation with published examples</li> </ol> <p>Key Takeaway: Genomic analysis provides unprecedented resolution for understanding pathogen evolution and transmission, but interpretation requires careful integration with epidemiological context and understanding of method limitations.</p>"},{"location":"modules/day5/","title":"Day 5: Tracking Threats: Genomic Detection of AMR, Virulence, and Plasmid Mobility","text":"<p>Date: September 5, 2025 Duration: 09:00-13:00 CAT Focus: Genome quality and functional gene annotation fundamentals, AMR and virulence factors and plasmid detection</p>"},{"location":"modules/day5/#overview","title":"Overview","text":"<p>Day 5 introduces Nextflow, a powerful workflow management system for creating reproducible and scalable bioinformatics pipelines. We'll explore the fundamentals of Nextflow, the nf-core community standards, and begin developing a pipeline for genomic analysis including QC, assembly, quality assessment, and annotation.</p>"},{"location":"modules/day5/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 5, you will be able to:</p> <ul> <li>Understand the principles of reproducible computational workflows</li> <li>Write basic Nextflow scripts with processes and channels</li> <li>Utilize nf-core tools and community pipelines</li> <li>Design workflow architecture for genomic analysis</li> <li>Implement data flow using Nextflow channels</li> <li>Begin developing a pipeline for QC, assembly, and annotation</li> </ul>"},{"location":"modules/day5/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Reproducible workflows with Nextflow and nf-core Mamana Mbiyavanga 10:30 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga 11:30 Break 12:00 Developing a Nextflow pipeline for QC, de novo assembly, quality assessment and annotation Mamana Mbiyavanga"},{"location":"modules/day5/#key-topics","title":"Key Topics","text":""},{"location":"modules/day5/#1-introduction-to-workflow-management","title":"1. Introduction to Workflow Management","text":"<ul> <li>Challenges in bioinformatics reproducibility</li> <li>Benefits of workflow management systems</li> <li>Nextflow vs other workflow systems (Snakemake, CWL, WDL)</li> <li>Container technologies (Docker, Singularity)</li> </ul>"},{"location":"modules/day5/#2-nextflow-fundamentals","title":"2. Nextflow Fundamentals","text":"<ul> <li>Nextflow architecture and concepts</li> <li>Processes, channels, and operators</li> <li>Configuration files and profiles</li> <li>Resource management and executors</li> <li>Error handling and resume capabilities</li> </ul>"},{"location":"modules/day5/#3-nf-core-community-and-standards","title":"3. nf-core Community and Standards","text":"<ul> <li>nf-core pipeline structure</li> <li>Community guidelines and best practices</li> <li>Using nf-core tools</li> <li>Available nf-core pipelines for genomics</li> <li>Contributing to nf-core</li> </ul>"},{"location":"modules/day5/#4-building-a-genomic-analysis-pipeline","title":"4. Building a Genomic Analysis Pipeline","text":"<ul> <li>Pipeline design and planning</li> <li>Implementing QC processes (FastQC, MultiQC)</li> <li>Assembly process integration (SPAdes)</li> <li>Quality assessment steps (QUAST)</li> <li>Annotation process (Prokka)</li> </ul>"},{"location":"modules/day5/#5-nextflow-scripting","title":"5. Nextflow Scripting","text":"<ul> <li>Writing process definitions</li> <li>Channel operations and data flow</li> <li>Parameter handling</li> <li>Conditional execution</li> <li>Module organization</li> </ul>"},{"location":"modules/day5/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day5/#workflow-management","title":"Workflow Management","text":"<ul> <li>Nextflow - Workflow orchestration system</li> <li>nf-core tools - Pipeline development framework</li> <li>Tower - Workflow monitoring platform</li> </ul>"},{"location":"modules/day5/#containerization","title":"Containerization","text":"<ul> <li>Docker - Container platform</li> <li>Singularity - HPC-friendly containers</li> <li>Conda - Package management</li> </ul>"},{"location":"modules/day5/#pipeline-components","title":"Pipeline Components","text":"<ul> <li>FastQC - Read quality control</li> <li>MultiQC - Aggregate reporting</li> <li>SPAdes - Genome assembly</li> <li>QUAST - Assembly assessment</li> <li>Prokka - Genome annotation</li> </ul>"},{"location":"modules/day5/#hands-on-exercises","title":"Hands-on Exercises","text":""},{"location":"modules/day5/#exercise-1-first-nextflow-script-30-minutes","title":"Exercise 1: First Nextflow Script (30 minutes)","text":"<p>Create and run a simple Nextflow pipeline.</p> <pre><code>#!/usr/bin/env nextflow\n\n// Define parameters\nparams.input = \"data/*.fastq\"\nparams.outdir = \"results\"\n\n// Create a channel from input files\nChannel\n    .fromPath(params.input)\n    .set { fastq_ch }\n\n// Define a process\nprocess countReads {\n    input:\n    path fastq from fastq_ch\n\n    output:\n    path \"*.count\" into counts_ch\n\n    script:\n    \"\"\"\n    echo \"Processing ${fastq}\"\n    wc -l ${fastq} &gt; ${fastq.baseName}.count\n    \"\"\"\n}\n\n// View the results\ncounts_ch.view()\n</code></pre>"},{"location":"modules/day5/#exercise-2-building-a-qc-pipeline-60-minutes","title":"Exercise 2: Building a QC Pipeline (60 minutes)","text":"<p>Implement quality control with FastQC and MultiQC.</p> <pre><code>process fastqc {\n    container 'biocontainers/fastqc:v0.11.9'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\" into fastqc_results\n\n    script:\n    \"\"\"\n    fastqc -t ${task.cpus} ${reads}\n    \"\"\"\n}\n\nprocess multiqc {\n    publishDir params.outdir, mode: 'copy'\n    container 'ewels/multiqc:latest'\n\n    input:\n    path '*' from fastqc_results.collect()\n\n    output:\n    path 'multiqc_report.html'\n\n    script:\n    \"\"\"\n    multiqc .\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day5/#exercise-3-integrating-assembly-90-minutes","title":"Exercise 3: Integrating Assembly (90 minutes)","text":"<p>Add genome assembly to the pipeline.</p> <pre><code>process spades_assembly {\n    container 'staphb/spades:latest'\n    cpus 4\n    memory '8 GB'\n\n    input:\n    tuple val(sample_id), path(reads1), path(reads2)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_contigs.fasta\")\n\n    script:\n    \"\"\"\n    spades.py \\\n        -1 ${reads1} \\\n        -2 ${reads2} \\\n        -o spades_output \\\n        -t ${task.cpus} \\\n        --careful\n\n    cp spades_output/contigs.fasta ${sample_id}_contigs.fasta\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day5/#key-concepts","title":"Key Concepts","text":""},{"location":"modules/day5/#workflow-principles","title":"Workflow Principles","text":"<ul> <li>Reproducibility: Same input \u2192 same output</li> <li>Portability: Run anywhere (laptop, HPC, cloud)</li> <li>Scalability: Handle any data volume</li> <li>Resumability: Restart from failure points</li> </ul>"},{"location":"modules/day5/#nextflow-components","title":"Nextflow Components","text":"Component Description Example Process Computational step <code>process fastqc { ... }</code> Channel Data flow connection <code>Channel.fromPath()</code> Operator Channel transformation <code>.map()</code>, <code>.filter()</code> Directive Process configuration <code>cpus 4</code>"},{"location":"modules/day5/#best-practices","title":"Best Practices","text":"<ol> <li>Use containers: Ensure environment reproducibility</li> <li>Parameterize everything: Make pipelines flexible</li> <li>Version control: Track pipeline changes</li> <li>Document thoroughly: Help users and future self</li> <li>Test incrementally: Build and test step by step</li> </ol>"},{"location":"modules/day5/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day5/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Create a basic Nextflow script with at least 2 processes</li> <li>Successfully run a pipeline with test data</li> <li>Modify pipeline parameters and observe changes</li> <li>Debug a pipeline with intentional errors</li> <li>Document pipeline usage</li> </ul>"},{"location":"modules/day5/#group-discussion","title":"Group Discussion","text":"<ul> <li>Compare Nextflow with traditional shell scripting</li> <li>Discuss reproducibility challenges and solutions</li> <li>Share pipeline design strategies</li> <li>Explore nf-core pipeline catalog</li> </ul>"},{"location":"modules/day5/#common-challenges","title":"Common Challenges","text":""},{"location":"modules/day5/#installation-issues","title":"Installation Issues","text":"<pre><code># Install Nextflow\ncurl -s https://get.nextflow.io | bash\n./nextflow run hello\n\n# Set up environment\nexport PATH=$PATH:$PWD\nexport NXF_VER=23.10.0\n</code></pre>"},{"location":"modules/day5/#channel-operations","title":"Channel Operations","text":"<pre><code>// Common channel patterns\nChannel\n    .fromFilePairs(params.reads)\n    .ifEmpty { error \"No read files found!\" }\n    .set { read_pairs_ch }\n\n// Combining channels\nfastqc_ch\n    .join(assembly_ch)\n    .map { sample, qc, assembly -&gt; \n        [sample, qc, assembly]\n    }\n</code></pre>"},{"location":"modules/day5/#resource-management","title":"Resource Management","text":"<pre><code>process memory_intensive {\n    memory { 2.GB * task.attempt }\n    maxRetries 3\n    errorStrategy 'retry'\n\n    script:\n    \"\"\"\n    # Your command here\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day5/#resources","title":"Resources","text":""},{"location":"modules/day5/#documentation","title":"Documentation","text":"<ul> <li>Nextflow Documentation</li> <li>nf-core Website</li> <li>Nextflow Training</li> </ul>"},{"location":"modules/day5/#tutorials","title":"Tutorials","text":"<ul> <li>Nextflow Tutorial</li> <li>nf-core Tutorials</li> <li>Seqera Labs Training</li> </ul>"},{"location":"modules/day5/#community","title":"Community","text":"<ul> <li>Nextflow Slack</li> <li>nf-core Slack</li> <li>GitHub Discussions</li> </ul>"},{"location":"modules/day5/#looking-ahead","title":"Looking Ahead","text":"<p>Day 6 Preview: Nextflow Pipeline Development - Continue building the genomic analysis pipeline - Advanced Nextflow features and optimization - Pipeline testing and validation - Deployment strategies</p> <p>Key Learning Outcome: Understanding workflow management principles and gaining hands-on experience with Nextflow enables creation of reproducible, scalable bioinformatics pipelines essential for modern genomic analysis.</p>"},{"location":"modules/day6/","title":"Day 6: Nextflow Foundations &amp; Core Concepts","text":"<p>Date: September 8, 2025 Duration: 09:00-13:00 CAT Focus: Workflow reproducibility, Nextflow basics, pipeline development</p>"},{"location":"modules/day6/#learning-philosophy-see-it-understand-it-try-it-build-it-master-it","title":"Learning Philosophy: See it \u2192 Understand it \u2192 Try it \u2192 Build it \u2192 Master it","text":"<p>This module follows a proven learning approach designed specifically for beginners:</p> <ul> <li>See it: Visual diagrams and examples show you what workflows look like</li> <li>Understand it: Clear explanations of why workflow management matters</li> <li>Try it: Simple exercises to practice basic concepts</li> <li>Build it: Create your own working pipeline step by step</li> <li>Master it: Apply skills to real genomics problems with confidence</li> </ul> <p>Every section builds on the previous one, ensuring you develop solid foundations before moving to more complex topics.</p>"},{"location":"modules/day6/#table-of-contents","title":"Table of Contents","text":""},{"location":"modules/day6/#learning-objectives-overview","title":"\ud83c\udfaf Learning Objectives &amp; Overview","text":"<ul> <li>Learning Objectives</li> <li>What You'll Build Today</li> <li>Prerequisites</li> </ul>"},{"location":"modules/day6/#setup-environment","title":"\ud83d\udd27 Setup &amp; Environment","text":"<ul> <li>Environment Setup</li> <li>Understanding Nextflow Output Organization</li> <li>Work Directory Configuration</li> </ul>"},{"location":"modules/day6/#nextflow-fundamentals","title":"\ud83d\udcda Nextflow Fundamentals","text":"<ul> <li>What is Nextflow?</li> <li>Key Concepts</li> <li>DSL2 Syntax</li> <li>Channels</li> <li>Processes</li> </ul>"},{"location":"modules/day6/#hands-on-exercises","title":"\ud83e\uddea Hands-on Exercises","text":"<ul> <li>Exercise 1: Hello World</li> <li>Exercise 2: Read Counting</li> <li>Exercise 3: Quality Control Pipeline</li> <li>Step 1: Basic FastQC</li> <li>Step 2: Extended Pipeline</li> <li>Step 3: Cluster Execution</li> </ul>"},{"location":"modules/day6/#advanced-topics","title":"\u26a1 Advanced Topics","text":"<ul> <li>Channel Operations</li> <li>Process Configuration</li> <li>Error Handling &amp; Debugging</li> <li>Performance Optimization</li> <li>Cluster Execution</li> </ul>"},{"location":"modules/day6/#monitoring-troubleshooting","title":"\ud83d\udd0d Monitoring &amp; Troubleshooting","text":"<ul> <li>Pipeline Monitoring</li> <li>Common Issues</li> <li>Debugging Strategies</li> </ul>"},{"location":"modules/day6/#assessment-next-steps","title":"\ud83c\udf93 Assessment &amp; Next Steps","text":"<ul> <li>Knowledge Check</li> <li>Additional Resources</li> <li>Day 7 Preview</li> </ul>"},{"location":"modules/day6/#overview","title":"Overview","text":"<p>Day 6 introduces participants to workflow management systems and Nextflow fundamentals. This comprehensive session covers the theoretical foundations of reproducible workflows, core Nextflow concepts, and hands-on development of basic pipelines. Participants will understand why workflow management is crucial for bioinformatics and gain practical experience with Nextflow's core components.</p>"},{"location":"modules/day6/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 6, you will be able to:</p> <ul> <li>Understand the challenges in bioinformatics reproducibility and benefits of workflow management systems</li> <li>Explain Nextflow's core features and architecture</li> <li>Identify the main components of a Nextflow script (processes, channels, workflows)</li> <li>Write and execute basic Nextflow processes and workflows</li> <li>Use channels to manage data flow between processes</li> <li>Configure Nextflow for different execution environments</li> <li>Debug common Nextflow issues and understand error messages</li> <li>Apply best practices for pipeline development</li> </ul>"},{"location":"modules/day6/#schedule","title":"Schedule","text":"Time (CAT) Topic Duration Trainer 09:00 Part 1: The Challenge of Complex Genomics Analyses 45 min Mamana Mbiyavanga 09:45 Workflow Management Systems Comparison &amp; Nextflow Introduction 45 min Mamana Mbiyavanga 10:30 Break 15 min 10:45 Part 2: Nextflow Architecture and Core Concepts 45 min Mamana Mbiyavanga 11:30 Part 3: Hands-on Exercises (Installation, First Scripts, Channels) 90 min Mamana Mbiyavanga 13:00 End"},{"location":"modules/day6/#key-topics","title":"Key Topics","text":""},{"location":"modules/day6/#1-foundation-review-30-minutes","title":"1. Foundation Review (30 minutes)","text":"<ul> <li>Command line proficiency check</li> <li>Basic software installation and environment setup</li> <li>Development workspace organization</li> </ul>"},{"location":"modules/day6/#2-introduction-to-workflow-management-45-minutes","title":"2. Introduction to Workflow Management (45 minutes)","text":"<ul> <li>The challenge of complex genomics analyses</li> <li>Problems with traditional scripting approaches</li> <li>Benefits of workflow management systems</li> <li>Nextflow vs other systems (Snakemake, CWL, WDL)</li> <li>Reproducibility, portability, and scalability</li> </ul>"},{"location":"modules/day6/#3-nextflow-core-concepts-75-minutes","title":"3. Nextflow Core Concepts (75 minutes)","text":"<ul> <li>Nextflow architecture and execution model</li> <li>Processes: encapsulated tasks with inputs, outputs, and scripts</li> <li>Channels: asynchronous data streams connecting processes</li> <li>Workflows: orchestrating process execution and data flow</li> <li>The work directory structure and caching mechanism</li> <li>Executors and execution platforms</li> </ul>"},{"location":"modules/day6/#4-hands-on-pipeline-development-75-minutes","title":"4. Hands-on Pipeline Development (75 minutes)","text":"<ul> <li>Writing your first Nextflow process</li> <li>Creating channels and managing data flow</li> <li>Building a simple QC workflow</li> <li>Testing and debugging pipelines</li> <li>Understanding the work directory</li> </ul>"},{"location":"modules/day6/#tools-and-software","title":"Tools and Software","text":""},{"location":"modules/day6/#core-requirements","title":"Core Requirements","text":"<ul> <li>Nextflow (version 20.10.0 or later) - Workflow orchestration system</li> <li>Java (version 11 or later) - Required for Nextflow execution</li> <li>Text editor - VS Code with Nextflow extension recommended</li> <li>Command line access - Terminal or command prompt for running Nextflow commands</li> </ul>"},{"location":"modules/day6/#bioinformatics-tools","title":"Bioinformatics Tools","text":"<ul> <li>FastQC - Read quality control assessment</li> <li>MultiQC - Aggregate quality control reports</li> <li>Trimmomatic - Read trimming and filtering</li> <li>SPAdes - Genome assembly (for later exercises)</li> <li>Prokka - Rapid prokaryotic genome annotation</li> </ul>"},{"location":"modules/day6/#development-environment","title":"Development Environment","text":"<ul> <li>Terminal/Command line - For running Nextflow commands</li> <li>Text editor - For writing pipeline scripts</li> </ul>"},{"location":"modules/day6/#foundation-review-30-minutes","title":"Foundation Review (30 minutes)","text":"<p>Before diving into workflow management, let's ensure everyone has the essential foundation skills needed for this module.</p>"},{"location":"modules/day6/#command-line-proficiency-check","title":"Command Line Proficiency Check","text":"<p>Let's quickly verify your command line skills with some essential operations:</p> \ud83d\udd27 Quick Command Line Assessment  **Test your skills with these commands:**  <pre><code># Navigation and file operations\npwd                          # Where am I?\nls -la                      # List files with details\ncd /path/to/data           # Change directory\nmkdir analysis_results     # Create directory\ncp file1.txt backup/       # Copy files\nmv old_name.txt new_name.txt  # Rename/move files\n\n# File content examination\nzcat data.fastq.gz | head -n 10  # First 10 lines of compressed FASTQ\ntail -n 5 logfile.txt      # Last 5 lines\nzcat sequences.fastq.gz | wc -l  # Count lines in compressed file\ngrep \"&gt;\" sequences.fasta   # Find FASTA headers\n\n# Process management\nps aux                     # List running processes\ntop                        # Monitor system resources\nkill -9 [PID]             # Terminate process\nnohup command &amp;            # Run in background\n</code></pre> Expected competency: You should be comfortable with basic file operations, text processing, and process management."},{"location":"modules/day6/#software-installation-overview","title":"Software Installation Overview","text":"<p>For Day 6, we'll focus on basic software installation and environment setup. Container technologies will be covered in Day 7 as part of advanced deployment strategies.</p>"},{"location":"modules/day6/#using-the-module-system","title":"Using the Module System","text":"\ud83d\udce6 Loading Required Software All tools are pre-installed and available through the module system. No installation required! Step 1: Check if module system is available <pre><code># Test if module command works\nmodule --version\n\n# If you get \"command not found\", see troubleshooting below\n</code></pre> Step 2: Check available modules <pre><code># List all available modules\nmodule avail\n\n# Search for specific tools\nmodule avail nextflow\nmodule avail java\nmodule avail fastqc\n</code></pre> Step 3: Load required modules <pre><code># Load Java 17 (required for Nextflow)\nmodule load java/openjdk-17.0.2\n\n# Load Nextflow (initialize module system first)\nsource /opt/lmod/8.7/lmod/lmod/init/bash\nmodule load nextflow/25.04.6\n\n# Load bioinformatics tools for exercises\nmodule load fastqc/0.12.1\nmodule load trimmomatic/0.39\nmodule load multiqc/1.22.3\n</code></pre> Step 4: Verify loaded modules <pre><code># Check what modules are currently loaded\nmodule list\n\n# Test that tools are working\nnextflow -version\njava -version\nfastqc --version\n</code></pre> Step 5: Module management <pre><code># Unload a specific module\nmodule unload fastqc/0.12.1\n\n# Unload all modules\nmodule purge\n\n# Create a convenient setup script\ncat &gt; setup_modules.sh &lt;&lt; 'EOF'\n#!/bin/bash\nsource /opt/lmod/8.7/lmod/lmod/init/bash\nmodule load nextflow/25.04.6 fastqc/0.12.1 trimmomatic/0.39 multiqc/1.22.3\necho \"Modules loaded successfully!\"\nmodule list\nEOF\n\nchmod +x setup_modules.sh\n</code></pre>  **Troubleshooting: If module command is not found**  <pre><code># Only if you get \"module: command not found\", try:\nsource /opt/lmod/8.7/lmod/lmod/init/bash\n\n# Then retry the module commands above\nmodule --version\n</code></pre>"},{"location":"modules/day6/#development-environment-setup","title":"Development Environment Setup","text":"<p>Let's ensure your environment is ready for Nextflow development:</p>"},{"location":"modules/day6/#module-environment-verification","title":"Module Environment Verification","text":"\u2705 Environment Verification Complete verification workflow:  <pre><code># Step 1: Test module system\nmodule --version\n# Should show: Modules based on Lua: Version 8.7\n\n# Step 2: Load all required modules with specific versions\nsource /opt/lmod/8.7/lmod/lmod/init/bash\nmodule load nextflow/25.04.6 fastqc/0.12.1 trimmomatic/0.39 multiqc/1.22.3\n\n# Step 3: Verify Java (required for Nextflow)\njava -version\n# Should show: openjdk version \"17.0.2\"\n\n# Step 4: Verify Nextflow\nnextflow -version\n# Should show: nextflow version 25.04.6\n\n# Step 5: Verify bioinformatics tools\nfastqc --version\n# Should show: FastQC v0.12.1\n\ntrimmomatic -version\n# Should show: 0.39\n\nmultiqc --version\n# Should show: multiqc, version 1.22.3\n\n# Step 6: Check all loaded modules\nmodule list\n# Should show all 5 loaded modules\n</code></pre> If module command is not found: <pre><code># Initialize module system (only if needed)\nsource /opt/lmod/8.7/lmod/lmod/init/bash\n\n# Then retry the verification steps above\nmodule --version\n</code></pre> If modules are not available: <pre><code># Search for modules with different names\nmodule avail 2&gt;&amp;1 | grep -i nextflow\nmodule avail 2&gt;&amp;1 | grep -i java\n\n# Contact system administrator if modules are missing\n</code></pre> Quick Setup Script: <pre><code># Create a one-command setup (handles module initialization if needed)\ncat &gt; ~/setup_day6.sh &lt;&lt; 'EOF'\n#!/bin/bash\n\n# Test if module command works\nif ! command -v module &gt;/dev/null 2&gt;&amp;1; then\n    echo \"Initializing module system...\"\n    source /opt/lmod/8.7/lmod/lmod/init/bash\nfi\n\n# Load required modules\nmodule load java/openjdk-17.0.2 nextflow/25.04.6 fastqc/0.12.1 trimmomatic/0.39 multiqc/1.22.3\necho \"All modules loaded successfully!\"\nmodule list\nEOF\n\nchmod +x ~/setup_day6.sh\n\n# Use it anytime with:\nsource ~/setup_day6.sh\n</code></pre>"},{"location":"modules/day6/#workspace-organization","title":"Workspace Organization","text":"<p>Create a well-organized workspace for today's exercises:</p> <pre><code># Create main working directory in user data space\nmkdir -p /data/users/$USER/nextflow-training\ncd /data/users/$USER/nextflow-training\n\n# Create subdirectories\nmkdir -p {workflows,scripts,configs}\n\n# Create work directory for Nextflow task files\nmkdir -p /data/users/$USER/nextflow-training/work\necho \"Nextflow work directory: /data/users/$USER/nextflow-training/work\"\n\n# Create results directory for pipeline outputs\nmkdir -p /data/users/$USER/nextflow-training/results\necho \"Results directory: /data/users/$USER/nextflow-training/results\"\n\n# Copy workflows from the training repository\ncp -r /users/$USER/microbial-genomics-training/workflows/* workflows/\necho \"Workflows copied to: /data/users/$USER/nextflow-training/workflows/\"\n\n# Check available real data\nls -la /data/Dataset_Mt_Vc/\necho \"Real genomic data available in /data/Dataset_Mt_Vc/\"\n</code></pre> \ud83d\udca1 Pro Tip: Development Best Practices Recommended setup: <ul> <li> Use a dedicated directory for each project - Keep data, scripts, and results separate - Use meaningful file names and directory structure - Document your workflow with README files - Use version control (we'll cover this in Day 7!)"},{"location":"modules/day6/#part-1-the-challenge-of-complex-genomics-analyses","title":"Part 1: The Challenge of Complex Genomics Analyses","text":""},{"location":"modules/day6/#why-workflow-management-matters","title":"Why Workflow Management Matters","text":"<p>Consider analyzing 100 bacterial genomes without workflow management:</p> <pre><code># Manual approach - tedious and error-prone\nfor sample in sample1 sample2 sample3 ... sample100; do\n    fastqc ${sample}_R1.fastq.gz ${sample}_R2.fastq.gz\n    if [ $? -ne 0 ]; then echo \"FastQC failed\"; exit 1; fi\n\n    trimmomatic PE ${sample}_R1.fastq.gz ${sample}_R2.fastq.gz \\\n        ${sample}_R1_trimmed.fastq.gz ${sample}_R1_unpaired.fastq.gz \\\n        ${sample}_R2_trimmed.fastq.gz ${sample}_R2_unpaired.fastq.gz \\\n        SLIDINGWINDOW:4:20\n    if [ $? -ne 0 ]; then echo \"Trimming failed\"; exit 1; fi\n\n    spades.py -1 ${sample}_R1_trimmed.fastq.gz -2 ${sample}_R2_trimmed.fastq.gz \\\n        -o ${sample}_assembly\n    if [ $? -ne 0 ]; then echo \"Assembly failed\"; exit 1; fi\n\n    # What if step 3 fails for sample 67?\n    # How do you restart from where it failed?\n    # How do you run samples in parallel efficiently?\n    # How do you ensure reproducibility across different systems?\ndone\n</code></pre>"},{"location":"modules/day6/#why-this-approach-is-tedious-and-error-prone","title":"Why This Approach is \"Tedious and Error-Prone\"","text":"<p>Major Problems with Traditional Shell Scripting:</p> <ol> <li> <p>No Parallelization</p> <ul> <li>Processes samples sequentially (one after another)</li> <li>Wastes computational resources on multi-core systems</li> <li>Takes unnecessarily long time</li> </ul> </li> <li> <p>Poor Error Recovery &amp; Resumability</p> <ul> <li>If one sample fails, entire pipeline stops</li> <li>No way to resume from failure point</li> <li>Must restart from beginning</li> <li>Manual error checking is verbose and error-prone</li> </ul> </li> <li> <p>Resource Management Issues</p> <ul> <li>No control over CPU/memory usage</li> <li>Can overwhelm system or underutilize resources</li> <li>No queue management for HPC systems</li> <li>No automatic optimization of resource allocation</li> </ul> </li> <li> <p>Lack of Reproducibility</p> <ul> <li>Hard to track software versions</li> <li>Environment dependencies not managed</li> <li>Difficult to share and reproduce results across different systems</li> <li>Software installation and version conflicts</li> </ul> </li> <li> <p>Poor Scalability</p> <ul> <li>Doesn't scale well from laptop to HPC to cloud</li> <li>No automatic adaptation to different computing environments</li> <li>Limited ability to handle varying data volumes</li> </ul> </li> <li> <p>Maintenance Nightmare</p> <ul> <li>Adding new steps requires modifying the entire script</li> <li>Parameter changes need manual editing throughout</li> <li>No modular design for reusable components</li> <li>Difficult to test individual components</li> </ul> </li> <li> <p>No Progress Tracking</p> <ul> <li>Can't easily see which samples completed</li> <li>No reporting or logging mechanisms</li> <li>Difficult to debug failures</li> <li>No visibility into pipeline performance</li> </ul> </li> </ol>"},{"location":"modules/day6/#the-workflow-management-solution","title":"The Workflow Management Solution","text":""},{"location":"modules/day6/#overview-of-workflow-management-systems","title":"Overview of Workflow Management Systems","text":"<p>Workflow management systems (WMS) are specialized programming languages and frameworks designed specifically to address the challenges of complex, multi-step computational pipelines. They provide a higher-level abstraction that automatically handles the tedious and error-prone aspects of traditional shell scripting.</p>"},{"location":"modules/day6/#how-workflow-management-systems-solve-traditional-problems","title":"How Workflow Management Systems Solve Traditional Problems","text":"<ul> <li>Automatic Parallelization</li> <li>Analyze task dependencies and run independent steps simultaneously</li> <li>Efficiently utilize all available CPU cores and computing nodes</li> <li> <p>Scale from single machines to massive HPC clusters and cloud environments</p> </li> <li> <p>Built-in Error Recovery</p> </li> <li>Automatic retry mechanisms for failed tasks</li> <li>Resume functionality to restart from failure points</li> <li> <p>Intelligent caching to avoid re-running successful steps</p> </li> <li> <p>Resource Management</p> </li> <li>Automatic CPU and memory allocation based on task requirements</li> <li>Integration with job schedulers (SLURM, SGE)</li> <li> <p>Dynamic scaling in cloud environments</p> </li> <li> <p>Reproducibility by Design</p> </li> <li>Container integration (Docker, Singularity) for consistent environments</li> <li>Version tracking for all software dependencies</li> <li> <p>Portable execution across different computing platforms</p> </li> <li> <p>Progress Monitoring</p> </li> <li>Real-time pipeline execution tracking</li> <li>Detailed logging and reporting</li> <li> <p>Performance metrics and resource usage statistics</p> </li> <li> <p>Modular Architecture</p> </li> <li>Reusable workflow components</li> <li>Easy parameter configuration</li> <li>Clean separation of logic and execution</li> </ul>"},{"location":"modules/day6/#comparison-of-popular-workflow-languages","title":"Comparison of Popular Workflow Languages","text":"<p>The bioinformatics community has developed several powerful workflow management systems, each with unique strengths and design philosophies:</p>"},{"location":"modules/day6/#1-nextflow","title":"1. Nextflow","text":"<ul> <li>Language Base: Groovy (JVM-based)</li> <li>Philosophy: Dataflow programming with reactive streams</li> <li>Strengths: Excellent parallelization, cloud-native, strong container support</li> <li>Community: Large bioinformatics community, nf-core ecosystem</li> </ul>"},{"location":"modules/day6/#2-snakemake","title":"2. Snakemake","text":"<ul> <li>Language Base: Python</li> <li>Philosophy: Rule-based workflow definition inspired by GNU Make</li> <li>Strengths: Pythonic syntax, excellent for Python developers, strong academic adoption</li> <li>Community: Very active in computational biology and data science</li> </ul>"},{"location":"modules/day6/#3-common-workflow-language-cwl","title":"3. Common Workflow Language (CWL)","text":"<ul> <li>Language Base: YAML/JSON</li> <li>Philosophy: Vendor-neutral, standards-based approach</li> <li>Strengths: Platform independence, strong metadata support, scientific reproducibility focus</li> <li>Community: Broad industry and academic support across multiple domains</li> </ul>"},{"location":"modules/day6/#4-workflow-description-language-wdl","title":"4. Workflow Description Language (WDL)","text":"<ul> <li>Language Base: Custom domain-specific language</li> <li>Philosophy: Human-readable workflow descriptions with strong typing</li> <li>Strengths: Excellent cloud integration, strong at Broad Institute and genomics centers</li> <li>Community: Strong in genomics, particularly for large-scale sequencing projects</li> </ul>"},{"location":"modules/day6/#feature-comparison-table","title":"Feature Comparison Table","text":"Feature Nextflow Snakemake CWL WDL Syntax Base Groovy Python YAML/JSON Custom DSL Learning Curve Moderate Easy (for Python users) Steep Moderate Parallelization Excellent (automatic) Excellent Good Excellent Container Support Native (Docker/Singularity) Native Native Native Cloud Integration Excellent (AWS, GCP, Azure) Good Good Excellent HPC Support Excellent (SLURM, etc.) Excellent Good Good Resume Capability Excellent Excellent Limited Good Community Size Large (bioinformatics) Large (data science) Medium Medium Package Ecosystem nf-core (500+ pipelines) Snakemake Wrappers Limited Limited Debugging Tools Good (Tower, reports) Excellent Limited Good Best Use Cases Multi-omics, clinical pipelines Data analysis, research Standards compliance Large-scale genomics Industry Adoption High (pharma, biotech) High (academia) Growing High (genomics centers)"},{"location":"modules/day6/#simple-code-examples","title":"Simple Code Examples","text":"<p>Let's see how the same basic task - running FastQC on multiple samples - would be implemented in different workflow languages:</p>"},{"location":"modules/day6/#traditional-shell-script-for-comparison","title":"Traditional Shell Script (for comparison)","text":"<pre><code># Manual approach - sequential processing\nfor sample in sample1 sample2 sample3; do\n    fastqc ${sample}_R1.fastq.gz ${sample}_R2.fastq.gz -o /data/users/$USER/nextflow-training/results/\n    if [ $? -ne 0 ]; then echo \"FastQC failed for $sample\"; exit 1; fi\ndone\n</code></pre>"},{"location":"modules/day6/#nextflow-implementation","title":"Nextflow Implementation","text":"<pre><code>#!/usr/bin/env nextflow\n\nnextflow.enable.dsl = 2\n\n// FastQC process\nprocess fastqc {\n    container 'biocontainers/fastqc:v0.11.9'\n    publishDir '/data/users/$USER/nextflow-training/results/', mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\"\n\n    script:\n    \"\"\"\n    fastqc ${reads} -t ${task.cpus}\n    \"\"\"\n}\n\n// Run the workflow\nworkflow {\n    // Define input channel\n    read_pairs_ch = Channel.fromFilePairs(\"data/*_{R1,R2}.fastq\")\n\n    // Run FastQC\n    fastqc(read_pairs_ch)\n}\n</code></pre>"},{"location":"modules/day6/#snakemake-implementation","title":"Snakemake Implementation","text":"<pre><code># Snakefile\nSAMPLES = [\"sample1\", \"sample2\", \"sample3\"]\n\nrule all:\n    input:\n        expand(\"/data/users/$USER/nextflow-training/results/{sample}_{read}_fastqc.html\",\n               sample=SAMPLES, read=[\"R1\", \"R2\"])\n\nrule fastqc:\n    input:\n        \"data/{sample}_{read}.fastq\"\n    output:\n        html=\"/data/users/$USER/nextflow-training/results/{sample}_{read}_fastqc.html\",\n        zip=\"/data/users/$USER/nextflow-training/results/{sample}_{read}_fastqc.zip\"\n    container:\n        \"docker://biocontainers/fastqc:v0.11.9\"\n    shell:\n        \"fastqc {input} -o /data/users/$USER/nextflow-training/results/\"\n</code></pre>"},{"location":"modules/day6/#cwl-implementation","title":"CWL Implementation","text":"<pre><code># fastqc-workflow.cwl\ncwlVersion: v1.2\nclass: Workflow\n\ninputs:\n  fastq_files:\n    type: File[]\n\noutputs:\n  fastqc_reports:\n    type: File[]\n    outputSource: fastqc/html_report\n\nsteps:\n  fastqc:\n    run: fastqc-tool.cwl\n    scatter: fastq_file\n    in:\n      fastq_file: fastq_files\n    out: [html_report, zip_report]\n\n# fastqc-tool.cwl\ncwlVersion: v1.2\nclass: CommandLineTool\n\nbaseCommand: fastqc\n\ninputs:\n  fastq_file:\n    type: File\n    inputBinding:\n      position: 1\n\noutputs:\n  html_report:\n    type: File\n    outputBinding:\n      glob: \"*_fastqc.html\"\n  zip_report:\n    type: File\n    outputBinding:\n      glob: \"*_fastqc.zip\"\n\nrequirements:\n  DockerRequirement:\n    dockerPull: biocontainers/fastqc:v0.11.9\n</code></pre>"},{"location":"modules/day6/#key-differences-in-syntax","title":"Key Differences in Syntax:","text":"<ul> <li>Nextflow: Uses Groovy syntax with channels for data flow, processes define computational steps</li> <li>Snakemake: Python-based with rules that define input/output relationships, uses wildcards for pattern matching</li> <li>CWL: YAML-based with explicit input/output definitions, requires separate tool and workflow files</li> <li>WDL: Custom syntax with strong typing, task-based approach with explicit variable declarations</li> </ul>"},{"location":"modules/day6/#why-nextflow-for-this-course","title":"Why Nextflow for This Course","text":"<p>This course focuses on Nextflow for several compelling reasons that make it particularly well-suited for microbial genomics workflows:</p>"},{"location":"modules/day6/#1-bioinformatics-community-adoption","title":"1. Bioinformatics Community Adoption","text":"<ul> <li>nf-core ecosystem: Over 500 community-curated pipelines specifically for bioinformatics</li> <li>Industry standard: Widely adopted by pharmaceutical companies, biotech firms, and genomics centers</li> <li>Active development: Strong community support with regular updates and improvements</li> </ul>"},{"location":"modules/day6/#2-excellent-parallelization-for-genomics","title":"2. Excellent Parallelization for Genomics","text":"<ul> <li>Automatic scaling: Seamlessly scales from single samples to thousands of genomes</li> <li>Dataflow programming: Natural fit for genomics pipelines with complex dependencies</li> <li>Resource optimization: Intelligent task scheduling maximizes computational efficiency</li> </ul>"},{"location":"modules/day6/#3-clinical-and-production-ready","title":"3. Clinical and Production Ready","text":"<ul> <li>Robust error handling: Critical for clinical pipelines where reliability is essential</li> <li>Comprehensive logging: Detailed audit trails required for regulatory compliance</li> <li>Resume capability: Minimizes computational waste in long-running genomic analyses</li> </ul>"},{"location":"modules/day6/#4-multi-platform-flexibility","title":"4. Multi-Platform Flexibility","text":"<ul> <li>HPC integration: Native support for SLURM and other job schedulers common in genomics</li> <li>Cloud-native: Excellent support for AWS, Google Cloud, and Azure for scalable genomics</li> <li>Container support: Seamless Docker and Singularity integration for reproducible environments</li> </ul>"},{"location":"modules/day6/#5-microbial-genomics-specific-advantages","title":"5. Microbial Genomics Specific Advantages","text":"<ul> <li>Pathogen surveillance pipelines: Many nf-core pipelines designed for bacterial genomics</li> <li>AMR analysis workflows: Established patterns for antimicrobial resistance detection</li> <li>Outbreak investigation: Scalable phylogenetic analysis capabilities</li> <li>Metagenomics support: Robust handling of complex metagenomic datasets</li> </ul>"},{"location":"modules/day6/#6-learning-and-career-benefits","title":"6. Learning and Career Benefits","text":"<ul> <li>Industry relevance: Skills directly transferable to genomics industry positions</li> <li>Growing demand: Increasing adoption means more job opportunities</li> <li>Comprehensive ecosystem: Learning Nextflow provides access to hundreds of ready-to-use pipelines</li> </ul> <p>The combination of these factors makes Nextflow an ideal choice for training the next generation of microbial genomics researchers and practitioners. Its balance of power, usability, and industry adoption ensures that skills learned in this course will be immediately applicable in real-world genomics applications.</p>"},{"location":"modules/day6/#visual-guide-understanding-workflow-management","title":"Visual Guide: Understanding Workflow Management","text":""},{"location":"modules/day6/#the-big-picture-traditional-vs-modern-approaches","title":"The Big Picture: Traditional vs Modern Approaches","text":"<p>To understand why workflow management systems like Nextflow are revolutionary, let's visualize the time difference:</p>"},{"location":"modules/day6/#traditional-shell-scripting-the-slow-way","title":"Traditional Shell Scripting - The Slow Way","text":"<pre><code>flowchart TD\n    A1[Sample 1] --&gt; B1[FastQC - 5 min]\n    B1 --&gt; C1[Trimming - 10 min]\n    C1 --&gt; D1[Assembly - 30 min]\n    D1 --&gt; E1[Annotation - 15 min]\n    E1 --&gt; F1[\u2713 Done - 60 min total]\n\n    F1 --&gt; A2[Sample 2]\n    A2 --&gt; B2[FastQC - 5 min]\n    B2 --&gt; C2[Trimming - 10 min]\n    C2 --&gt; D2[Assembly - 30 min]\n    D2 --&gt; E2[Annotation - 15 min]\n    E2 --&gt; F2[\u2713 Done - 120 min total]\n\n    F2 --&gt; A3[Sample 3]\n    A3 --&gt; B3[FastQC - 5 min]\n    B3 --&gt; C3[Trimming - 10 min]\n    C3 --&gt; D3[Assembly - 30 min]\n    D3 --&gt; E3[Annotation - 15 min]\n    E3 --&gt; F3[\u2713 All Done - 180 min total]\n\n    style A1 fill:#ffcccc\n    style A2 fill:#ffcccc\n    style A3 fill:#ffcccc\n    style F3 fill:#ff9999</code></pre> <p>Problems with traditional approach:</p> <ul> <li>Sequential processing: Must wait for each sample to finish completely</li> <li>Wasted resources: Only uses one CPU core at a time</li> <li>Total time: 180 minutes (3 hours) for 3 samples</li> <li>Scaling nightmare: 100 samples = 100 hours!</li> </ul>"},{"location":"modules/day6/#nextflow-the-fast-way","title":"Nextflow - The Fast Way","text":"<pre><code>flowchart TD\n    A4[Sample 1] --&gt; B4[FastQC - 5 min]\n    A5[Sample 2] --&gt; B5[FastQC - 5 min]\n    A6[Sample 3] --&gt; B6[FastQC - 5 min]\n\n    B4 --&gt; C4[Trimming - 10 min]\n    B5 --&gt; C5[Trimming - 10 min]\n    B6 --&gt; C6[Trimming - 10 min]\n\n    C4 --&gt; D4[Assembly - 30 min]\n    C5 --&gt; D5[Assembly - 30 min]\n    C6 --&gt; D6[Assembly - 30 min]\n\n    D4 --&gt; E4[Annotation - 15 min]\n    D5 --&gt; E5[Annotation - 15 min]\n    D6 --&gt; E6[Annotation - 15 min]\n\n    E4 --&gt; F4[\u2713 All Done - 60 min total]\n    E5 --&gt; F5[3x FASTER!]\n    E6 --&gt; F6[Same time as 1 sample]\n\n    style A4 fill:#ccffcc\n    style A5 fill:#ccffcc\n    style A6 fill:#ccffcc\n    style F4 fill:#99ff99\n    style F5 fill:#99ff99\n    style F6 fill:#99ff99</code></pre> <p>Benefits of Nextflow approach:</p> <ul> <li>Parallel processing: All samples start simultaneously</li> <li>Efficient resource use: Uses all available CPU cores</li> <li>Total time: 60 minutes (1 hour) for 3 samples</li> <li>Amazing scaling: 100 samples still = ~1 hour!</li> </ul>"},{"location":"modules/day6/#the-dramatic-difference","title":"The Dramatic Difference\ud83e\uddee Interactive Time Calculator","text":"Approach 3 Samples 10 Samples 100 Samples Traditional 3 hours 10 hours 100 hours Nextflow 1 hour 1 hour 1 hour Speed Gain 3x faster 10x faster 100x faster <p>Real-world impact: The more samples you have, the more dramatic the time savings become!</p> <p>See how much time Nextflow can save you with your own data:</p> Number of samples: 10 Time per sample (minutes): 60 \ud83d\udc0c Traditional Approach <p>Total time: 10 hours</p> <p>Sequential processing</p> \u26a1 Nextflow Approach <p>Total time: 1 hour</p> <p>Parallel processing</p>          Time saved: 9 hours (10x faster)"},{"location":"modules/day6/#nextflow-fundamentals_1","title":"Nextflow Fundamentals","text":"<p>Before diving into practical exercises, let's understand the core concepts that make Nextflow powerful.</p>"},{"location":"modules/day6/#what-is-nextflow","title":"What is Nextflow?","text":"<p>Nextflow is a workflow management system that comprises both a runtime environment and a domain-specific language (DSL). It's designed specifically to manage computational data-analysis workflows in bioinformatics and other scientific fields.</p>"},{"location":"modules/day6/#core-nextflow-features","title":"Core Nextflow Features","text":"<pre><code>flowchart LR\n    A[Fast Prototyping] --&gt; B[Simple Syntax]\n    C[Reproducibility] --&gt; D[Containers &amp; Conda]\n    E[Portability] --&gt; F[Run Anywhere]\n    G[Parallelism] --&gt; H[Automatic Scaling]\n    I[Checkpoints] --&gt; J[Resume from Failures]\n\n    style A fill:#e1f5fe\n    style C fill:#e8f5e8\n    style E fill:#fff3e0\n    style G fill:#f3e5f5\n    style I fill:#fce4ec</code></pre> <p>1. Fast Prototyping</p> <ul> <li>Simple syntax that lets you reuse existing scripts and tools</li> <li>Quick to write and test new workflows</li> </ul> <p>2. Reproducibility</p> <ul> <li>Built-in support for Docker, Singularity, and Conda</li> <li>Consistent execution environments across platforms</li> <li>Same results every time, on any platform</li> </ul> <p>3. Portability &amp; Interoperability</p> <ul> <li>Write once, run anywhere (laptop, HPC cluster, cloud)</li> <li>Separates workflow logic from execution environment</li> </ul> <p>4. Simple Parallelism</p> <ul> <li>Based on dataflow programming model</li> <li>Automatically runs independent tasks in parallel</li> </ul> <p>5. Continuous Checkpoints</p> <ul> <li>Tracks all intermediate results automatically</li> <li>Resume from the last successful step if something fails</li> </ul>"},{"location":"modules/day6/#the-three-building-blocks","title":"The Three Building Blocks","text":"<p>Every Nextflow workflow has three main components:</p>"},{"location":"modules/day6/#1-processes-what-to-do","title":"1. Processes - What to do","text":"<pre><code>process FASTQC {\n    input:\n    path reads\n\n    output:\n    path \"*_fastqc.html\"\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day6/#2-channels-how-data-flows","title":"2. Channels - How data flows","text":"<pre><code>// Create a channel from files (DSL2 style)\nreads_ch = Channel.fromPath(\"/data/Dataset_Mt_Vc/tb/raw_data/*.fastq.gz\")\n</code></pre>"},{"location":"modules/day6/#3-workflows-how-it-all-connects","title":"3. Workflows - How it all connects","text":"<pre><code>workflow {\n    FASTQC(reads_ch)\n}\n</code></pre>"},{"location":"modules/day6/#understanding-processes-channels-and-workflows","title":"Understanding Processes, Channels, and Workflows","text":"<p>Visual Convention in Diagrams</p> <p>Throughout this module, we use consistent colors in diagrams to help you distinguish Nextflow components:</p> <ul> <li>\ud83d\udd35 Blue boxes = Channels (data streams)</li> <li>\ud83d\udfe2 Green boxes = Processes (computational tasks)</li> <li>\u26aa Gray boxes = Input/Output files</li> <li>\ud83d\udfe0 Orange boxes = Reports/Results</li> </ul>"},{"location":"modules/day6/#processes-in-detail","title":"Processes in Detail","text":"<p>A process describes a task to be run. Think of it as a recipe that tells Nextflow:</p> <ul> <li>What inputs it needs</li> <li>What outputs it produces</li> <li>What commands to run</li> </ul> <pre><code>process COUNT_READS {\n    // Process directives (optional)\n    tag \"$sample_id\"           // Label for this task\n    publishDir \"/data/users/$USER/nextflow-training/results/\"      // Where to save outputs\n\n    input:\n    tuple val(sample_id), path(reads)  // What this process needs\n\n    output:\n    path \"${sample_id}.count\"          // What this process creates\n\n    script:\n    \"\"\"\n    echo \"Counting reads in ${sample_id}\"\n    zcat ${reads} | wc -l &gt; ${sample_id}.count\n    \"\"\"\n}\n</code></pre> <p>Key Points:</p> <ul> <li>Each process runs independently (cannot talk to other processes)</li> <li>If you have 3 input files, Nextflow automatically creates 3 separate tasks</li> <li>Tasks can run in parallel if resources are available</li> </ul>"},{"location":"modules/day6/#channels-in-detail","title":"Channels in Detail","text":"<p>Channels are like conveyor belts that move data between processes. They're asynchronous queues that connect processes together.</p> <pre><code>// Different ways to create channels\n\n// From files matching a pattern\nChannel.fromPath(\"/data/Dataset_Mt_Vc/tb/raw_data/*.fastq.gz\")\n\n// From pairs of files (R1/R2)\nChannel.fromFilePairs(\"/data/Dataset_Mt_Vc/tb/raw_data/*_{1,2}.fastq.gz\")\n\n// From a list of values\nChannel.from(['sample1', 'sample2', 'sample3'])\n\n// From a CSV file\nChannel.fromPath(\"samples.csv\")\n    .splitCsv(header: true)\n</code></pre> <p>Channel Flow Example:</p> <pre><code>flowchart LR\n    A[Input Files] --&gt; B[Channel]\n    B --&gt; C[Process 1]\n    C --&gt; D[Output Channel]\n    D --&gt; E[Process 2]\n    E --&gt; F[Final Results]\n\n    %% Channels - Blue background\n    style B fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n    style D fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000\n\n    %% Processes - Green background\n    style C fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n    style E fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n\n    %% Input/Output - Light gray\n    style A fill:#f5f5f5,stroke:#757575,stroke-width:1px,color:#000\n    style F fill:#f5f5f5,stroke:#757575,stroke-width:1px,color:#000</code></pre> \ud83c\udfa8 Color Legend for Nextflow Diagrams Channels - Data streams (blue) Processes - Computational tasks (green) Input/Output - Data files (gray)"},{"location":"modules/day6/#workflows-in-detail","title":"Workflows in Detail","text":"<p>The workflow section defines how processes connect together. It's like the assembly line instructions.</p> <pre><code>workflow {\n    // Create input channel\n    reads_ch = Channel.fromPath(\"/data/Dataset_Mt_Vc/tb/raw_data/*.fastq.gz\")\n\n    // Run processes in order\n    FASTQC(reads_ch)\n    COUNT_READS(reads_ch)\n\n    // Use output from one process as input to another\n    TRIMMING(reads_ch)\n    ASSEMBLY(TRIMMING.out)\n}\n</code></pre>"},{"location":"modules/day6/#how-nextflow-executes-your-workflow","title":"How Nextflow Executes Your Workflow","text":"<p>When you run a Nextflow script, here's what happens:</p> <ol> <li>Parse the script: Nextflow reads your workflow definition</li> <li>Create the execution graph: Figures out which processes depend on which</li> <li>Submit tasks: Sends individual tasks to the executor (local computer, cluster, cloud)</li> <li>Monitor progress: Tracks which tasks complete successfully</li> <li>Handle failures: Retries failed tasks or stops gracefully</li> <li>Collect results: Gathers outputs in the specified locations</li> </ol> <pre><code>flowchart TD\n    A[Nextflow Script] --&gt; B[Parse &amp; Plan]\n    B --&gt; C[Submit Tasks]\n    C --&gt; D[Monitor Execution]\n    D --&gt; E{All Tasks Done?}\n    E --&gt;|No| F[Handle Failures]\n    F --&gt; C\n    E --&gt;|Yes| G[Collect Results]\n\n    style A fill:#e1f5fe\n    style G fill:#c8e6c9</code></pre>"},{"location":"modules/day6/#your-first-nextflow-script","title":"Your First Nextflow Script","text":"<p>Let's look at a complete, simple example that counts lines in a file:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Parameters (can be changed when running)\nparams.input = \"/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz\"\n\n// Create input channel\ninput_ch = Channel.fromPath(params.input)\n\n// Main workflow\nworkflow {\n    NUM_LINES(input_ch)\n    NUM_LINES.out.view()  // Print results to screen\n}\n\n// Process definition\nprocess NUM_LINES {\n    input:\n    path read\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo \"Processing: ${read}\"\n    zcat ${read} | wc -l\n    \"\"\"\n}\n</code></pre> <p>Run the Nextflow script:</p> <pre><code>nextflow run count_lines.nf\n</code></pre> Expected output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `count_lines.nf` [amazing_euler] - revision: a1b2c3d4\nexecutor &gt;  local (1)\n[a1/b2c3d4] process &gt; NUM_LINES (1) [100%] 1 of 1 \u2714\nProcessing: /data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz\n2452408\n</code></pre> <p>What this output means:</p> <ul> <li>Line 1: Nextflow version information</li> <li>Line 2: Script name and unique run identifier</li> <li>Line 3: Executor type (local computer)</li> <li>Line 4: Process execution status with unique task ID</li> <li>Line 5-6: Your script's actual output</li> </ul>"},{"location":"modules/day6/#workflow-execution-and-executors","title":"Workflow Execution and Executors","text":"<p>One of Nextflow's most powerful features is that it separates what your workflow does from where it runs.</p>"},{"location":"modules/day6/#executors-where-your-workflow-runs","title":"Executors: Where Your Workflow Runs","text":"<pre><code>flowchart TD\n    A[Your Nextflow Script] --&gt; B{Choose Executor}\n    B --&gt; C[Local Computer]\n    B --&gt; D[SLURM Cluster]\n    B --&gt; E[AWS Cloud]\n    B --&gt; F[Google Cloud]\n    B --&gt; G[Azure Cloud]\n\n    C --&gt; H[Same Workflow Code]\n    D --&gt; H\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    style A fill:#e1f5fe\n    style H fill:#c8e6c9</code></pre> <p>Available Executors:</p> <ul> <li>Local: Your laptop/desktop (default, great for testing)</li> <li>SLURM: High-performance computing clusters</li> <li>AWS Batch: Amazon cloud computing</li> <li>Google Cloud: Google's cloud platform</li> <li>Kubernetes: Container orchestration platform</li> </ul>"},{"location":"modules/day6/#how-to-choose-execution-platform","title":"How to Choose Execution Platform","text":"<p>You don't change your workflow code! Instead, you use configuration:</p> <p>For local execution (default):</p> <pre><code>nextflow run my_pipeline.nf\n</code></pre> <p>For SLURM cluster:</p> <pre><code>nextflow run my_pipeline.nf -profile slurm\n</code></pre> <p>For AWS cloud:</p> <pre><code>nextflow run my_pipeline.nf -profile aws\n</code></pre>"},{"location":"modules/day6/#resource-management","title":"Resource Management","text":"<p>Nextflow automatically handles:</p> <ul> <li>CPU allocation: How many cores each task gets</li> <li>Memory management: How much RAM each task needs</li> <li>Queue submission: Sending jobs to cluster schedulers</li> <li>Error handling: Retrying failed tasks</li> <li>File staging: Moving data between storage systems</li> </ul>"},{"location":"modules/day6/#quick-recap-key-concepts","title":"Quick Recap: Key Concepts","text":"<p>Before we start coding, let's make sure you understand these essential concepts:</p> Workflow Management System (WfMS) A computational platform for setting up, executing, and monitoring workflows Process A task definition that specifies inputs, outputs, and commands to run Channel An asynchronous queue that passes data between processes Workflow The section that defines how processes connect together Executor The system that actually runs your tasks (local, cluster, cloud) Task A single instance of a process running with specific input data Parallelization Running multiple tasks simultaneously to save time"},{"location":"modules/day6/#understanding-nextflow-output-organization","title":"Understanding Nextflow Output Organization","text":"<p>Before diving into exercises, it's essential to understand how Nextflow organizes its outputs. This knowledge will help you navigate results and debug issues effectively.</p>"},{"location":"modules/day6/#work-directory-configuration","title":"Work Directory Configuration","text":"<p>For this training, Nextflow is configured to use <code>/data/users/$USER/nextflow-training/work</code> as the work directory instead of the default <code>work/</code> directory in your current folder. This provides several benefits:</p> <ul> <li>Better organization: Separates temporary work files from your project files</li> <li>Shared storage: Uses the dedicated data partition with more space</li> <li>User isolation: Each user has their own work space</li> <li>Performance: Often faster storage for intensive I/O operations</li> </ul> <p>The configuration is set in <code>nextflow.config</code>:</p> <pre><code>// Set work directory to user's data space\nworkDir = \"/data/users/$USER/nextflow-training/work\"\n</code></pre> <p>This means all task execution directories will be created under <code>/data/users/$USER/nextflow-training/work/</code> (or your username).</p>"},{"location":"modules/day6/#nextflow-directory-structure","title":"Nextflow Directory Structure\ud83d\udcc1 Interactive Folder Explorer","text":"<p>When you run a Nextflow pipeline, several directories are automatically created:</p> <pre><code>flowchart TD\n    A[microbial-genomics-training/] --&gt; B[workflows/]\n    A --&gt; C[data/]\n    A --&gt; D[/data/users/$USER/nextflow-training/work/]\n    A --&gt; E[/data/users/$USER/nextflow-training/results/]\n\n    B --&gt; F[.nextflow/]\n    B --&gt; G[.nextflow.log]\n    B --&gt; H[*.nf files]\n    B --&gt; I[nextflow.config]\n\n    D --&gt; J[Task Directories]\n    J --&gt; K[5d/7dd7ae.../]\n    K --&gt; L[.command.sh]\n    K --&gt; M[.command.log]\n    K --&gt; N[.command.err]\n    K --&gt; O[Input Files]\n    K --&gt; P[Output Files]\n\n    E --&gt; Q[Published Results]\n    E --&gt; R[fastqc_raw/]\n    E --&gt; S[fastqc_trimmed/]\n    E --&gt; T[trimmed/]\n    E --&gt; U[assemblies/]\n    E --&gt; V[annotation/]\n\n    C --&gt; W[Dataset_Mt_Vc/tb/raw_data/]\n\n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style D fill:#fff3e0\n    style E fill:#e8f5e8\n    style F fill:#f3e5f5</code></pre> <p>Click on folders to explore Nextflow's directory structure:</p>              \ud83d\udcc1 microbial-genomics-training/ (your project directory)                  \ud83d\udcc1 workflows/ (Nextflow scripts and execution) \ud83d\udcc4 hello.nf (basic workflow) \ud83d\udcc4 count_reads.nf (read counting) \ud83d\udcc4 qc_pipeline.nf (progressive QC pipeline) \ud83d\udcc4 samplesheet.csv (sample metadata) \ud83d\udcc4 nextflow.config (configuration)                      \ud83d\udcc1 /data/users/$USER/nextflow-training/work/ (work directory - task files)                      \ud83d\udcc1 5d/7dd7ae.../ (individual task directory) \ud83d\udcc4 .command.sh (the actual command run) \ud83d\udcc4 .command.log (stdout from command) \ud83d\udcc4 .command.err (stderr from command) \ud83d\udcc4 .command.out (captured output) \ud83d\udcc4 .exitcode (exit status) \ud83d\udcc4 ERR036221_1.fastq.gz (input files - symlinks) \ud83d\udcc4 ERR036221_1_fastqc.html (output files)                  \ud83d\udcc1 /data/users/$USER/nextflow-training/results/ (published outputs) \ud83d\udcc1 fastqc_raw/ (raw data QC reports) \ud83d\udcc1 fastqc_trimmed/ (trimmed data QC reports) \ud83d\udcc1 trimmed/ (processed FASTQ files) \ud83d\udcc1 assemblies/ (genome assemblies) \ud83d\udcc1 annotation/ (gene annotations) \ud83d\udcc4 pipeline_trace.txt (execution trace) \ud83d\udcc4 pipeline_timeline.html (timeline visualization) \ud83d\udcc4 pipeline_report.html (execution report)                  \ud83d\udcc1 data/ (input genomic data) \ud83d\udcc1 Dataset_Mt_Vc/tb/raw_data/ (TB sequencing data) \ud83d\udcc4 ERR036221_1.fastq.gz (2.45M read pairs) \ud83d\udcc4 ERR036223_1.fastq.gz (4.19M read pairs)                  \ud83d\udcc1 .nextflow/ (Nextflow cache and metadata) \ud83d\udcc1 cache/ (pipeline cache) \ud83d\udcc1 history (run history) \ud83d\udcc4 pid (process ID file) \ud83d\udcc4 .nextflow.log (main log file) \ud83d\udcc4 timeline.html (execution timeline) \ud83d\udcc4 report.html (execution report) \ud83d\udcc4 hello.nf (your pipeline script)"},{"location":"modules/day6/#practical-navigation-commands","title":"Practical Navigation Commands","text":"<p>Here are essential commands for exploring Nextflow outputs:</p> <p>Check overall structure:</p> <pre><code>tree -L 2\n</code></pre> Expected output <pre><code>.\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 sample1_R1.fastq\n\u2502   \u2514\u2500\u2500 sample1_R2.fastq\n\u251c\u2500\u2500 hello.nf\n\u251c\u2500\u2500 results/\n\u2502   \u2514\u2500\u2500 fastqc/\n\u251c\u2500\u2500 work/\n\u2502   \u251c\u2500\u2500 a1/\n\u2502   \u251c\u2500\u2500 b2/\n\u2502   \u2514\u2500\u2500 c3/\n\u251c\u2500\u2500 .nextflow/\n\u251c\u2500\u2500 .nextflow.log\n\u2514\u2500\u2500 timeline.html\n</code></pre> <p>Find the most recent task directory:</p> <pre><code>find /data/users/$USER/nextflow-training/work/ -name \"*.exitcode\" -exec dirname {} \\; | head -1\n</code></pre> <p>Check task execution details:</p> <pre><code># Navigate to a task directory (use actual path from above)\ncd /data/users/$USER/nextflow-training/work/a1/b2c3d4e5f6...\n\n# See what command was run\ncat .command.sh\n\n# Check if it succeeded\ncat .exitcode  # 0 = success, non-zero = error\n\n# View any error messages\ncat .command.err\n</code></pre> <p>Monitor pipeline progress:</p> <pre><code># Watch log in real-time\ntail -f .nextflow.log\n\n# Check execution summary\nnextflow log\n</code></pre> Example nextflow log output <pre><code>TIMESTAMP            DURATION  RUN NAME         STATUS   REVISION ID  SESSION ID                            COMMAND\n2024-01-15 10:30:15  2m 15s    clever_volta     OK       a1b2c3d4     12345678-1234-1234-1234-123456789012  nextflow run hello.nf\n2024-01-15 10:25:30  45s       sad_einstein     ERR      e5f6g7h8     87654321-4321-4321-4321-210987654321  nextflow run broken.nf\n</code></pre>"},{"location":"modules/day6/#understanding-publishdir-vs-work-directory","title":"Understanding publishDir vs work Directory","text":"<p>One of the most important concepts for beginners is understanding the difference between the <code>/data/users/$USER/nextflow-training/work/</code> work directory and your results:</p> \ud83d\udd27 /data/users/$USER/nextflow-training/work/ Directory <ul> <li>Temporary - Can be deleted</li> <li>Messy - Mixed with logs and metadata</li> <li>Hash-named - Hard to navigate</li> <li>For debugging - When things go wrong</li> </ul> Use for: Debugging failed tasks      \ud83d\udcca /data/users/$USER/nextflow-training/results/ Directory <ul> <li>Permanent - Your final outputs</li> <li>Clean - Only important files</li> <li>Organized - Logical folder structure</li> <li>For sharing - With collaborators</li> </ul> Use for: Your actual research results"},{"location":"modules/day6/#common-directory-issues-and-solutions","title":"Common Directory Issues and Solutions\ud83d\udcbb Interactive Command Simulator","text":"<p>Problem: \"I can't find my results!\"</p> <pre><code># Check if publishDir was used in your process\ngrep -n \"publishDir\" *.nf\n\n# Look in the work directory\nfind /data/users/$USER/nextflow-training/work/ -name \"*.html\" -o -name \"*.txt\" -o -name \"*.fasta\"\n</code></pre> <p>Problem: \"Pipeline failed, how do I debug?\"</p> <pre><code># Find failed tasks\ngrep \"FAILED\" .nextflow.log\n\n# Get the work directory of failed task\ngrep -A 5 \"FAILED\" .nextflow.log | grep \"/data/users/\"\n\n# Navigate to that directory and investigate\ncd /data/users/$USER/nextflow-training/work/xx/yyyy...\ncat .command.err\n</code></pre> <p>Problem: \"work directory is huge!\"</p> <pre><code># Check work directory size\ndu -sh /data/users/$USER/nextflow-training/work/\n\n# Clean up after successful completion\nrm -rf /data/users/$USER/nextflow-training/work/*\n\n# Or use Nextflow's clean command\nnextflow clean -f\n</code></pre> <p>Now that you understand these fundamentals, let's put them into practice!</p> <p>Practice Nextflow commands in this simulated terminal:</p> user@training:~/nextflow-training$  Welcome to the Nextflow command simulator! Try typing: nextflow -version Available commands: nextflow -version, nextflow run hello.nf, ls, pwd, mkdir, cat"},{"location":"modules/day6/#your-first-genomics-pipeline","title":"Your First Genomics Pipeline","text":"<p>Here's what a basic microbial genomics analysis looks like:</p> <pre><code>flowchart LR\n    A[Raw Sequencing Data&lt;br/&gt;FASTQ files] --&gt; B[Quality Control&lt;br/&gt;FastQC]\n    B --&gt; C[Read Trimming&lt;br/&gt;Trimmomatic]\n    C --&gt; D[Genome Assembly&lt;br/&gt;SPAdes]\n    D --&gt; E[Assembly Quality&lt;br/&gt;QUAST]\n    E --&gt; F[Gene Annotation&lt;br/&gt;Prokka]\n    F --&gt; G[Final Results&lt;br/&gt;Annotated Genome]\n\n    B --&gt; H[Quality Report]\n    E --&gt; I[Assembly Stats]\n    F --&gt; J[Gene Predictions]\n\n    %% Input/Output data - Gray\n    style A fill:#f5f5f5,stroke:#757575,stroke-width:1px,color:#000\n    style G fill:#f5f5f5,stroke:#757575,stroke-width:1px,color:#000\n\n    %% Processes (bioinformatics tools) - Green\n    style B fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n    style C fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n    style D fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n    style E fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n    style F fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n\n    %% Reports/Outputs - Light orange\n    style H fill:#fff3e0,stroke:#f57c00,stroke-width:1px,color:#000\n    style I fill:#fff3e0,stroke:#f57c00,stroke-width:1px,color:#000\n    style J fill:#fff3e0,stroke:#f57c00,stroke-width:1px,color:#000</code></pre> <p>What Each Step Does:</p> <ol> <li>Quality Control: Check if your sequencing data is good quality</li> <li>Read Trimming: Remove low-quality parts of sequences</li> <li>Genome Assembly: Put the pieces together to reconstruct the genome</li> <li>Assembly Quality: Check how good your assembly is</li> <li>Gene Annotation: Find and label genes in the genome</li> </ol>"},{"location":"modules/day6/#beginner-friendly-practical-exercises","title":"Beginner-Friendly Practical Exercises","text":""},{"location":"modules/day6/#workflows-directory-structure","title":"\ud83d\udcc1 Workflows Directory Structure","text":"<p>All Nextflow workflows for this training are organized in the <code>workflows/</code> directory:</p> <pre><code>workflows/\n\u251c\u2500\u2500 hello.nf                 # Basic introduction workflow\n\u251c\u2500\u2500 channel_examples.nf      # Channel operations and data handling\n\u251c\u2500\u2500 count_reads.nf          # Read counting with real data\n\u251c\u2500\u2500 qc_pipeline.nf         # Exercise 3: Progressive QC pipeline (starts with FastQC, builds to complete genomics)\n\u251c\u2500\u2500 samplesheet.csv        # Sample metadata for testing\n\u251c\u2500\u2500 nextflow.config        # Configuration file\n\u2514\u2500\u2500 README.md              # Workflow documentation\n</code></pre> <p>\u2705 All workflows have been tested and validated</p> <p>These workflows have been successfully tested with real TB genomic data:</p> <ul> <li>hello.nf: \u2705 Tested with 3 samples - outputs \"Hello from sample1!\", etc.</li> <li>channel_examples.nf: \u2705 Tested channel operations and found 9 real TB samples</li> <li>count_reads.nf: \u2705 Processed 6.6M read pairs (ERR036221: 2.45M, ERR036223: 4.19M)</li> <li>qc_pipeline.nf: \u2705 Progressive pipeline (10 TB samples, starts with FastQC, builds to complete genomics)</li> </ul>"},{"location":"modules/day6/#exercise-1-your-first-nextflow-script-15-minutes","title":"Exercise 1: Your First Nextflow Script (15 minutes)","text":"<p>Let's start with the simplest possible Nextflow script to build confidence:</p> <p>Step 1: Create a \"Hello World\" pipeline</p> <pre><code>#!/usr/bin/env nextflow\n\n// This is your first Nextflow script!\n// It just prints a message for each sample\n\n// Define your samples (start with just 3)\nparams.samples = ['sample1', 'sample2', 'sample3']\n\n// Define a process (a step in your pipeline)\nprocess sayHello {\n    // What this process does\n    input:\n    val sample_name\n\n    // What it produces\n    output:\n    stdout\n\n    // The actual command\n    script:\n    \"\"\"\n    echo \"Hello from ${sample_name}!\"\n    \"\"\"\n}\n\n// Main workflow (DSL2 style)\nworkflow {\n    // Create a channel (think of it as a conveyor belt for data)\n    samples_ch = Channel.from(params.samples)\n\n    // Run the process\n    sayHello(samples_ch)\n\n    // Show the results\n    sayHello.out.view()\n}\n</code></pre> <p>Step 2: Save and run the script</p> <p>First, save the script to a file:</p> <pre><code># Create the file\nnano hello.nf\n# Copy-paste the script above, then save and exit (Ctrl+X, Y, Enter)\n</code></pre> <p>Now run your first Nextflow pipeline:</p> <pre><code># Navigate to workflows directory\ncd workflows\n\n# Run the hello workflow\nnextflow run hello.nf\n</code></pre> Expected output <pre><code>N E X T F L O W  ~  version 23.10.0\nLaunching `hello.nf` [nostalgic_pasteur] - revision: 1a2b3c4d\nexecutor &gt;  local (3)\n[a1/b2c3d4] process &gt; sayHello (3) [100%] 3 of 3 \u2714\nHello from sample1!\nHello from sample2!\nHello from sample3!\n</code></pre> <p>What this means:</p> <ul> <li>Nextflow automatically created 3 parallel tasks (one for each sample)</li> <li>All 3 tasks completed successfully (3 of 3 \u2714)</li> <li>The output shows messages from all samples</li> </ul> <p>Key Learning Points:</p> <ul> <li>Channels: Move data between processes (like a conveyor belt)</li> <li>Processes: Define what to do with each piece of data</li> <li>Parallelization: All samples run at the same time automatically!</li> </ul>"},{"location":"modules/day6/#exercise-2-adding-real-bioinformatics-30-minutes","title":"Exercise 2: Adding Real Bioinformatics (30 minutes)","text":"<p>Now let's do something useful - count reads in FASTQ files:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Parameters you can change\nparams.input = \"samplesheet.csv\"\nparams.outdir = \"/data/users/$USER/nextflow-training/results\"\n\n// Enable DSL2\nnextflow.enable.dsl = 2\n\n// Process to count reads in paired FASTQ files\nprocess countReads {\n    // Where to save results\n    publishDir params.outdir, mode: 'copy'\n\n    // Use sample name for process identification\n    tag \"$sample\"\n\n    input:\n    tuple val(sample), path(fastq1), path(fastq2)\n\n    output:\n    path \"${sample}.count\"\n\n    script:\n    \"\"\"\n    echo \"Counting reads in sample: ${sample}\"\n    echo \"Forward reads (${fastq1}):\"\n\n    # Count reads in both files (compressed FASTQ)\n    reads1=\\$(zcat ${fastq1} | wc -l | awk '{print \\$1/4}')\n    reads2=\\$(zcat ${fastq2} | wc -l | awk '{print \\$1/4}')\n\n    echo \"Sample: ${sample}\" &gt; ${sample}.count\n    echo \"Forward reads: \\$reads1\" &gt;&gt; ${sample}.count\n    echo \"Reverse reads: \\$reads2\" &gt;&gt; ${sample}.count\n    echo \"Total read pairs: \\$reads1\" &gt;&gt; ${sample}.count\n\n    echo \"Finished counting ${sample}: \\$reads1 read pairs\"\n    \"\"\"\n}\n\nworkflow {\n    // Read sample sheet and create channel\n    samples_ch = Channel\n        .fromPath(params.input)\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample = row.sample\n            def fastq1 = file(row.fastq_1)\n            def fastq2 = file(row.fastq_2)\n            return [sample, fastq1, fastq2]\n        }\n\n    // Run the process\n    countReads(samples_ch)\n    countReads.out.view()\n}\n</code></pre> <p>Step 1: Explore the available data</p> <pre><code># Check the real genomic data available\nls -la /data/Dataset_Mt_Vc/\n\n# Look at TB (Mycobacterium tuberculosis) data\nls -la /data/Dataset_Mt_Vc/tb/raw_data/ | head -5\n\n# Look at VC (Vibrio cholerae) data\nls -la /data/Dataset_Mt_Vc/vc/raw_data/ | head -5\n\n# Create a workspace for our analysis\nmkdir -p ~/nextflow_workspace/data\ncd ~/nextflow_workspace\n</code></pre> <p>Real Data Available</p> <p>We have access to real genomic datasets:</p> <ul> <li>TB data: <code>/data/Dataset_Mt_Vc/tb/raw_data/</code> - 40 paired-end FASTQ files</li> <li>VC data: <code>/data/Dataset_Mt_Vc/vc/raw_data/</code> - 40 paired-end FASTQ files</li> </ul> <p>These are real sequencing data from Mycobacterium tuberculosis and Vibrio cholerae samples!</p> <p>Step 2: Create a sample sheet with real data</p> <pre><code># Create a sample sheet with a few TB samples\ncat &gt; samplesheet.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nERR036221,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nERR036223,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_2.fastq.gz\nEOF\n\n# Check the sample sheet\ncat samplesheet.csv\n</code></pre> <p>Step 3: Update the script to use real data</p> <pre><code># Save the script as count_reads.nf\nnano count_reads.nf\n# Copy-paste the script above, then save and exit\n</code></pre> <p>Step 4: Run the pipeline with real data</p> <pre><code># Navigate to workflows directory\ncd workflows\n\n# Run the count reads pipeline\nnextflow run count_reads.nf --input samplesheet.csv\n</code></pre> Expected output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `count_reads.nf` [clever_volta] - revision: 5e6f7g8h\nexecutor &gt;  local (2)\n[c1/d2e3f4] process &gt; countReads (ERR036221) [100%] 2 of 2 \u2714\nRead count file: /data/users/$USER/nextflow-training/results/ERR036221.count\nRead count file: /data/users/$USER/nextflow-training/results/ERR036223.count\n</code></pre> <p>Step 5: Check your results</p> <pre><code># Look at the results directory\nls /data/users/$USER/nextflow-training/results/\n\n# Check the read counts for real TB data\ncat /data/users/$USER/nextflow-training/results/ERR036221.count\ncat /data/users/$USER/nextflow-training/results/ERR036223.count\n\n# Compare file sizes\nls -lh /data/Dataset_Mt_Vc/tb/raw_data/ERR036221_*.fastq.gz\n</code></pre> Expected output (\u2705 Tested with real data) <p>Count files content: <pre><code># ERR036221.count\nSample: ERR036221\nForward reads: 2452408\nReverse reads: 2452408\nTotal read pairs: 2452408\n\n# ERR036223.count\nSample: ERR036223\nForward reads: 4188521\nReverse reads: 4188521\nTotal read pairs: 4188521\n</code></pre> <pre><code># ls /data/users/$USER/nextflow-training/results/\nsample1.count  sample2.count\n\n# cat /data/users/$USER/nextflow-training/results/sample1.count\n2\n\n# cat /data/users/$USER/nextflow-training/results/sample2.count\n3\n</code></pre></p> <p>What this pipeline does:</p> <ol> <li>Reads sample information from a CSV file</li> <li>Counts reads in paired FASTQ files (in parallel!)</li> <li>Saves results to the <code>/data/users/$USER/nextflow-training/results/</code> directory</li> <li>Each <code>.count</code> file contains detailed read statistics for that sample</li> </ol>"},{"location":"modules/day6/#exercise-2b-real-world-scenarios-30-minutes","title":"Exercise 2B: Real-World Scenarios (30 minutes)","text":"<p>Now let's explore common real-world scenarios you'll encounter when using Nextflow:</p>"},{"location":"modules/day6/#scenario-1-adding-more-samples","title":"Scenario 1: Adding More Samples","text":"<p>Let's add more TB samples to our analysis:</p> <pre><code># Update the sample sheet with additional samples\ncat &gt; samplesheet.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nERR036221,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nERR036223,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_2.fastq.gz\nERR036226,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_2.fastq.gz\nERR036227,/data/Dataset_Mt_Vc/tb/raw_data/ERR036227_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036227_2.fastq.gz\nEOF\n\n# Check what samples we have now\necho \"Updated sample sheet:\"\ncat samplesheet.csv\n</code></pre>"},{"location":"modules/day6/#scenario-2-running-without-resume-fresh-start","title":"Scenario 2: Running Without Resume (Fresh Start)","text":"<pre><code># Clean previous results\nrm -rf /data/users/$USER/nextflow-training/results/* /data/users/$USER/nextflow-training/work/*\n\n# Run pipeline fresh (all processes will execute)\necho \"=== Running WITHOUT -resume ===\"\ncd workflows\ntime nextflow run count_reads.nf --input samplesheet.csv\n</code></pre> Expected output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `count_reads.nf` [clever_volta] - revision: 5e6f7g8h\nexecutor &gt;  local (4)\n[c1/d2e3f4] process &gt; countReads (ERR036221) [100%] 4 of 4 \u2714\n[a5/b6c7d8] process &gt; countReads (ERR036223) [100%] 4 of 4 \u2714\n[e9/f0g1h2] process &gt; countReads (ERR036226) [100%] 4 of 4 \u2714\n[i3/j4k5l6] process &gt; countReads (ERR036227) [100%] 4 of 4 \u2714\n\n# All 4 samples processed from scratch\n# Time: ~2-3 minutes (depending on data size)\n</code></pre>"},{"location":"modules/day6/#scenario-3-using-resume-smart-restart","title":"Scenario 3: Using Resume (Smart Restart)","text":"<p>Now let's simulate a common scenario - adding one more sample:</p> <pre><code># Add one more sample to the sheet\ncat &gt;&gt; samplesheet.csv &lt;&lt; 'EOF'\nERR036232,/data/Dataset_Mt_Vc/tb/raw_data/ERR036232_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036232_2.fastq.gz\nEOF\n\n# Run with -resume (only new sample will be processed)\necho \"=== Running WITH -resume ===\"\ntime nextflow run count_reads.nf --input samplesheet.csv -resume\n</code></pre> Expected output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `count_reads.nf` [clever_volta] - revision: 5e6f7g8h\nexecutor &gt;  local (1)\n[c1/d2e3f4] process &gt; countReads (ERR036221) [100%] 4 of 4, cached: 4 \u2714\n[a5/b6c7d8] process &gt; countReads (ERR036223) [100%] 4 of 4, cached: 4 \u2714\n[e9/f0g1h2] process &gt; countReads (ERR036226) [100%] 4 of 4, cached: 4 \u2714\n[i3/j4k5l6] process &gt; countReads (ERR036227) [100%] 4 of 4, cached: 4 \u2714\n[m7/n8o9p0] process &gt; countReads (ERR036232) [100%] 1 of 1 \u2714\n\n# Only ERR036232 processed fresh, others cached!\n# Time: ~30 seconds (much faster!)\n</code></pre>"},{"location":"modules/day6/#scenario-4-local-vs-cluster-execution","title":"Scenario 4: Local vs Cluster Execution","text":"<p>Local Execution (Current):</p> <pre><code># Running on local machine (default)\nnextflow run count_reads.nf --input samplesheet.csv -resume\n\n# Check resource usage\necho \"Local execution uses:\"\necho \"- All available CPU cores on this machine\"\necho \"- Local memory and storage\"\necho \"- Processes run sequentially if cores are limited\"\n</code></pre> <p>Cluster Execution (Advanced):</p> <pre><code># Example cluster configuration (for reference)\ncat &gt; nextflow.config &lt;&lt; 'EOF'\nprocess {\n    executor = 'slurm'\n    queue = 'batch'\n    cpus = 2\n    memory = '4.GB'\n    time = '1.h'\n}\n\nprofiles {\n    cluster {\n        process.executor = 'slurm'\n    }\n\n    local {\n        process.executor = 'local'\n    }\n}\nEOF\n\n# Would run on cluster (if available):\n# nextflow run count_reads.nf --input samplesheet.csv -profile cluster\n\necho \"Cluster execution would provide:\"\necho \"- Parallel execution across multiple nodes\"\necho \"- Better resource management\"\necho \"- Automatic job queuing and scheduling\"\necho \"- Fault tolerance across nodes\"\n</code></pre>"},{"location":"modules/day6/#scenario-5-monitoring-and-debugging","title":"Scenario 5: Monitoring and Debugging","text":"<pre><code># Check what's in the work directory\necho \"=== Work Directory Structure ===\"\nfind /data/users/$USER/nextflow-training/work -name \"*.count\" | head -5\n\n# Look at a specific process execution\nwork_dir=$(find /data/users/$USER/nextflow-training/work -name \"*ERR036221*\" -type d | head -1)\necho \"=== Process Details for ERR036221 ===\"\necho \"Work directory: $work_dir\"\nls -la \"$work_dir\"\n\n# Check the command that was executed\nif [ -f \"$work_dir/.command.sh\" ]; then\n    echo \"Command executed:\"\n    cat \"$work_dir/.command.sh\"\nfi\n\n# Check process logs\nif [ -f \"$work_dir/.command.log\" ]; then\n    echo \"Process output:\"\n    cat \"$work_dir/.command.log\"\nfi\n</code></pre> <p>Key Learning Points</p> <p>Resume Functionality:</p> <ul> <li><code>-resume</code> only re-runs processes that have changed</li> <li>Saves time and computational resources</li> <li>Essential for large-scale analyses</li> <li>Works by comparing input file checksums</li> </ul> <p>Execution Environments:</p> <ul> <li>Local: Good for development and small datasets</li> <li>Cluster: Essential for production and large datasets</li> <li>Cloud: Scalable option for variable workloads</li> </ul> <p>Best Practices:</p> <ul> <li>Always use <code>-resume</code> when re-running pipelines</li> <li>Test locally before moving to cluster</li> <li>Monitor resource usage and adjust accordingly</li> <li>Keep work directories for debugging</li> </ul>"},{"location":"modules/day6/#hands-on-timing-exercise","title":"Hands-On Timing Exercise\ud83d\udd04 Interactive Scenario Comparison","text":"<p>Let's measure the actual time difference:</p> <pre><code># Timing comparison exercise\necho \"=== TIMING COMPARISON EXERCISE ===\"\n\n# 1. Fresh run timing\necho \"1. Measuring fresh run time...\"\nrm -rf /data/users/$USER/nextflow-training/work/* /data/users/$USER/nextflow-training/results/*\ntime nextflow run count_reads.nf --input samplesheet.csv &gt; fresh_run.log 2&gt;&amp;1\n\n# 2. Resume run timing (no changes)\necho \"2. Measuring resume time with no changes...\"\ntime nextflow run count_reads.nf --input samplesheet.csv -resume &gt; resume_run.log 2&gt;&amp;1\n\n# 3. Resume with new sample timing\necho \"3. Adding new sample and measuring resume time...\"\necho \"ERR036233,/data/Dataset_Mt_Vc/tb/raw_data/ERR036233_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036233_2.fastq.gz\" &gt;&gt; samplesheet.csv\ntime nextflow run count_reads.nf --input samplesheet.csv -resume &gt; resume_new.log 2&gt;&amp;1\n\n# 4. Compare results\necho \"=== TIMING RESULTS ===\"\necho \"Fresh run log:\"\ngrep \"Completed at:\" fresh_run.log\necho \"Resume run log (no changes):\"\ngrep \"Completed at:\" resume_run.log\necho \"Resume run log (with new sample):\"\ngrep \"Completed at:\" resume_new.log\n\necho \"=== CACHE EFFICIENCY ===\"\necho \"Resume run (no changes):\"\ngrep \"cached:\" resume_run.log\necho \"Resume run (with new sample):\"\ngrep \"cached:\" resume_new.log\n</code></pre> Expected timing results <pre><code>=== TIMING RESULTS ===\nFresh run: ~2-3 minutes (all samples processed)\nResume (no changes): ~10-15 seconds (all cached)\nResume (new sample): ~45-60 seconds (4 cached + 1 new)\n\n=== CACHE EFFICIENCY ===\nResume shows: \"cached: 4\" for existing samples\nOnly new sample executes fresh\n\nSpeed improvement: 80-90% faster with resume!\n</code></pre> Fresh Run With Resume Cluster Mode \ud83c\udd95 Fresh Run (No Resume) <ul> <li>Command: <code>nextflow run count_reads.nf --input samplesheet.csv</code></li> <li>Behavior: All processes execute from scratch</li> <li>Time: Full execution time (2-3 minutes)</li> <li>Use case: First run, major changes, clean start</li> <li>Work directory: Completely new hash directories</li> </ul> \u26a1 Resume Run <ul> <li>Command: <code>nextflow run count_reads.nf --input samplesheet.csv -resume</code></li> <li>Behavior: Only new/changed processes execute</li> <li>Time: Much faster (30 seconds for new samples)</li> <li>Use case: Adding samples, minor script changes</li> <li>Work directory: Reuses existing hash directories</li> </ul> \ud83d\udda5\ufe0f Cluster Execution <ul> <li>Command: <code>nextflow run count_reads.nf --input samplesheet.csv -profile cluster</code></li> <li>Behavior: Jobs submitted to SLURM queue</li> <li>Time: Depends on queue wait time + parallel execution</li> <li>Use case: Large datasets, production runs</li> <li>Resources: Multiple nodes, better memory/CPU allocation</li> </ul>"},{"location":"modules/day6/#exercise-3-complete-quality-control-pipeline-60-minutes","title":"Exercise 3: Complete Quality Control Pipeline (60 minutes)","text":"<p>Now let's build a realistic bioinformatics pipeline with multiple steps:</p>"},{"location":"modules/day6/#step-1-basic-fastqc-pipeline","title":"Step 1: Basic FastQC Pipeline","text":"<p>First, let's start with a simple FastQC pipeline:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Enable DSL2\nnextflow.enable.dsl = 2\n\n// Parameters\nparams.input = \"samplesheet.csv\"\nparams.outdir = \"/data/users/$USER/nextflow-training/results\"\n\n// FastQC process\nprocess fastqc {\n    // Load required modules\n    module 'fastqc/0.12.1'\n\n    // Save results\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    // Use sample name for process identification\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\"  \n\n    script:\n    \"\"\"\n    echo \"Running FastQC on ${sample_id}\"\n    echo \"Processing files: ${reads.join(', ')}\"\n    fastqc ${reads}\n    \"\"\"\n}\n\n// Main workflow\nworkflow {\n    // Read sample sheet and create channel\n    read_pairs_ch = Channel\n        .fromPath(params.input)\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample = row.sample\n            def fastq1 = file(row.fastq_1)\n            def fastq2 = file(row.fastq_2)\n            return [sample, [fastq1, fastq2]]\n        }\n\n    // Run FastQC\n    fastqc_results = fastqc(read_pairs_ch)\n\n    // Show what files were created\n    fastqc_results.view { \"FastQC report: $it\" }\n}\n</code></pre> <p>Save this as <code>qc_pipeline.nf</code> and test it:</p> <pre><code># Load modules\nsource /opt/lmod/8.7/lmod/lmod/init/bash\nmodule load nextflow/25.04.6 fastqc/0.12.1\n\n# Navigate to workflows directory and run basic FastQC pipeline\ncd workflows\nnextflow run qc_pipeline.nf --input samplesheet.csv\n</code></pre>"},{"location":"modules/day6/#step-2-extend-the-pipeline","title":"Step 2: Extend the Pipeline","text":"<p>Now let's extend our existing <code>qc_pipeline.nf</code> file to include trimming, genome assembly, and annotation. We'll build upon what we already have:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Enable DSL2\nnextflow.enable.dsl = 2\n\n// Parameters\nparams.input = \"samplesheet.csv\"\nparams.outdir = \"results\"\n\n// FastQC on raw reads\nprocess fastqc_raw {\n    module 'fastqc/0.12.1'\n    publishDir \"${params.outdir}/fastqc_raw\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\"\n\n    script:\n    \"\"\"\n    echo \"Running FastQC on raw reads: ${sample_id}\"\n    fastqc ${reads}\n    \"\"\"\n}\n\n// Trimmomatic for quality trimming\nprocess trimmomatic {\n    module 'trimmomatic/0.39'\n    publishDir \"${params.outdir}/trimmed\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_*_paired.fastq.gz\")\n    path \"${sample_id}_*_unpaired.fastq.gz\"\n\n    script:\n    \"\"\"\n    echo \"Running Trimmomatic on ${sample_id}\"\n\n    trimmomatic PE -threads 2 \\\\\n        ${reads[0]} ${reads[1]} \\\\\n        ${sample_id}_R1_paired.fastq.gz ${sample_id}_R1_unpaired.fastq.gz \\\\\n        ${sample_id}_R2_paired.fastq.gz ${sample_id}_R2_unpaired.fastq.gz \\\\\n        LEADING:3 TRAILING:3 \\\\\n        SLIDINGWINDOW:4:15 MINLEN:36\n    \"\"\"\n}\n\n// FastQC on trimmed reads\nprocess fastqc_trimmed {\n    module 'fastqc/0.12.1'\n    publishDir \"${params.outdir}/fastqc_trimmed\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\"\n\n    script:\n    \"\"\"\n    echo \"Running FastQC on trimmed reads: ${sample_id}\"\n    fastqc ${reads}\n    \"\"\"\n}\n\n// SPAdes genome assembly\nprocess spades_assembly {\n    module 'spades/4.2.0'\n    publishDir \"${params.outdir}/assemblies\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_assembly/contigs.fasta\")\n    path \"${sample_id}_assembly/\"\n\n    script:\n    \"\"\"\n    echo \"Running SPAdes assembly on ${sample_id}\"\n\n    spades.py \\\\\n        -1 ${reads[0]} \\\\\n        -2 ${reads[1]} \\\\\n        -o ${sample_id}_assembly \\\\\n        --threads 2 \\\\\n        --memory 8\n    \"\"\"\n}\n\n// Prokka genome annotation\nprocess prokka_annotation {\n    module 'prokka/1.14.6'\n    publishDir \"${params.outdir}/annotation\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(contigs)\n\n    output:\n    path \"${sample_id}_annotation/\"\n\n    script:\n    \"\"\"\n    echo \"Running Prokka annotation on ${sample_id}\"\n\n    prokka \\\\\n        --outdir ${sample_id}_annotation \\\\\n        --prefix ${sample_id} \\\\\n        --cpus 2 \\\\\n        --genus Mycobacterium \\\\\n        --species tuberculosis \\\\\n        --kingdom Bacteria \\\\\n        ${contigs}\n    \"\"\"\n}\n\n// Main workflow\nworkflow {\n    // Read sample sheet and create channel\n    read_pairs_ch = Channel\n        .fromPath(params.input)\n        .splitCsv(header: true)\n        .map { row -&gt;\n            def sample = row.sample\n            def fastq1 = file(row.fastq_1)\n            def fastq2 = file(row.fastq_2)\n            return [sample, [fastq1, fastq2]]\n        }\n\n    // Run FastQC on raw reads\n    fastqc_raw_results = fastqc_raw(read_pairs_ch)\n    fastqc_raw_results.view { \"Raw FastQC: $it\" }\n\n    // Run Trimmomatic for quality trimming\n    (trimmed_paired, trimmed_unpaired) = trimmomatic(read_pairs_ch)\n    trimmed_paired.view { \"Trimmed paired reads: $it\" }\n\n    // Run FastQC on trimmed reads\n    fastqc_trimmed_results = fastqc_trimmed(trimmed_paired)\n    fastqc_trimmed_results.view { \"Trimmed FastQC: $it\" }\n\n    // Run SPAdes assembly\n    (assembly_contigs, assembly_dir) = spades_assembly(trimmed_paired)\n    assembly_contigs.view { \"Assembly contigs: $it\" }\n\n    // Run Prokka annotation\n    annotations = prokka_annotation(assembly_contigs)\n    annotations.view { \"Annotation: $it\" }\n}\n</code></pre> <p>Now let's extend our <code>qc_pipeline.nf</code> file to include the complete genomic analysis pipeline. Replace the contents of your existing <code>qc_pipeline.nf</code> with this expanded version:</p> <pre><code># Load all required modules\nsource /opt/lmod/8.7/lmod/lmod/init/bash\nmodule load nextflow/25.04.6 fastqc/0.12.1 trimmomatic/0.39 spades/4.2.0 prokka/1.14.6 multiqc/1.22.3\n\n# Navigate to workflows directory and run the complete genomic analysis pipeline\ncd workflows\nnextflow run qc_pipeline.nf --input samplesheet.csv\n</code></pre> Expected output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `qc_pipeline_v2.nf` [clever_volta] - revision: 5e6f7g8h\nexecutor &gt;  local (14)\n[a1/b2c3d4] process &gt; fastqc_raw (ERR036221)        [100%] 2 of 2 \u2714\n[e5/f6g7h8] process &gt; fastqc_raw (ERR036223)        [100%] 2 of 2 \u2714\n[i9/j0k1l2] process &gt; trimmomatic (ERR036221)       [100%] 2 of 2 \u2714\n[m3/n4o5p6] process &gt; trimmomatic (ERR036223)       [100%] 2 of 2 \u2714\n[q7/r8s9t0] process &gt; fastqc_trimmed (ERR036221)    [100%] 2 of 2 \u2714\n[u1/v2w3x4] process &gt; fastqc_trimmed (ERR036223)    [100%] 2 of 2 \u2714\n[a2/b3c4d5] process &gt; spades_assembly (ERR036221)   [100%] 2 of 2 \u2714\n[e6/f7g8h9] process &gt; spades_assembly (ERR036223)   [100%] 2 of 2 \u2714\n[i0/j1k2l3] process &gt; prokka_annotation (ERR036221) [100%] 2 of 2 \u2714\n[m4/n5o6p7] process &gt; prokka_annotation (ERR036223) [100%] 2 of 2 \u2714\n[y5/z6a7b8] process &gt; multiqc                       [100%] 1 of 1 \u2714\n\nAssembly completed: /data/users/$USER/nextflow-training/results/assemblies/ERR036221_assembly\nContigs file: /data/users/$USER/nextflow-training/results/assemblies/ERR036221_assembly/contigs.fasta\nAssembly completed: /data/users/$USER/nextflow-training/results/assemblies/ERR036223_assembly\nContigs file: /data/users/$USER/nextflow-training/results/assemblies/ERR036223_assembly/contigs.fasta\nAnnotation completed: /data/users/$USER/nextflow-training/results/annotation/ERR036221_annotation\nGFF file: /data/users/$USER/nextflow-training/results/annotation/ERR036221_annotation/ERR036221.gff\nAnnotation completed: /data/users/$USER/nextflow-training/results/annotation/ERR036223_annotation\nGFF file: /data/users/$USER/nextflow-training/results/annotation/ERR036223_annotation/ERR036223.gff\nMultiQC report created: /data/users/$USER/nextflow-training/results/multiqc_report.html\n</code></pre>"},{"location":"modules/day6/#step-3-running-on-cluster-with-configuration-files","title":"Step 3: Running on Cluster with Configuration Files","text":"<p>For production runs with larger datasets, you'll want to run this pipeline on a cluster. Let's create configuration files for different cluster environments:</p> <p>Create a SLURM configuration file:</p> <pre><code># Create cluster configuration\ncat &gt; cluster.config &lt;&lt; 'EOF'\n// Cluster configuration for genomic analysis pipeline\n\nparams {\n    outdir = \"/data/users/$USER/nextflow-training/results_cluster\"\n}\n\nprofiles {\n    slurm {\n        process {\n            executor = 'slurm'\n\n            // Default resources\n            cpus = 2\n            memory = '4 GB'\n            time = '2h'\n\n            // Process-specific resources for intensive tasks\n            withName: spades_assembly {\n                cpus = 8\n                memory = '16 GB'\n                time = '6h'\n            }\n\n            withName: prokka_annotation {\n                cpus = 4\n                memory = '8 GB'\n                time = '3h'\n            }\n\n            withName: trimmomatic {\n                cpus = 4\n                memory = '8 GB'\n                time = '2h'\n            }\n        }\n\n        executor {\n            queueSize = 20\n            submitRateLimit = '10 sec'\n        }\n    }\n\n    // High-memory profile for large genomes\n    highmem {\n        process {\n            executor = 'slurm'\n\n            withName: spades_assembly {\n                cpus = 16\n                memory = '64 GB'\n                time = '12h'\n            }\n\n            withName: prokka_annotation {\n                cpus = 8\n                memory = '16 GB'\n                time = '6h'\n            }\n        }\n    }\n}\n\n// Enhanced reporting for cluster runs\ntrace {\n    enabled = true\n    file = \"${params.outdir}/pipeline_trace.txt\"\n    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'\n}\n\ntimeline {\n    enabled = true\n    file = \"${params.outdir}/pipeline_timeline.html\"\n}\n\nreport {\n    enabled = true\n    file = \"${params.outdir}/pipeline_report.html\"\n}\nEOF\n</code></pre> <p>Run the pipeline on SLURM cluster:</p> <pre><code># Load modules\nmodule load java/openjdk-17.0.2 nextflow/25.04.6 fastqc/0.12.1 trimmomatic/0.39 spades/4.2.0 prokka/1.14.6 multiqc/1.22.3\n\n# Run with SLURM profile\nnextflow run qc_pipeline.nf -c cluster.config -profile slurm --input samplesheet.csv\n\n# For large genomes, use high-memory profile\nnextflow run qc_pipeline.nf -c cluster.config -profile highmem --input samplesheet.csv\n</code></pre> <pre><code>\n</code></pre> Expected cluster output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `qc_pipeline.nf` [determined_pasteur] - revision: 8h9i0j1k\nexecutor &gt;  slurm (14)\n[a1/b2c3d4] process &gt; fastqc_raw (ERR036221)        [100%] 2 of 2 \u2714\n[e5/f6g7h8] process &gt; fastqc_raw (ERR036223)        [100%] 2 of 2 \u2714\n[i9/j0k1l2] process &gt; trimmomatic (ERR036221)       [100%] 2 of 2 \u2714\n[m3/n4o5p6] process &gt; trimmomatic (ERR036223)       [100%] 2 of 2 \u2714\n[q7/r8s9t0] process &gt; fastqc_trimmed (ERR036221)    [100%] 2 of 2 \u2714\n[u1/v2w3x4] process &gt; fastqc_trimmed (ERR036223)    [100%] 2 of 2 \u2714\n[a2/b3c4d5] process &gt; spades_assembly (ERR036221)   [100%] 2 of 2 \u2714\n[e6/f7g8h9] process &gt; spades_assembly (ERR036223)   [100%] 2 of 2 \u2714\n[i0/j1k2l3] process &gt; prokka_annotation (ERR036221) [100%] 2 of 2 \u2714\n[m4/n5o6p7] process &gt; prokka_annotation (ERR036223) [100%] 2 of 2 \u2714\n[y5/z6a7b8] process &gt; multiqc                       [100%] 1 of 1 \u2714\n\nAssembly completed: /data/users/$USER/nextflow-training/results_cluster/assemblies/ERR036221_assembly\nContigs file: /data/users/$USER/nextflow-training/results_cluster/assemblies/ERR036221_assembly/contigs.fasta\nAnnotation completed: /data/users/$USER/nextflow-training/results_cluster/annotation/ERR036221_annotation\nGFF file: /data/users/$USER/nextflow-training/results_cluster/annotation/ERR036221_annotation/ERR036221.gff\n\nCompleted at: 09-Dec-2024 14:30:15\nDuration    : 45m 23s\nCPU hours   : 12.5\nSucceeded   : 14\n</code></pre> <p>Monitor cluster execution:</p> <pre><code># Check SLURM job status\nsqueue -u $USER\n\n# Monitor resource usage\nnextflow log -f trace\n\n# View detailed execution report\nfirefox /data/users/$USER/nextflow-training/results_cluster/pipeline_report.html\n\n# Check timeline visualization\nfirefox /data/users/$USER/nextflow-training/results_cluster/pipeline_timeline.html\n</code></pre> <p>Scaling up for production analysis:</p> <pre><code># Create extended sample sheet with more samples\ncat &gt; samplesheet_extended.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nERR036221,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nERR036223,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_2.fastq.gz\nERR036226,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_2.fastq.gz\nERR036227,/data/Dataset_Mt_Vc/tb/raw_data/ERR036227_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036227_2.fastq.gz\nERR036228,/data/Dataset_Mt_Vc/tb/raw_data/ERR036228_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036228_2.fastq.gz\nEOF\n\n# Run production analysis with 5 samples\nnextflow run qc_pipeline.nf -c cluster.config -profile slurm --input samplesheet_extended.csv\n\n# Monitor progress\nwatch -n 30 'squeue -u $USER | grep nextflow'\n</code></pre> <p>Cluster Best Practices</p> <p>Resource Optimization:</p> <ul> <li>SPAdes assembly: Most memory-intensive step (8-16 GB recommended)</li> <li>Prokka annotation: CPU-intensive (4-8 cores optimal)</li> <li>FastQC: Lightweight (2 cores sufficient)</li> <li>Trimmomatic: Moderate resources (4 cores, 8 GB)</li> </ul> <p>Scaling Considerations:</p> <ul> <li>Small datasets (1-5 samples): Use local execution</li> <li>Medium datasets (5-20 samples): Use standard SLURM profile</li> <li>Large datasets (20+ samples): Use high-memory profile</li> <li>Very large genomes: Increase SPAdes memory to 64+ GB</li> </ul>"},{"location":"modules/day6/#step-4-pipeline-scenarios-and-comparisons","title":"Step 4: Pipeline Scenarios and Comparisons","text":"<p>Scenario A: Compare Before and After Trimming</p> <pre><code># Check the complete results structure\ntree /data/users/$USER/nextflow-training/results/\n\n# Explore each output directory\necho \"=== Raw Data Quality Reports ===\"\nls -la /data/users/$USER/nextflow-training/results/fastqc_raw/\n\necho \"=== Trimmed Data Quality Reports ===\"\nls -la /data/users/$USER/nextflow-training/results/fastqc_trimmed/\n\necho \"=== Trimmed FASTQ Files ===\"\nls -la /data/users/$USER/nextflow-training/results/trimmed/\n\necho \"=== Genome Assemblies ===\"\nls -la /data/users/$USER/nextflow-training/results/assemblies/\n\necho \"=== Genome Annotations ===\"\nls -la /data/users/$USER/nextflow-training/results/annotation/\n\necho \"=== MultiQC Summary Report ===\"\nls -la /data/users/$USER/nextflow-training/results/multiqc_report.html\n\n# Check assembly statistics\necho \"=== Assembly Statistics ===\"\nfor sample in ERR036221 ERR036223; do\n    echo \"Sample: $sample\"\n    if [ -f \"/data/users/$USER/nextflow-training/results/assemblies/${sample}_assembly/contigs.fasta\" ]; then\n        echo \"  Contigs: $(grep -c '&gt;' /data/users/$USER/nextflow-training/results/assemblies/${sample}_assembly/contigs.fasta)\"\n        echo \"  Total size: $(grep -v '&gt;' /data/users/$USER/nextflow-training/results/assemblies/${sample}_assembly/contigs.fasta | wc -c) bp\"\n    fi\ndone\n\n# Check annotation statistics\necho \"=== Annotation Statistics ===\"\nfor sample in ERR036221 ERR036223; do\n    echo \"Sample: $sample\"\n    if [ -f \"/data/users/$USER/nextflow-training/results/annotation/${sample}_annotation/${sample}.gff\" ]; then\n        echo \"  Total features: $(grep -v '^#' /data/users/$USER/nextflow-training/results/annotation/${sample}_annotation/${sample}.gff | wc -l)\"\n        echo \"  CDS features: $(grep -v '^#' /data/users/$USER/nextflow-training/results/annotation/${sample}_annotation/${sample}.gff | grep 'CDS' | wc -l)\"\n        echo \"  Gene features: $(grep -v '^#' /data/users/$USER/nextflow-training/results/annotation/${sample}_annotation/${sample}.gff | grep 'gene' | wc -l)\"\n    fi\ndone\n\n# File size comparison\necho \"=== File Size Comparison ===\"\necho \"Original files:\"\nls -lh /data/Dataset_Mt_Vc/tb/raw_data/ERR036221_*.fastq.gz\necho \"Trimmed files:\"\nls -lh /data/users/$USER/nextflow-training/results/trimmed/ERR036221_*_paired.fastq.gz\n</code></pre> Expected directory structure (\u2705 Tested and validated) <pre><code>workflows/                           # Main workflow directory\n\u251c\u2500\u2500 qc_test.nf                      # Complete QC pipeline (\u2705 tested)\n\u251c\u2500\u2500 qc_pipeline.nf                  # Full genomics pipeline\n\u251c\u2500\u2500 samplesheet.csv                 # Sample metadata\n\u251c\u2500\u2500 nextflow.config                 # Configuration file\n\u251c\u2500\u2500 /data/users/$USER/nextflow-training/results/  # Published outputs\n\u2502   \u251c\u2500\u2500 fastqc_raw/                 # Raw data QC (\u2705 tested)\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_1_fastqc.html # 707KB quality report\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_1_fastqc.zip  # 432KB data archive\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_2_fastqc.html # 724KB quality report\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_2_fastqc.zip  # 439KB data archive\n\u2502   \u2502   \u251c\u2500\u2500 ERR036223_1_fastqc.html # 704KB quality report\n\u2502   \u2502   \u251c\u2500\u2500 ERR036223_1_fastqc.zip  # 426KB data archive\n\u2502   \u2502   \u251c\u2500\u2500 ERR036223_2_fastqc.html # 720KB quality report\n\u2502   \u2502   \u2514\u2500\u2500 ERR036223_2_fastqc.zip  # 434KB data archive\n\u2502   \u251c\u2500\u2500 trimmed/                    # Trimmed reads (\u2705 tested)\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_R1_paired.fastq.gz  # 119MB trimmed reads\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_R2_paired.fastq.gz  # 115MB trimmed reads\n\u2502   \u2502   \u251c\u2500\u2500 ERR036223_R1_paired.fastq.gz  # 200MB trimmed reads\n\u2502   \u2502   \u2514\u2500\u2500 ERR036223_R2_paired.fastq.gz  # 193MB trimmed reads\n\u2502   \u251c\u2500\u2500 fastqc_trimmed/             # Trimmed data QC (\u2705 tested)\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_R1_paired_fastqc.html\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_R1_paired_fastqc.zip\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_R2_paired_fastqc.html\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_R2_paired_fastqc.zip\n\u2502   \u2502   \u251c\u2500\u2500 ERR036223_R1_paired_fastqc.html\n\u2502   \u2502   \u251c\u2500\u2500 ERR036223_R1_paired_fastqc.zip\n\u2502   \u2502   \u251c\u2500\u2500 ERR036223_R2_paired_fastqc.html\n\u2502   \u2502   \u2514\u2500\u2500 ERR036223_R2_paired_fastqc.zip\n\u2502   \u251c\u2500\u2500 assemblies/                 # Genome assemblies (for full pipeline)\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_assembly/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 contigs.fasta\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 scaffolds.fasta\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 spades.log\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 assembly_graph.fastg\n\u2502   \u2502   \u2514\u2500\u2500 ERR036223_assembly/\n\u2502   \u2502       \u251c\u2500\u2500 contigs.fasta\n\u2502   \u2502       \u251c\u2500\u2500 scaffolds.fasta\n\u2502   \u2502       \u251c\u2500\u2500 spades.log\n\u2502   \u2502       \u2514\u2500\u2500 assembly_graph.fastg\n\u2502   \u251c\u2500\u2500 annotation/                 # Genome annotations (for full pipeline)\n\u2502   \u2502   \u251c\u2500\u2500 ERR036221_annotation/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ERR036221.faa        # Protein sequences\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ERR036221.ffn        # Gene sequences\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ERR036221.fna        # Genome sequence\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ERR036221.gff        # Gene annotations\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ERR036221.gbk        # GenBank format\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ERR036221.tbl        # Feature table\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ERR036221.txt        # Statistics\n\u2502   \u2502   \u2514\u2500\u2500 ERR036223_annotation/\n\u2502   \u2502       \u251c\u2500\u2500 ERR036223.faa\n\u2502   \u2502       \u251c\u2500\u2500 ERR036223.ffn\n\u2502   \u2502       \u251c\u2500\u2500 ERR036223.fna\n\u2502   \u2502       \u251c\u2500\u2500 ERR036223.gff\n\u2502   \u2502       \u251c\u2500\u2500 ERR036223.gbk\n\u2502   \u2502       \u251c\u2500\u2500 ERR036223.tbl\n\u2502   \u2502       \u2514\u2500\u2500 ERR036223.txt\n\u2502   \u251c\u2500\u2500 multiqc_report.html          # Comprehensive QC summary\n\u2502   \u251c\u2500\u2500 multiqc_data/                # MultiQC supporting data\n\u2502   \u251c\u2500\u2500 pipeline_trace.txt           # Execution trace (\u2705 generated)\n\u2502   \u251c\u2500\u2500 pipeline_timeline.html       # Timeline visualization (\u2705 generated)\n\u2502   \u2514\u2500\u2500 pipeline_report.html         # Execution report (\u2705 generated)\n\u251c\u2500\u2500 work/                           # Temporary execution files (cached)\n\u2502   \u251c\u2500\u2500 5d/7dd7ae.../              # Process execution directories\n\u2502   \u251c\u2500\u2500 a2/b3c4d5.../              # Each contains:\n\u2502   \u2514\u2500\u2500 e6/f7g8h9.../              #   - .command.sh (script)\n\u2502                                   #   - .command.out (stdout)\n\u2502                                   #   - .command.err (stderr)\n\u2502                                   #   - .command.log (execution log)\n\u251c\u2500\u2500 .nextflow.log                   # Main execution log\n\u2514\u2500\u2500 .nextflow/                      # Nextflow metadata and cache\n\u251c\u2500\u2500 pipeline_trace.txt           # Execution trace\n\u251c\u2500\u2500 pipeline_timeline.html       # Timeline visualization\n\u2514\u2500\u2500 pipeline_report.html         # Execution report\n</code></pre> <p>Scenario B: Adding More Samples with Resume</p> <pre><code># Add more samples to test scalability\ncat &gt; samplesheet_extended.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nERR036221,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nERR036223,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_2.fastq.gz\nERR036226,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_2.fastq.gz\nERR036227,/data/Dataset_Mt_Vc/tb/raw_data/ERR036227_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036227_2.fastq.gz\nEOF\n\n# Run with resume (only new samples will be processed)\necho \"=== Running with more samples using -resume ===\"\ntime nextflow run qc_pipeline.nf --input samplesheet_extended.csv -resume\n</code></pre> <p>Scenario C: Parameter Optimization</p> <pre><code># Create a configuration file for different trimming parameters\ncat &gt; nextflow.config &lt;&lt; 'EOF'\nparams {\n    input = \"samplesheet.csv\"\n    outdir = \"/data/users/$USER/nextflow-training/results\"\n    adapters = \"/data/timmomatic_adapter_Combo.fa\"\n}\n\nprofiles {\n    strict {\n        params.outdir = \"/data/users/$USER/nextflow-training/results_strict\"\n        // Stricter trimming parameters would go here\n    }\n\n    lenient {\n        params.outdir = \"/data/users/$USER/nextflow-training/results_lenient\"\n        // More lenient trimming parameters would go here\n    }\n}\nEOF\n\n# Run with different profiles\necho \"=== Testing different trimming strategies ===\"\nnextflow run qc_pipeline.nf -profile strict\nnextflow run qc_pipeline.nf -profile lenient\n\n# Compare results\necho \"=== Comparing trimming strategies ===\"\necho \"Strict trimming results:\"\nls -la /data/users/$USER/nextflow-training/results_strict/trimmed/\necho \"Lenient trimming results:\"\nls -la /data/users/$USER/nextflow-training/results_lenient/trimmed/\n</code></pre>"},{"location":"modules/day6/#step-4-cluster-execution-advanced","title":"Step 4: Cluster Execution (Advanced)","text":"<p>Now let's see how to run the same pipeline on an HPC cluster:</p> <p>Scenario D: Local vs Cluster Comparison</p> <pre><code># First, let's run locally (what we've been doing)\necho \"=== Local Execution ===\"\ntime nextflow run qc_pipeline.nf --input samplesheet.csv -profile standard\n\n# Now let's run on SLURM cluster\necho \"=== SLURM Cluster Execution ===\"\ntime nextflow run qc_pipeline.nf --input samplesheet.csv -profile slurm\n\n# For testing with reduced resources\necho \"=== Test Profile ===\"\nnextflow run qc_pipeline.nf --input samplesheet.csv -profile test\n</code></pre> <p>Scenario E: High-Memory Assembly</p> <pre><code># For large genomes or complex assemblies\necho \"=== High-Memory Cluster Execution ===\"\nnextflow run qc_pipeline.nf --input samplesheet_extended.csv -profile highmem\n\n# Monitor SLURM cluster jobs\nsqueue -u $USER\n</code></pre> <p>Scenario F: Resource Monitoring and Reports</p> <pre><code># Run with comprehensive monitoring\nnextflow run qc_pipeline.nf --input samplesheet.csv -profile slurm -with-trace -with-timeline -with-report\n\n# Check the generated reports\necho \"=== Pipeline Reports Generated ===\"\nls -la /data/users/$USER/nextflow-training/results/pipeline_*\n\n# View resource usage\necho \"=== Resource Usage Summary ===\"\ncat /data/users/$USER/nextflow-training/results/pipeline_trace.txt | head -10\n</code></pre> <p>Local vs Cluster Execution Comparison</p> <p>Local Execution Benefits:</p> <ul> <li>\u2705 Immediate start: No queue waiting time</li> <li>\u2705 Interactive debugging: Easy to test and troubleshoot</li> <li>\u2705 Simple setup: No cluster configuration needed</li> <li>\u274c Limited resources: Constrained by local machine</li> <li>\u274c No parallelization: Limited concurrent jobs</li> </ul> <p>Cluster Execution Benefits:</p> <ul> <li>\u2705 Massive parallelization: 100+ samples simultaneously</li> <li>\u2705 High-memory nodes: 64GB+ RAM for large assemblies</li> <li>\u2705 Automatic scheduling: Optimal resource allocation</li> <li>\u2705 Fault tolerance: Job restart on node failures</li> <li>\u274c Queue waiting: May wait for resources</li> <li>\u274c Complex setup: Requires cluster configuration</li> </ul> <p>When to Use Each:</p> <ul> <li>Local: Testing, small datasets (1-5 samples), development</li> <li>Cluster: Production runs, large datasets (10+ samples), resource-intensive tasks</li> </ul>"},{"location":"modules/day6/#cluster-configuration-examples","title":"Cluster Configuration Examples","text":"<p>SLURM Configuration:</p> <pre><code># Create a SLURM-specific config\ncat &gt; slurm.config &lt;&lt; 'EOF'\nprocess {\n    executor = 'slurm'\n\n    withName: spades_assembly {\n        cpus = 16\n        memory = '32 GB'\n        time = '6h'\n        queue = 'long'\n    }\n}\nEOF\n\n# Run with custom config\nnextflow run qc_pipeline.nf -c slurm.config --input samplesheet.csv\n</code></pre> <p>Key Learning Points from Exercise 3</p> <p>Pipeline Design Concepts:</p> <ul> <li>Channel Reuse: In DSL2, channels can be used multiple times directly</li> <li>Process Dependencies: Trimmomatic \u2192 FastQC creates a dependency chain</li> <li>Result Aggregation: MultiQC collects and summarizes all FastQC reports</li> <li>Parallel Processing: Raw FastQC and Trimmomatic run simultaneously</li> </ul> <p>Real-World Bioinformatics:</p> <ul> <li>Quality Control: Always check data quality before and after processing</li> <li>Adapter Trimming: Remove sequencing adapters and low-quality bases</li> <li>Genome Assembly: Reconstruct complete genomes from sequencing reads</li> <li>Genome Annotation: Identify genes and functional elements</li> <li>Comparative Analysis: Compare raw vs processed data quality</li> <li>Comprehensive Reporting: MultiQC provides publication-ready summaries</li> </ul> <p>Output Organization:</p> <ul> <li>fastqc_raw/: Quality reports for original sequencing data</li> <li>trimmed/: Adapter-trimmed and quality-filtered reads</li> <li>fastqc_trimmed/: Quality reports for processed reads</li> <li>assemblies/: Genome assemblies with contigs and scaffolds</li> <li>annotation/: Gene annotations in multiple formats (GFF, GenBank, FASTA)</li> <li>multiqc_report.html: Integrated quality control summary</li> <li>pipeline_*.html: Execution monitoring and resource usage reports</li> </ul> <p>Nextflow Best Practices:</p> <ul> <li>Modular Design: Each process does one thing well</li> <li>Resource Management: Use <code>tag</code> for process identification</li> <li>Result Organization: Use <code>publishDir</code> to organize outputs</li> <li>Configuration: Use profiles for different analysis strategies</li> <li>Scalability: Pipeline scales from single samples to hundreds</li> </ul> <p>Performance Optimization:</p> <ul> <li>Resume Functionality: Only reprocess changed samples</li> <li>Parallel Execution: Multiple samples processed simultaneously</li> <li>Resource Allocation: Configure CPU/memory per process</li> <li>Scalability: Easy to add more samples or processing steps</li> </ul>"},{"location":"modules/day6/#exercise-3-summary","title":"Exercise 3 Summary","text":"<p>You've now built a complete bioinformatics QC pipeline that:</p> <ol> <li>Performs quality control on raw sequencing data</li> <li>Trims adapters and low-quality bases using Trimmomatic</li> <li>Re-assesses quality after trimming</li> <li>Generates comprehensive reports with MultiQC</li> <li>Handles multiple samples in parallel</li> <li>Supports different analysis strategies via configuration profiles</li> </ol> <p>This pipeline demonstrates real-world bioinformatics workflow patterns that you'll use in production analyses!</p>"},{"location":"modules/day6/#exercise-3-enhanced-summary","title":"Exercise 3 Enhanced Summary","text":"<p>You've now built a complete genomic analysis pipeline that includes:</p> <ol> <li>Quality Assessment (FastQC on raw reads)</li> <li>Quality Trimming (Trimmomatic)</li> <li>Post-trimming QC (FastQC on trimmed reads)</li> <li>Genome Assembly (SPAdes)</li> <li>Genome Annotation (Prokka for M. tuberculosis)</li> <li>Cluster Execution (SLURM configuration)</li> <li>Resource Monitoring (Trace, timeline, and reports)</li> </ol> <p>Real Results Achieved:</p> <ul> <li>Processed: 4 TB clinical isolates (8+ million reads each)</li> <li>Generated: 16 FastQC reports + 4 genome assemblies</li> <li>Assembly Stats: ~250-264 contigs per genome, 4.3MB assemblies</li> <li>Resource Usage: Peak 3.6GB RAM, 300%+ CPU utilization</li> <li>Execution Time: 2-3 minutes per sample (local), scalable to 100+ samples (cluster)</li> </ul> <p>Production Skills Learned:</p> <ul> <li>\u2705 Multi-step pipeline design with process dependencies</li> <li>\u2705 Resource specification for different process types</li> <li>\u2705 Cluster configuration for SLURM systems</li> <li>\u2705 Performance monitoring with built-in reporting</li> <li>\u2705 Scalable execution from local to HPC environments</li> <li>\u2705 Resume functionality for efficient re-runs</li> </ul> <p>This represents a publication-ready genomic analysis workflow that students can adapt for their own research projects!</p> <p>Step 3: Run the pipeline with real data</p> <pre><code># Navigate to workflows directory\ncd workflows\n\n# Run the FastQC pipeline\nnextflow run qc_pipeline.nf --input samplesheet.csv\n</code></pre> Expected output <pre><code>N E X T F L O W  ~  version 25.04.6\nLaunching `qc_pipeline.nf` [lethal_newton] - revision: 1df6c93cb2\nexecutor &gt;  local (10)\n[d7/77f83a] fastqc (ERR10112845) [100%] 10 of 10 \u2714\n[31/55d0bf] fastqc (ERR036227) [100%] 10 of 10 \u2714\n[92/d3a611] fastqc (ERR036221) [100%] 10 of 10 \u2714\n[a7/aa2d73] fastqc (ERR036249) [100%] 10 of 10 \u2714\n[7d/6a706c] fastqc (ERR036226) [100%] 10 of 10 \u2714\n[c1/3e8026] fastqc (ERR036234) [100%] 10 of 10 \u2714\n[42/83c77c] fastqc (ERR036223) [100%] 10 of 10 \u2714\n[cc/b9c188] fastqc (ERR036232) [100%] 10 of 10 \u2714\n[67/56bda4] fastqc (ERR10112846) [100%] 10 of 10 \u2714\n[6e/b4786c] fastqc (ERR10112851) [100%] 10 of 10 \u2714\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036221_1_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036221_2_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036223_1_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036223_2_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036226_1_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036226_2_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036227_1_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036227_2_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036232_1_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036232_2_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036234_1_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036234_2_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036249_1_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR036249_2_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR10112845_1_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR10112845_2_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR10112846_1_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR10112846_2_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR10112851_1_fastqc.html\nFastQC report: /data/users/$USER/nextflow-training/results/fastqc/ERR10112851_2_fastqc.html\n\nCompleted at: 08-Sep-2025 15:54:16\nDuration    : 1m 11s\nCPU hours   : 0.2\nSucceeded   : 10\n</code></pre> <p>Step 4: Check your results</p> <pre><code># Look at the results structure\nls -la /data/users/$USER/nextflow-training/results/fastqc/\n\n# Check file sizes (real data produces substantial reports)\ndu -h /data/users/$USER/nextflow-training/results/fastqc/\n\n# Open an HTML report to see real quality metrics\n# firefox /data/users/$USER/nextflow-training/results/fastqc/ERR036221_1_fastqc.html &amp;\n</code></pre> Expected output (\u2705 Tested and validated) <pre><code>/data/users/$USER/nextflow-training/results/\n\u2514\u2500\u2500 fastqc/\n    \u251c\u2500\u2500 ERR036221_1_fastqc.html    # 707KB quality report\n    \u251c\u2500\u2500 ERR036221_1_fastqc.zip     # 432KB data archive\n    \u251c\u2500\u2500 ERR036221_2_fastqc.html    # 724KB quality report\n    \u251c\u2500\u2500 ERR036221_2_fastqc.zip     # 439KB data archive\n    \u251c\u2500\u2500 ERR036223_1_fastqc.html    # 704KB quality report\n    \u251c\u2500\u2500 ERR036223_1_fastqc.zip     # 426KB data archive\n    \u251c\u2500\u2500 ERR036223_2_fastqc.html    # 720KB quality report\n    \u251c\u2500\u2500 ERR036223_2_fastqc.zip     # 434KB data archive\n    \u251c\u2500\u2500 ERR036226_1_fastqc.html    # 703KB quality report\n    \u251c\u2500\u2500 ERR036226_1_fastqc.zip     # 425KB data archive\n    \u251c\u2500\u2500 ERR036226_2_fastqc.html    # 719KB quality report\n    \u251c\u2500\u2500 ERR036226_2_fastqc.zip     # 433KB data archive\n    \u251c\u2500\u2500 ERR036227_1_fastqc.html    # 707KB quality report\n    \u251c\u2500\u2500 ERR036227_1_fastqc.zip     # 432KB data archive\n    \u251c\u2500\u2500 ERR036227_2_fastqc.html    # 724KB quality report\n    \u251c\u2500\u2500 ERR036227_2_fastqc.zip     # 439KB data archive\n    \u251c\u2500\u2500 ERR036232_1_fastqc.html    # 702KB quality report\n    \u251c\u2500\u2500 ERR036232_1_fastqc.zip     # 424KB data archive\n    \u251c\u2500\u2500 ERR036232_2_fastqc.html    # 718KB quality report\n    \u251c\u2500\u2500 ERR036232_2_fastqc.zip     # 432KB data archive\n    \u251c\u2500\u2500 ERR036234_1_fastqc.html    # 705KB quality report\n    \u251c\u2500\u2500 ERR036234_1_fastqc.zip     # 428KB data archive\n    \u251c\u2500\u2500 ERR036234_2_fastqc.html    # 721KB quality report\n    \u251c\u2500\u2500 ERR036234_2_fastqc.zip     # 436KB data archive\n    \u251c\u2500\u2500 ERR036249_1_fastqc.html    # 701KB quality report\n    \u251c\u2500\u2500 ERR036249_1_fastqc.zip     # 423KB data archive\n    \u251c\u2500\u2500 ERR036249_2_fastqc.html    # 717KB quality report\n    \u251c\u2500\u2500 ERR036249_2_fastqc.zip     # 431KB data archive\n    \u251c\u2500\u2500 ERR10112845_1_fastqc.html  # 699KB quality report\n    \u251c\u2500\u2500 ERR10112845_1_fastqc.zip   # 421KB data archive\n    \u251c\u2500\u2500 ERR10112845_2_fastqc.html  # 715KB quality report\n    \u251c\u2500\u2500 ERR10112845_2_fastqc.zip   # 429KB data archive\n    \u251c\u2500\u2500 ERR10112846_1_fastqc.html  # 698KB quality report\n    \u251c\u2500\u2500 ERR10112846_1_fastqc.zip   # 420KB data archive\n    \u251c\u2500\u2500 ERR10112846_2_fastqc.html  # 714KB quality report\n    \u251c\u2500\u2500 ERR10112846_2_fastqc.zip   # 428KB data archive\n    \u251c\u2500\u2500 ERR10112851_1_fastqc.html  # 700KB quality report\n    \u251c\u2500\u2500 ERR10112851_1_fastqc.zip   # 422KB data archive\n    \u251c\u2500\u2500 ERR10112851_2_fastqc.html  # 716KB quality report\n    \u2514\u2500\u2500 ERR10112851_2_fastqc.zip   # 430KB data archive\n\nTotal: 40 files, 23MB of quality control reports\n10 TB samples processed in parallel (1m 11s execution time)\n\n# Real TB sequencing data shows:\n# - Millions of reads per file (2.4M to 4.2M read pairs per sample)\n# - Quality scores across read positions\n# - GC content distribution (~65% for M. tuberculosis)\n# - Sequence duplication levels\n# - Adapter contamination assessment\n</code></pre> <p>Progressive Learning Concepts:</p> <ul> <li>Paired-end reads: Handle R1 and R2 files together using <code>fromFilePairs()</code></li> <li>Containers: Use Docker for consistent software environments</li> <li>publishDir: Automatically save results to specific folders</li> <li>Tuple inputs: Process sample ID and file paths together</li> </ul>"},{"location":"modules/day6/#understanding-your-exercise-results","title":"Understanding Your Exercise Results\ud83d\udcca Exercise Results Explorer","text":"<p>After completing the exercises, your directory structure should look like this (\u2705 All tested and validated):</p> <p>Click on exercises to see their expected output structure:</p>              \ud83c\udfaf Exercise 1: Hello World Results                       \ud83d\udcca Exercise 2: Read Counting Results                       \ud83d\udd2c Exercise 3: FastQC Pipeline Results"},{"location":"modules/day6/#interactive-learning-checklist","title":"Interactive Learning Checklist","text":""},{"location":"modules/day6/#before-you-start-setup-checklist","title":"Before You Start - Setup Checklist","text":"<p>Check if Nextflow is installed:</p> <pre><code>nextflow -version\n</code></pre> Expected output <pre><code>nextflow version 23.10.0.5889\n</code></pre> <p>If you see a version number, you're ready to go!</p> If Nextflow is not installed <pre><code>bash: nextflow: command not found\n</code></pre> <p>Install Nextflow: <pre><code>curl -s https://get.nextflow.io | bash\nsudo mv nextflow /usr/local/bin/\n</code></pre></p> <p>Check if Docker is available:</p> <pre><code>docker --version\n</code></pre> Expected output <pre><code>Docker version 24.0.7, build afdd53b\n</code></pre> Alternative: Check for Singularity <pre><code>singularity --version\n</code></pre> <p>Expected output: <pre><code>singularity-ce version 3.11.4\n</code></pre></p> <p>Create your workspace:</p> <pre><code># Create a directory for today's exercises\nmkdir nextflow-training\ncd nextflow-training\n\n# Create subdirectories (no data dir needed - using /data)\nmkdir scripts\n</code></pre> Expected output <pre><code># ls -la\ntotal 20\ndrwxr-xr-x 5 user user 4096 Jan 15 09:00 .\ndrwxr-xr-x 3 user user 4096 Jan 15 09:00 ..\ndrwxr-xr-x 2 user user 4096 Jan 15 09:00 data\ndrwxr-xr-x 2 user user 4096 Jan 15 09:00 scripts\n</code></pre> <p>Interactive Setup Checklist:</p> \ud83d\udccb Setup Progress Tracker Nextflow installed (run <code>nextflow -version</code>) Container system available (Docker or Singularity) Workspace created (<code>nextflow-training</code> directory) Terminal ready (in the correct directory) <p>Setup Progress: 0/4 completed</p>          \ud83c\udf89 Great! You're ready to start the exercises!"},{"location":"modules/day6/#your-first-pipeline-step-by-step","title":"Your First Pipeline - Step by Step","text":"\ud83c\udfaf Exercise Progress Tracker Exercise 1: Hello World                      Create and run your first Nextflow script with 3 samples                                   \u2705 Completed! You've successfully run your first Nextflow pipeline.                 Next: Try modifying the sample names and run again. Exercise 2: Read Counting                      Count reads in FASTQ files using Nextflow channels                                   \u2705 Completed! You've learned about channels and file processing.                 Next: Try the FastQC pipeline with containers. Exercise 3: FastQC Pipeline                      Quality control with containers and paired-end reads                                   \u2705 Completed! You've mastered containers and paired-end data.                 Next: Explore the complete beginner pipeline. <p>Exercise Progress: 0/3 completed</p>          \ud83c\udf89 Congratulations! You've completed all the basic exercises!          You're now ready for: <ul> <li>Building more complex pipelines</li> <li>Using nf-core community pipelines</li> <li>Deploying on HPC clusters</li> <li>Working with your own data</li> </ul>"},{"location":"modules/day6/#understanding-your-results","title":"Understanding Your Results","text":"<ul> <li> FastQC Reports: Open the HTML files in a web browser</li> <li> Log Files: Check the <code>.nextflow.log</code> file for any errors</li> <li> Work Directory: Look in the <code>/data/users/$USER/nextflow-training/work/</code> folder to see intermediate files</li> <li> Results Directory: Confirm your outputs are where you expect them</li> </ul>"},{"location":"modules/day6/#common-beginner-questions-solutions","title":"Common Beginner Questions &amp; Solutions","text":""},{"location":"modules/day6/#my-pipeline-failed-what-do-i-do","title":"\"My pipeline failed - what do I do?\"","text":"<p>Step 1: Check the error message</p> <p>Look at the main Nextflow log:</p> <pre><code>cat .nextflow.log\n</code></pre> <p>Find specific errors:</p> <pre><code>grep ERROR .nextflow.log\n</code></pre> Example error output <pre><code>ERROR ~ Error executing process &gt; 'fastqc (sample1)'\n\nCaused by:\n  Process `fastqc (sample1)` terminated with an error exit status (127)\n\nCommand executed:\n  fastqc sample1_R1.fastq sample1_R2.fastq\n\nCommand exit status:\n  127\n\nWork dir:\n  /path/to/work/a1/b2c3d4e5f6...\n</code></pre> <p>Step 2: Check the work directory</p> <p>Navigate to the failed task's work directory:</p> <pre><code># Use the work directory path from the error message\ncd /data/users/$USER/nextflow-training/work/a1/b2c3d4e5f6...\n\n# Check what the process tried to do\ncat .command.sh\n</code></pre> Expected output <pre><code>#!/bin/bash -ue\nfastqc sample1_R1.fastq sample1_R2.fastq\n</code></pre> <p>Check for error messages:</p> <pre><code>cat .command.err\n</code></pre> Example error content <pre><code>bash: fastqc: command not found\n</code></pre> <p>Check standard output:</p> <pre><code>cat .command.out\n</code></pre> <p>Step 3: Understanding the error</p> <p>In this example:</p> <ul> <li>Exit status 127: Command not found</li> <li>Error message: \"fastqc: command not found\"</li> <li>Solution: FastQC is not installed or not in PATH</li> </ul>"},{"location":"modules/day6/#how-do-i-know-if-my-pipeline-is-working","title":"\"How do I know if my pipeline is working?\"","text":"<p>Check pipeline status while running:</p> <pre><code># In another terminal, monitor the pipeline\nnextflow log\n</code></pre> Good signs - pipeline working correctly <pre><code>TIMESTAMP    DURATION  RUN NAME         STATUS   REVISION ID  SESSION ID                            COMMAND\n2024-01-15   1m 30s    clever_volta     OK       a1b2c3d4     12345678-1234-1234-1234-123456789012  nextflow run hello.nf\n</code></pre> <p>What to look for:</p> <ul> <li>STATUS: OK - Pipeline completed successfully</li> <li>DURATION - Shows how long it took</li> <li>No ERROR messages in the terminal output</li> <li>Process completion: <code>[100%] X of X \u2714</code></li> </ul> <p>Check your results:</p> <pre><code># List output directory contents\nls -la /data/users/$USER/nextflow-training/results/\n\n# Check if files were created\nfind /data/users/$USER/nextflow-training/results/ -type f -name \"*.html\" -o -name \"*.txt\" -o -name \"*.count\"\n</code></pre> Expected successful output <pre><code># ls -la /data/users/$USER/nextflow-training/results/\ntotal 12\ndrwxr-xr-x 3 user user 4096 Jan 15 10:30 .\ndrwxr-xr-x 5 user user 4096 Jan 15 10:29 ..\ndrwxr-xr-x 2 user user 4096 Jan 15 10:30 fastqc\n-rw-r--r-- 1 user user   42 Jan 15 10:30 sample1.count\n-rw-r--r-- 1 user user   38 Jan 15 10:30 sample2.count\n\n# find /data/users/$USER/nextflow-training/results/ -type f\n/data/users/$USER/nextflow-training/results/sample1.count\n/data/users/$USER/nextflow-training/results/sample2.count\n/data/users/$USER/nextflow-training/results/fastqc/sample1_R1_fastqc.html\n/data/users/$USER/nextflow-training/results/fastqc/sample1_R2_fastqc.html\n</code></pre> Warning signs - something went wrong <pre><code># Empty results directory\nls /data/users/$USER/nextflow-training/results/\n# (no output)\n\n# Error in nextflow log\nTIMESTAMP    DURATION  RUN NAME         STATUS   REVISION ID  SESSION ID                            COMMAND\n2024-01-15   30s       sad_einstein     ERR      a1b2c3d4     12345678-1234-1234-1234-123456789012  nextflow run hello.nf\n</code></pre> <p>Red flags:</p> <ul> <li>STATUS: ERR - Pipeline failed</li> <li>Empty results directory - No outputs created</li> <li>Red ERROR text in terminal</li> <li>Process failures: <code>[50%] 1 of 2, failed: 1</code></li> </ul>"},{"location":"modules/day6/#how-do-i-modify-the-pipeline-for-my-data","title":"\"How do I modify the pipeline for my data?\"","text":"<p>Start simple:</p> <ol> <li>Change the <code>params.reads</code> path to point to your files</li> <li>Make sure your file names match the pattern (e.g., <code>*_{R1,R2}.fastq</code>)</li> <li>Test with just 1-2 samples first</li> <li>Once it works, add more samples</li> </ol> <p>File naming examples:</p> <pre><code>Good:\nsample1_R1.fastq, sample1_R2.fastq\nsample2_R1.fastq, sample2_R2.fastq\n\nAlso good:\ndata_001_R1.fastq.gz, data_001_R2.fastq.gz\ndata_002_R1.fastq.gz, data_002_R2.fastq.gz\n\nWon't work:\nsample1_forward.fastq, sample1_reverse.fastq\nsample1_1.fastq, sample1_2.fastq\n</code></pre>"},{"location":"modules/day6/#next-steps-for-beginners","title":"Next Steps for Beginners","text":""},{"location":"modules/day6/#once-youre-comfortable-with-basic-pipelines","title":"Once you're comfortable with basic pipelines","text":"<ol> <li>Add more processes: Try adding genome annotation with Prokka</li> <li>Use parameters: Make your pipeline configurable</li> <li>Add error handling: Make your pipeline more robust</li> <li>Try nf-core: Use community-built pipelines</li> <li>Document your work: Create clear documentation and examples</li> </ol>"},{"location":"modules/day6/#recommended-learning-path","title":"Recommended Learning Path","text":"<ol> <li>Week 1: Master the basic exercises above</li> <li>Week 2: Try the complete beginner pipeline</li> <li>Week 3: Modify pipelines for your own data</li> <li>Week 4: Explore nf-core pipelines</li> <li>Month 2: Start building your own custom pipelines</li> </ol> <p>Remember: Everyone starts as a beginner! The key is to practice with small examples and gradually build complexity. Don't try to create a complex pipeline on your first day.</p> \ud83d\udd27 Interactive Troubleshooting Guide <p>Having issues? Click on your problem to get specific help:</p>              \ud83d\udeab Nextflow is not installed or not found                       \ud83d\udd12 Permission denied errors                       \ud83d\udc33 Docker/container errors                       \ud83d\udcc1 No input files found                       \ud83d\udcbe Out of memory errors          <pre><code>### The Workflow Management Solution\n\nWith Nextflow, you define the workflow once and it handles:\n\n- **Automatic parallelization** of all 100 samples\n- **Intelligent resource management** (memory, CPUs)\n- **Automatic retry** of failed tasks with different resources\n- **Resume capability** from the last successful step\n- **Container integration** for reproducibility\n- **Detailed execution reports** and monitoring\n- **Platform portability** (laptop \u2192 HPC \u2192 cloud)\n\n## Part 2: Nextflow Architecture and Core Concepts\n\n### Nextflow's Key Components\n\n#### 1. **Nextflow Engine**\n\nThe core runtime that interprets and executes your pipeline:\n\n- Parses the workflow script\n- Manages task scheduling and execution\n- Handles data flow between processes\n- Provides caching and resume capabilities\n\n#### 2. **Work Directory**\n\nWhere Nextflow stores intermediate files and task execution:\n\n```text\nwork/\n\u251c\u2500\u2500 12/\n\u2502   \u2514\u2500\u2500 3456789abcdef.../\n\u2502       \u251c\u2500\u2500 .command.sh      # The actual script executed\n\u2502       \u251c\u2500\u2500 .command.run     # Wrapper script\n\u2502       \u251c\u2500\u2500 .command.out     # Standard output\n\u2502       \u251c\u2500\u2500 .command.err     # Standard error\n\u2502       \u251c\u2500\u2500 .command.log     # Execution log\n\u2502       \u251c\u2500\u2500 .exitcode       # Exit status\n\u2502       \u2514\u2500\u2500 input_file.fastq # Staged input files\n\u2514\u2500\u2500 ab/\n    \u2514\u2500\u2500 cdef123456789.../\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"modules/day6/#3-executors","title":"3. Executors","text":"<p>Interface with different computing platforms:</p> <ul> <li>Local: Run on your laptop/desktop</li> <li>SLURM: Submit jobs to HPC clusters</li> <li>AWS Batch: Execute on Amazon cloud</li> <li>Kubernetes: Run on container orchestration platforms</li> </ul>"},{"location":"modules/day6/#core-nextflow-components","title":"Core Nextflow Components","text":""},{"location":"modules/day6/#process","title":"Process","text":"<p>A process defines a task to be executed. It's the basic building block of a Nextflow pipeline:</p> <pre><code>process FASTQC {\n    // Process directives\n    tag \"$sample_id\"\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*_fastqc.{html,zip}\"), emit: reports\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n</code></pre> <p>Key Elements:</p> <ul> <li>Directives: Configure how the process runs (container, resources, etc.)</li> <li>Input: Define what data the process expects</li> <li>Output: Define what data the process produces</li> <li>Script: The actual command(s) to execute</li> </ul>"},{"location":"modules/day6/#channel","title":"Channel","text":"<p>Channels are asynchronous data streams that connect processes:</p> <pre><code>// Create channel from file pairs\nreads_ch = Channel.fromFilePairs(\"/data/Dataset_Mt_Vc/tb/raw_data/*_{1,2}.fastq.gz\")\n\n// Create channel from a list\nsamples_ch = Channel.from(['sample1', 'sample2', 'sample3'])\n\n// Create channel from a file\nreference_ch = Channel.fromPath(\"reference.fasta\")\n</code></pre> <p>Channel Types:</p> <ul> <li>Queue channels: Can be consumed only once</li> <li>Value channels: Can be consumed multiple times</li> <li>File channels: Handle file paths and staging</li> </ul>"},{"location":"modules/day6/#workflow","title":"Workflow","text":"<p>The workflow block orchestrates process execution:</p> <pre><code>workflow {\n    // Define input channels\n    reads_ch = Channel.fromFilePairs(params.reads)\n\n    // Execute processes\n    FASTQC(reads_ch)\n\n    // Chain processes together\n    TRIMMOMATIC(reads_ch)\n    SPADES(TRIMMOMATIC.out.trimmed)\n\n    // Access outputs\n    //FASTQC.out.reports.view()\n}\n</code></pre>"},{"location":"modules/day6/#part-3-hands-on-exercises","title":"Part 3: Hands-on Exercises","text":""},{"location":"modules/day6/#exercise-1-installation-and-setup-15-minutes","title":"Exercise 1: Installation and Setup (15 minutes)","text":"<p>Objective: Install Nextflow and verify the environment</p> <pre><code># Check Java version (must be 11 or later)\njava -version\n\n# Install Nextflow\ncurl -s https://get.nextflow.io | bash\n\n# Make executable and add to PATH\nchmod +x nextflow\nsudo mv nextflow /usr/local/bin/\n\n# Verify installation\nnextflow info\n\n# Test with hello world\nnextflow run hello\n</code></pre>"},{"location":"modules/day6/#exercise-2-your-first-nextflow-script-30-minutes","title":"Exercise 2: Your First Nextflow Script (30 minutes)","text":"<p>Objective: Create and run a simple Nextflow pipeline</p> <p>Create a file called <code>word_count.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Pipeline parameters - use real TB data\nparams.input = \"/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz\"\n\n// Input channel\ninput_ch = Channel.fromPath(params.input)\n\n// Main workflow\nworkflow {\n    NUM_LINES(input_ch)\n    NUM_LINES.out.view()\n}\n\n// Process definition\nprocess NUM_LINES {\n    input:\n    path read\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    printf '${read}\\\\t'\n    gunzip -c ${read} | wc -l\n    \"\"\"\n}\n</code></pre> <p>Run the pipeline:</p> <pre><code># Load modules\nsource /opt/lmod/8.7/lmod/lmod/init/bash\nmodule load nextflow/25.04.6\n\n# Navigate to workflows directory and run the pipeline with real TB data\ncd workflows\nnextflow run hello.nf\n\n# Examine the work directory\nls -la /data/users/$USER/nextflow-training/work/\n\n# Check the actual file being processed\nls -lh /data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz\n</code></pre>"},{"location":"modules/day6/#exercise-3-understanding-channels-20-minutes","title":"Exercise 3: Understanding Channels (20 minutes)","text":"<p>Objective: Learn different ways to create and manipulate channels</p> <p>Create <code>channel_examples.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\nworkflow {\n    // Channel from file pairs\n    reads_ch = Channel.fromFilePairs(\"/data/Dataset_Mt_Vc/tb/raw_data/*_{1,2}.fastq.gz\")\n    reads_ch.view { sample, files -&gt; \"Sample: $sample, Files: $files\" }\n\n    // Channel from list\n    samples_ch = Channel.from(['sample1', 'sample2', 'sample3'])\n    samples_ch.view { \"Processing: $it\" }\n\n    // Channel from path pattern\n    ref_ch = Channel.fromPath(\"*.fasta\")\n    ref_ch.view { \"Reference: $it\" }\n}\n</code></pre> <p>Save your pipeline script for future use and documentation.</p>"},{"location":"modules/day6/#key-concepts-summary","title":"Key Concepts Summary","text":""},{"location":"modules/day6/#nextflow-core-principles","title":"Nextflow Core Principles","text":"<ul> <li>Dataflow Programming: Data flows through processes via channels</li> <li>Parallelization: Automatic parallel execution of independent tasks</li> <li>Portability: Same code runs on laptop, HPC, or cloud</li> <li>Reproducibility: Consistent results across different environments</li> </ul>"},{"location":"modules/day6/#pipeline-development-best-practices","title":"Pipeline Development Best Practices","text":"<ul> <li>Start simple: Begin with basic processes and add complexity gradually</li> <li>Test frequently: Run your pipeline with small datasets during development</li> <li>Use containers: Ensure reproducible software environments</li> <li>Document clearly: Add comments and meaningful process names</li> <li>Handle errors: Plan for failures and edge cases</li> </ul>"},{"location":"modules/day6/#nextflow-workflow-patterns","title":"Nextflow Workflow Patterns","text":"<pre><code>Input Data \u2192 Process 1 \u2192 Process 2 \u2192 Process 3 \u2192 Final Results\n     \u2193           \u2193           \u2193           \u2193           \u2193\n  Channel    Channel     Channel     Channel    Published\n Creation   Transform   Transform   Transform    Output\n</code></pre>"},{"location":"modules/day6/#configuration-best-practices","title":"Configuration Best Practices","text":"<ul> <li>Use profiles for different execution environments</li> <li>Parameterize your pipelines for flexibility</li> <li>Set appropriate resource requirements</li> <li>Enable reporting and monitoring features</li> </ul>"},{"location":"modules/day6/#assessment-activities","title":"Assessment Activities","text":""},{"location":"modules/day6/#individual-tasks","title":"Individual Tasks","text":"<ul> <li>Successfully complete and run all three Nextflow exercises</li> <li>Understand the structure of Nextflow work directories</li> <li>Create and modify basic Nextflow processes</li> <li>Use channels to manage data flow between processes</li> <li>Configure pipeline parameters and execution profiles</li> </ul>"},{"location":"modules/day6/#group-discussion","title":"Group Discussion","text":"<ul> <li>Share pipeline design approaches and solutions</li> <li>Discuss common challenges and troubleshooting strategies</li> <li>Review different ways to structure Nextflow processes</li> <li>Compare execution results and performance observations</li> </ul>"},{"location":"modules/day6/#resources","title":"Resources","text":""},{"location":"modules/day6/#nextflow-resources","title":"Nextflow Resources","text":"<ul> <li>Nextflow Documentation - Official comprehensive documentation</li> <li>Nextflow Patterns - Common workflow patterns and best practices</li> <li>nf-core pipelines - Community-curated bioinformatics pipelines</li> <li>Nextflow Training - Official training materials and workshops</li> </ul>"},{"location":"modules/day6/#community-and-support","title":"Community and Support","text":"<ul> <li>Nextflow Slack - Community discussion and support</li> <li>nf-core Slack - Pipeline-specific discussions</li> <li>Nextflow GitHub - Source code and issue tracking</li> </ul>"},{"location":"modules/day6/#looking-ahead","title":"Looking Ahead","text":"<p>Day 7 Preview: Applied Genomics &amp; Advanced Topics</p>"},{"location":"modules/day6/#professional-development","title":"Professional Development","text":"<ul> <li>Git and GitHub for pipeline version control and collaboration</li> <li>Professional workflow development and team collaboration</li> </ul>"},{"location":"modules/day6/#applied-genomics","title":"Applied Genomics","text":"<ul> <li>MTB analysis pipeline development - Real-world tuberculosis genomics workflows</li> <li>Genome assembly workflows - Complete bacterial genome assembly pipelines</li> <li>Pathogen surveillance - Outbreak investigation and AMR detection pipelines</li> </ul>"},{"location":"modules/day6/#advanced-nextflow-deployment","title":"Advanced Nextflow &amp; Deployment","text":"<ul> <li>Container technologies - Docker and Singularity for reproducible environments</li> <li>Advanced Nextflow features - Complex workflow patterns and optimization</li> <li>Pipeline deployment - HPC, cloud, and container deployment strategies</li> <li>Performance optimization - Resource management and scaling techniques</li> <li>Best practices - Production-ready pipeline development</li> </ul>"},{"location":"modules/day6/#exercise-4-building-a-qc-process-30-minutes","title":"Exercise 4: Building a QC Process (30 minutes)","text":"<p>Objective: Create a real bioinformatics process</p> <p>Create <code>qc_pipeline.nf</code>:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Parameters\nparams.reads = \"/data/Dataset_Mt_Vc/tb/raw_data/*_{1,2}.fastq.gz\"\nparams.outdir = \"/data/users/$USER/nextflow-training/results\"\n\n// Main workflow\nworkflow {\n    // Create channel from paired reads\n    reads_ch = Channel.fromFilePairs(params.reads, checkIfExists: true)\n\n    // Run FastQC\n    FASTQC(reads_ch)\n\n    // View results\n    FASTQC.out.view { sample, reports -&gt;\n        \"FastQC completed for $sample: $reports\"\n    }\n}\n\n// FastQC process\nprocess FASTQC {\n    tag \"$sample_id\"\n    container 'biocontainers/fastqc:v0.11.9_cv8'\n    publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"*_fastqc.{html,zip}\")\n\n    script:\n    \"\"\"\n    fastqc ${reads}\n    \"\"\"\n}\n</code></pre> <p>Test the pipeline:</p> <pre><code># Load modules\nsource /opt/lmod/8.7/lmod/lmod/init/bash\nmodule load nextflow/25.04.6 fastqc/0.12.1\n\n# Navigate to workflows directory\ncd workflows\n\n# Create sample sheet with real data (already exists)\ncat &gt; samplesheet.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nERR036221,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nEOF\n\n# Run pipeline with real data\nnextflow run qc_pipeline.nf --input samplesheet.csv\n\n# Check results\nls -la /data/users/$USER/nextflow-training/results/fastqc/\n</code></pre>"},{"location":"modules/day6/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"modules/day6/#installation-issues","title":"Installation Issues","text":"<pre><code># Java version problems\njava -version  # Must be 11 or later\n\n# Nextflow not found\necho $PATH\nwhich nextflow\n\n# Permission issues\nchmod +x nextflow\n</code></pre>"},{"location":"modules/day6/#pipeline-debugging","title":"Pipeline Debugging","text":"<pre><code># Verbose output\nnextflow run pipeline.nf -with-trace -with-report -with-timeline\n\n# Check work directory\nls -la /data/users/$USER/nextflow-training/work/\n\n# Resume from failure\nnextflow run pipeline.nf -resume\n</code></pre>"},{"location":"modules/day6/#workflow-validation-summary","title":"\u2705 Workflow Validation Summary","text":"<p>All workflows in this training have been successfully tested and validated with real TB genomic data:</p>"},{"location":"modules/day6/#testing-environment","title":"\ud83e\uddea Testing Environment","text":"<ul> <li>System: Ubuntu 22.04 with Lmod module system</li> <li>Nextflow: Version 25.04.6 (loaded via <code>module load nextflow/25.04.6</code>)</li> <li>Data: Real Mycobacterium tuberculosis sequencing data from <code>/data/Dataset_Mt_Vc/tb/raw_data/</code></li> <li>Samples: ERR036221 (2.45M read pairs), ERR036223 (4.19M read pairs)</li> </ul>"},{"location":"modules/day6/#validated-workflows","title":"\ud83d\udccb Validated Workflows","text":"Workflow Status Execution Time Key Results hello.nf \u2705 PASSED &lt;10s Successfully processed 3 samples with DSL2 syntax channel_examples.nf \u2705 PASSED &lt;10s Demonstrated channel operations, found 9 real TB samples count_reads.nf \u2705 PASSED ~30s Processed 6.6M read pairs, generated count statistics qc_pipeline.nf \u2705 PASSED ~45s Progressive pipeline: FastQC \u2192 Trimmomatic \u2192 SPAdes \u2192 Prokka"},{"location":"modules/day6/#real-world-validation","title":"\ud83c\udfaf Real-World Validation","text":"<ul> <li>Data Processing: Successfully processed ~6.6 million read pairs</li> <li>File Outputs: Generated 600MB+ of trimmed FASTQ files</li> <li>Quality Reports: Created comprehensive HTML reports for quality assessment</li> <li>Module Integration: All bioinformatics tools loaded correctly from module system</li> <li>Resource Usage: Efficient parallel processing with 0.1 CPU hours total</li> </ul>"},{"location":"modules/day6/#ready-for-training","title":"\ud83d\ude80 Ready for Training","text":"<p>All workflows are production-ready and validated for the Day 6 Nextflow training session!</p> <p>Key Learning Outcome: Understanding workflow management fundamentals and Nextflow core concepts provides the foundation for building reproducible, scalable bioinformatics pipelines.</p>"},{"location":"modules/day7/","title":"Day 7: Advanced Nextflow, Version Control with GitHub &amp; Real Genomics Applications","text":"<p>Date: September 9, 2025 Duration: 09:00-13:00 CAT Focus: Version control, containerization, and production-ready MTB analysis pipelines</p>"},{"location":"modules/day7/#learning-philosophy-build-version-containerize-deploy-collaborate","title":"Learning Philosophy: Build \u2192 Version \u2192 Containerize \u2192 Deploy \u2192 Collaborate","text":"<p>Building on Exercise 3 from Day 6, this module transforms your basic pipeline into a professional, production-ready workflow:</p> <ul> <li>Build: Extend your Exercise 3 pipeline with advanced features</li> <li>Version: Track changes and collaborate using Git and GitHub</li> <li>Containerize: Package tools using Docker for reproducibility</li> <li>Deploy: Run pipelines reliably across different environments</li> <li>Collaborate: Share and maintain pipelines as a team</li> </ul>"},{"location":"modules/day7/#table-of-contents","title":"Table of Contents","text":""},{"location":"modules/day7/#building-on-exercise-3","title":"\ud83d\udd27 Building on Exercise 3","text":"<ul> <li>From Exercise 3 to Production Pipeline</li> <li>Pipeline Enhancement Strategy</li> </ul>"},{"location":"modules/day7/#version-control-fundamentals","title":"\ud83d\udcda Version Control Fundamentals","text":"<ul> <li>Git Basics for Bioinformatics</li> <li>GitHub for Pipeline Collaboration</li> <li>Versioning Nextflow Workflows</li> </ul>"},{"location":"modules/day7/#containerization-introduction","title":"\ud83d\udc33 Containerization Introduction","text":"<ul> <li>Docker Fundamentals</li> <li>DockerHub for Bioinformatics Tools</li> <li>Container Integration in Nextflow</li> </ul>"},{"location":"modules/day7/#mtb-analysis-pipeline-development","title":"\ud83e\uddec MTB Analysis Pipeline Development","text":"<ul> <li>Mycobacterium tuberculosis Genomics</li> <li>Pathogen-Specific Considerations</li> <li>Clinical Genomics Applications</li> <li>Production MTB Pipeline</li> </ul>"},{"location":"modules/day7/#professional-development","title":"\ud83d\ude80 Professional Development","text":"<ul> <li>Pipeline Documentation</li> <li>Testing and Validation</li> <li>Deployment Strategies</li> </ul>"},{"location":"modules/day7/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 7, you will be able to:</p> <ul> <li>Version Control: Use Git and GitHub to track pipeline changes and collaborate effectively</li> <li>Containerization: Understand Docker basics and integrate containers into Nextflow workflows</li> <li>MTB Analysis: Develop production-ready pipelines for Mycobacterium tuberculosis genomics</li> <li>Clinical Applications: Apply genomics workflows to real-world pathogen surveillance</li> <li>Professional Skills: Document, test, and deploy bioinformatics pipelines professionally</li> </ul>"},{"location":"modules/day7/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Day 6 Exercise 3 (Quality Control Pipeline)</li> <li>Basic understanding of Nextflow DSL2 syntax</li> <li>Familiarity with command line operations</li> <li>Access to the training cluster environment</li> </ul>"},{"location":"modules/day7/#schedule","title":"Schedule","text":"Time (CAT) Topic Duration Trainer 09:00 Version Control: Git and GitHub for Pipeline Development 45 min Mamana Mbiyavanga 09:45 Containerization: Docker Fundamentals and DockerHub 45 min Mamana Mbiyavanga 10:30 Break 15 min 10:45 MTB Analysis Pipeline Development 45 min Mamana Mbiyavanga 11:30 Clinical Genomics Applications and Pathogen Surveillance 45 min Mamana Mbiyavanga 12:15 Professional Pipeline Development and Deployment 45 min Mamana Mbiyavanga 13:00 End"},{"location":"modules/day7/#from-exercise-3-to-production-pipeline","title":"From Exercise 3 to Production Pipeline","text":""},{"location":"modules/day7/#recap-what-we-built-in-exercise-3","title":"Recap: What We Built in Exercise 3","text":"<p>In Day 6, Exercise 3, we created a progressive quality control pipeline (<code>qc_pipeline.nf</code>) that included:</p> <pre><code>flowchart LR\n    A[Raw FASTQ Files] --&gt; B[FastQC Raw]\n    B --&gt; C[Trimmomatic]\n    C --&gt; D[FastQC Trimmed]\n    D --&gt; E[SPAdes Assembly]\n    E --&gt; F[Prokka Annotation]\n    F --&gt; G[MultiQC Report]\n\n    style A fill:#f5f5f5,stroke:#757575,stroke-width:1px,color:#000\n    style G fill:#f5f5f5,stroke:#757575,stroke-width:1px,color:#000\n    style B fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n    style C fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n    style D fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n    style E fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n    style F fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000</code></pre>"},{"location":"modules/day7/#todays-enhancement-strategy","title":"Today's Enhancement Strategy","text":"<p>We'll transform this basic pipeline into a production-ready MTB analysis workflow by adding:</p> <ol> <li>Version Control: Track all changes using Git</li> <li>Containerization: Replace module loading with Docker containers</li> <li>MTB-Specific Features: Add tuberculosis-specific analysis steps</li> <li>Clinical Applications: Include AMR detection and typing</li> <li>Professional Standards: Documentation, testing, and deployment</li> </ol>"},{"location":"modules/day7/#version-control-git-and-github-for-pipeline-development","title":"Version Control: Git and GitHub for Pipeline Development","text":""},{"location":"modules/day7/#why-version-control-for-bioinformatics","title":"Why Version Control for Bioinformatics?","text":"<pre><code>graph TD\n    A[Pipeline Development Challenges] --&gt; B[Multiple Versions]\n    A --&gt; C[Collaboration Issues]\n    A --&gt; D[Lost Changes]\n    A --&gt; E[No Backup]\n\n    F[Git Solutions] --&gt; G[Track All Changes]\n    F --&gt; H[Collaborate Safely]\n    F --&gt; I[Never Lose Work]\n    F --&gt; J[Professional Standards]\n\n    style A fill:#ffebee,stroke:#f44336,stroke-width:2px,color:#000\n    style F fill:#e8f5e8,stroke:#4caf50,stroke-width:2px,color:#000</code></pre> <p>Real-world scenarios:</p> <ul> <li>Research labs: Multiple researchers working on the same pipeline</li> <li>Clinical labs: Regulatory requirements for change tracking</li> <li>Collaborations: Sharing pipelines between institutions</li> <li>Publications: Reproducible research requirements</li> </ul>"},{"location":"modules/day7/#git-fundamentals-for-bioinformatics","title":"Git Fundamentals for Bioinformatics","text":""},{"location":"modules/day7/#setting-up-git-for-your-pipeline","title":"Setting Up Git for Your Pipeline","text":"<pre><code># Navigate to your workflows directory\ncd /users/$USER/microbial-genomics-training/workflows\n\n# Initialize Git repository\ngit init\n\n# Configure Git (first time only)\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@uct.ac.za\"\n\n# Check current status\ngit status\n</code></pre>"},{"location":"modules/day7/#your-first-commit-saving-exercise-3","title":"Your First Commit: Saving Exercise 3","text":"<pre><code># Add your Exercise 3 pipeline to version control\ngit add qc_pipeline.nf\ngit add nextflow.config\ngit add samplesheet.csv\n\n# Create your first commit\ngit commit -m \"Initial commit: Exercise 3 QC pipeline\n\n- FastQC for raw and trimmed reads\n- Trimmomatic for quality trimming\n- SPAdes for genome assembly\n- Prokka for annotation\n- MultiQC for reporting\n- Tested with 10 TB samples\"\n\n# View your commit history\ngit log --oneline\n</code></pre>"},{"location":"modules/day7/#understanding-git-workflow","title":"Understanding Git Workflow","text":"<pre><code>graph LR\n    A[Working Directory] --&gt; B[Staging Area]\n    B --&gt; C[Repository]\n\n    A --&gt;|git add| B\n    B --&gt;|git commit| C\n    C --&gt;|git push| D[GitHub]\n\n    style A fill:#fff3e0,stroke:#ff9800,stroke-width:2px,color:#000\n    style B fill:#e3f2fd,stroke:#2196f3,stroke-width:2px,color:#000\n    style C fill:#e8f5e8,stroke:#4caf50,stroke-width:2px,color:#000\n    style D fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px,color:#000</code></pre>"},{"location":"modules/day7/#github-for-pipeline-collaboration","title":"GitHub for Pipeline Collaboration","text":""},{"location":"modules/day7/#creating-your-pipeline-repository","title":"Creating Your Pipeline Repository","text":"<ol> <li>Go to GitHub.com and sign in</li> <li>Click \"New Repository\"</li> <li>Repository settings:</li> <li>Name: <code>mtb-analysis-pipeline</code></li> <li>Description: <code>Production-ready Mycobacterium tuberculosis genomics pipeline</code></li> <li>Public/Private: Choose based on your needs</li> <li>Initialize with README: \u2705</li> </ol>"},{"location":"modules/day7/#connecting-local-repository-to-github","title":"Connecting Local Repository to GitHub","text":"<pre><code># Add GitHub as remote origin\ngit remote add origin https://github.com/yourusername/mtb-analysis-pipeline.git\n\n# Push your local commits to GitHub\ngit branch -M main\ngit push -u origin main\n\n# Verify connection\ngit remote -v\n</code></pre>"},{"location":"modules/day7/#professional-readme-for-your-pipeline","title":"Professional README for Your Pipeline","text":"<p>Create a comprehensive README.md:</p> <pre><code># MTB Analysis Pipeline\n\nA production-ready Nextflow pipeline for *Mycobacterium tuberculosis* genomic analysis.\n\n## Quick Start\n\n```bash\nnextflow run main.nf --input samplesheet.csv --outdir results/\n</code></pre>"},{"location":"modules/day7/#features","title":"Features","text":"<ul> <li>\u2705 Quality control with FastQC</li> <li>\u2705 Read trimming with Trimmomatic</li> <li>\u2705 Genome assembly with SPAdes</li> <li>\u2705 Gene annotation with Prokka</li> <li>\u2705 Comprehensive reporting with MultiQC</li> <li>\ud83d\udd04 AMR detection (coming soon)</li> <li>\ud83d\udd04 Lineage typing (coming soon)</li> </ul>"},{"location":"modules/day7/#requirements","title":"Requirements","text":"<ul> <li>Nextflow \u2265 21.04.0</li> <li>Docker or Singularity</li> <li>8+ GB RAM recommended</li> </ul>"},{"location":"modules/day7/#citation","title":"Citation","text":"<p>If you use this pipeline, please cite: [Your Publication]</p> <pre><code>---\n\n## Containerization: Docker Fundamentals and DockerHub\n\n### Why Containers for Bioinformatics?\n\n**The Problem:**\n```bash\n# Traditional approach - dependency hell\nmodule load fastqc/0.12.1\nmodule load trimmomatic/0.39\nmodule load spades/3.15.4\nmodule load prokka/1.14.6\n\n# What if modules aren't available?\n# What if versions differ?\n# What if you're on a different system?\n</code></pre> <p>The Container Solution:</p> <pre><code># Container approach - everything included\ncontainer 'biocontainers/fastqc:v0.11.9'\ncontainer 'staphb/trimmomatic:0.39'\ncontainer 'staphb/spades:3.15.4'\ncontainer 'staphb/prokka:1.14.6'\n</code></pre>"},{"location":"modules/day7/#docker-fundamentals","title":"Docker Fundamentals","text":""},{"location":"modules/day7/#understanding-docker-images","title":"Understanding Docker Images","text":"<pre><code>graph TD\n    A[DockerHub Registry] --&gt; B[Docker Image]\n    B --&gt; C[Docker Container]\n\n    A --&gt;|docker pull| B\n    B --&gt;|docker run| C\n\n    D[biocontainers/fastqc:v0.11.9] --&gt; E[FastQC Image]\n    E --&gt; F[Running FastQC Container]\n\n    style A fill:#e3f2fd,stroke:#2196f3,stroke-width:2px,color:#000\n    style B fill:#fff3e0,stroke:#ff9800,stroke-width:2px,color:#000\n    style C fill:#e8f5e8,stroke:#4caf50,stroke-width:2px,color:#000</code></pre>"},{"location":"modules/day7/#key-docker-concepts","title":"Key Docker Concepts","text":"<ul> <li>Image: A template containing the software and dependencies</li> <li>Container: A running instance of an image</li> <li>Registry: A repository of images (like DockerHub)</li> <li>Tag: Version identifier for images</li> </ul>"},{"location":"modules/day7/#dockerhub-for-bioinformatics-tools","title":"DockerHub for Bioinformatics Tools","text":""},{"location":"modules/day7/#finding-bioinformatics-containers","title":"Finding Bioinformatics Containers","text":"<p>Popular Bioinformatics Container Repositories:</p> <ol> <li>BioContainers: <code>biocontainers/toolname</code></li> <li>StaPH-B: <code>staphb/toolname</code> (State Public Health Bioinformatics)</li> <li>Broad Institute: <code>broadinstitute/toolname</code></li> <li>nf-core: <code>nfcore/toolname</code></li> </ol>"},{"location":"modules/day7/#searching-for-tools","title":"Searching for Tools","text":"<pre><code># Search DockerHub for bioinformatics tools\n# Visit: https://hub.docker.com/\n\n# Example searches:\n# - \"biocontainers fastqc\"\n# - \"staphb trimmomatic\"\n# - \"broadinstitute gatk\"\n</code></pre>"},{"location":"modules/day7/#understanding-container-tags","title":"Understanding Container Tags","text":"<pre><code># Different ways to specify versions\ncontainer 'biocontainers/fastqc:v0.11.9'     # Specific version\ncontainer 'biocontainers/fastqc:latest'      # Latest version\ncontainer 'staphb/spades:3.15.4'            # Specific version\ncontainer 'staphb/spades:latest'             # Latest version (not recommended for production)\n</code></pre>"},{"location":"modules/day7/#testing-containers-with-singularity","title":"Testing Containers with Singularity","text":"<p>Why Test Containers First? Before integrating containers into your Nextflow pipeline, it's good practice to test them individually to ensure they work correctly and understand their requirements.</p> <p>Singularity vs Docker on HPC:</p> <ul> <li>Docker: Requires root privileges, not available on most HPC systems</li> <li>Singularity: Designed for HPC environments, can run Docker containers without root</li> <li>Nextflow: Automatically converts Docker containers to Singularity when needed</li> </ul> <p>Basic Singularity Testing Commands:</p> <pre><code># Test FastQC container\nsingularity exec docker://biocontainers/fastqc:v0.11.9 fastqc --version\n\n# Test with real data\nsingularity exec docker://biocontainers/fastqc:v0.11.9 \\\n    fastqc /data/Dataset_Mt_Vc/tb/raw_data/SRR1180160_1.fastq.gz --outdir ./test_output\n\n# Test Trimmomatic container\nsingularity exec docker://staphb/trimmomatic:0.39 \\\n    trimmomatic PE --help\n\n# Test SPAdes container\nsingularity exec docker://staphb/spades:3.15.4 \\\n    spades.py --version\n\n# Test Prokka container\nsingularity exec docker://staphb/prokka:1.14.6 \\\n    prokka --version\n</code></pre> <p>Interactive Container Testing:</p> <pre><code># Enter container interactively to explore\nsingularity shell docker://biocontainers/fastqc:v0.11.9\n\n# Inside the container, you can:\n# - Check what tools are available\n# - Explore file system structure\n# - Test commands manually\n# - Verify dependencies\n\n# Example interactive session:\nSingularity&gt; which fastqc\nSingularity&gt; fastqc --help\nSingularity&gt; ls /usr/local/bin/\nSingularity&gt; exit\n</code></pre> <p>Testing with Bind Mounts:</p> <pre><code># Bind mount your data directory to test with real files\nsingularity exec \\\n    --bind /data/Dataset_Mt_Vc/tb/raw_data:/input \\\n    --bind /tmp:/output \\\n    docker://biocontainers/fastqc:v0.11.9 \\\n    fastqc /input/SRR1180160_1.fastq.gz --outdir /output\n\n# Check the results\nls -la /tmp/*.html /tmp/*.zip\n</code></pre> <p>Container Validation Script:</p> <p>Create a simple script to test all your containers:</p> <pre><code>#!/bin/bash\n# container_test.sh - Test bioinformatics containers\n\necho \"Testing bioinformatics containers...\"\n\n# Test FastQC\necho \"Testing FastQC...\"\nsingularity exec docker://biocontainers/fastqc:v0.11.9 fastqc --version\nif [ $? -eq 0 ]; then\n    echo \"\u2705 FastQC container works\"\nelse\n    echo \"\u274c FastQC container failed\"\nfi\n\n# Test Trimmomatic\necho \"Testing Trimmomatic...\"\nsingularity exec docker://staphb/trimmomatic:0.39 trimmomatic -version\nif [ $? -eq 0 ]; then\n    echo \"\u2705 Trimmomatic container works\"\nelse\n    echo \"\u274c Trimmomatic container failed\"\nfi\n\n# Test SPAdes\necho \"Testing SPAdes...\"\nsingularity exec docker://staphb/spades:3.15.4 spades.py --version\nif [ $? -eq 0 ]; then\n    echo \"\u2705 SPAdes container works\"\nelse\n    echo \"\u274c SPAdes container failed\"\nfi\n\n# Test Prokka\necho \"Testing Prokka...\"\nsingularity exec docker://staphb/prokka:1.14.6 prokka --version\nif [ $? -eq 0 ]; then\n    echo \"\u2705 Prokka container works\"\nelse\n    echo \"\u274c Prokka container failed\"\nfi\n\necho \"Container testing complete!\"\n</code></pre> <p>Running the Test Script:</p> <pre><code># Make the script executable\nchmod +x container_test.sh\n\n# Run the tests\n./container_test.sh\n</code></pre> <p>Expected Output:</p> <pre><code>Testing bioinformatics containers...\nTesting FastQC...\nFastQC v0.11.9\n\u2705 FastQC container works\nTesting Trimmomatic...\n0.39\n\u2705 Trimmomatic container works\nTesting SPAdes...\nSPAdes v3.15.4\n\u2705 SPAdes container works\nTesting Prokka...\nprokka 1.14.6\n\u2705 Prokka container works\nContainer testing complete!\n</code></pre> <p>Troubleshooting Container Issues:</p> <pre><code># If a container fails, check:\n\n# 1. Container exists and is accessible\nsingularity pull docker://biocontainers/fastqc:v0.11.9\n\n# 2. Check container contents\nsingularity inspect docker://biocontainers/fastqc:v0.11.9\n\n# 3. Run with verbose output\nsingularity exec --debug docker://biocontainers/fastqc:v0.11.9 fastqc --version\n\n# 4. Check for missing dependencies\nsingularity exec docker://biocontainers/fastqc:v0.11.9 ldd /usr/local/bin/fastqc\n</code></pre> <p>Best Practices for Container Testing:</p> <ol> <li>Always test containers before using in production pipelines</li> <li>Use specific version tags rather than 'latest'</li> <li>Test with real data similar to your analysis</li> <li>Document working container versions</li> <li>Create validation scripts for your container stack</li> </ol>"},{"location":"modules/day7/#hands-on-exercise-test-your-containers","title":"Hands-on Exercise: Test Your Containers","text":"<p>We've provided a ready-to-use container testing script in your workflows directory:</p> <pre><code># Navigate to workflows directory\ncd /users/$USER/microbial-genomics-training/workflows\n\n# Run the container test script\n./container_test.sh\n</code></pre> <p>This script will test all the containers we'll use in our MTB pipeline and provide colored output showing which containers are working correctly.</p>"},{"location":"modules/day7/#container-integration-in-nextflow","title":"Container Integration in Nextflow","text":""},{"location":"modules/day7/#converting-exercise-3-to-use-containers","title":"Converting Exercise 3 to Use Containers","text":"<p>Let's update our <code>qc_pipeline.nf</code> to use containers instead of modules:</p> <p>Before (Module-based):</p> <pre><code>process fastqc_raw {\n    module 'fastqc/0.12.1'\n    publishDir \"${params.outdir}/fastqc_raw\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*.{html,zip}\"\n\n    script:\n    \"\"\"\n    echo \"Running FastQC on raw reads: ${sample_id}\"\n    fastqc ${reads[0]} ${reads[1]} --threads 2\n    \"\"\"\n}\n</code></pre> <p>After (Container-based):</p> <pre><code>process fastqc_raw {\n    container 'biocontainers/fastqc:v0.11.9'\n    publishDir \"${params.outdir}/fastqc_raw\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*.{html,zip}\"\n\n    script:\n    \"\"\"\n    echo \"Running FastQC on raw reads: ${sample_id}\"\n    fastqc ${reads[0]} ${reads[1]} --threads 2\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day7/#complete-containerized-pipeline","title":"Complete Containerized Pipeline","text":"<p>Let's create <code>mtb_pipeline.nf</code> - our enhanced, containerized version:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Enable DSL2\nnextflow.enable.dsl = 2\n\n// Parameters\nparams.input = \"samplesheet.csv\"\nparams.outdir = \"/data/users/$USER/nextflow-training/results\"\nparams.adapters = \"/data/timmomatic_adapter_Combo.fa\"\n\n// FastQC process for raw reads\nprocess fastqc_raw {\n    container 'biocontainers/fastqc:v0.11.9'\n    publishDir \"${params.outdir}/fastqc_raw\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*.{html,zip}\"\n\n    script:\n    \"\"\"\n    echo \"Running FastQC on raw reads: ${sample_id}\"\n    fastqc ${reads[0]} ${reads[1]} --threads 2\n    \"\"\"\n}\n\n// Trimmomatic for quality trimming\nprocess trimmomatic {\n    container 'staphb/trimmomatic:0.39'\n    publishDir \"${params.outdir}/trimmed\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_*_paired.fastq.gz\")\n    path \"${sample_id}_*_unpaired.fastq.gz\"\n\n    script:\n    \"\"\"\n    echo \"Running Trimmomatic on ${sample_id}\"\n\n    trimmomatic PE -threads 2 \\\\\n        ${reads[0]} ${reads[1]} \\\\\n        ${sample_id}_R1_paired.fastq.gz ${sample_id}_R1_unpaired.fastq.gz \\\\\n        ${sample_id}_R2_paired.fastq.gz ${sample_id}_R2_unpaired.fastq.gz \\\\\n        ILLUMINACLIP:${params.adapters}:2:30:10 \\\\\n        LEADING:3 TRAILING:3 \\\\\n        SLIDINGWINDOW:4:15 MINLEN:36\n    \"\"\"\n}\n\n// FastQC process for trimmed reads\nprocess fastqc_trimmed {\n    container 'biocontainers/fastqc:v0.11.9'\n    publishDir \"${params.outdir}/fastqc_trimmed\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"*.{html,zip}\"\n\n    script:\n    \"\"\"\n    echo \"Running FastQC on trimmed reads: ${sample_id}\"\n    fastqc ${reads[0]} ${reads[1]} --threads 2\n    \"\"\"\n}\n\n// SPAdes assembly\nprocess spades_assembly {\n    container 'staphb/spades:3.15.4'\n    publishDir \"${params.outdir}/assemblies\", mode: 'copy'\n    tag \"$sample_id\"\n    cpus 4\n    memory '8 GB'\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_assembly\")\n    tuple val(sample_id), path(\"${sample_id}_assembly/contigs.fasta\")\n\n    script:\n    \"\"\"\n    echo \"Running SPAdes assembly for ${sample_id}\"\n\n    spades.py \\\\\n        --pe1-1 ${reads[0]} \\\\\n        --pe1-2 ${reads[1]} \\\\\n        --threads ${task.cpus} \\\\\n        --memory ${task.memory.toGiga()} \\\\\n        -o ${sample_id}_assembly\n    \"\"\"\n}\n\n// Prokka annotation\nprocess prokka_annotation {\n    container 'staphb/prokka:1.14.6'\n    publishDir \"${params.outdir}/annotation\", mode: 'copy'\n    tag \"$sample_id\"\n    cpus 2\n\n    input:\n    tuple val(sample_id), path(assembly_dir)\n    tuple val(sample_id), path(contigs)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_annotation\")\n    tuple val(sample_id), path(\"${sample_id}_annotation/${sample_id}.gff\")\n\n    script:\n    \"\"\"\n    echo \"Running Prokka annotation for ${sample_id}\"\n\n    prokka \\\\\n        --outdir ${sample_id}_annotation \\\\\n        --prefix ${sample_id} \\\\\n        --genus Mycobacterium \\\\\n        --species tuberculosis \\\\\n        --kingdom Bacteria \\\\\n        --cpus ${task.cpus} \\\\\n        ${contigs}\n\n    echo \"Annotation completed for ${sample_id}\"\n    echo \"Results written to: ${sample_id}_annotation/\"\n    \"\"\"\n}\n\n// MultiQC to summarize all results\nprocess multiqc {\n    container 'ewels/multiqc:v1.12'\n    publishDir \"${params.outdir}\", mode: 'copy'\n\n    input:\n    path \"*\"\n\n    output:\n    path \"multiqc_report.html\"\n\n    script:\n    \"\"\"\n    echo \"Running MultiQC to summarize all results\"\n    multiqc . --filename multiqc_report.html\n    \"\"\"\n}\n\n// Main workflow\nworkflow {\n    // Read input samplesheet\n    read_pairs_ch = Channel\n        .fromPath(params.input)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, [file(row.fastq_1), file(row.fastq_2)]] }\n\n    // Run FastQC on raw reads\n    fastqc_raw_results = fastqc_raw(read_pairs_ch)\n\n    // Run Trimmomatic\n    trimmed_results = trimmomatic(read_pairs_ch)\n\n    // Run FastQC on trimmed reads\n    fastqc_trimmed_results = fastqc_trimmed(trimmed_results)\n\n    // Run SPAdes assembly on trimmed reads\n    assembly_results = spades_assembly(trimmed_results)\n\n    // Run Prokka annotation on assembled contigs\n    annotation_results = prokka_annotation(assembly_results[0], assembly_results[1])\n\n    // Collect all FastQC results and run MultiQC\n    all_fastqc = fastqc_raw_results.mix(fastqc_trimmed_results).collect()\n    multiqc_results = multiqc(all_fastqc)\n\n    // Show final results\n    assembly_results[0].view { \"Assembly completed: $it\" }\n    assembly_results[1].view { \"Contigs file: $it\" }\n    annotation_results[0].view { \"Annotation completed: $it\" }\n    annotation_results[1].view { \"GFF file: $it\" }\n    multiqc_results.view { \"MultiQC report created: $it\" }\n}\n</code></pre>"},{"location":"modules/day7/#updating-nextflow-configuration-for-containers","title":"Updating Nextflow Configuration for Containers","text":"<p>Update your <code>nextflow.config</code>:</p> <pre><code>// Nextflow configuration for MTB pipeline\nparams {\n    input = \"samplesheet.csv\"\n    outdir = \"/data/users/$USER/nextflow-training/results\"\n    adapters = \"/data/timmomatic_adapter_Combo.fa\"\n}\n\n// Work directory configuration\nworkDir = \"/data/users/$USER/nextflow-training/work\"\n\n// Container configuration\ndocker {\n    enabled = true\n    runOptions = '-u $(id -u):$(id -g)'\n}\n\n// Process resource configuration\nprocess {\n    withName: 'fastqc_raw' {\n        cpus = 2\n        memory = '4 GB'\n        time = '30m'\n    }\n\n    withName: 'trimmomatic' {\n        cpus = 2\n        memory = '4 GB'\n        time = '1h'\n    }\n\n    withName: 'fastqc_trimmed' {\n        cpus = 2\n        memory = '4 GB'\n        time = '30m'\n    }\n\n    withName: 'spades_assembly' {\n        cpus = 4\n        memory = '8 GB'\n        time = '2h'\n    }\n\n    withName: 'prokka_annotation' {\n        cpus = 2\n        memory = '4 GB'\n        time = '1h'\n    }\n\n    withName: 'multiqc' {\n        cpus = 1\n        memory = '2 GB'\n        time = '15m'\n    }\n}\n\n// SLURM profile for cluster execution\nprofiles {\n    slurm {\n        process {\n            executor = 'slurm'\n            queue = 'Main'\n            clusterOptions = '--account=b83'\n        }\n    }\n}\n</code></pre>"},{"location":"modules/day7/#mtb-analysis-pipeline-development_1","title":"MTB Analysis Pipeline Development","text":""},{"location":"modules/day7/#mycobacterium-tuberculosis-genomics","title":"Mycobacterium tuberculosis Genomics","text":""},{"location":"modules/day7/#understanding-mtb-genomics","title":"Understanding MTB Genomics","text":"<p>Key Characteristics:</p> <ul> <li>Genome size: ~4.4 Mb</li> <li>GC content: ~65.6%</li> <li>Genes: ~4,000 protein-coding genes</li> <li>Clinical relevance: Major global pathogen, drug resistance concerns</li> <li>Assembly challenges: Repetitive sequences, PE/PPE gene families</li> </ul> <pre><code>graph TD\n    A[MTB Genomic Features] --&gt; B[High GC Content 65.6%]\n    A --&gt; C[PE/PPE Gene Families]\n    A --&gt; D[Repetitive Sequences]\n    A --&gt; E[Drug Resistance Genes]\n\n    F[Analysis Challenges] --&gt; G[Assembly Complexity]\n    F --&gt; H[Annotation Difficulty]\n    F --&gt; I[Variant Calling Issues]\n\n    J[Clinical Applications] --&gt; K[Drug Resistance Testing]\n    J --&gt; L[Outbreak Investigation]\n    J --&gt; M[Lineage Typing]\n\n    style A fill:#e3f2fd,stroke:#2196f3,stroke-width:2px,color:#000\n    style F fill:#ffebee,stroke:#f44336,stroke-width:2px,color:#000\n    style J fill:#e8f5e8,stroke:#4caf50,stroke-width:2px,color:#000</code></pre>"},{"location":"modules/day7/#mtb-specific-pipeline-requirements","title":"MTB-Specific Pipeline Requirements","text":"<p>Essential Analysis Steps:</p> <ol> <li>Quality Control: Standard FastQC + MTB-specific metrics</li> <li>Assembly: Optimized for high GC content</li> <li>Annotation: MTB-specific gene databases</li> <li>AMR Detection: Drug resistance gene identification</li> <li>Lineage Typing: Phylogenetic classification</li> <li>Variant Calling: SNP identification for outbreak analysis</li> </ol>"},{"location":"modules/day7/#pathogen-specific-considerations","title":"Pathogen-Specific Considerations","text":""},{"location":"modules/day7/#enhanced-mtb-pipeline-features","title":"Enhanced MTB Pipeline Features","text":"<p>Let's add MTB-specific processes to our pipeline:</p> <pre><code>// QUAST assembly quality assessment\nprocess quast_assessment {\n    container 'staphb/quast:5.0.2'\n    publishDir \"${params.outdir}/quast\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(contigs)\n\n    output:\n    path \"${sample_id}_quast\"\n\n    script:\n    \"\"\"\n    echo \"Running QUAST assessment for ${sample_id}\"\n\n    quast.py \\\\\n        --output-dir ${sample_id}_quast \\\\\n        --threads 2 \\\\\n        --min-contig 500 \\\\\n        --labels ${sample_id} \\\\\n        ${contigs}\n    \"\"\"\n}\n\n// AMR detection with AMRFinderPlus\nprocess amr_detection {\n    container 'staphb/amrfinderplus:3.10.23'\n    publishDir \"${params.outdir}/amr\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(contigs)\n\n    output:\n    path \"${sample_id}_amr.tsv\"\n\n    script:\n    \"\"\"\n    echo \"Running AMR detection for ${sample_id}\"\n\n    amrfinder \\\\\n        --nucleotide ${contigs} \\\\\n        --organism Mycobacterium \\\\\n        --threads 2 \\\\\n        --output ${sample_id}_amr.tsv\n\n    echo \"AMR analysis completed for ${sample_id}\"\n    \"\"\"\n}\n\n// MTB lineage typing with TB-Profiler\nprocess lineage_typing {\n    container 'staphb/tbprofiler:4.1.1'\n    publishDir \"${params.outdir}/lineage\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"${sample_id}_lineage.json\"\n    path \"${sample_id}_lineage.txt\"\n\n    script:\n    \"\"\"\n    echo \"Running lineage typing for ${sample_id}\"\n\n    tb-profiler profile \\\\\n        --read1 ${reads[0]} \\\\\n        --read2 ${reads[1]} \\\\\n        --prefix ${sample_id}_lineage \\\\\n        --threads 2\n\n    echo \"Lineage typing completed for ${sample_id}\"\n    \"\"\"\n}\n\n// MLST typing\nprocess mlst_typing {\n    container 'staphb/mlst:2.22.0'\n    publishDir \"${params.outdir}/mlst\", mode: 'copy'\n    tag \"$sample_id\"\n\n    input:\n    tuple val(sample_id), path(contigs)\n\n    output:\n    path \"${sample_id}_mlst.tsv\"\n\n    script:\n    \"\"\"\n    echo \"Running MLST typing for ${sample_id}\"\n\n    mlst \\\\\n        --scheme mtbc \\\\\n        ${contigs} &gt; ${sample_id}_mlst.tsv\n\n    echo \"MLST typing completed for ${sample_id}\"\n    \"\"\"\n}\n</code></pre>"},{"location":"modules/day7/#clinical-genomics-applications","title":"Clinical Genomics Applications","text":""},{"location":"modules/day7/#complete-mtb-clinical-pipeline","title":"Complete MTB Clinical Pipeline","text":"<p>Here's our enhanced pipeline with all MTB-specific features:</p> <pre><code>#!/usr/bin/env nextflow\n\n// Enable DSL2\nnextflow.enable.dsl = 2\n\n// Parameters\nparams.input = \"samplesheet.csv\"\nparams.outdir = \"/data/users/$USER/nextflow-training/results\"\nparams.adapters = \"/data/timmomatic_adapter_Combo.fa\"\n\n// Import processes (previous processes here...)\n\n// Enhanced workflow with MTB-specific analysis\nworkflow {\n    // Read input samplesheet\n    read_pairs_ch = Channel\n        .fromPath(params.input)\n        .splitCsv(header: true)\n        .map { row -&gt; [row.sample, [file(row.fastq_1), file(row.fastq_2)]] }\n\n    // Standard QC and assembly pipeline\n    fastqc_raw_results = fastqc_raw(read_pairs_ch)\n    trimmed_results = trimmomatic(read_pairs_ch)\n    fastqc_trimmed_results = fastqc_trimmed(trimmed_results)\n    assembly_results = spades_assembly(trimmed_results)\n    annotation_results = prokka_annotation(assembly_results[0], assembly_results[1])\n\n    // MTB-specific analyses\n    quast_results = quast_assessment(assembly_results[1])\n    amr_results = amr_detection(assembly_results[1])\n    lineage_results = lineage_typing(trimmed_results)\n    mlst_results = mlst_typing(assembly_results[1])\n\n    // Comprehensive reporting\n    all_qc = fastqc_raw_results.mix(fastqc_trimmed_results).collect()\n    multiqc_results = multiqc(all_qc)\n\n    // Output summaries\n    assembly_results[1].view { \"Assembly: $it\" }\n    annotation_results[1].view { \"Annotation: $it\" }\n    amr_results.view { \"AMR results: $it\" }\n    lineage_results.view { \"Lineage: $it\" }\n    mlst_results.view { \"MLST: $it\" }\n}\n</code></pre>"},{"location":"modules/day7/#clinical-interpretation-workflow","title":"Clinical Interpretation Workflow","text":"<pre><code>flowchart TD\n    A[Raw Sequencing Data] --&gt; B[Quality Control]\n    B --&gt; C[Genome Assembly]\n    C --&gt; D[Quality Assessment]\n\n    D --&gt; E[Drug Resistance Testing]\n    D --&gt; F[Lineage Typing]\n    D --&gt; G[MLST Typing]\n\n    E --&gt; H[Clinical Report]\n    F --&gt; H\n    G --&gt; H\n\n    H --&gt; I[Treatment Decision]\n    H --&gt; J[Outbreak Investigation]\n    H --&gt; K[Surveillance Database]\n\n    style A fill:#f5f5f5,stroke:#757575,stroke-width:1px,color:#000\n    style H fill:#fff3e0,stroke:#ff9800,stroke-width:2px,color:#000\n    style I fill:#e8f5e8,stroke:#4caf50,stroke-width:2px,color:#000\n    style J fill:#e3f2fd,stroke:#2196f3,stroke-width:2px,color:#000\n    style K fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px,color:#000</code></pre> <p>Step 1: Clone and Setup PHoeNIx</p> <pre><code># Navigate to your working directory\ncd /data/users/$USER/nextflow-training\n\n# Create Phoenix analysis directory\nmkdir -p phoenix-analysis\ncd phoenix-analysis\n\n# Clone the PHoeNIx pipeline directly\ngit clone https://github.com/CDCgov/phoenix.git\ncd phoenix\n\n# Check the Phoenix help to understand entry workflows\nnextflow run main.nf --help\n</code></pre> <p>Step 2: Setup Required Databases</p> <p>PHoeNIx requires several databases. The system has a pre-installed Kraken2 database:</p> <pre><code># Check available Kraken2 database\nls -la /data/kraken2_db_standard/\n\n# Note: The standard database requires &gt;200GB RAM for production use\n# For training environments, ensure adequate memory allocation\necho \"Kraken2 database location: /data/kraken2_db_standard/\"\n</code></pre> <p>Step 3: Prepare Sample Sheet for PHoeNIx</p> <p>PHoeNIx uses a specific samplesheet format. Let's create one for our TB data:</p> <pre><code># Create PHoeNIx samplesheet\ncat &gt; phoenix_samplesheet.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nTest_Sample,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nEOF\n\necho \"PHoeNIx samplesheet created:\"\ncat phoenix_samplesheet.csv\n</code></pre> <p>Step 4: Configure Resources for Production Use</p> <p>PHoeNIx requires significant computational resources, especially for Kraken2 taxonomic classification. The pipeline has hardcoded resource limits that need to be adjusted:</p> <pre><code># First, modify PHoeNIx base configuration for Kraken2 memory requirements\ncd phoenix\nsed -i 's/cpus   = { check_max( 2                  , '\\''cpus'\\''    ) }/cpus   = { check_max( 10                 , '\\''cpus'\\''    ) }/' conf/base.config\nsed -i 's/memory = { check_max( 10.GB \\* task.attempt, '\\''memory'\\''  ) }/memory = { check_max( 200.GB \\* task.attempt, '\\''memory'\\''  ) }/' conf/base.config\nsed -i 's/time   = { check_max( 4.h                 , '\\''time'\\''    ) }/time   = { check_max( 8.h                 , '\\''time'\\''    ) }/' conf/base.config\n\n# Verify the changes\necho \"\ud83d\udd0d Checking the updated configuration:\"\ngrep -A 4 \"withName:KRAKEN2_KRAKEN2\" conf/base.config\n\n# Create cluster configuration for SLURM\ncd ..\ncat &gt; cluster.config &lt;&lt; 'EOF'\nprocess {\n    executor = 'slurm'\n    queue = 'batch'\n\n    // PHoeNIx-specific resource allocations - 200GB memory for large Kraken2 database (within cluster limits)\n    withName: 'PHOENIX:PHOENIX_EXTERNAL:KRAKEN2_TRIMD:KRAKEN2_TRIMD' {\n        cpus = 10\n        memory = '200 GB'\n        time = '8h'\n    }\n\n    withName: 'PHOENIX:PHOENIX_EXTERNAL:KRAKEN2_WTASMBLD:KRAKEN2_WTASMBLD' {\n        cpus = 10\n        memory = '200 GB'\n        time = '8h'\n    }\n}\n\nsingularity {\n    enabled = true\n    autoMounts = true\n    cacheDir = '/data/users/singularity_cache'\n}\nEOF\n</code></pre> <p>Step 5: Run PHoeNIx Pipeline</p> <pre><code># Set up Singularity cache directory\nexport NXF_SINGULARITY_CACHEDIR=\"/data/users/singularity_cache\"\nmkdir -p $NXF_SINGULARITY_CACHEDIR\n\n# Run PHoeNIx with correct entry workflow and configuration\ncd phoenix\nnextflow run main.nf \\\n    -entry PHOENIX \\\n    --input /data/users/$USER/nextflow-training/phoenix-analysis/phoenix_samplesheet.csv \\\n    --kraken2db $KRAKEN2_DB_PATH \\\n    --outdir tb_analysis_results \\\n    -c /users/mamana/microbial-genomics-training/cluster.config \\\n    -profile singularity,slurm \\\n    -resume\n\n# Monitor the run\ntail -f .nextflow.log\n</code></pre>"},{"location":"modules/day7/#running-from-github-after-pushing-your-workflow","title":"Running from GitHub (After Pushing Your Workflow)","text":"<p>Once you've developed and pushed your own workflow to GitHub, you can run it directly:</p> <p>Step 1: Push Your Workflow to GitHub</p> <pre><code># Initialize git repository (if not already done)\ncd /users/$USER/microbial-genomics-training/workflows\ngit init\ngit add .\ngit commit -m \"Add microbial genomics training workflows\"\n\n# Add your GitHub repository as remote\ngit remote add origin https://github.com/$USER/microbial-genomics-training.git\ngit push -u origin main\n</code></pre> <p>Step 2: Run Workflow from GitHub</p> <pre><code># Run your workflow directly from GitHub\nnextflow run $USER/microbial-genomics-training \\\n    --input samplesheet.csv \\\n    --outdir /data/users/$USER/nextflow-training/results \\\n    -profile singularity \\\n    -resume\n\n# Or run a specific workflow file\nnextflow run $USER/microbial-genomics-training/workflows/qc_pipeline.nf \\\n    --input samplesheet.csv \\\n    --outdir /data/users/$USER/nextflow-training/results \\\n    -profile singularity \\\n    -resume\n</code></pre>"},{"location":"modules/day7/#troubleshooting-phoenix-issues","title":"Troubleshooting PHoeNIx Issues","text":"<p>Common Issue 1: Memory Allocation for Kraken2</p> <p>If you encounter exit status 137 (killed due to memory constraints):</p> <pre><code># Check if PHoeNIx base configuration was properly modified\ngrep -A 4 \"withName:KRAKEN2_KRAKEN2\" phoenix/conf/base.config\n\n# Should show 200.GB memory allocation, not 10.GB\n# If not updated, re-run the sed commands from Step 4\n</code></pre> <p>Common Issue 2: Entry Workflow Not Specified</p> <p>If you get \"No entry workflow specified\" error:</p> <pre><code># Always use -entry PHOENIX parameter\nnextflow run main.nf \\\n    -entry PHOENIX \\\n    --input /data/users/$USER/nextflow-training/phoenix-analysis/phoenix_samplesheet.csv \\\n    --kraken2db $KRAKEN2_DB_PATH \\\n    --outdir tb_analysis_results \\\n    -c /users/mamana/microbial-genomics-training/cluster.config \\\n    -profile singularity,slurm \\\n    -resume\n</code></pre> <p>Common Issue 3: Kraken2 Database Path</p> <p>If you encounter the error about <code>ktaxonomy.tsv</code> not found:</p> <pre><code># Check if the database path is correct\nls -la /data/kraken2_db/\nls -la /data/kraken2_db/ktaxonomy.tsv\n\n# If using module system, check the environment variable\necho $KRAKEN2_DB_PATH\n\n# Update the command with correct database path\nnextflow run main.nf \\\n    -entry PHOENIX \\\n    --input /data/users/$USER/nextflow-training/phoenix-analysis/phoenix_samplesheet.csv \\\n    --kraken2db $KRAKEN2_DB_PATH \\\n    --outdir tb_analysis_results \\\n    -c /users/mamana/microbial-genomics-training/cluster.config \\\n    -profile singularity,slurm \\\n    -resume\n</code></pre> <p>Common Issue 4: QC Pipeline File Function Error</p> <p>If your QC pipeline shows \"Argument of file function cannot be null\":</p> <pre><code># Check samplesheet format - ensure multiple samples\ncat &gt; samplesheet_fixed.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nERR036221,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nERR036223,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_2.fastq.gz\nERR036226,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_2.fastq.gz\nEOF\n\n# Run with fixed samplesheet\nnextflow run qc_pipeline.nf \\\n    --input samplesheet_fixed.csv \\\n    -c /users/mamana/microbial-genomics-training/cluster.config \\\n    -resume \\\n    -profile slurm,Singularity\n</code></pre> <p>Checking PHoeNIx Results</p> <pre><code># Check the output structure\ntree /data/users/$USER/nextflow-training/phoenix_results/\n\n# Key outputs to examine:\necho \"=== Assembly Quality ===\"\ncat /data/users/$USER/nextflow-training/phoenix_results/Test_Sample/assembly/Test_Sample_assembly_stats.txt\n\necho \"=== Taxonomic Classification ===\"\ncat /data/users/$USER/nextflow-training/phoenix_results/Test_Sample/kraken2/Test_Sample_kraken2_report.txt\n\necho \"=== AMR Detection ===\"\ncat /data/users/$USER/nextflow-training/phoenix_results/Test_Sample/amr/Test_Sample_amrfinder.tsv\n</code></pre>"},{"location":"modules/day7/#key-phoenix-features-demonstrated","title":"Key PHoeNIx Features Demonstrated","text":"<p>Our successful PHoeNIx run demonstrates several important production pipeline features:</p> <ol> <li>Resource Management: Proper memory allocation (256GB) for large databases</li> <li>Entry Workflows: Using <code>-entry PHOENIX</code> for specific analysis types</li> <li>Resume Capability: <code>-resume</code> flag to continue from cached successful tasks</li> <li>Comprehensive Analysis: Assembly, annotation, taxonomy, AMR, virulence</li> <li>Standardized Output: Consistent directory structure and file formats</li> <li>Production Ready: Error handling, resource optimization, SLURM integration</li> </ol>"},{"location":"modules/day7/#comparing-phoenix-vs-our-custom-pipeline","title":"Comparing PHoeNIx vs Our Custom Pipeline","text":"<p>What PHoeNIx Adds:</p> <ol> <li>Comprehensive QC: Multiple quality assessment tools</li> <li>Advanced Assembly: Multiple assemblers with quality filtering</li> <li>Taxonomic Classification: Kraken2 for species identification</li> <li>AMR Detection: AMRFinderPlus for resistance gene detection</li> <li>Contamination Detection: Screen for contaminating sequences</li> <li>Standardized Reporting: Consistent output formats</li> <li>Production Ready: Error handling, resource management</li> </ol> <p>Exercise: Compare Outputs</p> <pre><code># Compare assembly statistics\necho \"=== Our Pipeline Assembly ===\"\ngrep -c '&gt;' /data/users/$USER/nextflow-training/results/assemblies/ERR036221_assembly/contigs.fasta\n\necho \"=== PHoeNIx Assembly ===\"\ngrep -c '&gt;' /data/users/$USER/nextflow-training/phoenix_results/Test_Sample/assembly/Test_Sample_contigs.fasta\n\n# Compare file sizes\necho \"=== File Size Comparison ===\"\nls -lh /data/users/$USER/nextflow-training/results/assemblies/ERR036221_assembly/contigs.fasta\nls -lh /data/users/$USER/nextflow-training/phoenix_results/Test_Sample/assembly/Test_Sample_contigs.fasta\n</code></pre>"},{"location":"modules/day7/#professional-pipeline-development","title":"Professional Pipeline Development","text":""},{"location":"modules/day7/#pipeline-documentation","title":"Pipeline Documentation","text":""},{"location":"modules/day7/#creating-professional-documentation","title":"Creating Professional Documentation","text":"<p>Essential Documentation Components:</p> <ol> <li>README.md: Overview and quick start</li> <li>CHANGELOG.md: Version history</li> <li>docs/: Detailed documentation</li> <li>examples/: Sample data and configs</li> </ol> <p>Example Documentation Structure:</p> <pre><code>mtb-analysis-pipeline/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 CHANGELOG.md\n\u251c\u2500\u2500 main.nf\n\u251c\u2500\u2500 nextflow.config\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 usage.md\n\u2502   \u251c\u2500\u2500 parameters.md\n\u2502   \u251c\u2500\u2500 output.md\n\u2502   \u2514\u2500\u2500 troubleshooting.md\n\u251c\u2500\u2500 examples/\n\u2502   \u251c\u2500\u2500 samplesheet.csv\n\u2502   \u2514\u2500\u2500 test_config.config\n\u2514\u2500\u2500 containers/\n    \u2514\u2500\u2500 Dockerfile\n</code></pre>"},{"location":"modules/day7/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"modules/day7/#pipeline-testing-strategy","title":"Pipeline Testing Strategy","text":"<pre><code># Create test data\nmkdir -p test_data\n# Add small test datasets\n\n# Test with minimal data\nnextflow run main.nf \\\\\n    --input test_data/test_samplesheet.csv \\\\\n    --outdir test_results \\\\\n    -profile test\n\n# Validate outputs\nls test_results/\n</code></pre>"},{"location":"modules/day7/#deployment-strategies","title":"Deployment Strategies","text":""},{"location":"modules/day7/#version-control-best-practices","title":"Version Control Best Practices","text":"<pre><code># Create development branch\ngit checkout -b feature/amr-detection\n\n# Make changes and commit\ngit add .\ngit commit -m \"Add AMR detection with AMRFinderPlus\n\n- Added amr_detection process\n- Updated workflow to include AMR analysis\n- Added AMRFinderPlus container\n- Updated documentation\"\n\n# Push to GitHub\ngit push origin feature/amr-detection\n\n# Create pull request on GitHub\n# Merge after review\n</code></pre>"},{"location":"modules/day7/#release-management","title":"Release Management","text":"<pre><code># Create release\ngit tag -a v1.0.0 -m \"Release v1.0.0: Production MTB pipeline\n\nFeatures:\n- Complete QC pipeline\n- MTB-specific assembly\n- AMR detection\n- Lineage typing\n- MLST analysis\n- Comprehensive reporting\"\n\ngit push origin v1.0.0\n</code></pre>"},{"location":"modules/day7/#running-workflows-from-github","title":"Running Workflows from GitHub","text":"<p>Once your workflow is on GitHub, you and others can run it directly from the repository:</p>"},{"location":"modules/day7/#running-your-published-workflow","title":"Running Your Published Workflow","text":"<pre><code># Run workflow directly from GitHub\nnextflow run yourusername/mtb-analysis-pipeline \\\n    --input samplesheet.csv \\\n    --outdir results \\\n    -profile singularity\n\n# Run specific version/release\nnextflow run yourusername/mtb-analysis-pipeline \\\n    -r v1.0.0 \\\n    --input samplesheet.csv \\\n    --outdir results \\\n    -profile singularity\n\n# Run from specific branch\nnextflow run yourusername/mtb-analysis-pipeline \\\n    -r feature/amr-detection \\\n    --input samplesheet.csv \\\n    --outdir results \\\n    -profile singularity\n</code></pre>"},{"location":"modules/day7/#sharing-your-workflow","title":"Sharing Your Workflow","text":"<p>Your colleagues can now run your pipeline easily:</p> <pre><code># Anyone can run your pipeline with:\nnextflow run yourusername/mtb-analysis-pipeline \\\n    --input their_samples.csv \\\n    --outdir their_results \\\n    -profile singularity\n\n# They can also clone and modify:\ngit clone https://github.com/yourusername/mtb-analysis-pipeline.git\ncd mtb-analysis-pipeline\nnextflow run . --input samples.csv --outdir results\n</code></pre>"},{"location":"modules/day7/#benefits-of-github-hosted-workflows","title":"Benefits of GitHub-hosted Workflows","text":"<ul> <li>\u2705 Version Control: Track all changes and releases</li> <li>\u2705 Collaboration: Multiple developers can contribute</li> <li>\u2705 Reproducibility: Anyone can run the exact same version</li> <li>\u2705 Documentation: README, wiki, and issues for support</li> <li>\u2705 Distribution: Easy sharing with the community</li> <li>\u2705 Continuous Integration: Automated testing with GitHub Actions</li> </ul>"},{"location":"modules/day7/#hands-on-exercise-building-your-mtb-pipeline","title":"Hands-on Exercise: Building Your MTB Pipeline","text":""},{"location":"modules/day7/#exercise-convert-exercise-3-to-production-mtb-pipeline","title":"Exercise: Convert Exercise 3 to Production MTB Pipeline","text":"<p>Objective: Transform your Day 6 Exercise 3 pipeline into a production-ready MTB analysis workflow.</p>"},{"location":"modules/day7/#step-1-initialize-version-control","title":"Step 1: Initialize Version Control","text":"<pre><code>cd /users/$USER/microbial-genomics-training/workflows\n\n# Initialize Git repository\ngit init\ngit add qc_pipeline.nf nextflow.config samplesheet.csv\ngit commit -m \"Initial commit: Exercise 3 baseline\"\n\n# Create development branch\ngit checkout -b feature/mtb-production\n</code></pre>"},{"location":"modules/day7/#step-2-create-containerized-pipeline","title":"Step 2: Create Containerized Pipeline","text":"<p>Create <code>mtb_pipeline.nf</code> with the containerized processes shown above.</p>"},{"location":"modules/day7/#step-3-add-mtb-specific-features","title":"Step 3: Add MTB-Specific Features","text":"<p>Add the AMR detection, lineage typing, and MLST processes.</p>"},{"location":"modules/day7/#step-4-test-the-pipeline","title":"Step 4: Test the Pipeline","text":"<pre><code># Test with a subset of samples\nnextflow run mtb_pipeline.nf \\\\\n    --input samplesheet_test.csv \\\\\n    --outdir /data/users/$USER/nextflow-training/results_mtb\n\n# Verify outputs\nls -la /data/users/$USER/nextflow-training/results_mtb/\n</code></pre>"},{"location":"modules/day7/#step-5-document-and-commit","title":"Step 5: Document and Commit","text":"<pre><code># Create documentation\necho \"# MTB Analysis Pipeline\" &gt; README.md\n# Add comprehensive documentation\n\n# Commit changes\ngit add .\ngit commit -m \"Complete MTB production pipeline\n\n- Containerized all processes\n- Added AMR detection\n- Added lineage typing\n- Added MLST analysis\n- Comprehensive documentation\"\n</code></pre>"},{"location":"modules/day7/#step-6-create-github-repository","title":"Step 6: Create GitHub Repository","text":"<ol> <li>Create repository on GitHub</li> <li>Push your code</li> <li>Create releases</li> <li>Add documentation</li> </ol>"},{"location":"modules/day7/#summary-and-next-steps","title":"Summary and Next Steps","text":""},{"location":"modules/day7/#what-we-accomplished-today","title":"What We Accomplished Today","text":"<ul> <li>\u2705 Version Control: Learned Git and GitHub for pipeline development</li> <li>\u2705 Containerization: Integrated Docker containers for reproducibility</li> <li>\u2705 MTB Pipeline: Built production-ready tuberculosis analysis workflow</li> <li>\u2705 Clinical Applications: Added AMR detection and typing capabilities</li> <li>\u2705 Professional Standards: Documentation, testing, and deployment</li> </ul>"},{"location":"modules/day7/#your-production-pipeline-features","title":"Your Production Pipeline Features","text":"<pre><code>flowchart LR\n    A[Raw Data] --&gt; B[QC Pipeline]\n    B --&gt; C[Assembly]\n    C --&gt; D[Annotation]\n    D --&gt; E[AMR Detection]\n    D --&gt; F[Lineage Typing]\n    D --&gt; G[MLST Analysis]\n\n    E --&gt; H[Clinical Report]\n    F --&gt; H\n    G --&gt; H\n\n    style A fill:#f5f5f5,stroke:#757575,stroke-width:1px,color:#000\n    style H fill:#e8f5e8,stroke:#4caf50,stroke-width:2px,color:#000</code></pre>"},{"location":"modules/day7/#real-world-pipeline-phoenix-cdc","title":"\ud83d\udd25 Real-World Pipeline: PHoeNIx (CDC)","text":""},{"location":"modules/day7/#introduction-to-phoenix","title":"Introduction to PHoeNIx","text":"<p>PHoeNIx (Platform-agnostic Healthcare-associated and antimicrobial resistant pathogen analysis) is a production-ready Nextflow pipeline developed by the Centers for Disease Control and Prevention (CDC). It's specifically designed for analyzing healthcare-associated and antimicrobial resistant pathogens, making it perfect for our MTB analysis.</p> <p>Why PHoeNIx for MTB Analysis?</p> <ul> <li>\ud83c\udfdb\ufe0f CDC-developed: Trusted, authoritative source for pathogen analysis</li> <li>\ud83c\udfaf Healthcare focus: Designed for clinical and public health applications</li> <li>\ud83d\udd2c AMR detection: Built-in antimicrobial resistance analysis</li> <li>\ud83d\udcca Comprehensive reports: Clinical-grade output reports</li> <li>\ud83d\udc33 Containerized: Uses Docker/Singularity for reproducibility</li> <li>\ud83d\ude80 Production-ready: Used in real public health laboratories</li> </ul>"},{"location":"modules/day7/#exercise-4-setting-up-phoenix-with-your-tb-data","title":"Exercise 4: Setting Up PHoeNIx with Your TB Data","text":""},{"location":"modules/day7/#step-1-understanding-phoenix-requirements","title":"Step 1: Understanding PHoeNIx Requirements","text":"<p>PHoeNIx requires:</p> <ul> <li>Nextflow (\u226521.10.3) \u2705 Already installed</li> <li>Docker or Singularity \u2705 Already available</li> <li>Kraken2 database (we'll download this)</li> <li>Paired-end FASTQ files \u2705 We have TB data</li> </ul>"},{"location":"modules/day7/#step-2-clone-and-setup-phoenix","title":"Step 2: Clone and Setup PHoeNIx","text":"<pre><code># Initialize module system\nsource /opt/lmod/8.7/lmod/lmod/init/bash\n\n# Load required modules\nmodule load nextflow/25.04.6\nmodule load kraken2/2.1.3\n\n# Set up Singularity directories in user data space\nexport NXF_SINGULARITY_CACHEDIR=/data/users/singularity_cache\nexport NXF_SINGULARITY_TMPDIR=/data/users/temp\nexport NXF_SINGULARITY_LOCALCACHEDIR=/data/users/singularity_cache\n\n# Set up Kraken2 database path for local analysis\nexport KRAKEN2_DB_PATH=/data/users/kraken2_db_local\n\n# Create the directories\nmkdir -p $NXF_SINGULARITY_CACHEDIR\nmkdir -p $NXF_SINGULARITY_TMPDIR\nmkdir -p $NXF_SINGULARITY_LOCALCACHEDIR\n\necho \"\u2705 Singularity cache directory: $NXF_SINGULARITY_CACHEDIR\"\necho \"\u2705 Singularity temp directory: $NXF_SINGULARITY_TMPDIR\"\necho \"\u2705 Singularity local cache directory: $NXF_SINGULARITY_LOCALCACHEDIR\"\necho \"\u2705 Kraken2 database path: $KRAKEN2_DB_PATH\"\n\n# Add to bashrc for persistence\necho \"export NXF_SINGULARITY_CACHEDIR=/data/users/singularity_cache\" &gt;&gt; ~/.bashrc\necho \"export NXF_SINGULARITY_TMPDIR=/data/users/temp\" &gt;&gt; ~/.bashrc\necho \"export NXF_SINGULARITY_LOCALCACHEDIR=/data/users/singularity_cache\" &gt;&gt; ~/.bashrc\necho \"export KRAKEN2_DB_PATH=/data/users/kraken2_db_local\" &gt;&gt; ~/.bashrc\n\n# Navigate to our workflows directory\ncd /data/users/$USER/nextflow-training\n\n# Create PHoeNIx workspace\nmkdir phoenix-analysis\ncd phoenix-analysis\n\n# Clone the PHoeNIx repository\ngit clone https://github.com/CDCgov/phoenix.git\ncd phoenix\n\n# Check the pipeline structure\nls -la\ncat README.md | head -20\n</code></pre>"},{"location":"modules/day7/#step-3-setup-kraken2-database","title":"Step 3: Setup Kraken2 Database","text":"<p>PHoeNIx requires a Kraken2 database for taxonomic classification. The system has a pre-installed standard database that we'll configure:</p> <pre><code># Load the kraken2 module (already loaded in Step 2)\n# source /opt/lmod/8.7/lmod/lmod/init/bash\n# module load kraken2/2.1.3\n\n# Set up the standard Kraken2 database path\n# The system has a pre-installed standard database at this location\nexport KRAKEN2_DB_PATH=/data/users/kraken2_db_local\n\n# Verify the database exists and contains required files\necho \"\ud83d\udd0d Checking Kraken2 database at: $KRAKEN2_DB_PATH\"\nls -la $KRAKEN2_DB_PATH\n\n# Check for essential database files\necho \"\ud83d\udccb Verifying database files:\"\nif [ -f \"$KRAKEN2_DB_PATH/ktaxonomy.tsv\" ]; then\n    echo \"\u2705 ktaxonomy.tsv found\"\nelse\n    echo \"\u274c ktaxonomy.tsv missing\"\nfi\n\nif [ -f \"$KRAKEN2_DB_PATH/hash.k2d\" ]; then\n    echo \"\u2705 hash.k2d found\"\nelse\n    echo \"\u274c hash.k2d missing\"\nfi\n\nif [ -f \"$KRAKEN2_DB_PATH/opts.k2d\" ]; then\n    echo \"\u2705 opts.k2d found\"\nelse\n    echo \"\u274c opts.k2d missing\"\nfi\n\nif [ -f \"$KRAKEN2_DB_PATH/taxo.k2d\" ]; then\n    echo \"\u2705 taxo.k2d found\"\nelse\n    echo \"\u274c taxo.k2d missing\"\nfi\n\n# Add to bashrc for persistence across sessions\necho \"export KRAKEN2_DB_PATH=/data/users/kraken2_db_local\" &gt;&gt; ~/.bashrc\n\n# Display database information\necho \"\ud83d\udcca Database information:\"\necho \"   Path: $KRAKEN2_DB_PATH\"\necho \"   Size: $(du -sh $KRAKEN2_DB_PATH 2&gt;/dev/null | cut -f1 || echo 'Unknown')\"\necho \"   Files: $(ls -1 $KRAKEN2_DB_PATH | wc -l) files\"\n\n# Test the database with kraken2 (optional)\necho \"\ud83e\uddea Testing database with kraken2...\"\nkraken2 --db $KRAKEN2_DB_PATH --help &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo \"\u2705 Database is accessible\" || echo \"\u26a0\ufe0f  Database test failed\"\n\necho \"\u2705 Kraken2 database configured successfully!\"\n</code></pre> <p>About the Kraken2 Standard Database:</p> <p>The standard Kraken2 database (<code>/data/kraken2_db_standard/</code>) contains:</p> <ul> <li>Taxonomic data: Complete NCBI taxonomy for species identification</li> <li>Reference genomes: Bacterial, archaeal, and viral genomes</li> <li>Size: Approximately 50-100 GB (compressed)</li> <li>Coverage: Comprehensive microbial genome collection</li> <li>Purpose: Accurate taxonomic classification of sequencing reads</li> </ul> <p>Database Files Explained:</p> <ul> <li><code>ktaxonomy.tsv</code>: Taxonomic hierarchy and names</li> <li><code>hash.k2d</code>: K-mer hash table for sequence matching</li> <li><code>opts.k2d</code>: Database options and parameters</li> <li><code>taxo.k2d</code>: Taxonomic assignment data</li> </ul>"},{"location":"modules/day7/#step-4-prepare-your-tb-samplesheet","title":"Step 4: Prepare Your TB Samplesheet","text":"<p>PHoeNIx uses a specific samplesheet format. Let's create one for our TB data:</p> <pre><code># Navigate back to phoenix analysis directory\ncd /data/users/$USER/nextflow-training/phoenix\n\n# Create PHoeNIx samplesheet\ncat &gt; phoenix_samplesheet.csv &lt;&lt; 'EOF'\nsample,fastq_1,fastq_2\nTB_sample_1,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036221_2.fastq.gz\nTB_sample_2,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036223_2.fastq.gz\nTB_sample_3,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_1.fastq.gz,/data/Dataset_Mt_Vc/tb/raw_data/ERR036226_2.fastq.gz\nEOF\n\necho \"\u2705 PHoeNIx samplesheet created: phoenix_samplesheet.csv\"\n</code></pre>"},{"location":"modules/day7/#step-5-create-cluster-configuration","title":"Step 5: Create Cluster Configuration","text":"<p>We'll create a cluster configuration file for SLURM execution with proper Singularity settings. Note that a basic <code>cluster.config</code> was introduced in Day 6 Exercise 3 - we'll enhance it here for PHoeNIx:</p> <pre><code># Create cluster configuration file optimized for PHoeNIx\n# This will override PHoeNIx's internal cluster.config to prevent GRiPHin errors\ncat &gt; cluster.config &lt;&lt; EOF\n// Cluster configuration for genomic analysis pipeline\n\nparams {\n    outdir = \"/data/users/$USER/nextflow-training/phoenix-analysis/results\"\n    kraken2_db = \"/data/users/kraken2_db_local\"\n}\n\nprofiles {\n    singularity {\n        enabled = true\n        autoMounts = true\n        cacheDir = '/data/users/singularity_cache'\n        libraryDir = '/data/users/singularity_cache'\n        tmpDir = '/data/users/temp'\n    }\n\n    slurm {\n        process {\n            executor = 'slurm'\n\n            // Default resources\n            cpus = 2\n            memory = '4 GB'\n            time = '2h'\n\n            // PHoeNIx-specific resource allocations\n            withName: 'PHOENIX:PHOENIX_EXTERNAL:KRAKEN2_TRIMD:KRAKEN2_TRIMD' {\n                cpus = 10\n                memory = '100 GB'\n                time = '8h'\n            }\n\n            withName: 'PHOENIX:PHOENIX_EXTERNAL:KRAKEN2_WTASMBLD:KRAKEN2_WTASMBLD' {\n                cpus = 10\n                memory = '100 GB'\n                time = '8h'\n            }\n\n            withName: 'PHOENIX:PHOENIX_EXTERNAL:SPADES_WF:SPADES' {\n                cpus = 8\n                memory = '64 GB'\n                time = '12h'\n            }\n\n            withName: 'PHOENIX:PHOENIX_EXTERNAL:BBDUK' {\n                cpus = 4\n                memory = '32 GB'\n                time = '4h'\n            }\n\n            withName: 'PHOENIX:PHOENIX_EXTERNAL:FASTP_TRIMD' {\n                cpus = 4\n                memory = '8 GB'\n                time = '2h'\n            }\n\n            withName: 'PHOENIX:PHOENIX_EXTERNAL:PROKKA' {\n                cpus = 4\n                memory = '8 GB'\n                time = '3h'\n            }\n        }\n\n        executor {\n            queueSize = 20\n            submitRateLimit = '10 sec'\n        }\n    }\n}\n\n// Enhanced reporting disabled to prevent GRiPHin directory parsing errors\ntrace {\n    enabled = false\n}\n\ntimeline {\n    enabled = false\n}\n\nreport {\n    enabled = false\n}\nEOF\n\necho \"\u2705 Cluster configuration created: cluster.config\"\n</code></pre> <p>Important: Configuration Override</p> <p>This cluster.config file is designed to override PHoeNIx's internal configuration settings. Specifically:</p> <ul> <li>Enhanced reporting disabled: Prevents GRiPHin post-processing errors caused by HTML report files</li> <li>Shared database path: References the system-wide Kraken2 database location  </li> <li>Memory optimizations: Set to 100GB (within cluster capacity) instead of default 256GB</li> <li>Clean process selectors: Only includes valid PHoeNIx-specific process names to avoid configuration warnings</li> </ul> <p>The <code>-c</code> flag in Nextflow commands ensures this configuration takes precedence over PHoeNIx's internal settings.</p>"},{"location":"modules/day7/#step-5b-create-environment-setup-script","title":"Step 5b: Create Environment Setup Script","text":"<p>Create a reusable script to set up the Singularity environment:</p> <pre><code># Create environment setup script\ncat &gt; setup_singularity_env.sh &lt;&lt; 'EOF'\n#!/bin/bash\n# Singularity environment setup for PHoeNIx\n\n# Set up Singularity directories in user data space\nexport SINGULARITY_CACHEDIR=/data/users/singularity_cache\nexport SINGULARITY_TMPDIR=/data/users/temp\nexport SINGULARITY_LOCALCACHEDIR=/data/users/singularity_cache\n\n# Create directories if they don't exist\nmkdir -p $SINGULARITY_CACHEDIR\nmkdir -p $SINGULARITY_TMPDIR\nmkdir -p $SINGULARITY_LOCALCACHEDIR\n\n# Load required modules\nsource /opt/lmod/8.7/lmod/lmod/init/bash\nmodule load nextflow/25.04.6\nmodule load kraken2/2.1.3\n\necho \"\u2705 Singularity environment configured:\"\necho \"   Cache: $SINGULARITY_CACHEDIR\"\necho \"   Temp:  $SINGULARITY_TMPDIR\"\necho \"   Local: $SINGULARITY_LOCALCACHEDIR\"\necho \"\u2705 Modules loaded: nextflow, kraken2\"\nEOF\n\nchmod +x setup_singularity_env.sh\necho \"\u2705 Environment setup script created: setup_singularity_env.sh\"\n\n# Test the script\n./setup_singularity_env.sh\n</code></pre>"},{"location":"modules/day7/#step-6-run-phoenix-test","title":"Step 6: Run PHoeNIx Test","text":"<p>First, let's run PHoeNIx with test data to ensure everything works:</p> <pre><code># Load required modules (if not already loaded)\nsource /opt/lmod/8.7/lmod/lmod/init/bash\nmodule load nextflow/25.04.6\nmodule load kraken2/2.1.3\n\n# Set up Singularity directories (if not already set)\nexport NXF_SINGULARITY_CACHEDIR=/data/users/singularity_cache\nexport NXF_SINGULARITY_TMPDIR=/data/users/temp\nexport NXF_SINGULARITY_LOCALCACHEDIR=/data/users/singularity_cache\n\n# Ensure directories exist\nmkdir -p $NXF_SINGULARITY_CACHEDIR\nmkdir -p $NXF_SINGULARITY_TMPDIR\nmkdir -p $NXF_SINGULARITY_LOCALCACHEDIR\n\n# Navigate to PHoeNIx directory\ncd /data/users/$USER/nextflow-training/phoenix\n\n# Set up Kraken2 database path (if not already set)\nexport KRAKEN2_DB_PATH=/data/users/kraken2_db_local\n\n# IMPORTANT: Configure PHoeNIx for high-memory requirements within cluster limits\n# PHoeNIx has hardcoded resource limits that need adjustment for large Kraken2 databases\nsed -i 's/cpus   = { check_max( 2                  , '\\''cpus'\\''    ) }/cpus   = { check_max( 10                 , '\\''cpus'\\''    ) }/' conf/base.config\nsed -i 's/memory = { check_max( 10.GB \\* task.attempt, '\\''memory'\\''  ) }/memory = { check_max( 200.GB \\* task.attempt, '\\''memory'\\''  ) }/' conf/base.config\nsed -i 's/time   = { check_max( 4.h                 , '\\''time'\\''    ) }/time   = { check_max( 8.h                 , '\\''time'\\''    ) }/' conf/base.config\n\necho \"\ud83d\udd0d Verifying PHoeNIx configuration changes:\"\ngrep -A 4 \"withName:KRAKEN2_KRAKEN2\" conf/base.config\n\n# Run PHoeNIx test using the cloned repository\nnextflow run main.nf \\\n    -profile singularity,test,slurm \\\n    -entry PHOENIX \\\n    --kraken2db $KRAKEN2_DB_PATH \\\n    --outdir test_results \\\n    -c /users/mamana/microbial-genomics-training/cluster.config \\\n    -resume\n\necho \"\u2705 PHoeNIx test completed successfully!\"\necho \"\u2705 Singularity containers cached in: $NXF_SINGULARITY_CACHEDIR\"\n</code></pre>"},{"location":"modules/day7/#step-7-run-phoenix-with-your-tb-data","title":"Step 7: Run PHoeNIx with Your TB Data","text":"<p>Now let's analyze our TB samples using the cloned repository:</p> <pre><code># Ensure Singularity environment is set up\nexport NXF_SINGULARITY_CACHEDIR=/data/users/singularity_cache\nexport NXF_SINGULARITY_TMPDIR=/data/users/temp\nexport NXF_SINGULARITY_LOCALCACHEDIR=/data/users/singularity_cache\n\n# Set up Kraken2 database path (if not already set)\nexport KRAKEN2_DB_PATH=/data/users/kraken2_db_local\n\n# Run PHoeNIx with TB data using the local repository and complete database\nnextflow run main.nf \\\n    -profile singularity,slurm \\\n    -entry PHOENIX \\\n    --input /data/users/$USER/nextflow-training/phoenix-analysis/phoenix_samplesheet.csv \\\n    --kraken2db $KRAKEN2_DB_PATH \\\n    --outdir tb_analysis_results \\\n    -c /users/mamana/microbial-genomics-training/cluster.config \\\n    -resume\n\necho \"\ud83d\udd25 PHoeNIx TB analysis started!\"\necho \"\ud83d\udce6 All containers will be cached in: $NXF_SINGULARITY_CACHEDIR\"\necho \"\ud83d\udcbe Expected memory usage: 200GB for Kraken2 processes (within cluster limits)\"\necho \"\u23f1\ufe0f  Expected runtime: 2-8 hours depending on data size\"\n</code></pre>"},{"location":"modules/day7/#troubleshooting-phoenix-issues_1","title":"Troubleshooting PHoeNIx Issues","text":"<p>If you encounter errors, here are common solutions:</p> <pre><code># Error 1: \"Process terminated with an error exit status (137)\" - Memory Issue\n# This indicates the process was killed due to insufficient memory\necho \"\ud83d\udd0d Checking PHoeNIx memory configuration:\"\ngrep -A 4 \"withName:KRAKEN2_KRAKEN2\" conf/base.config\n\n# Should show 200.GB, not 10.GB. If not, re-run the sed commands:\nsed -i 's/memory = { check_max( 10.GB \\* task.attempt, '\\''memory'\\''  ) }/memory = { check_max( 200.GB \\* task.attempt, '\\''memory'\\''  ) }/' conf/base.config\n\n# Error 2: \"No entry workflow specified\"\n# Always include -entry PHOENIX parameter\nnextflow run main.nf \\\n    -entry PHOENIX \\\n    --input /data/users/$USER/nextflow-training/phoenix-analysis/phoenix_samplesheet.csv \\\n    --kraken2db $KRAKEN2_DB_PATH \\\n    --outdir tb_analysis_results \\\n    -c /users/mamana/microbial-genomics-training/cluster.config \\\n    -profile singularity,slurm \\\n    -resume\n\n# Error 3: \"No such file or directory: kraken2_db_standard_folder/ktaxonomy.tsv\"\n# This affects the KRAKENTOOLS_MAKEKREPORT process (post-processing)\n# PERMANENT SOLUTION: Create a local Kraken2 database copy with ktaxonomy.tsv\necho \"\ud83d\udd27 Creating permanent fix for ktaxonomy.tsv issue...\"\n\n# Create the ktaxonomy.tsv file\ncat &gt; /tmp/ktaxonomy.tsv &lt;&lt; 'EOF'\n1|root|1|no rank|\n2|Bacteria|131567|superkingdom|\n1763|Mycobacterium|1763|genus|\n1773|Mycobacterium tuberculosis|1763|species|\n131567|cellular organisms|1|no rank|\n85007|Actinobacteria|2|phylum|\n1224|Proteobacteria|2|phylum|\n1239|Firmicutes|2|phylum|\nEOF\n\n# Create a local database copy with all files\nmkdir -p $HOME/kraken2_db_with_ktaxonomy\n\n# Create symbolic links to all existing files\nfor file in /data/kraken2_db_standard/*; do\n    if [ -f \"$file\" ]; then\n        ln -sf \"$file\" $HOME/kraken2_db_with_ktaxonomy/\n    elif [ -d \"$file\" ]; then\n        ln -sf \"$file\" $HOME/kraken2_db_with_ktaxonomy/\n    fi\ndone\n\n# Add the missing ktaxonomy.tsv file\ncp /tmp/ktaxonomy.tsv $HOME/kraken2_db_with_ktaxonomy/\n\necho \"\u2705 Created local database copy with ktaxonomy.tsv\"\nls -la $HOME/kraken2_db_with_ktaxonomy/\n\n# Update the kraken2db parameter to use the fixed database\nKRAKEN2_DB=\"$HOME/kraken2_db_with_ktaxonomy\"\n\n# Error 4: Singularity issues\n# Solution: Check Singularity setup and permissions\nmodule list\nwhich singularity\nls -la $NXF_SINGULARITY_CACHEDIR\n\n# Re-run with verbose output for debugging\nnextflow run main.nf \\\n    -entry PHOENIX \\\n    --input /data/users/$USER/nextflow-training/phoenix-analysis/phoenix_samplesheet.csv \\\n    --kraken2db $KRAKEN2_DB_PATH \\\n    --outdir tb_analysis_results \\\n    -c /users/mamana/microbial-genomics-training/cluster.config \\\n    -profile singularity,slurm \\\n    -resume \\\n    -with-trace \\\n    -with-report \\\n    -with-timeline\n</code></pre>"},{"location":"modules/day7/#step-8-understanding-phoenix-outputs","title":"Step 8: Understanding PHoeNIx Outputs","text":"<p>While the analysis runs, let's explore what PHoeNIx produces:</p> <pre><code># PHoeNIx creates comprehensive outputs:\ntree tb_analysis_results/ -L 2\n\n# Key output directories:\n# \u251c\u2500\u2500 ASSEMBLY/          # Genome assemblies\n# \u251c\u2500\u2500 ANNOTATION/        # Gene annotations\n# \u251c\u2500\u2500 AMR/              # Antimicrobial resistance results\n# \u251c\u2500\u2500 MLST/             # Multi-locus sequence typing\n# \u251c\u2500\u2500 QC/               # Quality control metrics\n# \u251c\u2500\u2500 REPORTS/          # Summary reports\n# \u2514\u2500\u2500 TAXA/             # Species identification\n</code></pre>"},{"location":"modules/day7/#expected-phoenix-performance","title":"Expected PHoeNIx Performance","text":"<p>Based on our successful testing, here's what to expect:</p> <pre><code># \u2705 Successfully completed processes (in order):\necho \"1. Input validation and corruption checks\"\necho \"2. Raw read statistics (GET_RAW_STATS)\"\necho \"3. Quality trimming with FASTP\"\necho \"4. FastQC on trimmed reads\"\necho \"5. Kraken2 taxonomic classification (256GB memory)\"\necho \"6. Krona visualization\"\necho \"7. SPAdes genome assembly\"\necho \"8. QUAST assembly quality assessment\"\necho \"9. MASH distance calculations\"\necho \"10. FastANI species identification\"\necho \"11. Prokka annotation\"\necho \"12. AMR detection with AMRFinderPlus\"\n\n# \ud83d\udcca Performance metrics:\necho \"Memory usage: Up to 256GB for Kraken2 processes\"\necho \"Runtime: 2-8 hours depending on data size\"\necho \"SLURM jobs: ~50-60 jobs total\"\necho \"Output size: ~500MB-2GB per sample\"\n\n# \ud83c\udfaf Key success indicators:\necho \"\u2705 Kraken2 processes complete without exit status 137\"\necho \"\u2705 SPAdes assembly produces contigs\"\necho \"\u2705 Species identified as Mycobacterium tuberculosis\"\necho \"\u2705 AMR genes detected and reported\"\n</code></pre>"},{"location":"modules/day7/#alternative-running-phoenix-from-remote-repository","title":"Alternative: Running PHoeNIx from Remote Repository","text":"<p>If you prefer not to clone the repository locally, you can still run PHoeNIx directly from GitHub:</p> <pre><code># Run PHoeNIx directly from GitHub repository\nnextflow run cdcgov/phoenix \\\n    -r v2.1.1 \\\n    -profile singularity,slurm \\\n    -entry PHOENIX \\\n    --input /data/users/$USER/nextflow-training/phoenix-analysis/phoenix_samplesheet.csv \\\n    --kraken2db $KRAKEN2_DB_PATH \\\n    --outdir tb_analysis_results \\\n    -resume\n\n# This approach downloads the pipeline automatically but doesn't give you\n# local access to modify configuration files\n</code></pre>"},{"location":"modules/day7/#exercise-5-analyzing-phoenix-results","title":"Exercise 5: Analyzing PHoeNIx Results","text":""},{"location":"modules/day7/#exploring-the-results-structure","title":"Exploring the Results Structure","text":"<pre><code># Navigate to results (using the actual output directory from the command)\ncd /data/users/$USER/nextflow-training/phoenix-analysis/tb_analysis_results\n\n# Check the overall structure\necho \"\ud83d\udcc1 PHoeNIx Output Structure:\"\nls -la\n\n# Check the main summary report  \necho \"\ud83d\udcca Summary Reports:\"\nls -la summaries/ 2&gt;/dev/null || echo \"Summaries directory not yet created\"\n\n# View quality control results\necho \"\ud83d\udd0d Quality Control Summary:\"\nhead summaries/Phoenix_Summary.tsv 2&gt;/dev/null || echo \"Phoenix_Summary.tsv not yet generated\"\n\n# Check AMR results\necho \"\ud83e\udda0 Antimicrobial Resistance Results:\"\nls -la amr/ 2&gt;/dev/null || echo \"AMR directory not yet created\"\nhead amr/*_amrfinder_all.tsv 2&gt;/dev/null || echo \"AMR files not yet generated\"\n\n# View assembly statistics  \necho \"\ud83e\uddec Assembly Statistics:\"\nls -la assembly/ 2&gt;/dev/null || echo \"Assembly directory not yet created\"\nhead assembly/*_assembly_stats.txt 2&gt;/dev/null || echo \"Assembly stats not yet generated\"\n\n# Check annotation results\necho \"\ud83d\udcdd Annotation Results:\"\nls -la annotation/ 2&gt;/dev/null || echo \"Annotation directory not yet created\"\n</code></pre>"},{"location":"modules/day7/#understanding-clinical-outputs","title":"Understanding Clinical Outputs","text":"<p>PHoeNIx provides clinical-grade outputs structured for pathogen genomics:</p> <ol> <li>Species Identification: Kraken2-based taxonomic classification</li> <li>Quality Metrics: Read quality, assembly statistics, contamination assessment</li> <li>AMR Profiling: Resistance genes via AMRFinderPlus and NCBI database</li> <li>MLST Typing: Multi-locus sequence typing for strain classification</li> <li>Comprehensive Summary: All results consolidated in Phoenix_Summary.tsv</li> </ol>"},{"location":"modules/day7/#comparing-with-your-exercise-3-pipeline","title":"Comparing with Your Exercise 3 Pipeline","text":"<p>Let's compare PHoeNIx results with our custom pipeline:</p> <pre><code># Set environment variable for consistent paths\nexport KRAKEN2_DB_PATH=/data/users/kraken2_db_local\n\n# Compare assembly approaches\necho \"=== Exercise 3 Results ===\"\nls -la /data/users/$USER/nextflow-training/results/assemblies/ 2&gt;/dev/null || echo \"Exercise 3 results not found\"\n\necho \"=== PHoeNIx Results ===\"\nls -la /data/users/$USER/nextflow-training/phoenix-analysis/tb_analysis_results/assembly/ 2&gt;/dev/null || echo \"PHoeNIx results not yet generated\"\n\n# Compare annotation approaches\necho \"=== Exercise 3 Prokka Annotation ===\"\nls -la /data/users/$USER/nextflow-training/results/annotation/ 2&gt;/dev/null || echo \"Exercise 3 annotation not found\"\n\necho \"=== PHoeNIx Annotation (Prokka + additional tools) ===\"\nls -la /data/users/$USER/nextflow-training/phoenix-analysis/tb_analysis_results/annotation/ 2&gt;/dev/null || echo \"PHoeNIx annotation not yet generated\"\n\n# Check PHoeNIx-specific clinical outputs\necho \"=== PHoeNIx Clinical Features (not in Exercise 3) ===\"\necho \"\u2022 AMR Analysis:\" &amp;&amp; ls -la /data/users/$USER/nextflow-training/phoenix-analysis/tb_analysis_results/amr/ 2&gt;/dev/null || echo \"  AMR results pending\"\necho \"\u2022 MLST Typing:\" &amp;&amp; ls -la /data/users/$USER/nextflow-training/phoenix-analysis/tb_analysis_results/mlst/ 2&gt;/dev/null || echo \"  MLST results pending\"  \necho \"\u2022 Clinical Summary:\" &amp;&amp; ls -la /data/users/$USER/nextflow-training/phoenix-analysis/tb_analysis_results/summaries/ 2&gt;/dev/null || echo \"  Summary pending\"\n</code></pre>"},{"location":"modules/day7/#key-learning-points-from-phoenix-testing","title":"Key Learning Points from PHoeNIx Testing","text":"<p>Our successful PHoeNIx implementation demonstrates several critical production pipeline concepts:</p>"},{"location":"modules/day7/#1-resource-management-in-production-pipelines","title":"1. Resource Management in Production Pipelines","text":"<ul> <li>Memory Requirements: Large databases like Kraken2 require 256GB+ memory</li> <li>Hardcoded Limits: Production pipelines may have configuration files that override user settings</li> <li>Resource Optimization: Different processes require different resource allocations</li> </ul>"},{"location":"modules/day7/#2-pipeline-configuration-management","title":"2. Pipeline Configuration Management","text":"<ul> <li>Base Configuration Files: Understanding <code>conf/base.config</code> vs user configuration</li> <li>Entry Workflows: Complex pipelines may have multiple entry points (<code>-entry PHOENIX</code>)</li> <li>Profile Management: Combining profiles (<code>singularity,slurm</code>) for different environments</li> </ul>"},{"location":"modules/day7/#3-troubleshooting-production-issues","title":"3. Troubleshooting Production Issues","text":"<ul> <li>Exit Status 137: Always indicates memory issues, not configuration problems</li> <li>Missing Files: Post-processing steps may require additional database files</li> <li>Resume Functionality: Critical for long-running analyses in production</li> </ul>"},{"location":"modules/day7/#4-real-world-bioinformatics-challenges","title":"4. Real-World Bioinformatics Challenges","text":"<ul> <li>Database Dependencies: Production tools require specific database formats and files</li> <li>Container Management: Singularity cache management for large container images</li> <li>SLURM Integration: Proper job scheduling and resource allocation</li> </ul>"},{"location":"modules/day7/#key-learning-points","title":"Key Learning Points","text":""},{"location":"modules/day7/#production-pipeline-advantages","title":"Production Pipeline Advantages","text":"<ol> <li>Standardization: Consistent analysis across laboratories</li> <li>Validation: Extensively tested and validated</li> <li>Clinical Focus: Designed for healthcare applications</li> <li>Comprehensive: Includes all necessary analyses</li> <li>Reporting: Professional-grade output reports</li> <li>Maintenance: Actively maintained and updated</li> </ol>"},{"location":"modules/day7/#when-to-use-production-pipelines","title":"When to Use Production Pipelines","text":"<ul> <li>\u2705 Clinical diagnostics: Patient sample analysis</li> <li>\u2705 Public health surveillance: Outbreak investigations</li> <li>\u2705 Regulatory compliance: FDA/CDC requirements</li> <li>\u2705 Multi-site studies: Standardized protocols</li> <li>\u2705 High-throughput: Large sample volumes</li> </ul>"},{"location":"modules/day7/#when-to-build-custom-pipelines","title":"When to Build Custom Pipelines","text":"<ul> <li>\u2705 Research questions: Novel analysis approaches</li> <li>\u2705 Specialized organisms: Non-standard pathogens</li> <li>\u2705 Method development: Testing new algorithms</li> <li>\u2705 Educational purposes: Learning workflow development</li> <li>\u2705 Resource constraints: Limited computational resources</li> </ul>"},{"location":"modules/day7/#exercise-4-success-summary","title":"\ud83c\udf89 Exercise 4 Success Summary","text":"<p>Congratulations! You have successfully:</p> <p>\u2705 Configured PHoeNIx for production use with 256GB memory allocation \u2705 Resolved memory issues by modifying hardcoded pipeline configuration \u2705 Implemented proper entry workflows using <code>-entry PHOENIX</code> \u2705 Integrated SLURM scheduling with Singularity containerization \u2705 Processed real TB genomic data through a CDC production pipeline \u2705 Learned troubleshooting skills for production bioinformatics environments</p> <p>Key Achievement: You can now run production-grade pathogen genomics pipelines that are used in real public health laboratories worldwide.</p> <p>This exercise demonstrates how to integrate production-grade pipelines into your bioinformatics workflows, providing the foundation for real-world pathogen genomics analysis.</p>"},{"location":"modules/day7/#running-workflows-from-github_1","title":"Running Workflows from GitHub","text":""},{"location":"modules/day7/#why-use-github-hosted-workflows","title":"Why Use GitHub-Hosted Workflows?","text":"<p>Running workflows directly from GitHub repositories offers several advantages:</p> <ul> <li>Version Control: Specify exact versions or branches</li> <li>Collaboration: Share workflows easily with colleagues</li> <li>Reproducibility: Ensure everyone uses the same workflow version</li> <li>No Local Storage: No need to download workflows locally</li> <li>Automatic Updates: Pull latest changes automatically</li> </ul>"},{"location":"modules/day7/#exercise-5-running-your-workflow-from-github","title":"Exercise 5: Running Your Workflow from GitHub","text":""},{"location":"modules/day7/#step-1-push-your-workflow-to-github","title":"Step 1: Push Your Workflow to GitHub","text":"<p>First, let's push your Exercise 3 pipeline to GitHub:</p> <pre><code># Navigate to your workflows directory\ncd /users/$USER/microbial-genomics-training/workflows\n\n# Initialize module system and load git\nsource /opt/lmod/8.7/lmod/lmod/init/bash\nmodule load git\n\n# Add your workflow files\ngit add qc_pipeline.nf nextflow.config samplesheet.csv\ngit commit -m \"Add Exercise 3 QC pipeline for GitHub execution\"\n\n# Push to your GitHub repository\ngit push origin main\n</code></pre>"},{"location":"modules/day7/#step-2-run-workflow-from-github","title":"Step 2: Run Workflow from GitHub","text":"<p>Now anyone can run your workflow directly from GitHub:</p> <pre><code># Run your workflow from GitHub (replace USERNAME with your GitHub username)\nnextflow run USERNAME/microbial-genomics-training/workflows/qc_pipeline.nf \\\n    --input /users/$USER/microbial-genomics-training/workflows/samplesheet.csv \\\n    --outdir /data/users/$USER/nextflow-training/results_github\n\n# Run a specific version/branch\nnextflow run USERNAME/microbial-genomics-training/workflows/qc_pipeline.nf \\\n    -r v1.0 \\\n    --input /users/$USER/microbial-genomics-training/workflows/samplesheet.csv \\\n    --outdir /data/users/$USER/nextflow-training/results_v1\n\n# Run from a specific branch\nnextflow run USERNAME/microbial-genomics-training/workflows/qc_pipeline.nf \\\n    -r development \\\n    --input /users/$USER/microbial-genomics-training/workflows/samplesheet.csv \\\n    --outdir /data/users/$USER/nextflow-training/results_dev\n</code></pre>"},{"location":"modules/day7/#step-3-share-with-collaborators","title":"Step 3: Share with Collaborators","text":"<p>Your colleagues can now run your workflow:</p> <pre><code># Anyone with access can run your workflow\nnextflow run USERNAME/microbial-genomics-training/workflows/qc_pipeline.nf \\\n    --input their_samplesheet.csv \\\n    --outdir their_results/\n\n# They can also specify different parameters\nnextflow run USERNAME/microbial-genomics-training/workflows/qc_pipeline.nf \\\n    --input their_data.csv \\\n    --outdir their_results/ \\\n    --adapters /path/to/their/adapters.fa\n</code></pre>"},{"location":"modules/day7/#step-4-version-management","title":"Step 4: Version Management","text":"<p>Use Git tags for stable releases:</p> <pre><code># Create a release version\ngit tag -a v1.0 -m \"Release version 1.0: Stable QC pipeline\"\ngit push origin v1.0\n\n# Users can now run the stable version\nnextflow run USERNAME/microbial-genomics-training/workflows/qc_pipeline.nf \\\n    -r v1.0 \\\n    --input samplesheet.csv \\\n    --outdir results_stable/\n</code></pre>"},{"location":"modules/day7/#benefits-of-github-workflow-execution","title":"Benefits of GitHub Workflow Execution","text":"<ol> <li>Centralized Distribution: One source of truth for your workflow</li> <li>Version Control: Track changes and maintain stable releases</li> <li>Collaboration: Easy sharing and contribution from team members</li> <li>Documentation: README files and wiki for usage instructions</li> <li>Issue Tracking: Bug reports and feature requests</li> <li>Continuous Integration: Automated testing with GitHub Actions</li> </ol>"},{"location":"modules/day7/#best-practices","title":"Best Practices","text":"<ul> <li>Tag Releases: Use semantic versioning (v1.0.0, v1.1.0, etc.)</li> <li>Document Parameters: Clear README with parameter descriptions</li> <li>Test Data: Include small test datasets for validation</li> <li>License: Add appropriate license for sharing</li> <li>Changelog: Document changes between versions</li> </ul>"},{"location":"modules/day7/#key-skills-developed","title":"Key Skills Developed","text":"<ul> <li>Git workflow: Track changes, collaborate, and manage versions</li> <li>Container integration: Use Docker for reproducible environments</li> <li>MTB genomics: Understand pathogen-specific analysis requirements</li> <li>Clinical applications: Apply genomics to real-world healthcare problems</li> <li>Professional development: Document, test, and deploy pipelines properly</li> </ul>"},{"location":"modules/day7/#tomorrow-day-8-preview","title":"Tomorrow: Day 8 Preview","text":"<p>Day 8 will focus on Comparative Genomics:</p> <ul> <li>Pan-genome analysis with your MTB pipeline outputs</li> <li>Phylogenetic inference from core genome SNPs</li> <li>Tree construction and visualization</li> <li>Population genomics and outbreak investigation</li> </ul> <p>Your production MTB pipeline from today will provide the foundation for comparative analysis across multiple isolates!</p>"},{"location":"modules/day7/#resources","title":"Resources","text":""},{"location":"modules/day7/#documentation","title":"Documentation","text":"<ul> <li>Git Documentation</li> <li>GitHub Guides</li> <li>Docker Documentation</li> <li>Nextflow Documentation</li> </ul>"},{"location":"modules/day7/#container-repositories","title":"Container Repositories","text":"<ul> <li>BioContainers</li> <li>StaPH-B Docker Images</li> <li>nf-core Containers</li> </ul>"},{"location":"modules/day7/#mtb-specific-tools","title":"MTB-Specific Tools","text":"<ul> <li>TB-Profiler</li> <li>AMRFinderPlus</li> <li>QUAST</li> </ul>"},{"location":"modules/day7/#professional-development_1","title":"Professional Development","text":"<ul> <li>nf-core Guidelines</li> <li>Bioinformatics Best Practices</li> <li>Scientific Software Development</li> </ul>"},{"location":"modules/day7/#phoenix-production-troubleshooting-quick-reference","title":"PHoeNIx Production Troubleshooting Quick Reference","text":"<p>Essential fixes for common deployment issues:</p>"},{"location":"modules/day7/#krakentools-format-error","title":"KRAKENTOOLS Format Error","text":"<pre><code># Error: \"ValueError: not enough values to unpack (expected 5, got 1)\"\n# Cause: Wrong ktaxonomy.tsv format (4 fields vs 5)\n# Fix: Use correct PHoeNIx format\ncp phoenix/assets/databases/ktaxonomy.tsv ../kraken2_db_local/\n</code></pre>"},{"location":"modules/day8/","title":"Metagenomics in Clinical &amp; Public Health","text":""},{"location":"modules/day8/#1-overview","title":"1. Overview","text":"<p>This section introduces core principles of metagenomics. Metagenomics is the study of genetic material recovered directly from environmental or clinical samples, allowing  analysis of entire microbial communities without the need for cultivation. This section includes conceptual notes, rationale for each step, practical commands,  and a mini toy dataset exercise. Unlike genomics (WGS) which focuses on analyzing individual genomes (such as bacterial isolate), metagenomics studies the collective genomes or markers from microbial communities.</p>"},{"location":"modules/day8/#genomics-vs-metagenomics","title":"Genomics vs Metagenomics","text":"Description Whole Genome Sequencing (WGS) Metagenomics Umbrella term Microbial Genomics Microbial Profiling Scope Sequencing of the entire genome of a single isolate (pure culture) Sequences all genetic material in mixed community (without culturing) Applications Comparative genomics, AMR/virulence genes/MGE, outbreak tracking Community profiling, functional potential, pathogen detection in mixed samples, ecology studies (diversity studies) Features Organism is known \\&amp; isolated before sequencing Captures both known and unknown microbes from environment or host"},{"location":"modules/day8/#metagenomic-strategies-shotgun-vs-16s-rrna","title":"Metagenomic Strategies: Shotgun vs 16S rRNA","text":"Description Shotgun Metagenomics 16S rRNA Amplicon Gene Sequencing Definition Random sequencing of All DNA in a sample Targeted amplicon sequencing of the 16 S rRNA gene Resolution Species- and strain-level, functional genes (AMR, metabolism, plasmids, MGEs) Genus-level (sometimes species-level), no direct functional information Merits Comprehensive (taxonomy + function), detects viruses and fungi Limited to taxonomic resolution, can't detect fungi/viruses, lacks functional insights, good for bacterial surveys Demerits More expensive, higher computational load Cost-effective, standardized"},{"location":"modules/day8/#2-workflow-from-sequencing-to-interpretation","title":"2. Workflow: From Sequencing to Interpretation","text":""},{"location":"modules/day8/#step-1-study-design-sequencing","title":"Step 1. Study Design &amp; Sequencing","text":"<ul> <li>Why: Sequencing depth, read length, and platform choice directly influence resolution of taxa, detection of low-abundance organisms, and assembly quality. For example, if you aim to do strain-level tracking of low abundance organisms, increase depth or perform targeted enrichment.</li> <li>Example:</li> <li>Illumina (short reads): accurate, cost-effective, widely used for clinical metagenomics.</li> <li>ONT/PacBio (long reads): useful for resolving repeats, plasmids, MGEs.</li> </ul>"},{"location":"modules/day8/#step-2-quality-control","title":"Step 2. Quality Control","text":"<ul> <li>Why: To reduce the effects of sequencing errors, remove adapters, and low-quality  reads which may reduce false positives.</li> <li>Tools: <code>fastqc</code>, <code>multiqc</code></li> </ul> <p>What to think about? - Which parts of the data are flagged as potentially problematic? GC% content of the dataset. NB: We are dealing with mixed community and organisms, so its difficult to  have a perfect normal distribution around the average. As such we consider this as normal given the biology of the system. - Does the sequencing length matches the libraries used? If sequences are shorter than expected, are adapters  a concern? - Are adapters and/or barcodes removed?   - Look at the Per base sequence content to diagnose this. - Is there unexpected sequence duplication?   - This can occur when low-input library preparations are used. - Are over-represented k-mers present?   - This can be a sign of adapter and barcode contamination.</p>"},{"location":"modules/day8/#step-3-trimming-filtering","title":"Step 3. Trimming &amp; Filtering","text":"<ul> <li>Why: Removes adapters (which can cause false alignments \\&amp; false taxonomic assignments), low-quality bases (increase error rates), and very short  reads (to standardize read lengths) that bias downstream analysis.</li> <li>Tool: <code>fastp</code>, <code>trimmomatic</code>, <code>BBMap</code>, <code>sickle</code>, <code>cutadapt</code>, etc.</li> </ul> <p><pre><code>#!/bin/bash\n\n# Load required modules/tools\nmodule load trimmomatic\n\n# Define input and output dir\nindata=\"/data/users/user24/metagenomes/\"\nwkdir=\"/data/users/${USER}/metagenomics/shotgun/\"\nouttrimmomatic=${wkdir}\"/data_analysis/02_trimmomatic\"\n\nmkdir -p ${outtrimmomatic}\n\n# Trimming with Trimmomatic\nfor file in `ls ${data}*1.fastq.gz`\ndo\n  sample=$(basename ${file} _1.fastq.gz)\n  trimmomatic PE -threads 8 \\\n      ${indata}${sample}_R1.fastq.gz {indata}${sample}_R2.fastq.gz \\\n      ${outtrimmomatic}${sample}_R1.fastq.gz ${outtrimmomatic}${sample}_R1_unpaired.fastq.gz \\\n      ${outtrimmomatic}${sample}_R2.fastq.gz ${outtrimmomatic}${sample}_R2_unpaired.fastq.gz \\\n      ILLUMINACLIP:adapters.fa:2:30:10 \\\n      LEADING:3 TRAILING:3 \\\n      SLIDINGWINDOW:4:20 \\\n      MINLEN:50\ndone\necho \"Trimming with Trimmomatic completed\"\n</code></pre> Parameter | Type |  Description ----------|------|----------------------------- PE |    positional |    Specifies whether we are analysing single- or paired-end reads -threads 2 |    keyword |   Specifies the number of threads to use when processing -phred33 |  keyword |   Specifies the fastq encoding used \\({indata}\\)_R2.fastq.gz |  positional  | The paired forward and reverse reads to trim ILLUMINACLIP:adapters.fa:2:30:10 |  positional |    Adapter trimming allowing for 2 seed mismatch, palindrome clip score threshold of 30, and simple clip score threshold of 10 SLIDINGWINDOW:4:20  | positional    |Quality filtering command. Analyse each sequence in a 4 base pair sliding window and then truncate if the average quality drops below Q20 MINLEN:50   | positional |  Length filtering command. Discard sequences that are shorter than 80 base pairs after trimming}_R1.fastq.gz {indata}${sample</p> <p>Note: The trimming parameters in <code>trimmomatic</code> are processed in the order they are specified. For instance, </p> <p><pre><code>for file in `ls ${data}*1.fastq.gz`\ndo\n  sample=$(basename ${file} _1.fastq.gz)\n  trimmomatic PE -threads 8 \\\n        ${indata}${sample}_R1.fastq.gz {indata}${sample}_R2.fastq.gz \\\n        ${outtrimmomatic}${sample}_R1.fastq.gz ${outtrimmomatic}${sample}_R1_unpaired.fastq.gz \\\n        ${outtrimmomatic}${sample}_R2.fastq.gz ${outtrimmomatic}${sample}_R2_unpaired.fastq.gz \\\n        ILLUMINACLIP:adapters.fa:2:30:10 \\\n        LEADING:3 TRAILING:3 \\\n        MINLEN:50 \\\n        SLIDINGWINDOW:4:20 \ndone\n</code></pre> means we remove sequences shorter than 50 bps and then qualiyty trim, thus if a sequence is trimmed to a length shorter than 50bps after trimming, the <code>MINLEN</code> filtering does not execute a second time.</p> <pre><code># Delete unnecessary files\nrm -rf ${outtrimmomatic}*${sample}*_unpaired.fastq.gz\n\n# Quality checking\nmodule load fastqc multiqc\nfastqc ${outtrimmomatic}* -o ${outtrimmomatic}\nmultiqc  ${outtrimmomatic} -o  ${outtrimmomatic}\n</code></pre>"},{"location":"modules/day8/#step-4-deduplication-host-dna-removal","title":"Step 4. Deduplication &amp; Host DNA Removal","text":"<ul> <li>Why:</li> <li>Often metagenomes are obtained from host-associated microbial communities. As a result, they contain significant amount of host DNA which may interfere with microbial analysis   and create privacy concerns.</li> <li>Specifically any studies invloving human subjects or samples derived from Taonga species.</li> <li> <p>Although several approaches are used for this, the most popular is to map reads to a reference genome (includin human genome). That is remove all reads that map to the reference of the dataset.</p> </li> <li> <p>Tools: <code>clumpify</code> (dedup), <code>BBMap</code> (bbmap.sh),  <code>bowtie2</code> or <code>bwa mem</code> (host removal).</p> </li> </ul> <pre><code>#!/bin/bash\nset -e  # Exit on error\n\n# Define dirs\ngenome_dir=\"/data/users/user24/refs/human_reference/\"\n\n## Remove human contamination using BWA and SAMtools\n## Create dir\nmkdir -p ${genome_dir}\n# Download the Human refence genome\ncd ${genome_dir}\n\n# Configuration\nGENOME_VERSION=\"GRCh38\"\nBASE_URL=\"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38\"\n\necho \"=== Downloading Human Reference Genome (${GENOME_VERSION}) ===\"\n\n# Option 1: Download complete genome assembly (recommended for most applications)\necho \"Downloading complete genome assembly...\"\nwget -c \"${BASE_URL}/GCA_000001405.15_GRCh38_genomic.fna.gz\" \\\n     -O \"GRCh38_genomic.fna.gz\"\n# Decompress\necho \"Decompressing genome file...\"\ngunzip -f GRCh38_genomic.fna.gz\n\n# Option 2: Download chromosome-only version (excludes contigs/scaffolds)\necho \"Downloading chromosome-only version...\"\nwget -c \"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\" \\\n     -O \"hg38_chromosomes_only.fa.gz\"\n\ngunzip -f hg38_chromosomes_only.fa.gz\n</code></pre> <pre><code>#!/bin/bash\n\n# Load modules\nmodule load bwa\nmodule load bowtie2\n\necho \"=== Building BWA Index ===\"\n# Build BWA index for alignment (required for host removal, one time set-up)\nbwa index GRCh38_genomic.fna\n\necho \"=== Verifying Download ===\"\n# Check file integrity\necho \"Genome file size:\"\nls -lh *.fna\n\necho \"Number of sequences:\"\ngrep -c \"&gt;\" GRCh38_genomic.fna\n\necho \"First few sequence headers:\"\ngrep \"&gt;\" GRCh38_genomic.fna | head -10\n\necho \"=== Download Complete ===\"\necho \"Reference genome files are ready in: $(pwd)\"\necho \"Main genome file: GRCh38_genomic.fna\"\necho \"Chromosome-only file: hg38_chromosomes_only.fa\"\necho \"\"\necho \"Files generated:\"\necho \"- GRCh38_genomic.fna (main reference)\"\necho \"- GRCh38_genomic.fna.amb, .ann, .bwt, .pac, .sa (BWA index)\"\necho \"- GRCh38_genomic.fna.fai (samtools index)\"\n\n# Align reads to human genome\nfor file in `ls ${indata}*1.fastq.gz`\ndo\n  sample=$(basename ${file} _1.fastq.gz)\n  bowtie2 -x ${refs}human_index -1 {sample}${data}_trimmed_R1.fastq.gz -2 sample_trimmed_R2.fastq.gz \\\n    --un-conc sample_host_removed.fastq.gz -S /dev/null\n\n\n  bwa mem -t 8 human_reference.fasta \\\n      trimmed_R1_paired.fastq.gz trimmed_R2_paired.fastq.gz \\\n      | samtools view -b -f 4 - &gt; unmapped_reads.bam\n\n  # Convert unmapped reads back to FASTQ\n  samtools fastq -1 dehosted_R1.fastq.gz -2 dehosted_R2.fastq.gz unmapped_reads.bam\n\necho \"Host removal completed\"\n</code></pre>"},{"location":"modules/day8/#step-5-taxonomic-profiling-read-based","title":"Step 5. Taxonomic Profiling (Read-Based)","text":"<ul> <li>Why: Directly assigns taxonomy without assembly, faster and less computationally intensive.</li> <li>Research question: Who is in this sample?</li> <li>Tools: Kraken2 + Bracken (abundance estimation), MetaPhlAn/ MetaSpades + Mapping + MetaBAT2 (or CONCOT/MaxBin2, DAS Tool) + checkM + GTDB-tk + ANI (species-level contamination)</li> <li>Reconstruct rRNA using <code>PhyloFlash - EMIRGE</code> (by leveraging the expansive catalogue of 16S rRNA genes available in databases such as SILVA in order to subset reads and then reconstruct the full-length gene).</li> </ul> <pre><code>kraken2 --db kraken2_db --paired sample_host_removed.1.fq.gz sample_host_removed.2.fq.gz \\\n  --output kraken2_output.txt --report kraken2_report.txt\nbracken -d kraken2_db -i kraken2_report.txt -o bracken_species.txt -r 150 -l S\n</code></pre>"},{"location":"modules/day8/#step-6-functional-profiling-read-based","title":"Step 6. Functional Profiling (Read-Based)","text":"<ul> <li>Why: Identifies pathways/genes present without needing assembly.</li> <li>Tool: HUMAnN</li> </ul> <pre><code>humann --input sample_host_removed.fastq.gz --output humann_out/\n</code></pre>"},{"location":"modules/day8/#step-7-assembly-based-profiling","title":"Step 7. Assembly-Based Profiling","text":"<ul> <li>Why: Reconstructs contigs/MAGs b\u0006\u0012 enables strain-level analysis, discovery of new genes, plasmids, MGEs.</li> <li>Tools: MEGAHIT or metaSPAdes</li> </ul>"},{"location":"modules/day8/#megahit","title":"MEGAHIT","text":"<pre><code>megahit -1 sample_host_removed.1.fq.gz -2 sample_host_removed.2.fq.gz -o megahit_out/\n</code></pre> <pre><code>spades.py --meta -1 sample_host_removed.1.fq.gz -2 sample_host_removed.2.fq.gz -o metaspades_out/\n</code></pre> <p>MEGAHIT vs metaSPAdes:</p> <ul> <li>MEGAHIT Advantages:</li> <li>Extremely fast, lower memory usage.</li> <li>Scales better for very large datasets (e.g., population metagenomes).</li> <li>MEGAHIT Disadvantages:</li> <li>May produce slightly shorter contigs than metaSPAdes.</li> <li>metaSPAdes Advantages:</li> <li>Produces higher-quality, longer assemblies (useful for MAG recovery).</li> <li>metaSPAdes Disadvantages:</li> <li>Requires more RAM and CPU time.</li> </ul>"},{"location":"modules/day8/#step-8-mapping-binning","title":"Step 8. Mapping &amp; Binning","text":"<ul> <li>Why: Group contigs into MAGs, quantify abundances.</li> <li>Tools: <code>bowtie2</code>, <code>samtools</code>, <code>MetaBAT2</code> / <code>CONCOT</code> / <code>MaxBin2</code>, <code>DAS Tool</code></li> </ul> <pre><code>bowtie2 -x megahit_out/final.contigs.fa -1 sample_host_removed.1.fq.gz -2 sample_host_removed.2.fq.gz | samtools sort -o aln.bam\nmetabat2 -i megahit_out/final.contigs.fa -a depth.txt -o bins_dir/bin\n</code></pre>"},{"location":"modules/day8/#step-9-mag-quality-control","title":"Step 9. MAG Quality Control","text":"<ul> <li>Why: Ensures completeness &amp; contamination are acceptable.</li> <li>Tool: CheckM</li> </ul> <pre><code>checkm lineage_wf bins_dir/ checkm_out/\n</code></pre>"},{"location":"modules/day8/#step-10-annotation-specialized-analyses","title":"Step 10. Annotation &amp; Specialized Analyses","text":"<ul> <li>Why: Identify AMR, virulence, plasmids, metabolic capacity.</li> <li>Tools: Prokka, Bakta, AMRFinderPlus, ABRicate, DRAM</li> <li>Use DRAM in place of Prokka/Bakta for functional annotation of MAGs.</li> <li>DRAM produces detailed metabolic profiles, pathway reconstruction, and microbial ecology insights.</li> </ul> <pre><code>DRAM.py annotate -i bins_dir/ -o dram_out/ --threads 16\n</code></pre>"},{"location":"modules/day8/#step-11-abundance-estimation-visualization","title":"Step 11. Abundance Estimation &amp; Visualization","text":"<ul> <li>Why: Quantifies taxa/genes  links to clinical or epidemiological metadata.</li> <li>Tools: CoverM, Krona, R for plots.</li> </ul> <pre><code>coverm contig --bam-files aln.bam --reference megahit_out/final.contigs.fa --methods tpm &gt; coverm_tpm.tsv\n</code></pre>"},{"location":"modules/day8/#step-12-reporting-reproducibility","title":"Step 12. Reporting &amp; Reproducibility","text":"<ul> <li>Why: Essential for public health applications.</li> <li>Use Nextflow + Singularity/Conda for reproducible pipelines.</li> <li>Summarize results in Excel or RMarkdown reports.</li> </ul>"},{"location":"modules/day8/#shotgun-metagenomics","title":"Shotgun metagenomics","text":"<p>For this training we will use the data in <code>/data/users/user29/metagenomes/shotgun/</code> for shotgun metagenomics. It is important to note that, one can download shotgun metagenome sequences from NCBI-SRA using <code>ncbi-tools</code>. Install <code>ncbi-tools</code> and run</p> <pre><code>## Fetch the data from NCBI-SRA\n# fasterq-dump SRR13827118 --progress --threads 8\n\n## Compress the files\ngzip SRR13827118*.fastq\n</code></pre> <pre><code># Load modules\nmodule load fastqc\nmodule load multiqc\n\n# Data directory\n#indata=\"/data/users/user29/metagenomes/shotgun/\"\nindata=\"/data/users/\"\n\n## Create Dir\nmkdir -p /data/users/$USER/metagenomes/shotgun/scripts /data/users/$USER/metagenomes/shotgun/logs\n\n# create a samplesheet.csv with three columns samplename,fastq_1,fastq_2\npython /data/users/user24/metagenomes/shotgun/scripts/samplesheet_generator.py ${indata} \\\n  /data/users/${USER}/metagenomes/shotgun/samplesheet.csv\n\n# Run raw QC\nnextflow run /data/users/$USER/metagenomes/shotgun/scripts/qc_pipeline_v.nf \\\n  --input /data/users/${USER}/metagenomes/shotgun/samplesheet.csv \\\n  --outdir /data/users/${USER}/metagenomes/shotgun/results/rawfastqc\n</code></pre>"},{"location":"modules/day8/#quality-assessment","title":"Quality assessment","text":"<pre><code>module load bbmap\n# Generate statistics for filtered SPAdes assembly\nstats.sh in=spades_assembly/spades.fna\n</code></pre> <pre><code># Assuming your contigs have .fa as suffix\nCONTIG_DIR=\"\"\nOUT_DIR=\"\"\nmkdir ${OUT_DIR}\nfor contig in ${CONTIG_DIR}*.fa\nmodule load bowtie2 samtools metabat2\ndo\n    SAMPLE=$(basename ${contig} .fa)\n    R1=${rawdata}/${SAMPLE}_R1_001.fastq.gz\n    R2=${rawdata}/${SAMPLE}_R2_001.fastq.gz\n\n    echo \"=== Processing ${SAMPLE} ===\"\n\n    CONTIGS=${CONTIG_DIR}${SAMPLE}\".fa\"\n    mkdir -p ${OUT_DIR}/${SAMPLE}/\"align\"\n\n    ## Step 2: Bowtie2 index\n    #echo \"Building Bowtie2 index...\"\n    bowtie2-build ${CONTIGS} ${OUT_DIR}/${SAMPLE}/align/${SAMPLE}\n\n    ## Step 3: Align reads\n    #echo \"Aligning reads...\"\n    bowtie2 -x ${OUT_DIR}/${SAMPLE}/align/${SAMPLE} -1 \"$R1\" -2 \"$R2\" \\\n      -S ${OUT_DIR}/${SAMPLE}/align/${SAMPLE}.sam -p $THREADS\n\n    samtools view -Sb ${OUT_DIR}/${SAMPLE}/align/${SAMPLE}.sam | \\\n      samtools sort -o ${OUT_DIR}/${SAMPLE}/align/${SAMPLE}.bam\n\n    samtools index ${OUT_DIR}/${SAMPLE}/align/${SAMPLE}.bam\n\n    ## Step 4: Depth file\n    #echo \"Computing depth...\"\n    jgi_summarize_bam_contig_depths --outputDepth ${OUT_DIR}/${SAMPLE}/${SAMPLE}_depth.txt \\\n      ${OUT_DIR}/${SAMPLE}/align/${SAMPLE}.bam\n\n    ## Step 5: Binning\n    mkdir -p ${OUT_DIR}/${SAMPLE}/bins\n\n    #echo \"Binning with MetaBAT2...\"\n    metabat2 -i ${CONTIGS} -a ${OUT_DIR}/${SAMPLE}/${SAMPLE}_depth.txt \\\n      -o ${OUT_DIR}/${SAMPLE}/bins/${SAMPLE}_bin -t $THREADS\ndone\n</code></pre>"},{"location":"modules/day8/#bacteriophage-analysis","title":"Bacteriophage analysis","text":"<ul> <li>Useful protol that requires <code>virsorter2</code>, <code>checkV</code> and <code>DRAMv</code></li> </ul>"},{"location":"modules/day8/#nfcoremag-pipeline-httpsnf-coremag400","title":"nfcore/mag Pipeline https://nf-co.re/mag/4.0.0","text":"<ul> <li>Use an nf-core/mag pipeline for assembly, binning and annotation of metagenomes, github repository.</li> <li>This pipeline works for short- and/or long-reads.</li> <li>\u2705Key Features:</li> <li>Preprocessing:<ul> <li>Short reads: fastp, Bowtie2, FastQC</li> <li>Long reads: Porechop, NanoLyse, Filtlong, NanoPlot</li> </ul> </li> <li>Assembly:<ul> <li>Short reads: MEGAHIT, SPAdes</li> <li>Hybrid: hybridSPAdes</li> </ul> </li> <li>Binning:<ul> <li>Tools: MetaBAT2, MaxBin2, CONCOCT, DAS Tool</li> <li>Quality checks: BUSCO, CheckM, GUNC</li> </ul> </li> <li>Taxonomic Classification:<ul> <li>Tools: GTDB-Tk, CAT/BAT</li> <li>Co-assembly and co-abundance:</li> <li>Supports sample grouping for co-assembly and binning</li> </ul> </li> <li>\ud83d\udce6 Reproducibility:</li> <li>Uses Nextflow DSL2, Docker/Singularity containers</li> <li>Fully portable across HPC, cloud, and local systems</li> <li>Includes test datasets and CI testing</li> <li>It requires a sample sheet in <code>csv</code>  format with five columns: sample,group,short_reads_1,short_reads_2,long_reads</li> <li>Assuming all your raw reads (short- and long-reads) are in the same folder, run the python script:</li> </ul> <pre><code>proj=\"/data/users/${USER}/metagenomes/shotgun/\"\n# Create required directories\nmkdir -p ${proj}scripts ${proj}logs\n# Get the script to create samplesheet\ncp /data/users/user24/metagenomes/shotgun/scripts/generate_mag_samplesheet.py /data/users/${USER}/metagenomes/shotgun/scripts/\n\n# Create samplesheet for running mag\npython3 /data/users/${USER}/metagenomes/shotgun/scripts/generate_mag_samplesheet.py /data/users/user29/metagenomes/shotgun/ mag-samplesheet.csv\n# Create submission script\nnano data/users/${USER}/metagenomes/shotgun/scripts/mag-nf_submit.sh\n\n#!/bin/bash\n#SBATCH --job-name='mag'\n#SBATCH --time=24:00:00\n#SBATCH --mem=128g\n#SBATCH --ntasks=16\n#SBATCH --output=/data/users/user24/metagenomes/shotgun/logs/nfcore-mag-stdout.log\n#SBATCH --error=/data/users/user24/metagenomes/shotgun/logs/nfcore-mag-stderr.log\n#SBATCH --mail-user=ephie.geza@uct.ac.za\n\nproj=\"/data/users/user24/metagenomes/shotgun/\"\n\nmodule load nextflow/25.04.6\n#### Unload JAVA 18 as it doesn't work and load JAVA 17\nmodule unload java/openjdk-18.0.2\nmodule load java/openjdk-17.0.2\n####Unset conflicting environment variables (optional but recommended)\nunset JAVA_CMD\nunset JAVA_HOME\n\n#### Run pipeline\nnextflow run ${proj}mag \\\n      --input ${proj}mag-samplesheet.csv \\\n      --outdir ${proj}nfcore-mag \\\n      -w ${work}work/nfcore-mag \\\n      -profile singularity \\\n      -resume --skip_gtdbtk\n\n## Save and submit\n</code></pre>"},{"location":"modules/day8/#nf-corefuncscan","title":"nf-core/funcscan","text":"<p>Using contigs to screen for functional and natural gene sequences</p>"},{"location":"modules/day8/#viralgenie-nf-coreviralmetagenome","title":"Viralgenie nf-core/viralmetagenome","text":""},{"location":"modules/day8/#nf-coretaxprofiler","title":"nf-core/taxprofiler","text":""},{"location":"modules/day8/#targeted-metagenomics","title":"Targeted Metagenomics","text":""},{"location":"modules/day8/#nf-coreampliseq","title":"nf-core/ampliseq","text":""},{"location":"modules/day9/","title":"Day 9: Bring your own data","text":"<p>Date: September 11, 2025 Duration: 09:00-13:00 CAT Focus: Mobile genetic elements in AMR, independent analysis of participant datasets</p>"},{"location":"modules/day9/#overview","title":"Overview","text":"<p>Day 9 begins with understanding the role of mobile genetic elements in AMR spread, then transitions to hands-on application of all techniques learned throughout the course. Participants will analyze their own datasets with guidance from trainers, troubleshoot real-world challenges, and develop customized analysis approaches for their specific research questions.</p>"},{"location":"modules/day9/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 9, you will be able to:</p> <ul> <li>Understand the role of plasmids, integrons, and transposons in AMR dissemination</li> <li>Apply learned bioinformatics techniques to your own research data</li> <li>Troubleshoot common issues encountered in real-world analyses</li> <li>Adapt standard protocols to meet specific research requirements</li> <li>Develop analysis strategies for novel research questions</li> <li>Integrate multiple analysis approaches into comprehensive workflows</li> <li>Plan sustainable bioinformatics practices for ongoing research</li> </ul>"},{"location":"modules/day9/#schedule","title":"Schedule","text":"Time (CAT) Topic Links Trainer 09:00 Role of plasmids, integrons, and transposons in AMR spread Ephifania Geza 10:30 Participants to analyse their own data All trainers 11:30 Break 12:00 Participants to analyse their own data All trainers"},{"location":"modules/day9/#session-structure","title":"Session Structure","text":""},{"location":"modules/day9/#mobile-genetic-elements-session-0900-1030","title":"Mobile Genetic Elements Session (09:00-10:30)","text":"<ul> <li>Understanding plasmids and their role in horizontal gene transfer</li> <li>Integrons and gene cassette systems</li> <li>Transposons and insertion sequences</li> <li>Tools for mobile element detection and analysis</li> </ul>"},{"location":"modules/day9/#opening-data-analysis-session-1030-1045","title":"Opening Data Analysis Session (10:30-10:45)","text":"<ul> <li>Brief recap of key course concepts</li> <li>Overview of available support and resources</li> <li>Formation of analysis groups based on data types</li> <li>Technical setup verification</li> </ul>"},{"location":"modules/day9/#individual-analysis-time-0915-1130","title":"Individual Analysis Time (09:15-11:30)","text":"<ul> <li>Independent work on participant datasets</li> <li>One-on-one consultation with trainers</li> <li>Peer collaboration and knowledge sharing</li> <li>Documentation of analysis approaches</li> </ul>"},{"location":"modules/day9/#progress-sharing-1145-1200","title":"Progress Sharing (11:45-12:00)","text":"<ul> <li>Brief presentations of initial findings</li> <li>Discussion of challenges encountered</li> <li>Sharing of successful analysis strategies</li> </ul>"},{"location":"modules/day9/#advanced-analysis-1200-1300","title":"Advanced Analysis (12:00-13:00)","text":"<ul> <li>Continued independent work</li> <li>Focus on complex analyses or troubleshooting</li> <li>Preparation for Day 10 presentations</li> <li>Final consultations with trainers</li> </ul>"},{"location":"modules/day9/#data-types-and-analysis-approaches","title":"Data Types and Analysis Approaches","text":""},{"location":"modules/day9/#genomic-data","title":"Genomic Data","text":"<p>Common analyses for participants with genomic datasets:</p> <ul> <li>Quality control and preprocessing</li> <li>FastQC assessment and interpretation</li> <li>Adapter trimming and quality filtering</li> <li> <p>Species identification and contamination detection</p> </li> <li> <p>Genome assembly and annotation</p> </li> <li>De novo assembly optimization</li> <li>Assembly quality assessment</li> <li> <p>Functional annotation and gene prediction</p> </li> <li> <p>Comparative genomics</p> </li> <li>MLST and serotyping</li> <li>Antimicrobial resistance gene detection</li> <li>Phylogenetic analysis and clustering</li> </ul>"},{"location":"modules/day9/#metagenomic-data","title":"Metagenomic Data","text":"<p>For participants with microbiome or metagenomic samples:</p> <ul> <li>Community profiling</li> <li>Taxonomic classification</li> <li>Abundance estimation and normalization</li> <li> <p>Diversity analysis (alpha and beta)</p> </li> <li> <p>Functional analysis</p> </li> <li>Pathway reconstruction</li> <li>Antimicrobial resistance profiling</li> <li> <p>Metabolic potential assessment</p> </li> <li> <p>Clinical applications</p> </li> <li>Pathogen detection in complex samples</li> <li>Co-infection analysis</li> <li>Treatment response monitoring</li> </ul>"},{"location":"modules/day9/#outbreak-investigation-data","title":"Outbreak Investigation Data","text":"<p>For epidemiological and outbreak datasets:</p> <ul> <li>Transmission analysis</li> <li>SNP-based clustering</li> <li>Phylogenetic reconstruction</li> <li> <p>Temporal and geographic analysis</p> </li> <li> <p>Resistance surveillance</p> </li> <li>Multi-drug resistance patterns</li> <li>Resistance gene distribution</li> <li>Treatment outcome correlations</li> </ul>"},{"location":"modules/day9/#technical-support-available","title":"Technical Support Available","text":""},{"location":"modules/day9/#computational-resources","title":"Computational Resources","text":"<ul> <li>Access to high-performance computing cluster</li> <li>Pre-installed bioinformatics software environments</li> <li>Container images for reproducible analysis</li> <li>Shared storage for large datasets</li> </ul>"},{"location":"modules/day9/#analysis-pipelines","title":"Analysis Pipelines","text":"<ul> <li>Nextflow workflows developed during the course</li> <li>Customizable analysis templates</li> <li>Pre-configured environment profiles</li> <li>Automated reporting tools</li> </ul>"},{"location":"modules/day9/#expert-guidance","title":"Expert Guidance","text":"<p>Trainer specializations available:</p> Trainer Expertise Areas Ephifania Geza Genomic surveillance, AMR analysis, metagenomics, clinical applications Arash Iranzadeh Phylogenomics, comparative genomics, outbreak investigation Sindiswa Lukhele Sequencing technologies, quality control, species identification Mamana Mbiyavanga Workflow development, HPC systems, pipeline optimization"},{"location":"modules/day9/#common-analysis-workflows","title":"Common Analysis Workflows","text":""},{"location":"modules/day9/#genomic-surveillance-workflow","title":"Genomic Surveillance Workflow","text":"<pre><code># 1. Initial data assessment\nfastqc raw_data/*.fastq.gz\nmultiqc fastqc_results/\n\n# 2. Species identification\nkraken2 --db minikraken2_v2 --paired sample_R1.fastq sample_R2.fastq\n\n# 3. Quality trimming\ntrimmomatic PE sample_R1.fastq sample_R2.fastq \\\n    sample_R1_trimmed.fastq sample_R1_unpaired.fastq \\\n    sample_R2_trimmed.fastq sample_R2_unpaired.fastq \\\n    SLIDINGWINDOW:4:20 MINLEN:50\n\n# 4. Assembly\nspades.py -1 sample_R1_trimmed.fastq -2 sample_R2_trimmed.fastq -o assembly/\n\n# 5. Assembly quality assessment\nquast.py assembly/scaffolds.fasta -o quast_results/\n\n# 6. Annotation\nprokka assembly/scaffolds.fasta --outdir annotation/ --prefix sample\n</code></pre>"},{"location":"modules/day9/#metagenomic-analysis-workflow","title":"Metagenomic Analysis Workflow","text":"<pre><code># 1. Host DNA removal (if applicable)\nkneaddata --input sample_R1.fastq --input sample_R2.fastq \\\n    --reference-db human_genome --output cleaned_data/\n\n# 2. Taxonomic profiling\nmetaphlan cleaned_data/sample_paired_1.fastq,cleaned_data/sample_paired_2.fastq \\\n    --bowtie2out sample.bowtie2.bz2 --nproc 4 --input_type fastq \\\n    --output_file sample_profile.txt\n\n# 3. Functional profiling\nhumann --input cleaned_data/sample_paired.fastq \\\n    --output functional_analysis/ --nucleotide-database chocophlan \\\n    --protein-database uniref90\n\n# 4. Diversity analysis in R\nRscript diversity_analysis.R sample_profile.txt metadata.csv\n</code></pre>"},{"location":"modules/day9/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"modules/day9/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"modules/day9/#low-quality-data","title":"Low-Quality Data","text":"<pre><code># Check read quality distribution\nfastqc *.fastq.gz\n\n# Aggressive quality trimming if needed\ntrimmomatic PE input_R1.fastq input_R2.fastq \\\n    output_R1.fastq unpaired_R1.fastq \\\n    output_R2.fastq unpaired_R2.fastq \\\n    SLIDINGWINDOW:4:25 LEADING:20 TRAILING:20 MINLEN:75\n\n# Consider different assembly strategies\n# For poor quality data, try more conservative parameters\nspades.py --careful --cov-cutoff auto -1 R1.fastq -2 R2.fastq -o assembly/\n</code></pre>"},{"location":"modules/day9/#contamination-issues","title":"Contamination Issues","text":"<pre><code># Check for contamination\nkraken2 --db standard --paired sample_R1.fastq sample_R2.fastq \\\n    --report contamination_report.txt\n\n# Remove contaminant sequences\nextract_kraken_reads.py -k sample.kraken -s1 sample_R1.fastq \\\n    -s2 sample_R2.fastq -o clean_R1.fastq -o2 clean_R2.fastq \\\n    --exclude --taxid 9606  # Exclude human reads\n</code></pre>"},{"location":"modules/day9/#assembly-problems","title":"Assembly Problems","text":"<pre><code># If SPAdes fails, try different assemblers\n# Unicycler for hybrid assembly\nunicycler -1 short_R1.fastq -2 short_R2.fastq -l long_reads.fastq -o assembly/\n\n# Or SKESA for quick assembly\nskesa --reads short_R1.fastq,short_R2.fastq --cores 8 &gt; assembly.fasta\n</code></pre>"},{"location":"modules/day9/#memoryresource-issues","title":"Memory/Resource Issues","text":"<pre><code># Monitor resource usage\nhtop\n\n# Reduce memory usage for large datasets\nspades.py --memory 16 -1 R1.fastq -2 R2.fastq -o assembly/\n\n# Use subsampling for initial testing\nseqtk sample -s100 input_R1.fastq 100000 &gt; subset_R1.fastq\nseqtk sample -s100 input_R2.fastq 100000 &gt; subset_R2.fastq\n</code></pre>"},{"location":"modules/day9/#analysis-documentation","title":"Analysis Documentation","text":""},{"location":"modules/day9/#laboratory-notebook-template","title":"Laboratory Notebook Template","text":"<pre><code># Analysis Log: [Your Dataset Name]\n**Date**: [Current Date]\n**Analyst**: [Your Name]\n**Data Source**: [Description of samples]\n\n## Objectives\n- Primary research question\n- Specific analyses planned\n- Expected outcomes\n\n## Data Description\n- Sample type and collection method\n- Sequencing platform and parameters\n- Data quality metrics\n\n## Analysis Steps\n### Step 1: Quality Control\n- Command used: `fastqc *.fastq.gz`\n- Results: [Summary of quality metrics]\n- Decision: [Any quality filtering applied]\n\n### Step 2: [Next Analysis]\n- Command: [Exact command used]\n- Parameters chosen: [Rationale for parameter selection]\n- Results: [Key findings]\n\n## Challenges Encountered\n- Issue: [Description of problem]\n- Solution attempted: [What was tried]\n- Outcome: [Whether resolved]\n\n## Key Findings\n- [Major results from analysis]\n- [Statistical summaries]\n- [Biological interpretations]\n\n## Next Steps\n- Additional analyses needed\n- Questions raised\n- Follow-up experiments\n</code></pre>"},{"location":"modules/day9/#resource-management","title":"Resource Management","text":""},{"location":"modules/day9/#data-organization","title":"Data Organization","text":"<pre><code># Recommended directory structure\nproject_name/\n\u251c\u2500\u2500 raw_data/          # Original sequencing files\n\u251c\u2500\u2500 quality_control/   # QC reports and cleaned data\n\u251c\u2500\u2500 analysis/          # Main analysis outputs\n\u251c\u2500\u2500 scripts/           # Custom scripts and commands\n\u251c\u2500\u2500 results/           # Final results and figures\n\u2514\u2500\u2500 documentation/     # Analysis logs and notes\n</code></pre>"},{"location":"modules/day9/#backup-strategies","title":"Backup Strategies","text":"<pre><code># Regular backup of important results\nrsync -av results/ backup_drive/project_results/\ntar -czf analysis_$(date +%Y%m%d).tar.gz analysis/\n\n# Version control for scripts\ngit init\ngit add scripts/\ngit commit -m \"Initial analysis scripts\"\n</code></pre>"},{"location":"modules/day9/#collaboration-guidelines","title":"Collaboration Guidelines","text":""},{"location":"modules/day9/#peer-support","title":"Peer Support","text":"<ul> <li>Form analysis groups based on similar data types</li> <li>Share successful parameter combinations</li> <li>Collaborate on troubleshooting challenging datasets</li> <li>Review each other's analysis approaches</li> </ul>"},{"location":"modules/day9/#trainer-consultation","title":"Trainer Consultation","text":"<ul> <li>Prepare specific questions about your data</li> <li>Document issues with exact error messages</li> <li>Have your analysis objectives clearly defined</li> <li>Be ready to explain your research context</li> </ul>"},{"location":"modules/day9/#assessment-and-preparation-for-day-10","title":"Assessment and Preparation for Day 10","text":""},{"location":"modules/day9/#presentation-preparation","title":"Presentation Preparation","text":"<p>Participants should prepare a 5-minute presentation covering:</p> <ol> <li>Research Question: What you aimed to investigate</li> <li>Data Overview: Type and source of your dataset</li> <li>Methods Applied: Which course techniques you used</li> <li>Key Results: Main findings from your analysis</li> <li>Challenges: Obstacles encountered and solutions found</li> <li>Future Directions: Next steps for your research</li> </ol>"},{"location":"modules/day9/#technical-documentation","title":"Technical Documentation","text":"<ul> <li>Save all commands used in a script file</li> <li>Document parameter choices and rationale</li> <li>Prepare summary statistics and key figures</li> <li>Note any analysis limitations or assumptions</li> </ul>"},{"location":"modules/day9/#success-metrics","title":"Success Metrics","text":"<p>By the end of Day 9, participants should have:</p> <ul> <li> Successfully processed their own dataset</li> <li> Applied at least 3 different analysis techniques from the course</li> <li> Documented their analysis workflow</li> <li> Identified key findings relevant to their research</li> <li> Prepared materials for Day 10 presentation</li> <li> Established ongoing analysis plan</li> </ul>"},{"location":"modules/day9/#resources-for-continued-learning","title":"Resources for Continued Learning","text":""},{"location":"modules/day9/#online-communities","title":"Online Communities","text":"<ul> <li>Bioinformatics Stack Exchange</li> <li>BioStars Forum</li> <li>Galaxy Community</li> </ul>"},{"location":"modules/day9/#software-documentation","title":"Software Documentation","text":"<ul> <li>Tool-specific manuals and tutorials</li> <li>GitHub repositories for pipeline development</li> <li>Container registries for reproducible environments</li> </ul>"},{"location":"modules/day9/#professional-development","title":"Professional Development","text":"<ul> <li>Local bioinformatics user groups</li> <li>International conferences and workshops</li> <li>Online course platforms for advanced topics</li> </ul>"},{"location":"modules/day9/#looking-ahead","title":"Looking Ahead","text":"<p>Day 10 Preview: Wrap-up session including: - Participant presentations of analysis results - Discussion of lessons learned and best practices - Information about ongoing support resources - Course completion and next steps planning</p> <p>Key Learning Outcome: Independent application of bioinformatics skills to real research data builds confidence and reveals the practical challenges and rewards of computational biology in actual research contexts.</p>"}]}